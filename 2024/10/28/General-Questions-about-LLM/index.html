<!DOCTYPE html>


<html lang="en">


<head>
  <meta charset="utf-8" />
    
  <meta name="description" content="I am a second PhD student at Renmin University of China. My research interests include federated learning, high dimensional data, machine learning, and optimization. I am currently working on latent graph learning in Prof.Renjie Liao&#39;s group." />
  
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1" />
  <title>
    General Questions about LLM |  Welcome to XueYu&#39;s Blog
  </title>
  <meta name="generator" content="hexo-theme-ayer">
  
  <link rel="shortcut icon" href="/favicon.ico" />
  
  
<link rel="stylesheet" href="/dist/main.css">

  
<link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/Shen-Yu/cdn/css/remixicon.min.css">

  
<link rel="stylesheet" href="/css/custom.css">

  
  
<script src="https://cdn.jsdelivr.net/npm/pace-js@1.0.2/pace.min.js"></script>

  
  

  

<link rel="alternate" href="/atom.xml" title="Welcome to XueYu's Blog" type="application/atom+xml">

<style>.github-emoji { position: relative; display: inline-block; width: 1.2em; min-height: 1.2em; overflow: hidden; vertical-align: top; color: transparent; }  .github-emoji > span { position: relative; z-index: 10; }  .github-emoji img, .github-emoji .fancybox { margin: 0 !important; padding: 0 !important; border: none !important; outline: none !important; text-decoration: none !important; user-select: none !important; cursor: auto !important; }  .github-emoji img { height: 1.2em !important; width: 1.2em !important; position: absolute !important; left: 50% !important; top: 50% !important; transform: translate(-50%, -50%) !important; user-select: none !important; cursor: auto !important; } .github-emoji-fallback { color: inherit; } .github-emoji-fallback img { opacity: 0 !important; }</style>
<link href="https://cdn.bootcss.com/KaTeX/0.11.1/katex.min.css" rel="stylesheet" /></head>

</html>
<script src="/js/hexo_resize_image.js"></script>
<body>
  <div id="app">
    
      
    <main class="content on">
      <section class="outer">
  <article
  id="post-General-Questions-about-LLM"
  class="article article-type-post"
  itemscope
  itemprop="blogPost"
  data-scroll-reveal
>
  <div class="article-inner">
    
    <header class="article-header">
       
<h1 class="article-title sea-center" style="border-left:0" itemprop="name">
  General Questions about LLM
</h1>
 

    </header>
     
    <div class="article-meta">
      <a href="/2024/10/28/General-Questions-about-LLM/" class="article-date">
  <time datetime="2024-10-28T07:00:00.000Z" itemprop="datePublished">2024-10-28</time>
</a> 
  <div class="article-category">
    <a class="article-category-link" href="/categories/Foundation-Model/">Foundation Model</a>
  </div>
  
<div class="word_count">
    <span class="post-time">
        <span class="post-meta-item-icon">
            <i class="ri-quill-pen-line"></i>
            <span class="post-meta-item-text"> Word count:</span>
            <span class="post-count">16.9k</span>
        </span>
    </span>

    <span class="post-time">
        &nbsp; | &nbsp;
        <span class="post-meta-item-icon">
            <i class="ri-book-open-line"></i>
            <span class="post-meta-item-text"> Reading time≈</span>
            <span class="post-count">60 min</span>
        </span>
    </span>
</div>
 
    </div>
      
    <div class="article-entry" itemprop="articleBody">
       
  <h2 id="大模型常用微调方法lora和ptuning的原理">1.
大模型常用微调方法LORA和Ptuning的原理</h2>
<p>LORA: Low-Rank Adaptation.
核心是在大型语言模型上对指定参数增加额外的低秩矩阵，也就是在原始pre-trained
LM
旁边增加一个旁路，做一个降维再升维的操作。假设模型中有一个需要更新的权重矩阵
<span class="math inline">\(W \in \mathbb{R}^{d \times
k}\)</span>，LORA的思想是修改为： <span class="math inline">\(W' = W
+ \delta W\)</span>，其中 <span class="math inline">\(\Delta W = A
\times B\)</span>, <span class="math inline">\(A \in \mathbb{R}^{d
\times r}\)</span>, <span class="math inline">\(B \in \mathbb{R}^{r
\times k}\)</span>, 且 <span class="math inline">\(r &lt;&lt; \min\{d,
k\}\)</span>。在模型训练过程中，固定PLM的参数，只训练降维矩阵 <span class="math inline">\(A\)</span> 与升维矩阵 <span class="math inline">\(B\)</span>。</p>
<p>Ptuning: Prompt Tuning.</p>
<h2 id="diffusion-models-and-stable-diffusion">2. Diffusion models and
Stable diffusion</h2>
<ul>
<li><p><a target="_blank" rel="noopener" href="https://lilianweng.github.io/posts/2021-07-11-diffusion-models/#prog-distll">What
are Diffusion Models?</a></p></li>
<li><p><a target="_blank" rel="noopener" href="https://lilianweng.github.io/posts/2024-04-12-diffusion-video/">Diffusion
Models for Video Generation</a></p></li>
</ul>
<h2 id="llm的幻觉的问题">3. LLM的幻觉的问题</h2>
<p>Great thanks to this blog: <a target="_blank" rel="noopener" href="https://aman.ai/primers/ai/hallucination/">NLP • Hallucination
Mitigation</a></p>
<p>AI文本生成中的<strong>幻觉</strong>现象指的是模型生成的文本虽然在语法上可能是正确的，并且看起来合理，但与输入内容并不一致，甚至可能是事实错误的。这种问题在像GPT-3这样的系统中尤为常见，生成的细节可能会偏离甚至与输入内容相矛盾。</p>
<h3 id="幻觉产生的原因">幻觉产生的原因</h3>
<p>造成幻觉的原因可以归结为以下几个方面：</p>
<p><strong>1.
训练数据不足</strong>：如果模型在训练中没有接触到多样化的数据，它可能无法准确地建立输入与合适输出之间的关联，从而导致幻觉内容的产生。</p>
<p><strong>2.
模型过拟合</strong>：过拟合于训练数据会导致模型生成的输出过于依赖训练集，但在面对新的或不同的输入时与实际不符。</p>
<p><strong>3.
监督不足</strong>：如果没有充分的指导，模型可能会过度依赖其内部逻辑，导致生成的内容出现“幻觉”。</p>
<p><strong>4.
知识截止</strong>：像ChatGPT这样的语言模型有知识截止日期，因此对于截止日期之后的信息一无所知。在这种情况下，它可能在不知情的情况下提供过时或不再相关的回答。</p>
<h3 id="如何解决幻觉">如何解决幻觉</h3>
<h4 id="训练阶段">训练阶段</h4>
<p><strong>Reinforcement Learning from Human Feedback
(RLHF)</strong>。使用RLHF来减少幻觉的核心思想是让人类提供有关模型响应准确性和相关性的反馈。通过将这些反馈融入训练过程，模型可以逐步学习区分准确信息和不准确信息，从而降低产生幻觉的可能性。此外，RLHF还能帮助模型理解其输出带来的影响，进而提高生成相关且符合事实的回应能力。</p>
<h4 id="训练之后">训练之后</h4>
<p>在对 LLM 进行训练之后，可以使用 <strong>Prompting</strong>
减轻幻觉。</p>
<p><strong>1. Retrieval Augmented Generation（RAG）</strong>。
通过在生成过程中提供额外的上下文信息，有助于消除大语言模型中的幻觉问题。幻觉现象通常发生在LLM基于训练数据中的模式生成响应，而不是依赖真实知识时，尤其当模型缺乏特定领域的信息或难以识别其知识边界时更容易出现。RAG通过将外部知识源整合到生成过程中来解决这一问题。它使LLM能够在生成响应时访问来自外部数据库的最新或特定上下文的数据。这种方法为模型注入了更多的上下文信息，帮助其更好地理解主题，降低幻觉出现的概率。例如，在设计用于提供汽车信息的聊天机器人中，RAG可以从外部数据库中检索产品的具体细节和上下文信息，以补充用户的输入。这样，LLM可以接收到更全面和详细的提示，从而生成更准确和相关的响应。</p>
<p><strong>2. Contextual
Prompting</strong>。旨在通过为模型提供明确的上下文或背景信息来改善其生成的输出。这种方法通过在提示（prompt）中包含相关的上下文信息，帮助模型更好地理解任务，并生成更准确、相关性更高的回答或内容。在给大语言模型（LLM）提供问题和上下文时，附加的上下文段落通常是来自维基百科文章、书籍章节等的摘要。这些上下文片段通过在句末插入唯一标识符进行标记，例如“(source
1234)”或“(source 4567)”。例如：</p>
<ul>
<li><p>“巴黎是法国的首都。(source 1234)”</p></li>
<li><p>“法国位于西欧。(source 4567)”</p>
<p>这些来源标签是与原始上下文片段中的特定句子相对应的唯一编号。具体来说，Contextual
Prompting
涉及将一段上下文或背景知识与问题或任务一起输入模型。这段上下文可以是来自外部知识库的文本、前面对话中的信息、或任何与当前任务相关的数据。上下文为模型提供了额外的信息，使其能够更好地理解用户的意图，并在生成内容时参考这些背景知识。在使用这些带标签的上下文提示LLM时，研究方法还会在问题后附加指令，例如“提供细节并在答案中包含来源。”
通过这种方式，LLM在生成响应时被引导引用这些标记的来源。这些标签为验证LLM的响应是否基于提供的上下文信息提供了参考。如果响应中包含匹配的来源标签，就表明LLM依赖于提供的上下文，而不是凭空生成（幻觉）的内容。</p></li>
</ul>
<p><strong>3. Chain of Verification
（CoVe）</strong>。CoVe方法让大语言模型（LLM）在生成初始回答后，经过多个步骤来提升准确性：(1).
生成初始回答，可能包含不准确或幻觉。(2).
规划验证问题——模型生成一系列验证问题以自我查证。(3).
执行验证——模型独立回答这些验证问题 (Verification questions are often
answered more accurately than facts stated in long passages)。(4).
基于验证结果修正初始回答，生成最终答案。 <img src="/2024/10/28/General-Questions-about-LLM/cove.png"></p>
<h2 id="llm-alignment">4. LLM Alignment</h2>
<p>Great thanks to this blog: <a target="_blank" rel="noopener" href="https://aman.ai/primers/ai/llm-alignment/">LLM Alignment</a></p>
<h3 id="overview">Overview</h3>
<ul>
<li>2017 年，OpenAI 在其论文 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1706.03741">Deep reinforcement learning from
human preferences</a> 中提出了一种开创性的机器学习方法，称为
"从人类反馈出发的强化学习"
(RLHF)，特别关注人类偏好。这一创新概念自此激发了该领域的进一步研究和发展。</li>
<li>RLHF 概念:
使用一个预先训练好的语言模型，由人类评估员对其输出进行排序。然后，这种排序会让模型对某些类型的回答产生偏好，从而产生更可靠、更安全的输出。</li>
<li>RLHF
可以有效利用人类反馈来提高语言模型的性能。它将强化学习算法的优势与对人类输入的细微理解相结合，促进了模型的持续学习和改进。结合人类反馈，RLHF
不仅能提高模型的自然语言理解和生成能力，还能提高其在文本分类或翻译等特定任务中的效率。此外，RLHF
在解决语言模型中的偏差方面也发挥着至关重要的作用。通过允许人工输入来指导和纠正模型的语言使用，它可以促进更加公平和包容的交流。不过，在这一过程中，必须注意人为因素可能导致的偏差。</li>
</ul>
<h3 id="reinforcement-learning-强化学习基础概念">Reinforcement Learning
强化学习基础概念</h3>
<p><img src="/2024/10/28/General-Questions-about-LLM/rl.png"></p>
<p>如图所示，agent 采取一定的 action，对于当前的
action，环境会反馈其状态 state 以及 给出
reward。其中，reward是要优化的目标，state是环境当前的状态，policy用于根据state选择
action.</p>
<h3 id="reinforcement-learning-from-human-feedback-rlhf">Reinforcement
Learning from Human Feedback (RLHF)</h3>
<p>LLM
的最初目标是准确地预测下一个token。但是，这种方式无法保证输出的结果是有用、无害且诚实的，有可能产生不符合人类道德或安全标准的内容。为解决这一问题，需要有一种方式来引导模型输出符合人类价值观的结果。</p>
<p><img src="/2024/10/28/General-Questions-about-LLM/rlhf.png"></p>
<p>图中给出了使用RLHF训练LM的三个步骤，具体来说，</p>
<ol type="1">
<li><p>Collect Demonstration Data, and Train a Supervised Policy.
首先，从 prompts 中选择一个
prompt；然后人类标注者给出希望得到的输出；最后这些经过标注后的数据用于对LM进行
supervised fine-tune.</p></li>
<li><p>Collect Comparison Data, and Train a Reward Model.
首先，选取一个prompt，模型给出几个可能的输出结果；标注者根据有用性、准确性等准则对结果进行从好到差的排序；这些排序后的数据用来训练一个
reward model. Reward model用来评估模型输出结果的质量。</p></li>
<li><p>Optimize a Policy Against the Reward Model Using Reinforcement
Learning. 产生新的prompt, 基于当前的policy, model 得到新的输出 response;
Reward model 评估 response，然后得到 reward；基于得到的 reward
以及一些强化学习算法，比如PPO，对 policy 进行更新。调整 policy
是为了增加未来产生 higher-reward outputs 的可能性。</p></li>
</ol>
<p>Chip Huyen provides a zoomed out view of how the overall process
works in her flowchart below:</p>
<p><img src="/2024/10/28/General-Questions-about-LLM/rlhf1.jpeg"></p>
<h4 id="reward-model">REWARD MODEL</h4>
<p>Reward model 的主要功能是评估给定的输入（如文本序列）并产生scalar
reward。这种reward 量化了输出与人类偏好或期望行为的一致程度。</p>
<p><img src="/2024/10/28/General-Questions-about-LLM/rlhf2.png"></p>
<p>Reward 模型的结构包括 - LM 分类器：一个二元分类器微调的
LLM，可对哪种反应更符合人类偏好进行评分。 - value
networks：一个回归模型，根据输入预测人类偏好评分。 -
评论生成器：经过训练的
LM，可生成评价性评论，解释哪种回答更好以及原因。该评论可用于指令调整。</p>
<h4 id="optimizing-the-policy">Optimizing the Policy</h4>
<p><strong>策略（policy）</strong>：在强化学习中，策略是一组规则或决策机制，指导智能体（agent）根据它所处的环境状态或观察结果来选择行动。也就是说，策略定义了智能体如何在不同的情境下采取什么样的行为。</p>
<p><strong>PPO（Proximal Policy
Optimization，邻近策略优化）</strong>：是一种常用的强化学习算法。在PPO中，策略是通过反复迭代来优化的。其目标是最大化奖励，即让智能体的行为逐步改善，获得更高的回报。
但是，PPO会确保策略的更新不会发生剧烈变化。这是通过引入一种约束，使更新后的策略保持与之前的策略相似性，以避免不稳定性或训练失败的情况。</p>
<p><strong>DPO（Direct Preference
Optimization，直接偏好优化）</strong>：是一种不同的策略优化方法。在DPO中，策略直接基于人类偏好进行优化。具体来说，它通过二元交叉熵损失函数（binary
cross entropy
loss），增加模型生成的优选输出的相对对数概率，而减少非优选输出的概率。这种方法直接根据人类的反馈进行优化，旨在使模型生成更符合人类期望的输出。
与此同时，DPO也通过KL散度约束来保持平衡，防止策略发生过大的偏离。</p>
<h4 id="training-llama-2">Training Llama 2</h4>
<p><img src="/2024/10/28/General-Questions-about-LLM/llama.jpeg"></p>
<p>以下是Llama 2 的主要训练阶段的介绍：</p>
<ol type="1">
<li><strong>预训练阶段</strong>（Pretraining）：
<ul>
<li>在最初的预训练阶段，Llama 2
使用大量数据通过<strong>自监督学习</strong>进行训练。这一阶段让模型学习语言模式和上下文的基本结构，使其能够理解语言的基本规则和含义。</li>
<li>自监督学习的方式通常是通过预测文本中隐藏的部分（如下一句话或遮盖的单词）来训练模型，帮助它积累广泛的语言知识。</li>
</ul></li>
<li><strong>有监督微调阶段</strong>（Supervised Fine-Tuning）：
<ul>
<li>在此阶段，模型进一步通过<strong>指令数据</strong>进行有监督微调。具体来说，模型会根据特定的指令进行训练，学习如何对不同的提示做出合适的响应。</li>
<li>这个过程使模型能够在实际应用中根据明确的要求或任务生成准确、相关的回答。</li>
</ul></li>
<li><strong>奖励模型创建（RLHF步骤1）</strong>（Reward Models Creation -
RLHF Step 1）：
<ul>
<li>为了进一步优化模型输出的质量，Llama 2
创建了两个<strong>奖励模型</strong>，一个针对<strong>帮助性（helpfulness）</strong>，另一个针对<strong>安全性（safety）</strong>。</li>
<li>这些奖励模型通过<strong>人类偏好数据</strong>训练，预测在两种不同的输出中哪一个更符合人类的判断。此阶段基于二元比较，模型通过评估每对输出的优劣来学习。</li>
</ul></li>
<li><strong>边际损失与排名</strong>（Margin Loss and Ranking）：
<ul>
<li>Llama 2
使用二元比较数据集来优化排名。在每次比较中，标注者只需要选择两种响应中的一个，并通过<strong>边际标签</strong>来表示偏好的强度。这种边际标签可以用于进一步计算<strong>排名损失</strong>，提高模型对不同偏好的敏感性。</li>
</ul></li>
<li><strong>拒绝采样与PPO对齐（RLHF步骤2）</strong>（Rejection Sampling
and PPO - RLHF Step 2）：
<ul>
<li>在最后一步，Llama 2
使用<strong>拒绝采样</strong>和<strong>邻近策略优化（PPO）</strong>来进一步优化模型。</li>
<li>拒绝采样是指从模型生成的多个输出中，选择<strong>奖励最高</strong>的输出用于更新梯度，从而增强模型生成高质量输出的能力。</li>
<li>之后通过PPO算法对模型进行进一步对齐，使其生成的回答更加安全且有帮助，同时确保优化过程中策略更新的稳定性。</li>
</ul></li>
</ol>
<p>总的来说，Llama 2
的训练流程结合了大规模的自监督学习、基于指令的有监督微调，以及基于人类偏好的强化学习，通过一系列精细的步骤来提升模型的语言理解、输出的帮助性和安全性。</p>
<h4 id="proximal-policy-optimization-ppo">Proximal Policy Optimization
(PPO)</h4>
<p>建议先阅读以下两篇优秀博客： - <a target="_blank" rel="noopener" href="https://www.cnblogs.com/xingzheai/p/15826847.html">详解策略梯度算法</a>
- <a target="_blank" rel="noopener" href="https://www.cnblogs.com/xingzheai/p/15931681.html">详解近端策略优化</a></p>
<p><strong>PPO-clip</strong>:
在PPO（邻近策略优化）中，代理损失函数（surrogate loss）
是通过当前策略和参考策略下执行同一动作的概率比率来定义的。这一比率用于引导策略向那些能够获得更高奖励的动作倾斜，同时确保策略更新的幅度不会过大，从而保持训练的稳定性。为防止策略的更新幅度过大，PPO引入了剪裁，限制比率在一定范围内。通过在一定阈值外“剪裁”比率的变化，模型可以避免发生过大的更新，从而保证训练过程的稳定性。</p>
<p>定义 <span class="math inline">\(\pi_{\theta}\)</span>为当前策略（参数为 <span class="math inline">\(\theta\)</span> 的一个网络），<span class="math inline">\(\pi_{ref}\)</span>
是实际的、可参考的策略空间。<span class="math inline">\(A(s_t,
a_t)\)</span>为在状态 <span class="math inline">\(s_t\)</span>
下采取行为 <span class="math inline">\(a_t\)</span>
时得到的奖励。近端策略优化裁剪函数为： <span class="math display">\[
L(\theta) = E_{(s_t, a_t) \sim \pi_{ref}} \min{(\frac{p_{\theta}(a_t |
s_t)}{p_{\pi_{ref}}(a_t|s_t)} A(s_t, a_t), clip(\frac{p_{\theta}(a_t |
s_t)}{p_{\pi_{ref}}(a_t|s_t)}, 1-\epsilon, 1+\epsilon)A(s_t, a_t))},
\]</span> <span class="math inline">\(\epsilon\)</span>
是一个超参数，要需要我们调整的，一般设置为0.1或0.2。</p>
<p><strong>PPO-penalty</strong>: 在PPO中，除了使用剪裁目标函数（clipped
objective）外，另一种常见的方法是直接在目标函数中加入KL散度惩罚项。这意味着算法会根据新策略与参考策略的偏离程度对目标函数进行惩罚。具体损失函数为：
<span class="math display">\[
L(\theta) = E_{(s_t, a_t) \sim \pi_{ref}} \frac{p_{\theta}(a_t |
s_t)}{p_{\pi_{ref}}(a_t|s_t)} A(s_t, a_t) - \beta
KL(\pi_{ref}||\pi_{\theta}),
\]</span></p>
<p>通过<strong>最大化目标函数</strong>得到最优策略。对于大规模语言模型（LLM）来说，这个目标函数反映了模型对齐的目标，比如生成<strong>有帮助</strong>、<strong>真实</strong>、<strong>无害</strong>的回答。</p>
<p><strong>参考策略 (Reference
Policy)</strong>：参考策略是训练过程中用作<strong>基准</strong>或<strong>对照</strong>的一套策略。它通常是一个<strong>稳定的策略</strong>，模型可以从这个基准出发，或者在训练过程中参考该策略来指导学习。它确保最优策略的更新不会偏离初始策略太远，防止训练过程中产生剧烈变化或不稳定的行为。</p>
<h3 id="reinforcement-learning-with-ai-feedback-rlaif">Reinforcement
Learning with AI Feedback (RLAIF)</h3>
<p>RLAIF
使用AI生成的偏好（而不是人工标注的偏好）来训练大规模语言模型（LLMs）。这种方法通过利用强大的预训练模型（如GPT-4）生成反馈，为训练其他LLM提供高效、成本更低的替代方案。在RLAIF中，反馈生成的语言模型相当于充当了“虚拟人工标注者”的角色。它评估训练中的模型生成的多个输出，选择优选响应或提供改进建议。</p>
<h4 id="direct-preference-optimization-dpo">Direct Preference
Optimization (DPO)</h4>
<p>本文前面讨论的 RLHF
主要包括两个阶段：根据人类偏好标签训练奖励模型，然后使用强化学习（RL）对
LM 进行微调，使其与这些偏好保持一致。然而，RLHF
存在复杂性和不稳定性问题，它需要拟合一个奖励模型，然后训练一个策略来优化该奖励，这就容易产生稳定性问题。</p>
<p>DPO算法摆脱了传统RL方法中的两个阶段。通过定义新的损失函数来训练LLM，以避免不稳定性问题。
DPO使用一种特殊格式的数据集，形式为：&lt;prompt, worse completion,
better
completion&gt;（即“提示，较差的完成，较好的完成”）。在训练过程中，DPO的损失函数鼓励模型增加较好完成的概率，同时降低较差完成的概率。这个过程是通过加权实现的，权重基于隐含的奖励模型。这里的关键在于，LLM本身充当了奖励模型，因此不再需要一个显式的奖励模型。下图给出了DPO和RLHF的区别。</p>
<p><img src="/2024/10/28/General-Questions-about-LLM/DPO.jpg"></p>
<p><strong>Binary Cross-Entropy Loss</strong> DPO
通过使用二元交叉熵（Binary Cross-Entropy,
BCE）损失函数来优化语言模型以更好地与人类偏好对齐的训练方法。对于每个输入，模型会生成两个响应，并由人类标注者指明他们的偏好（哪个响应更好）。DPO通过比较模型生成的响应对（即优选响应和不优选响应）与人类偏好进行训练。</p>
<p>损失定义如下： <span class="math display">\[
L_{DPO}(\theta) = -E_{(x, y_w, y_l) \sim D} [\log \sigma (\beta
\log\frac{\pi_{\theta}(y_w| x)}{\pi_{ref}(y_w|x)} - \beta \beta
\log\frac{\pi_{\theta}(y_l| x)}{\pi_{ref}(y_l|x)})],
\]</span> 其中，<span class="math inline">\(\pi_{\theta}\)</span>
为要训练的策略模型， <span class="math inline">\(\pi_{ref}\)</span>
是参考的策略模型；<span class="math inline">\(y_w\)</span> 和 <span class="math inline">\(y_l\)</span> 分别表示优选response 和 不优选的
response. <span class="math inline">\(\beta\)</span>
控制待训练模型与参考策略模型的接近程度。<span class="math inline">\(\sigma\)</span> 为 logistic 函数。</p>
<ul>
<li>DPO标志着语言模型训练方法的转变，通过将强化学习与人类反馈（RLHF）过程整合为<strong>单个的端到端</strong>优化步骤，简化了模型的训练。</li>
</ul>
<p><strong>DPO 的训练过程</strong> -
选择一个已经经过基础指令调优的语言模型作为参考模型，这个模型提供了良好的基础。
-
使用不同的采样/解码方法（例如不同的温度设置）对同一提示生成成对输出，并让人类选择他们喜欢的哪一个。这一过程将产生一个人类偏好/反馈的数据集。
-
在LLM上添加一个线性层，使得模型能够输出一个标量值。这一层将帮助模型在训练过程中产生更具体的数值输出。
-
使用DPO损失，该损失函数基于二元交叉熵损失。计算参考模型和正在调优模型的标量输出的对数比率，并乘以一个散度参数，以调整模型的输出。
-
在训练完成后，去掉最后的线性层，这样就得到了一个基于人类反馈微调的LLM。</p>
<p>通过以上步骤，DPO方法通过简化RLHF过程，去掉了复杂的强化学习步骤和专门的奖励模型，使得模型训练更为高效和直接。这样，最终得到的模型能够更好地反映人类的偏好，提供更优质的输出</p>
<h4 id="kahneman-tversky-optimization-kto">Kahneman-Tversky Optimization
(KTO)</h4>
<p>人类在面对不确定事件时，由于‘厌恶损失’，往往会做出无法最大化期望值的决策。直接以人的偏好指导大模型的训练，其训练的数据中包含了大量的人类偏好，往往无法做出期望最大的决策。KTO是一种对齐手段，将重点从传统训练目标（如下一个标记预测或拟合配对偏好数据）转向直接优化被<strong>认为有价值或可取</strong>的输出。</p>
<p>KTO消除了对配对偏好排名或比较数据的需求，显著简化了数据要求。它只需要二元标签，指示某个LLM输出是可取的还是不可取的。这种二元偏好数据的需求使KTO在现实场景中更为实用，因为收集详细的偏好数据往往比较困难。</p>
<p><strong>前景理论 (prospect theory)</strong></p>
<p>KTO 的灵感来自 Daniel Kahneman 和 Amos Tversky
提出的决策行为模型，特别是他们的前景理论 (prospect theory)。KTO
将这些概念调整为损失函数，通过捕捉人类的偏差（如损失规避和风险敏感性），使
LLM 与人类反馈保持一致。</p>
<p>在前景理论中，人类在不确定性下的决策行为偏离了预期效用最大化的原则，主要是因为一些心理偏差，如损失厌恶（loss
aversion）和非线性概率加权（nonlinear probability
weighting）。这些概念是KTO损失函数的基础。</p>
<p><strong>1. 价值函数 (Value
Function)</strong>：前景理论中的价值函数用于描述人们如何看待收益和损失的差异。它具有以下特征：</p>
<ul>
<li><p><strong>对收益的凹性</strong>：当收益增加时，价值函数是凹的，这意味着人们在获得相同金额的收益时，所感受到的价值增加会逐渐减小。这反映了人们在面对收益时的风险厌恶（risk
aversion）。</p></li>
<li><p><strong>对损失的凸性</strong>：当面临损失时，价值函数是凸的，这意味着在损失相同金额时，所感受到的损失会逐渐增大，反映了人们在面对损失时的风险寻求（risk-seeking）行为。</p></li>
<li><p><strong>损失的影响大于收益</strong>：损失对人们的情感影响通常大于收益，这一点通过损失厌恶参数
<span class="math inline">\(\lambda\)</span>
来建模。该参数通常大于1，意味着人们在面对损失时的感受强于获得相同金额收益时的感受。</p></li>
</ul>
<p><strong>2. 数学表达式</strong>. 价值函数 <span class="math inline">\(v(x)\)</span> 可以用以下公式表示： <span class="math display">\[
v(x) = \begin{cases}
x^\alpha &amp; \text{if } x \geq 0 \\
-\lambda (-x)^\beta &amp; \text{if } x &lt; 0
\end{cases}
\]</span> 其中：</p>
<ul>
<li>$ (0,1)$ 和 <span class="math inline">\(\beta \in (0,1)\)</span>
控制对收益和损失的减敏感性（diminishing
sensitivity）。这意味着随着收益或损失的增加，人们的感知效应会逐渐减弱。</li>
<li>$ $
是损失厌恶因子，通常大于1，这表示人们对损失的反应比对收益更为强烈。</li>
</ul>
<p><strong>3. 概率加权函数 (Probability Weighting Function)</strong>:
人们在判断概率时，往往会倾向于高估小概率事件和低估大概率事件。尽管这一元素并非KTO的核心部分，但它强调了主观不确定性感知如何影响决策。这种加权使得人们在面对不确定性时的决策并不是完全理性的，而是受到了心理因素的影响。</p>
<p>Kahneman-Tversky Optimization (KTO)
的损失函数是基于前景理论构建的，其设计目标是直接最大化语言模型生成输出的效用。以下是
KTO 损失函数的关键要素及其解释：</p>
<p><strong>KTO‘s loss function</strong></p>
<ul>
<li><p>KTO 使用了一个 <strong>逻辑函数 <span class="math inline">\(\sigma\)</span></strong>，而不是经典前景理论中的分段价值函数。这种逻辑函数保持了对收益的<strong>凹性</strong>和对损失的<strong>凸性</strong>，反映了人类对风险的感知。</p></li>
<li><p><strong>风险厌恶参数 <span class="math inline">\(\beta\)</span></strong>
被纳入模型中，用于控制风险厌恶程度。这一参数影响价值函数饱和的陡峭程度，进而影响模型如何感知收益和损失。</p></li>
<li><p>在 KTO 中，传统的损失厌恶参数 <span class="math inline">\(\lambda\)</span>
被替换为两个独立的超参数：<strong><span class="math inline">\(\lambda_D\)</span></strong>（用于积极反馈的输出）和
<strong><span class="math inline">\(\lambda_U\)</span></strong>（用于消极反馈的输出）。允许模型根据输出类型的不同（积极或消极），以更细致的控制方式来处理反馈，从而更好地反映人类的风险厌恶特性。</p></li>
<li><p>模型的参考点通过 <strong>KL 散度</strong>
来定义，表示当前模型策略 <span class="math inline">\(\pi_\theta\)</span>
与参考策略 <span class="math inline">\(\pi_{\text{ref}}\)</span>
之间的差异。KL
散度项控制当前模型输出与预训练参考模型的偏离程度，并作为优化中评估收益和损失的参考点
<span class="math inline">\(z_0\)</span>。</p></li>
</ul>
<p>KTO（Kahneman-Tversky Optimization）损失函数的数学公式如下： <span class="math display">\[
L_{KTO}(\pi_\theta, \pi_{\text{ref}}) = \mathbb{E}_{x,y \sim
D}[\lambda_y - v(x,y)], \\
\quad \\
v(x,y) =
   \begin{cases}
   \lambda_D \sigma(\beta(r_\theta(x,y) - z_0)), &amp; \text{if } y \sim
\text{desirable} \\
   \lambda_U \sigma(\beta(z_0 - r_\theta(x,y))), &amp; \text{if } y \sim
\text{undesirable}
   \end{cases}
\]</span></p>
<p>其中：</p>
<ul>
<li><p><strong><span class="math inline">\(\mathbb{E}_{x,y \sim
D}\)</span></strong>：表示对数据集 <span class="math inline">\(D\)</span> 中的样本进行期望计算，其中 <span class="math inline">\(x\)</span> 是输入，<span class="math inline">\(y\)</span> 是模型生成的输出。</p></li>
<li><p><strong><span class="math inline">\(\lambda_y\)</span></strong>：代表与输出 <span class="math inline">\(y\)</span> 相关的损失厌恶参数，可以是
<strong><span class="math inline">\(\lambda_D\)</span></strong>（用于积极输出）或
<strong><span class="math inline">\(\lambda_U\)</span></strong>（用于消极输出），用于表示人类对损失的厌恶程度。</p></li>
<li><p><strong><span class="math inline">\(r_\theta(x,y)\)</span></strong>： $ r_(x,y) = . $
该函数表示在当前策略 <span class="math inline">\(\pi_\theta\)</span>
下生成输出 <span class="math inline">\(y\)</span> 的对数概率与参考策略
<span class="math inline">\(\pi_{\text{ref}}\)</span>
下生成同一输出的对数概率之比。它衡量了当前模型与参考模型在生成特定输出时的相对表现。</p></li>
<li><p><strong><span class="math inline">\(z_0\)</span></strong>： <span class="math inline">\(z_0 = KL(\pi_\theta(y'|x) \|
\pi_{\text{ref}}(y'|x))\)</span>. 这里量化当前策略 <span class="math inline">\(\pi_\theta\)</span> 和参考策略 <span class="math inline">\(\pi_{\text{ref}}\)</span>
之间的差异。它作为评估当前策略与参考策略偏离程度的参考点。</p></li>
<li><p><strong><span class="math inline">\(v(x,y)\)</span></strong>：价值函数，依赖于输出
<span class="math inline">\(y\)</span> 的性质. <strong><span class="math inline">\(\sigma\)</span></strong>：逻辑函数，用于对价值函数进行调整，使其保持凹性（对于收益）和凸性（对于损失），模型就会在收益时更加规避风险，在损失时更加追求风险。<strong><span class="math inline">\(\beta\)</span></strong>：风险厌恶参数，控制风险厌恶的程度。增加
<span class="math inline">\(\beta\)</span>会增加收益时的风险规避行为和损失时的风险追求行为。</p></li>
</ul>
<h3 id="ppo-dpo-以及-kto-的对比">PPO, DPO 以及 KTO 的对比</h3>
<table>
<colgroup>
<col style="width: 11%">
<col style="width: 29%">
<col style="width: 29%">
<col style="width: 29%">
</colgroup>
<thead>
<tr>
<th>Aspect</th>
<th>PPO</th>
<th>DPO</th>
<th>KTO</th>
</tr>
</thead>
<tbody>
<tr>
<td>目标</td>
<td>最大化预期奖励，同时防止策略更新过大（目标函数clip）。</td>
<td>根据人类偏好直接优化策略，使用二元分类目标（使用 KL
散度约束）。</td>
<td>通过最大化 LLM
生成的效用对齐模型，基于前景理论，不需要详细的偏好对。</td>
</tr>
<tr>
<td>输入</td>
<td>来自环境的状态和奖励。</td>
<td>来自环境的状态和人类偏好反馈。</td>
<td>带有二元标签（可取或不可取结果）的 LLM 输出。</td>
</tr>
<tr>
<td>输出</td>
<td>在环境中采取的行动。</td>
<td>在环境中采取的行动，与人类偏好对齐。</td>
<td>与简化人类效用函数对齐的 LLM 生成结果。</td>
</tr>
<tr>
<td>学习机制</td>
<td>使用clip替代目标的策略梯度来更新策略和价值网络。</td>
<td>在人类偏好数据上进行二元交叉熵优化，更新单个策略网络。</td>
<td>基于 LLM 输出与二元反馈的对齐进行优化，无需复杂的偏好模型。</td>
</tr>
<tr>
<td>网络结构</td>
<td>独立的策略网络和价值网络。</td>
<td>单个策略网络。</td>
<td>针对 KTO 方法学调整的 LLM 框架。</td>
</tr>
<tr>
<td>反馈机制</td>
<td>使用来自环境的奖励作为学习的反馈。</td>
<td>使用人类偏好数据作为直接反馈进行学习。</td>
<td>利用对 LLM 输出的二元反馈来指导对齐，无需复杂的偏好数据。</td>
</tr>
<tr>
<td>稳定性</td>
<td>目标函数中的剪辑机制保持策略更新的稳定性。</td>
<td>通过直接优化偏好，利用动态逐例重要性加权实现内在稳定性。</td>
<td>通过简化反馈机制和聚焦于效用最大化来实现稳定的对齐。</td>
</tr>
<tr>
<td>复杂性</td>
<td>由于双网络结构和奖励最大化与策略更新稳定性之间的平衡，较复杂。</td>
<td>更简单，因为它绕过显式的奖励建模，直接从人类偏好优化政策。</td>
<td>通过消除对详细偏好建模的需求，专注于二元效用优化，降低复杂性。</td>
</tr>
<tr>
<td>适用性</td>
<td>适用于各种 RL 环境，其中奖励信号可用。</td>
<td>在与人类偏好对齐至关重要的场景中特别有效。</td>
<td>在快速和简化对齐人类反馈的场景中尤为有用。</td>
</tr>
</tbody>
</table>
<h3 id="对齐可能引入的偏差以及解决策略">对齐可能引入的偏差以及解决策略</h3>
<p>在讨论 <strong>强化学习人类反馈（RLHF）</strong> 和
<strong>强化学习人工反馈（RLAIF）</strong>
时，一个重要的问题是：这些方法是否会给模型引入偏见？答案是肯定的，正如任何依赖人类输入的机器学习方法，RLHF
也有引入偏见的潜力。</p>
<p><strong>可能引入的不同形式的偏见</strong></p>
<ol type="1">
<li><p><strong>选择偏见</strong>：RLHF
依赖于人类评估者的反馈，这些评估者可能会有自己的偏见和偏好，因此他们的反馈可能局限于他们能够关联的主题或情境。这可能导致模型没有接触到其在现实世界中将遇到的行为和结果的真实范围。</p></li>
<li><p><strong>确认偏见</strong>：人类评估者可能更倾向于提供确认他们已有信念或预期的反馈，而不是根据代理的表现提供客观反馈。这可能导致模型在某些行为或结果上受到强化，而这些行为或结果在长远来看可能并不理想或可取。</p></li>
<li><p><strong>评分者间的差异</strong>：不同的人类评估者可能对代理表现的质量有不同的看法或判断，导致agent收到的反馈不一致。这使得有效训练agent变得困难，并可能导致次优表现。</p></li>
<li><p><strong>反馈有限</strong>：人类评估者可能无法对agent表现的所有方面提供反馈，导致
agent 学习的缺口，可能在某些情况下表现不佳。</p></li>
</ol>
<p><strong>缓解策略</strong></p>
<ol type="1">
<li><p><strong>多样化评估者选择</strong>：选择具有不同背景和视角的评估者可以帮助减少反馈中的偏见，就像在工作场所中一样。这可以通过从不同的人口群体、地区或行业招募评估者来实现。</p></li>
<li><p><strong>共识评估</strong>：使用共识评估，即多个评估者对同一任务提供反馈，可以减少个体偏见的影响，提高反馈的可靠性。这几乎就像是对评估进行“归一化”。</p></li>
<li><p><strong>评估者的校准</strong>：通过提供培训和指导来校准评估者，帮助提高反馈的质量和一致性。</p></li>
<li><p><strong>反馈过程的评估</strong>：定期评估反馈过程，包括反馈质量和培训过程的有效性，可以帮助识别和解决可能存在的偏见。</p></li>
<li><p><strong>agent 表现的评估</strong>：定期评估 agent
在各种任务和不同环境中的表现，可以确保其没有过拟合于特定示例，并且能够推广到新的情境。</p></li>
<li><p><strong>平衡反馈</strong>：将人类评估者的反馈与其他反馈来源（如自我对话或专家演示）进行平衡，有助于减少反馈中的偏见影响，提高训练数据的整体质量。</p></li>
</ol>
<h3 id="trl---transformer-reinforcement-learning">TRL - Transformer
Reinforcement Learning</h3>
<p><strong>TRL</strong>（Transformer Reinforcement
Learning）库可用于通过
<strong>监督微调（SFT）</strong>、<strong>奖励建模（RM）</strong>、<strong>近端策略优化（PPO）</strong>
以及 <strong>直接偏好优化（DPO）</strong>
等方法，对转换器语言模型和扩散模型进行微调和对齐。</p>
<h2 id="mixture-of-experts">5. Mixture of Experts</h2>
<p><a target="_blank" rel="noopener" href="https://aman.ai/primers/ai/mixture-of-experts/">Mixture of
Experts</a></p>
<h2 id="encoder-vs.-decoder-vs.-encoder-decoder">6. Encoder vs. Decoder
vs. Encoder-Decoder？</h2>
<p><a target="_blank" rel="noopener" href="https://discuss.huggingface.co/t/suggestions-and-guidance-finetuning-bert-models-for-next-word-prediction/14043">模型总结</a>
<img src="./statistic/enc_dec.jpeg"></p>
<h3 id="编码器模型">编码器模型</h3>
<p>基于编码器的模型专注于理解输入文本。它们通过在包含重建损坏输入任务的预训练（如掩码部门tokens）中，捕获丰富的上下文信息。<strong>BERT</strong>（双向编码器表示的变换器）是编码器模型的一个重要例子。在
BERT 中，部分输入标记会被特殊的 [MASK]
标记替代，模型通过周围的上下文预测这些被掩蔽的标记。这使得 BERT
能够学习双向表示，从而捕捉标记左侧和右侧的上下文。编码器模型在自然语言理解（NLU）任务中尤其有效，例如文本分类、情感分析和抽取式问答。这些任务受益于模型深刻理解和表示输入文本的能力。</p>
<p>BERT 可以利用双向语境进行预测 masked
tokens，相较于自回归模型，使用双向信息可以提高性能。然而，BERT
在预训练时使用的 [MASK]
等人工符号在微调时并不存在于真实数据中，这就造成了预训练数据与微调数据分布之间的异质性。此外，由于预测的tokens在输入中被mask，BERT
无法像自回归那样使用乘积规则建立联合概率模型。换句话说，<strong>BERT
assumes the predicted tokens are independent of each other given the
unmasked tokens, which is oversimplified as high-order, long-range
dependency is prevalent in natural language.
（在给定其他未被掩码的tokens时，BERT
对每个被掩盖的单词独立地进行预测，所有需要预测的token之间被假设是相互独立的）</strong></p>
<p>BERT（及其所有变体，如 RoBERTa、DistilBERT、ALBERT 等）和 XLM
就是编码器模型的例子。</p>
<p><img src="/2024/10/28/General-Questions-about-LLM/bert.jpg"></p>
<p><strong>优点：</strong></p>
<ol type="1">
<li><strong>上下文依赖性：</strong>
在BERT等Encoder模型中，模型可以同时获取左右两侧的上下文信息（双向上下文）。相比之下，像GPT这样的自回归模型只能访问当前位置之前的内容（单向上下文）。双向上下文对于理解文本中的复杂依赖关系和语义结构至关重要，因此Encoder模型在许多自然语言处理任务上表现优异。</li>
<li><strong>丰富的上下文理解：</strong>
Encoder模型擅长捕捉输入数据中的复杂模式和关系，尤其适合需要深入理解和分析的任务，比如命名实体识别、文本分类、阅读理解等。这使得模型能够更好地理解句子的语义和结构，从而在相关任务中表现出色。</li>
</ol>
<p><strong>缺点：</strong></p>
<ol type="1">
<li><strong>输入噪声：</strong>
BERT在预训练阶段使用了人工符号（如[MASK]）来掩盖部分输入单词，但这些符号在实际应用和微调时并不存在。这种差异导致了预训练和微调之间数据分布的异质性，影响模型的迁移能力。</li>
<li><strong>独立性假设：</strong>
BERT在预测被掩盖的单词时，假设每个掩盖的单词是独立于其他掩盖词的，只与未被掩盖的单词相关。例如，在句子“it
shows that the housing crisis was turned into a banking
crisis”中，如果掩盖了“banking”和“crisis”两个词，模型会分别预测它们，而不考虑二者之间的隐含关系。这种独立性假设限制了模型对掩盖词之间复杂依赖关系的捕捉，影响了在需要更精细理解的任务中的表现。</li>
</ol>
<h3 id="解码器模型">解码器模型</h3>
<p>基于解码器的模型（自回归模型）旨在执行文本生成任务。这些模型一次生成一个token，并使用之前生成的token作为上下文来预测下一个token。<strong>GPT</strong>（生成式预训练变换器）、GPT-2
和 GPT-3
是解码器模型的典型例子。这些模型在大规模语料库上进行预训练，以预测句子中的下一个词，从而能够生成连贯且语境相关的文本。解码器模型在自然语言生成（NLG）任务中表现出色，例如语言翻译、文本摘要和对话生成。它们生成流利且上下文适当的文本的能力使其成为从头生成内容任务的理想选择。</p>
<p>自回归语言模型利用上下文中的词来预测下一个词，通过估计文本语料库的概率分布来实现。具体来说，给定一个文本序列
$ x = (x_1, , x_T) <span class="math inline">\(，自回归语言建模将似然分解为前向积：\)</span>$
p(x) = <em>{t=1}^{T} p(x_t | x</em>{&lt;t}) $$</p>
<p>或者反向的形式： <span class="math display">\[
p(x) = \prod_{t=1}^{T} p(x_t | x_{&gt;t})
\]</span></p>
<p>通常使用<strong>多项式分布</strong>（Multinomial
Distribution）来建模下一个词的生成过程。这是因为语言模型的目标是根据先前的词预测当前词的概率，而语言中词汇的生成通常是一个离散事件。因此，给定上下文
$x_{&lt;t} $，下一个词 $ x_t $ 的条件概率可以表示为： <span class="math display">\[
p(x_t \mid x_{&lt;t}) = \text{softmax}(z_t),
\]</span> 其中，$ z_t $
是通过神经网络计算得到的未归一化的得分，通常代表每个词在词汇表中的相对可能性。通过使用
softmax
函数，这些得分被转换为一个有效的概率分布，保证所有可能词的概率和为
1。</p>
<p>在这种情况下，模型会学习每个条件分布的参数模型（例如神经网络）。由于自回归语言模型只训练编码单向上下文（要么是前向的，要么是后向的），因此它在建模深层双向上下文方面并不有效。下面的图示展示了前向和后向的方向性。</p>
<p><img src="/2024/10/28/General-Questions-about-LLM/auto1.jpg"></p>
<p><img src="/2024/10/28/General-Questions-about-LLM/auto2.jpg"></p>
<p><strong>优点：</strong></p>
<ol type="1">
<li><strong>生成能力强</strong>：自回归语言模型非常适合生成式自然语言处理（NLP）任务。由于它们采用因果注意力机制来预测下一个标记，因此在内容生成方面自然适用。它们能够生成流畅且与上下文相关的文本，这使得它们在需要自然语言生成的任务中表现出色。</li>
<li><strong>训练数据生成简单</strong>：训练这些模型的数据生成相对简单，因为目标只是预测给定序列中的下一个标记。这利用了语言数据的固有结构，使得数据准备过程更加高效。</li>
</ol>
<p><strong>缺点：</strong></p>
<ol type="1">
<li><strong>上下文限制</strong>：自回归语言模型只能使用前向上下文或后向上下文，这意味着它们不能同时利用双向上下文。这种限制可能会影响它们在需要深刻理解双向上下文的任务中的表现。例如，在处理复杂句子结构或含义依赖时，缺乏双向上下文可能导致理解不够准确。</li>
</ol>
<h4 id="编码器-解码器模型">编码器-解码器模型</h4>
<p>编码器-解码器模型（也称为seq2seq
模型）结合了编码器和解码器架构的优势。这些模型使用编码器处理和理解输入序列，使用解码器生成输出序列。这种架构在输入和输出都是序列的任务中特别有效，这些序列可能具有不同的长度或格式，甚至是不同的语言。
编码器将输入序列转换为固定长度的上下文向量或中间表示，捕捉输入的含义和上下文。解码器然后接收这个上下文向量，逐个标记地生成输出序列，通常采用类似于解码器模型中的自回归技术。</p>
<ul>
<li><strong>T5 (Text-To-Text Transfer
Transformer)</strong>（文本到文本的迁移变换器）是编码器-解码器模型的一个显著例子。T5
将每个自然语言处理问题都视为一个文本到文本的问题，其中输入和输出都是文本序列。这种方法使
T5 能够应用于广泛的任务，包括翻译、摘要和问答。<br>
</li>
<li><strong>BART (Bidirectional and Auto-Regressive
Transformers)</strong>（双向自回归变换器）是另一个强大的编码器-解码器模型。BART
通过使用任意噪声函数破坏文本并学习重建原始文本进行预训练。这使得它在需要基于对输入的理解生成文本的任务（如摘要和对话生成）中非常有效。<br>
</li>
<li><strong>BigBird</strong>
使用稀疏注意力机制来处理较长的序列。这使得它适合处理长文档的任务，如文档分类和长篇问答。</li>
</ul>
<p><strong>优点：</strong></p>
<p>编码器-解码器模型能够同时处理输入的理解和输出的生成，使它们在以下任务中特别有效：</p>
<ul>
<li><p>机器翻译：将一种语言翻译成另一种语言。</p></li>
<li><p>文本摘要：生成文本的简要概述。</p></li>
<li><p>对话生成：在对话系统中生成合适的回应。</p></li>
</ul>
<p><strong>缺点：</strong></p>
<ol type="1">
<li><p><strong>计算资源需求高</strong>：编码器-解码器模型通常参数量庞大，尤其是在处理复杂任务时。它们需要大量的计算资源和内存，训练和推理速度可能较慢。</p></li>
<li><p><strong>长序列处理能力有限</strong>：虽然一些模型（如BigBird）专门针对长序列进行了优化，但传统的编码器-解码器模型在处理非常长的输入时仍然面临挑战，因为它们的输入长度受限。</p></li>
<li><p><strong>依赖于大量标注数据</strong>：这些模型通常需要大量的高质量标注数据进行训练，这在某些领域可能难以获得。此外，训练过程中数据的多样性和质量直接影响模型的性能。</p></li>
<li><p><strong>对输入输出长度不匹配的敏感性</strong>：编码器-解码器模型在处理输入和输出长度差异较大的任务时，可能会表现不佳。例如，当输入很长而输出很短时，模型可能难以有效地提取和生成信息。</p></li>
</ol>
<h3 id="总结">总结</h3>
<ul>
<li>编码器模型在需要理解和解释文本的任务中表现出色。由于其能够捕捉双向上下文，这使得它们适用于理解整个句子或文档上下文至关重要的任务。例如，命名实体识别、情感分析和文本分类等任务都依赖于模型对输入文本的深刻理解。</li>
<li>解码器模型则非常擅长生成文本，因此非常适合创意任务，如故事生成、聊天机器人回复和文本补全等。它们通过利用先前生成的内容作为上下文来预测下一个词，从而能够生成流畅且与上下文相关的文本。</li>
<li>编码器-解码器模型提供了一种灵活的架构，可以处理广泛的任务，从机器翻译、文本摘要到复杂的问题回答和文档生成。这种模型能够同时理解和生成文本，使其在需要深刻理解和流利文本生成的任务中非常有效。</li>
</ul>
<p>例如，在机器翻译中，编码器处理源语言的输入句子，生成一个上下文向量，而解码器则利用这个上下文向量生成目标语言的翻译。类似地，在文本摘要中，编码器阅读并理解原始文本，而解码器则生成一个简洁的摘要。这种架构的优势在于它能够结合理解和生成的能力，适用于多种自然语言处理任务。</p>
<h2 id="目标检测的常用评估指标">7. 目标检测的常用评估指标</h2>
<h3 id="intersection-over-union-iou">Intersection Over Union (IoU)</h3>
<p><img src="/2024/10/28/General-Questions-about-LLM/iou.png"> Image
credits to <a target="_blank" rel="noopener" href="https://jonathan-hui.medium.com/map-mean-average-precision-for-object-detection-45c121a31173">source.</a></p>
<p>IoU 越高，拟合效果越好。IoU
对任何大小和形状的物体都很有效。这一针对每个物体的指标与精确度和召回率共同构成了完整的物体检测指标--平均精确度
(mAP) 的基础。</p>
<h3 id="average-precision-ap">Average Precision (AP)</h3>
<p>目标检测器会生成多个预测结果：每张图片可能包含多个被预测的目标，而需要进行推理的图片也很多。每个预测目标都会被赋予一个置信度，表示检测器对该预测的信心程度。我们可以选择不同的<strong>置信度阈值</strong>，以决定接受哪些预测结果。例如，若将阈值设置为
0.7，那么只接受置信度大于 0.7
的预测，低置信度的预测将被舍弃。由于可以选择的阈值很多，使用精确率-召回率曲线进行评估。</p>
<p>模型越好，其的精确度和召回率就越高：这会将曲线的边界推向顶部和右侧。可以用曲线下的面积来概括模型的性能。这样就得到了一个介于
0 和 1 之间的数值，数值越大越好。这个指标通常称为平均精度 (AP)。</p>
<h3 id="mean-average-precision-map">Mean Average Precision (mAP)</h3>
<p>mAP
用来衡量模型的整体检测性能。它结合了精确率（precision）和召回率（recall），并对不同类别和不同阈值下的检测结果进行综合评价。这里的类别指的是目标检测任务中模型需要识别的不同目标种类或类型。例如，在物体检测任务中，每个目标物体都属于某个特定类别（如“人”、“车”、“猫”、“狗”等），这些类别就是检测任务中模型需要识别和区分的对象。</p>
<h4 id="计算过程">计算过程</h4>
<p>首先，我们只考虑单张图片和单个类别。假设网络在一张图片中预测了 10
个属于某一类别的目标：每个预测包含一个bounding
boxes、预测的类别以及预测的置信度（即网络对其预测的信心）。</p>
<ol type="1">
<li><p>使用IoU来决定每个预测是否正确。对于每个真实目标和相应的预测，如果同时满足：（1）预测的类别与真实类别匹配；（2）IoU
大于某个阈值。那么认为该预测是正确的（True
Positive）。否则，该预测被认为是错误的（False Positive）。</p></li>
<li><p>将所有的预测按置信度从高到低排序。在表格中，按置信度从高到低排列预测结果，右侧显示累积的精确率和召回率。</p>
<p><img src="/2024/10/28/General-Questions-about-LLM/table1.png"></p></li>
<li><p>根据这张表格，对于每个置信度（从最大到最小），计算出截至该点的精确度和召回率。将其绘制成图，然后得到该图像和类别的原始精确度-召回率曲线：
<img src="/2024/10/28/General-Questions-about-LLM/graph1.png"></p></li>
<li><p>将该曲线锯齿平滑化，从而绘制出网络针对该图像和类别的最终精确度-召回曲线。根据平滑后的精确度-召回率曲线计算平均精确度（曲线下的面积）：
<img src="/2024/10/28/General-Questions-about-LLM/graph2.png"> (Images
credits to <a target="_blank" rel="noopener" href="https://cs230.stanford.edu/section/8/">source</a>)</p></li>
<li><p>对每张图像和每个类别的 AP
进行平均，从而得出模型在整个数据集的平均精度 mAP.</p></li>
</ol>
<h2 id="文本生成模型的常用评估指标">8. 文本生成模型的常用评估指标</h2>
<h3 id="困惑度-perplexity">困惑度 Perplexity</h3>
<p>Perplexity
衡量语言模型生成文本流畅性和质量的一个常见指标，反映模型对词序预测的准确性。它本质上是模型对给定文本的“不确定性”的度量。困惑度得分越低，说明语言模型在计算给定序列中可能出现的下一个单词时越自信，而困惑度得分越高，说明模型对词的预测较为不确定，生成的文本可能不流畅，或者不符合语言结构。</p>
<p>熵是随关于机变量的不可预测性或随机性的度量。对于一个离散随机变量
<span class="math inline">\(x\)</span>，概率分布为 <span class="math inline">\(p(x)\)</span>，其熵定义为 <span class="math display">\[
H(x) = -\sum_x p(x) \log_2 p(x).
\]</span>
基于此，困惑度定义为一个序列的负对数似然的指数平均。具体来说，假设模型为一段文本
$ (w_1, w_2, ..., w_N)$ 生成概率，困惑度定义为： <span class="math display">\[
\begin{aligned}
\text{Perplexity} &amp;= 2^{-\frac{1}{N} \sum_{i=1}^{N} \log_2 P( w_1,
w_2, ..., w_{i-1}, w_i)} \\
&amp;= P(w_1, w_2, ..., w_{i-1}, w_i)^{-\frac{1}{N}} = \prod_{i=1}^{N}
P(w_i | w_1, w_2, ..., w_{i-1})^{-\frac{1}{N}}.
\end{aligned}
\]</span> 其中，$ P(w_i | w_1, w_2, ..., w_{i-1}) $ 是模型在前面 <span class="math inline">\(i-1\)</span> 个词的条件下预测第 $ i $
个词的概率。</p>
<h3 id="突发性-burstiness">突发性 Burstiness</h3>
<p>突发性意味着如果一个词在文档中使用了一次，那么它很可能会再次出现。这种现象称为突发性，它表明第二次及之后出现的词的重要性低于第一次出现的词。值得注意的是，词语的突发性与其语义内容呈正相关性：信息量更大的词也往往更加突发。突发性基本上衡量了一段内容的可预测性，体现在句子长度和结构的均一性上。</p>
<p>通过收集句子长度，将文本中的每个句子按照其包含的单词数计算长度。计算均值（句子长度的平均值）和方差（句子长度的波动程度）。突发性
<strong>b</strong> 的数学计算公式为：<br>
<span class="math display">\[
b = \left( \frac{\sigma_{\tau}}{m_{\tau}} - 1 \right) \left(
\frac{\sigma_{\tau}}{m_{\tau}} + 1 \right)^{-1}
\]</span> 其中，<strong>b</strong> 的取值范围在 <span class="math inline">\([-1, 1]\)</span> 之间。</p>
<p>一般来说，人工智能具有统一有规律的特点。因此可以假设人类作者的突发性高于
AI 的突发性: <span class="math display">\[
b_H - b_{AI} \geq 0
\]</span> 其中，<strong>b_H</strong>
表示人类作者的平均突发性，<strong>b_{AI}</strong> 表示
AI（如某个特定的大型语言模型）的平均突发性。</p>
<p>总之：</p>
<ul>
<li><p>Perplexity
主要用于评估模型的语言流畅性，适合衡量语言模型在生成或预测下一个词时的能力。它衡量的是整个文本的语言建模性能。</p></li>
<li><p>Burstiness
则侧重于文本中词语分布的不均匀性，尤其关注某些词或话题在文本中突然集中出现的现象。它更多地用于分析文本结构中的局部特征。</p></li>
</ul>
<h3 id="bleu">BLEU</h3>
<p>BLEU 是双语评估研究（Bilingual Evaluation
Understudy）的缩写，<strong>主要用于机器翻译</strong>。它通过与一组参考译文进行比较来量化机器翻译文本的质量。BLEU
分数计算的关键是机器翻译文本中 n-grams （给定文本样本中 n
个单词的连续序列）的精确度。不过，为了防止因句子较短而高估精确度，BLEU
包含一个简短度惩罚因子。尽管 BLEU
被广泛使用，但值得注意的是，<strong>BLEU
主要关注的是精确度，缺少召回率部分。</strong></p>
<p>对于 unigram (single word)，精度计算公式为 <span class="math display">\[
\text{precision} = \frac{Number of correct words in machine
translation}{Total words in machine translation}.
\]</span> <strong>Unigram matches tend to measure adequacy while longer
n-grams matches account for fluency.</strong> 为避免夸大精度，BLEU
采用修正的精度计算方法。计算方式如下：</p>
<ol type="1">
<li><p>对于 n-gram，计算预测序列中所有匹配的 n-gram
与该序列中所有n-gram的商。 <span class="math display">\[
p_n = \frac{\sum_{n-gram \in \text{hypothesis}}
\text{count}_{\text{match}}(n-gram)}{\sum_{n-gram \in \text{hypothesis}}
\text{count}(n-gram)}.
\]</span> 示例： <img src="/2024/10/28/General-Questions-about-LLM/bleu-unigrams.png"> Image
credits to <a target="_blank" rel="noopener" href="https://aman.ai/primers/ai/evaluation-metrics/#example-1">source.</a>
如图所示，对于unigram，计算得到的 <span class="math inline">\(p_n =
7/9\)</span>.</p></li>
<li><p>计算得到不同 n-grams 的精度，然后对其对数进行加权平均： <span class="math display">\[
\text{BLUE}_N = \text{BP} \exp(\sum_{n=1}^{N}w_n \log p_n).
\]</span></p></li>
</ol>
<ul>
<li>为了防止机器翻译生成的翻译过于简短，增加了简洁性惩罚 BP，BP
是参考序列和预测训练长度的函数。 <span class="math display">\[
\text{BP} = \begin{cases}
1 &amp; \text{if } l_{hyp} &gt; l_{ref} \\
e^{(1 - \frac{l_{ref}}{l_{hyp}})} &amp; \text{if } l_{hyp} \geq l_{ref}
\end{cases}
\]</span> BLEU 分数是一个介于 0 和 1 之间的标量值，0.6 或 0.7
分是当前可以达到的较好的分数。</li>
</ul>
<h3 id="rouge">ROUGE</h3>
<p>ROUGE 分数代表 <strong>Recall-Oriented Understudy for Gisting
Evaluation</strong>，最早在《ROUGE: A Package for Automatic Evaluation
of
Summaries》中提出，主要用于自动总结的评估，有时也用于机器翻译的评估。</p>
<p>ROUGE 的关键特征是其对召回率的重视，测量系统生成的摘要中有多少参考
n-gram 被找出。这使得 ROUGE 尤其适用于需要覆盖关键点的任务。ROUGE
的变体中，<strong>ROUGE-N</strong> 计算 n-gram
的重叠情况，<strong>ROUGE-L</strong>
使用最长公共子序列来衡量句子层级的结构相似性，<strong>ROUGE-S</strong>
则包含跳跃双字组的统计信息。</p>
<p><strong>ROUGE-N</strong>: <span class="math display">\[
\text{ROUGE-N} = \frac{\text{Number of N-grams in both system and
reference summary}}{\text{Total number of N-grams in reference summary}}
\]</span></p>
<p><strong>ROUGE-L</strong>: 或称 <strong>ROUGE-LCS</strong>。
基于最长公共子序列（LCS）的长度进行评估。为了弥补纯召回率指标（如
ROUGE-N）的不足，ROUGE-L 通过计算 <span class="math inline">\(F_{\beta}\)</span>
分数，结合了精确率和召回率。</p>
<p>ROUGE-LCS的优势在于，它不仅仅关注 n-gram
的连续词汇重叠，还考虑了序列匹配（即重叠的词不一定需要按照相同顺序出现）。另一个更大的优势是，它自动包含最长的序列内的公共
n-gram，因此不需要预先定义 n-gram 的长度。 <span class="math display">\[
\text{ROUGE-LCS} = \frac{(1 + \beta^2) R_{LCS} P_{LCS}}{R_{LCS} +
\beta^2 P_{LCS}}
\]</span> 其中： - <strong>LCS(reference,
hypothesis)</strong>：表示参考文本和生成文本的最长公共子序列。 - <span class="math inline">\(R_{LCS}\)</span>：表示基于参考文本的 LCS
召回率，公式为 $ $。 - <span class="math inline">\(P_{LCS}\)</span>：表示基于生成文本的 LCS
精确率，公式为 $ $。</p>
<p>示例： <img src="/2024/10/28/General-Questions-about-LLM/rouge-l.png"> Image
credits to <a target="_blank" rel="noopener" href="https://aman.ai/primers/ai/evaluation-metrics/#example-1">source.</a>。在该例子中，
可以计算得到 <span class="math inline">\(R_{LCS} = 7/10\)</span>, <span class="math inline">\(P_{LCS} = 7/9\)</span>, <span class="math inline">\(\text{ROUGE-LCS} = \frac{(1 + \beta^2) 49}{70 +
\beta^2 63}\)</span></p>
<h3 id="bertscore">BERTScore</h3>
<p><strong>BERTScore</strong> 和 <strong>MoverScore</strong>
是两个用于评估文本生成任务（如机器翻译、文本摘要等）质量的指标，它们基于语义相似性，弥补了传统
n-gram 匹配方法（如
BLEU、ROUGE）的不足。这两者通过引入深度学习模型（特别是
BERT）来捕捉文本的语义信息，更加注重内容的语义一致性。</p>
<p>BERTScore是一种基于 BERT
预训练模型的文本评估方法。它通过计算生成文本和参考文本在语义上的相似性，避免了传统
n-gram 方法忽略语义匹配的不足。BERTScore
主要是通过比较句子的词嵌入（word embeddings）来衡量语义相似度。</p>
<h4 id="关键步骤">关键步骤</h4>
<ol type="1">
<li><p><strong>词嵌入表示</strong>：使用 BERT
模型将生成文本和参考文本中的单词转换为高维的向量表示（嵌入表示）。</p></li>
<li><p><strong>相似性计算</strong>：计算生成文本和参考文本中每个词的嵌入表示的余弦相似度（cosine
similarity）。</p></li>
<li><p><strong>匹配</strong>：为每个生成文本中的词找到参考文本中最相似的词，并基于余弦相似度进行匹配。对于候选句子中的每个token，选择与参考句子中任何token的余弦相似度得分最高的token，反之亦然。这些得分用于计算精确度、召回率和
F1 score。</p></li>
<li><p><strong>平均相似度</strong>：基于这些匹配，对整个数据集的精确度、召回率和
F1 分数进行汇总，计算生成文本和参考文本之间的整体相似度分数。</p></li>
</ol>
<h4 id="bertscore-的优势">BERTScore 的优势</h4>
<ul>
<li><p>能够捕捉到词汇和语义之间的细微差别，特别适合于语义相似但词汇不完全相同的场景。</p></li>
<li><p>相比于 BLEU 和 ROUGE 等基于 n-gram
的方法，它更加关注语义层面的匹配，而不仅仅是词汇层面的匹配。</p></li>
</ul>
<h3 id="moverscore">MoverScore</h3>
<p><strong>MoverScore</strong>
是一种改进的文本相似度评估指标，它将<strong>词移动距离（Word Mover's
Distance, WMD）</strong>与<strong>BERT
嵌入</strong>相结合，用来衡量生成文本和参考文本之间的语义差异。MoverScore
不仅考虑了词汇的相似性，还考虑了将生成文本中的词映射到参考文本中的最小代价。</p>
<h4 id="关键步骤-1">关键步骤</h4>
<ol type="1">
<li><p><strong>词嵌入表示</strong>：与 BERTScore 类似，使用 BERT
模型将文本中的每个词转换为向量表示。</p></li>
<li><p><strong>计算词移动距离</strong>：计算生成文本和参考文本之间的词嵌入的最小移动代价，即参考文本中的词映射到生成文本中的词需要“移动”多少距离。</p></li>
<li><p><strong>匹配得分</strong>：基于词移动距离计算生成文本和参考文本的相似度分数，得分越高表示两者的语义越接近。</p></li>
</ol>
<h4 id="moverscore-的优势">MoverScore 的优势</h4>
<ul>
<li><p>结合了词嵌入的语义相似度和词移动距离，能有效捕捉句子层次的结构信息和语义变化。</p></li>
<li><p>对词序、句子结构、语义信息具有更高的鲁棒性，特别适用于更复杂的语言生成任务评估。</p></li>
</ul>
<h3 id="对比总结">对比总结</h3>
<ul>
<li><p>语义匹配与标记相似性：MoverScore 使用单词移动距离（Word Mover's
Distance）和预训练嵌入来衡量整体句子级语义相似性。相比之下，BERTScore
则侧重于使用上下文嵌入的标记级相似性。</p></li>
<li><p>评估目标：MoverScore
通过测量词嵌入之间的距离来提供精细的语义评估，而 BERTScore
则评估标记嵌入的精确度、召回率和 F1 分数。</p></li>
<li><p>对同义词的鲁棒性：MoverScore
依赖于嵌入距离，因此对同义词和意译具有固有的鲁棒性，而 BERTScore
通过上下文嵌入也能很好地处理同义词和意译。</p></li>
</ul>
<p>总之，MoverScore 提供了一种sentence-level
语义相似性测量方法，可以捕捉句子的整体含义和结构，而 BERTScore
则提供了一种详细的token-level相似性评估方法，重点关注精确度、召回率和 F1
分数。</p>
<h2 id="国内外基座大模型">9. 国内外基座大模型</h2>
<h3 id="国内">国内</h3>
<ul>
<li><p>百度文心一言，2023年3月。具有强大的自然语言理解和生成能力，在搜索、信息流推荐、广告投放、智能写作、对话系统等场景中实现智能化升级。
<img src="/2024/10/28/General-Questions-about-LLM/wenxin.jpg"> (image
credits to <a target="_blank" rel="noopener" href="https://wenxin.baidu.com">source</a>)</p></li>
<li><p>阿里通义千文，2023年4月。通义千问-Max（通义千问系列效果最好的模型，适合复杂、多步骤的任务），
通义千问-Plus（能力均衡，推理效果、成本和速度介于通义千问-Max和通义千问-Turbo之间，适合中等复杂任务），通义千问-Turbo（通义千问系列速度最快、成本很低的模型，适合简单任务）。2024年8月推出通义千问-72B（Qwen-72B）。通义千问-72B（Qwen-72B）是阿里云研发的通义千问大模型系列的720亿参数规模模型，它的预训练数据类型多样、覆盖广泛，包括大量网络文本、专业书籍、代码等。Qwen-72B-Chat是在Qwen-72B的基础上，使用对齐机制打造的基于大语言模型的AI助手。</p>
<p><img src="/2024/10/28/General-Questions-about-LLM/ali.jpg"> (image
credits to <a target="_blank" rel="noopener" href="https://help.aliyun.com/zh/model-studio/getting-started/models?spm=a2c4g.11186623.0.0.41f9253aof0b1F">source</a>)</p></li>
<li><p>上海人工智能实验室等联合推出的书生·视觉大模型（InternVL）。书生浦语，书生万象，书生风乌，书生翼飞，书生天际，书生济世。InternVL2-Llama3-76B.
[github]（https://github.com/OpenGVLab/InternVL）</p></li>
<li><p>科大讯飞星火大模型，2023年5月。星火大模型在教育、医疗、政务、司法等行业应用场景中广泛使用，尤其是在智能语音合成、语音识别、语义理解和知识图谱构建等方面表现突出。</p></li>
<li><p>百川智能-百川大模型，2023年6月。</p></li>
<li><p>华为盘古大模型，2023年7月。</p></li>
<li><p>腾讯混元大模型，2023年9月。</p></li>
<li><p>商汤日日新，2024年4月。<a target="_blank" rel="noopener" href="https://platform.sensenova.cn/doc?path=/model/llm/GeneralLLM.md">doc</a></p></li>
<li><p>字节豆包，2024年9月。</p></li>
<li><p>清华-ChatGLM</p></li>
<li><p>智谱AI，GLM-4。</p></li>
<li><p>Minimax系列模型</p></li>
<li><p>京东言犀大模型。专为其电商平台定制的人工智能模型，尤其擅长在智能客服、智能营销和智能供应链管理等方面发挥作用</p></li>
<li><p>360GPT大模型。</p></li>
</ul>
<h3 id="国外">国外</h3>
<ul>
<li>Meta：LLaMA系列。2023年2月LLaMA1（7B、13B、33B、65B（650
亿）），2023年7月LLaMA2（7B、13B、34B和70B），2024年4月LLaMA3（8B和70B）。</li>
</ul>
<p>相比于Llama 1，Llama
2将预训练的语料token数量扩充到了2T（万亿），同时将模型的上下文长度从2048翻倍到了4096，并引入了分组查询注意力机制（grouped-query
attention, GQA）技术，GQA可以在最佳性能(multi-query
attention，MQA)和最佳模型质量(multi-head
attention，MHA)之间做到很好的权衡。</p>
<p>相比Llama 2，Llama
3支持8K长文本，并采用了编码效率更高的tokenizer，词表的大小为128K。在预训练数据方面，Llama
3使用了超过15T token的语料，这比Llama 2的7倍还多。Llama
3在性能上取得了巨大飞跃，并在相同规模的大模型中取得了最优异的性能。</p>
<ul>
<li><p>ref:<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/694072728">万字长文详解LlaMA
3的前世今生</a></p></li>
<li><p>ref:<a target="_blank" rel="noopener" href="http://arthurchiao.art/blog/llama-paper-zh/">LLaMA：开放和高效的基础语言模型集</a></p></li>
<li><p>谷歌：Gemini系列。</p></li>
<li><p>微软：Bing。</p></li>
</ul>
<h2 id="智能体和多模态模型的区别">10. 智能体和多模态模型的区别</h2>
<h3 id="智能体agent"><strong>智能体（Agent）</strong></h3>
<p>智能体（Agent）是一种能够自主感知环境、做出决策并执行行动的系统，广泛应用于不同领域，如机器人、自动化系统、游戏角色、虚拟助理等。</p>
<h4 id="主要特点">主要特点：</h4>
<ul>
<li><p><strong>自主性</strong>：智能体能够根据感知到的环境信息，独立进行决策和行动，而不需要外部持续的控制。</p></li>
<li><p><strong>感知-决策-行动循环</strong>：智能体能够感知外部环境（通过传感器或输入），根据某种规则或策略进行决策，并在环境中执行相应的行为。这是智能体的核心特性。</p></li>
<li><p><strong>持续性</strong>：智能体通常在持续的时间框架中工作，不断与环境互动。</p></li>
<li><p><strong>适应性与学习</strong>：有些智能体可以通过学习（如强化学习）在复杂的环境中不断优化其行为。</p></li>
</ul>
<h4 id="举例">举例：</h4>
<ul>
<li><p>机器人智能体通过传感器感知周围环境，规划路径并自主导航。</p></li>
<li><p>自动驾驶汽车智能体根据道路情况实时调整驾驶策略。</p></li>
<li><p>游戏中的 AI 角色根据玩家行为做出回应并采取行动。</p></li>
</ul>
<h3 id="多模态-gpt"><strong>多模态 GPT</strong></h3>
<p>多模态 GPT 是基于 <strong>Transformer</strong>
架构的预训练语言模型（GPT），它能够处理和生成多种模态的数据，如文本、图像、音频等。传统
GPT 模型专注于自然语言处理，而多模态 GPT
可以跨越多种模态，将它们结合在一起进行任务处理，如从文本生成图像、理解图文组合等。</p>
<h4 id="主要特点-1">主要特点：</h4>
<ul>
<li><p><strong>多模态输入与输出</strong>：多模态 GPT
可以处理多种类型的数据。例如，它可以接收图像和文本作为输入，然后生成文本描述，或根据文本输入生成相关图像。</p></li>
<li><p><strong>基于 Transformer 架构</strong>：多模态 GPT 继承了 GPT 的
Transformer
架构，通过大规模的预训练进行自监督学习，从而具备强大的生成和理解能力。</p></li>
<li><p><strong>生成能力</strong>：多模态 GPT
强调生成能力，尤其在需要跨模态任务时表现出色，如生成图像、音频或视频，或通过对话生成文本内容。</p></li>
<li><p><strong>推理与回答</strong>：它可以通过整合不同模态的数据进行复杂的推理和回答，适用于许多生成和理解任务，如图文理解、文本生成等。</p></li>
</ul>
<h4 id="举例-1">举例：</h4>
<ul>
<li><p><strong>DALL·E</strong>：OpenAI 的 DALL·E 是多模态 GPT
的一个典型例子，能够根据文字描述生成高质量的图像。</p></li>
<li><p><strong>CLIP</strong>：CLIP
是一个多模态模型，可以理解图像和文本之间的关系，通过文本找到相关图像，或通过图像生成对应的文本描述。</p></li>
</ul>
<h3 id="智能体-vs-多模态-gpt区别与联系"><strong>智能体 vs 多模态
GPT：区别与联系</strong></h3>
<h4 id="区别"><strong>区别</strong></h4>
<ul>
<li><strong>核心功能</strong>：
<ul>
<li><p><strong>智能体</strong>：侧重于感知环境、决策与行动的闭环循环。智能体可以是物理的（如机器人），也可以是虚拟的（如自动化软件），并且通常需要与动态的环境进行交互。</p></li>
<li><p><strong>多模态
GPT</strong>：主要用于处理和生成多种模态的数据（如图像、文本），侧重于模态间的数据理解和生成。它并不具备自主的决策和行动能力。</p></li>
</ul></li>
<li><strong>任务性质</strong>：
<ul>
<li><p><strong>智能体</strong>：通常任务是交互性的，智能体在动态环境中持续工作，例如自动驾驶、游戏角色
AI、机器人执行任务等。智能体不仅需要感知，还需要执行行动。</p></li>
<li><p><strong>多模态
GPT</strong>：主要任务是生成式或理解式的。例如，生成图像、生成文本回答问题、或理解图文关系。它在一个静态输入的任务上更为强大，但并不在环境中主动采取行动。</p></li>
</ul></li>
<li><strong>学习机制</strong>：
<ul>
<li><p><strong>智能体</strong>：可能采用强化学习、进化算法等方法来在与环境的互动中学习最优策略。</p></li>
<li><p><strong>多模态
GPT</strong>：使用大规模预训练进行自监督学习，主要依赖于大量的跨模态数据进行学习。</p></li>
</ul></li>
</ul>
<h4 id="联系"><strong>联系</strong></h4>
<ul>
<li><p><strong>感知能力</strong>：虽然智能体和多模态 GPT
的主要目标不同，但两者都涉及感知能力。智能体可以使用多模态感知（如视觉、听觉），而多模态
GPT 直接处理多种模态的数据输入。未来的智能体可能会集成多模态 GPT
模型，使其在处理复杂多模态数据（如图像、文本）时更加智能。</p></li>
<li><p><strong>跨模态理解</strong>：多模态 GPT
可以为智能体提供更强大的理解和生成能力。例如，一个多模态 GPT
模型可以嵌入到智能体中，使其能够通过文本描述生成视觉信息（如在机器人视觉系统中辅助感知）或根据视觉信息生成文本描述（如在自动驾驶中生成自然语言报告）。</p></li>
<li><p><strong>语言生成</strong>：某些智能体，例如聊天机器人，可以使用多模态
GPT 的生成能力来与用户进行自然语言交互，提供图像或文本回答。</p></li>
</ul>
<h3 id="总结-1">总结</h3>
<ul>
<li><p><strong>智能体</strong>
是一个自主的实体，能够感知环境、决策和执行行动，强调的是行动循环和与环境的持续交互。</p></li>
<li><p><strong>多模态 GPT</strong>
是一个生成和理解多模态数据的语言模型，强调的是跨模态数据的处理和生成能力。</p></li>
</ul>
<p>设计一个 AI
智能体（Agent）是一个系统化的过程，涉及多个阶段，包括任务定义、感知环境、决策机制、行动执行和学习改进等。以下是详细的步骤来帮助设计一个
AI 智能体：</p>
<ul>
<li><p><strong>明确任务与目标</strong>：清楚智能体的作用和目标。</p></li>
<li><p><strong>感知模块</strong>：设计感知环境的方式，获取数据。</p></li>
<li><p><strong>决策模块</strong>：设计如何根据感知的数据做出决策，可以基于规则、规划、机器学习或强化学习。</p></li>
<li><p><strong>行为执行模块</strong>：设计如何执行智能体的决策。</p></li>
<li><p><strong>学习与优化</strong>：引入学习机制，让智能体能够根据经验或新数据不断改进。</p></li>
<li><p><strong>反馈与评估</strong>：持续评估智能体的表现，优化其任务执行效果。</p></li>
</ul>
<h1 id="reference">Reference</h1>
<p><a target="_blank" rel="noopener" href="https://aman.ai/primers/ai/">Distilled AI</a></p>
<p><a target="_blank" rel="noopener" href="https://arthurchiao.art/blog/ai-agent-white-paper-zh/#323-tree-of-thoughts-tot">[译]
AI Agent 白皮书</a></p>
 
      <!-- reward -->
      
    </div>
    

    <!-- copyright -->
    
    <div class="declare">
      <ul class="post-copyright">
        <li>
          <i class="ri-copyright-line"></i>
          <strong>Copyright： </strong>
          
          Copyright is owned by the author. For commercial reprints, please contact the author for authorization. For non-commercial reprints, please indicate the source.
          
        </li>
      </ul>
    </div>
    
    <footer class="article-footer">
       
<div class="share-btn">
      <span class="share-sns share-outer">
        <i class="ri-share-forward-line"></i>
        分享
      </span>
      <div class="share-wrap">
        <i class="arrow"></i>
        <div class="share-icons">
          
          <a class="weibo share-sns" href="javascript:;" data-type="weibo">
            <i class="ri-weibo-fill"></i>
          </a>
          <a class="weixin share-sns wxFab" href="javascript:;" data-type="weixin">
            <i class="ri-wechat-fill"></i>
          </a>
          <a class="qq share-sns" href="javascript:;" data-type="qq">
            <i class="ri-qq-fill"></i>
          </a>
          <a class="douban share-sns" href="javascript:;" data-type="douban">
            <i class="ri-douban-line"></i>
          </a>
          <!-- <a class="qzone share-sns" href="javascript:;" data-type="qzone">
            <i class="icon icon-qzone"></i>
          </a> -->
          
          <a class="facebook share-sns" href="javascript:;" data-type="facebook">
            <i class="ri-facebook-circle-fill"></i>
          </a>
          <a class="twitter share-sns" href="javascript:;" data-type="twitter">
            <i class="ri-twitter-fill"></i>
          </a>
          <a class="google share-sns" href="javascript:;" data-type="google">
            <i class="ri-google-fill"></i>
          </a>
        </div>
      </div>
</div>

<div class="wx-share-modal">
    <a class="modal-close" href="javascript:;"><i class="ri-close-circle-line"></i></a>
    <p>扫一扫，分享到微信</p>
    <div class="wx-qrcode">
      <img src="//api.qrserver.com/v1/create-qr-code/?size=150x150&data=https://xueyu-ubc.github.io/2024/10/28/General-Questions-about-LLM/" alt="微信分享二维码">
    </div>
</div>

<div id="share-mask"></div>  
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/LLM/" rel="tag">LLM</a></li></ul>

    </footer>
  </div>

   
  <nav class="article-nav">
    
      <a href="/2024/11/08/agents/" class="article-nav-link">
        <strong class="article-nav-caption">上一篇</strong>
        <div class="article-nav-title">
          
            AI Agents
          
        </div>
      </a>
    
    
      <a href="/2024/10/08/statistic/" class="article-nav-link">
        <strong class="article-nav-caption">下一篇</strong>
        <div class="article-nav-title">Machine Learning</div>
      </a>
    
  </nav>

   
<!-- valine评论 -->
<div id="vcomments-box">
  <div id="vcomments"></div>
</div>
<script src="//cdn1.lncld.net/static/js/3.0.4/av-min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/valine@1.4.14/dist/Valine.min.js"></script>
<script>
  new Valine({
    el: "#vcomments",
    app_id: "",
    app_key: "",
    path: window.location.pathname,
    avatar: "monsterid",
    placeholder: "给我的文章加点评论吧~",
    recordIP: true,
  });
  const infoEle = document.querySelector("#vcomments .info");
  if (infoEle && infoEle.childNodes && infoEle.childNodes.length > 0) {
    infoEle.childNodes.forEach(function (item) {
      item.parentNode.removeChild(item);
    });
  }
</script>
<style>
  #vcomments-box {
    padding: 5px 30px;
  }

  @media screen and (max-width: 800px) {
    #vcomments-box {
      padding: 5px 0px;
    }
  }

  #vcomments-box #vcomments {
    background-color: #fff;
  }

  .v .vlist .vcard .vh {
    padding-right: 20px;
  }

  .v .vlist .vcard {
    padding-left: 10px;
  }
</style>

 
   
     
</article>

</section>
      <footer class="footer">
  <div class="outer">
    <ul>
      <li>
        Copyrights &copy;
        2021-2025
        <i class="ri-heart-fill heart_icon"></i> Xue Yu
      </li>
    </ul>
    <ul>
      <li>
        
        
        
        Powered by <a href="https://hexo.io" target="_blank">Hexo</a>
        <span class="division">|</span>
        Theme - <a href="https://github.com/Shen-Yu/hexo-theme-ayer" target="_blank">Ayer</a>
        
      </li>
    </ul>
    <ul>
      <li>
        
        
        <span>
  <span><i class="ri-user-3-fill"></i>Visitors:<span id="busuanzi_value_site_uv"></span></s>
  <span class="division">|</span>
  <span><i class="ri-eye-fill"></i>Views:<span id="busuanzi_value_page_pv"></span></span>
</span>
        
      </li>
    </ul>
    <ul>
      
    </ul>
    <ul>
      
    </ul>
    <ul>
      <li>
        <!-- cnzz统计 -->
        
        <script type="text/javascript" src='https://s9.cnzz.com/z_stat.php?id=1278069914&amp;web_id=1278069914'></script>
        
      </li>
    </ul>
  </div>
</footer>
      <div class="float_btns">
        <div class="totop" id="totop">
  <i class="ri-arrow-up-line"></i>
</div>

<div class="todark" id="todark">
  <i class="ri-moon-line"></i>
</div>

      </div>
    </main>
    <aside class="sidebar on">
      <button class="navbar-toggle"></button>
<nav class="navbar">
  
  <div class="logo">
    <a href="/"><img src="/images/ayer-side.svg" alt="Welcome to XueYu&#39;s Blog"></a>
  </div>
  
  <ul class="nav nav-main">
    
    <li class="nav-item">
      <a class="nav-item-link" href="/">主页</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/archives">归档</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/categories">分类</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/tags">标签</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/tags/%E6%97%85%E8%A1%8C/">旅行</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/">摄影</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/friends">友链</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/2021/about">关于我</a>
    </li>
    
  </ul>
</nav>
<nav class="navbar navbar-bottom">
  <ul class="nav">
    <li class="nav-item">
      
      <a class="nav-item-link nav-item-search"  title="Search">
        <i class="ri-search-line"></i>
      </a>
      
      
      <a class="nav-item-link" target="_blank" href="/atom.xml" title="RSS Feed">
        <i class="ri-rss-line"></i>
      </a>
      
    </li>
  </ul>
</nav>
<div class="search-form-wrap">
  <div class="local-search local-search-plugin">
  <input type="search" id="local-search-input" class="local-search-input" placeholder="Search...">
  <div id="local-search-result" class="local-search-result"></div>
</div>
</div>
    </aside>
    <script>
      if (window.matchMedia("(max-width: 768px)").matches) {
        document.querySelector('.content').classList.remove('on');
        document.querySelector('.sidebar').classList.remove('on');
      }
    </script>
    <div id="mask"></div>

<!-- #reward -->
<div id="reward">
  <span class="close"><i class="ri-close-line"></i></span>
  <p class="reward-p"><i class="ri-cup-line"></i></p>
  <div class="reward-box">
    
    
  </div>
</div>
    
<script src="/js/jquery-2.0.3.min.js"></script>


<script src="/js/lazyload.min.js"></script>

<!-- Tocbot -->

<script src="https://cdn.jsdelivr.net/npm/jquery-modal@0.9.2/jquery.modal.min.js"></script>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/jquery-modal@0.9.2/jquery.modal.min.css">
<script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/js/jquery.justifiedGallery.min.js"></script>

<script src="/dist/main.js"></script>

<!-- ImageViewer -->

<!-- Root element of PhotoSwipe. Must have class pswp. -->
<div class="pswp" tabindex="-1" role="dialog" aria-hidden="true">

    <!-- Background of PhotoSwipe. 
         It's a separate element as animating opacity is faster than rgba(). -->
    <div class="pswp__bg"></div>

    <!-- Slides wrapper with overflow:hidden. -->
    <div class="pswp__scroll-wrap">

        <!-- Container that holds slides. 
            PhotoSwipe keeps only 3 of them in the DOM to save memory.
            Don't modify these 3 pswp__item elements, data is added later on. -->
        <div class="pswp__container">
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
        </div>

        <!-- Default (PhotoSwipeUI_Default) interface on top of sliding area. Can be changed. -->
        <div class="pswp__ui pswp__ui--hidden">

            <div class="pswp__top-bar">

                <!--  Controls are self-explanatory. Order can be changed. -->

                <div class="pswp__counter"></div>

                <button class="pswp__button pswp__button--close" title="Close (Esc)"></button>

                <button class="pswp__button pswp__button--share" style="display:none" title="Share"></button>

                <button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>

                <button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button>

                <!-- Preloader demo http://codepen.io/dimsemenov/pen/yyBWoR -->
                <!-- element will get class pswp__preloader--active when preloader is running -->
                <div class="pswp__preloader">
                    <div class="pswp__preloader__icn">
                        <div class="pswp__preloader__cut">
                            <div class="pswp__preloader__donut"></div>
                        </div>
                    </div>
                </div>
            </div>

            <div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap">
                <div class="pswp__share-tooltip"></div>
            </div>

            <button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
            </button>

            <button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)">
            </button>

            <div class="pswp__caption">
                <div class="pswp__caption__center"></div>
            </div>

        </div>

    </div>

</div>

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.css">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/default-skin/default-skin.min.css">
<script src="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe-ui-default.min.js"></script>

<script>
    function viewer_init() {
        let pswpElement = document.querySelectorAll('.pswp')[0];
        let $imgArr = document.querySelectorAll(('.article-entry img:not(.reward-img)'))

        $imgArr.forEach(($em, i) => {
            $em.onclick = () => {
                // slider展开状态
                // todo: 这样不好，后面改成状态
                if (document.querySelector('.left-col.show')) return
                let items = []
                $imgArr.forEach(($em2, i2) => {
                    let img = $em2.getAttribute('data-idx', i2)
                    let src = $em2.getAttribute('data-target') || $em2.getAttribute('src')
                    let title = $em2.getAttribute('alt')
                    // 获得原图尺寸
                    const image = new Image()
                    image.src = src
                    items.push({
                        src: src,
                        w: image.width || $em2.width,
                        h: image.height || $em2.height,
                        title: title
                    })
                })
                var gallery = new PhotoSwipe(pswpElement, PhotoSwipeUI_Default, items, {
                    index: parseInt(i)
                });
                gallery.init()
            }
        })
    }
    viewer_init()
</script>

<!-- MathJax -->

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
      tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      }
  });

  MathJax.Hub.Queue(function() {
      var all = MathJax.Hub.getAllJax(), i;
      for(i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
      }
  });
</script>

<script src="https://cdn.jsdelivr.net/npm/mathjax@2.7.6/unpacked/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
<script>
  var ayerConfig = {
    mathjax: true
  }
</script>

<!-- Katex -->

<!-- busuanzi  -->


<script src="/js/busuanzi-2.3.pure.min.js"></script>


<!-- ClickLove -->

<!-- ClickBoom1 -->

<!-- ClickBoom2 -->

<!-- CodeCopy -->


<link rel="stylesheet" href="/css/clipboard.css">

<script src="https://cdn.jsdelivr.net/npm/clipboard@2/dist/clipboard.min.js"></script>
<script>
  function wait(callback, seconds) {
    var timelag = null;
    timelag = window.setTimeout(callback, seconds);
  }
  !function (e, t, a) {
    var initCopyCode = function(){
      var copyHtml = '';
      copyHtml += '<button class="btn-copy" data-clipboard-snippet="">';
      copyHtml += '<i class="ri-file-copy-2-line"></i><span>COPY</span>';
      copyHtml += '</button>';
      $(".highlight .code pre").before(copyHtml);
      $(".article pre code").before(copyHtml);
      var clipboard = new ClipboardJS('.btn-copy', {
        target: function(trigger) {
          return trigger.nextElementSibling;
        }
      });
      clipboard.on('success', function(e) {
        let $btn = $(e.trigger);
        $btn.addClass('copied');
        let $icon = $($btn.find('i'));
        $icon.removeClass('ri-file-copy-2-line');
        $icon.addClass('ri-checkbox-circle-line');
        let $span = $($btn.find('span'));
        $span[0].innerText = 'COPIED';
        
        wait(function () { // 等待两秒钟后恢复
          $icon.removeClass('ri-checkbox-circle-line');
          $icon.addClass('ri-file-copy-2-line');
          $span[0].innerText = 'COPY';
        }, 2000);
      });
      clipboard.on('error', function(e) {
        e.clearSelection();
        let $btn = $(e.trigger);
        $btn.addClass('copy-failed');
        let $icon = $($btn.find('i'));
        $icon.removeClass('ri-file-copy-2-line');
        $icon.addClass('ri-time-line');
        let $span = $($btn.find('span'));
        $span[0].innerText = 'COPY FAILED';
        
        wait(function () { // 等待两秒钟后恢复
          $icon.removeClass('ri-time-line');
          $icon.addClass('ri-file-copy-2-line');
          $span[0].innerText = 'COPY';
        }, 2000);
      });
    }
    initCopyCode();
  }(window, document);
</script>


<!-- CanvasBackground -->


    
  </div>
</body>

</html>
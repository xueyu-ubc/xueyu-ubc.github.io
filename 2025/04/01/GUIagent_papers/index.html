<!DOCTYPE html>


<html lang="en">


<head>
  <meta charset="utf-8" />
    
  <meta name="description" content="I am a second PhD student at Renmin University of China. My research interests include federated learning, high dimensional data, machine learning, and optimization. I am currently working on latent graph learning in Prof.Renjie Liao&#39;s group." />
  
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1" />
  <title>
    (GUI) Agent Paper |  Welcome to XueYu&#39;s Blog
  </title>
  <meta name="generator" content="hexo-theme-ayer">
  
  <link rel="shortcut icon" href="/favicon.ico" />
  
  
<link rel="stylesheet" href="/dist/main.css">

  
<link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/Shen-Yu/cdn/css/remixicon.min.css">

  
<link rel="stylesheet" href="/css/custom.css">

  
  
<script src="https://cdn.jsdelivr.net/npm/pace-js@1.0.2/pace.min.js"></script>

  
  

  

<link rel="alternate" href="/atom.xml" title="Welcome to XueYu's Blog" type="application/atom+xml">

<style>.github-emoji { position: relative; display: inline-block; width: 1.2em; min-height: 1.2em; overflow: hidden; vertical-align: top; color: transparent; }  .github-emoji > span { position: relative; z-index: 10; }  .github-emoji img, .github-emoji .fancybox { margin: 0 !important; padding: 0 !important; border: none !important; outline: none !important; text-decoration: none !important; user-select: none !important; cursor: auto !important; }  .github-emoji img { height: 1.2em !important; width: 1.2em !important; position: absolute !important; left: 50% !important; top: 50% !important; transform: translate(-50%, -50%) !important; user-select: none !important; cursor: auto !important; } .github-emoji-fallback { color: inherit; } .github-emoji-fallback img { opacity: 0 !important; }</style>
<link href="https://cdn.bootcss.com/KaTeX/0.11.1/katex.min.css" rel="stylesheet" /></head>

</html>
<script src="/js/hexo_resize_image.js"></script>
<body>
  <div id="app">
    
      
    <main class="content on">
      <section class="outer">
  <article
  id="post-GUIagent_papers"
  class="article article-type-post"
  itemscope
  itemprop="blogPost"
  data-scroll-reveal
>
  <div class="article-inner">
    
    <header class="article-header">
       
<h1 class="article-title sea-center" style="border-left:0" itemprop="name">
  (GUI) Agent Paper
</h1>
 

    </header>
     
    <div class="article-meta">
      <a href="/2025/04/01/GUIagent_papers/" class="article-date">
  <time datetime="2025-04-01T00:00:00.000Z" itemprop="datePublished">2025-04-01</time>
</a> 
  <div class="article-category">
    <a class="article-category-link" href="/categories/research-works/">research works</a>
  </div>
  
<div class="word_count">
    <span class="post-time">
        <span class="post-meta-item-icon">
            <i class="ri-quill-pen-line"></i>
            <span class="post-meta-item-text"> Word count:</span>
            <span class="post-count">13.7k</span>
        </span>
    </span>

    <span class="post-time">
        &nbsp; | &nbsp;
        <span class="post-meta-item-icon">
            <i class="ri-book-open-line"></i>
            <span class="post-meta-item-text"> Reading time≈</span>
            <span class="post-count">51 min</span>
        </span>
    </span>
</div>
 
    </div>
      
    <div class="tocbot"></div>




  
    <div class="article-entry" itemprop="articleBody">
       
  <h1 id="seeclick-harnessing-gui-grounding-for-advanced-visual-gui-agents"><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2401.10935">（202402）SeeClick: Harnessing
GUI Grounding for Advanced Visual GUI Agents</a></h1>
<p>本文主要介绍了一种新型的基于视觉 (screenshot) 的图形用户界面
(Graphical User Interface, GUI) -- SeeClick。传统的 GUI
智能体通常依赖于结构化文本 (structured text)
与环境进行交互。但是这种方式有三种主要的局限，具体来说，</p>
<ul>
<li><p>在一些场景下，比如 ios
或桌面应用中，结构化文本可能无法直接获取。</p></li>
<li><p>结构化文本冗长，同时缺乏布局、图片、图标等视觉信息。</p></li>
<li><p>结构化文本格式繁多，比如 html, dom
等，导致需要根据不同任务设计不同的行为。</p></li>
</ul>
<p>针对上述问题，作者提出了一种基于大型视觉语言模型 (Large
Vision-Language Models, LVLMs) 的 视觉 GUI
智能体。它可以根据指令定位截图上的元素，进而执行任务，无需结构化文本。具体对比如
Figure 1 所示。 <img src="/2025/04/01/GUIagent_papers/seeclick1.png"></p>
<p>尽管目前 LVLMs 具有在自然图像中定位元素的能力，但是 GUI
的截图和自然图像存在显著差异，比如 GUI
图像包含大量密集文本，图标、控件较多。这些特点使得视觉 GUI
智能体面临的一个核心挑战是 GUI
grounding，即如何根据指令准确定位屏幕中元素。</p>
<h2 id="gui-grounding-for-lvlms">GUI grounding for LVLMs</h2>
<p>作者首先通过预训练 LVLM 生成关于元素位置的文本描述。给定界面截图
<span class="math inline">\(s\)</span> 和一组数据 <span class="math inline">\({(x_i, y_i)} (i =
0,...)\)</span>，目标是预测元素位置 <span class="math inline">\(y\)</span>，即计算 <span class="math inline">\(p(y
| s, x)\)</span>。传统方法是将图片进行划分为若干个 bins，然后通过
tokenization 来表示元素 <span class="math inline">\(x\)</span> 以及位置
<span class="math inline">\(y\)</span>。在这个工作中，作者直接将坐标视作语言生成文本，通过构造
prompt，例如：“In the UI, where should I click if I want to view the new
album of Jony J?”，ground truth 为：“click (0.49,
0.40)”。利用交叉熵损失优化 LVLM，对每个 token 的预测概率求 log
并加总，优化模型输出序列与真实坐标文本之间的一致性。</p>
<h2 id="data-construction">Data Construction</h2>
<p>作者构建了三个数据集，分别是：web UI data、mobile UI data
以及通用的视觉-语言指令数据。</p>
<h3 id="web-data">Web Data</h3>
<p>作者从开源的网页抓取数据库 Common Crawl 中提取了约 30
万个网页。对于每个网页 <span class="math inline">\(s\)</span>，作者从其
HTML 中收集两类元素，分别是（1）可见文本元素，（2）带有 title
属性的元素，即该属性在鼠标悬停时会显示描述性文字。具体示例可见 Figure
3。通过这种方式，作者构建了大量的训练数据：（文本描述 x, 目标位置
y)。除了基本的 grounding 任务 <span class="math inline">\(p(y∣s,x)\)</span>（给文本找位置），作者还引入了
Web OCR 任务 <span class="math inline">\(p(x∣s,y)\)</span>
（根据坐标位置预测对应的文本内容）。这样可以让模型从两种方向理解网页
UI，提升对界面内容的理解能力。</p>
<p><img src="/2025/04/01/GUIagent_papers/seeclick3.png"></p>
<p><strong><em>忽视全局信息</em></strong></p>
<h3 id="mobile-data">Mobile Data</h3>
<p>对于移动端 UI，作者考虑了三类数据，分别是 - 控件描述：Widget
Captioning。例如音乐播放器中播放按钮对应的描述是 “play
music”。作者使用了 Li 等人（2020b）提供的数据集训练集部分，包含约 2
万张截图、4 万个控件和 10 万条描述。用于 <span class="math inline">\(p(x∣s,y)\)</span> 任务。</p>
<ul>
<li><p>UI 定位：UI grounding。作者通过“反转”控件描述的过程构建 UI
grounding
数据。把描述语句视为“指令”，把对应控件当作“目标位置”，这样就能形成类似于
<span class="math inline">\(p(y | s, x)\)</span>
的任务样本。为了提高多样性，作者还使用了移动 UI 大型公开数据集 RICO
数据集中自动提取的控件与说明。</p></li>
<li><p>UI 总结：UI summarization。作者加入了 Wang 等人（2021）提出的移动
UI 总结数据，用于增强模型对整个界面的理解能力。</p></li>
</ul>
<h3 id="general-data">General Data</h3>
<p>为了维持大型视觉语言模型（LVLM）在自然图像上的通用理解能力，作者还使用了来自
LLaVA（Liu et al.,
2023a）的通用图文指令跟随数据，其中包括对话、详细描述以及复杂推理等任务。</p>
<h2 id="training-details">Training Details</h2>
<p>作者使用 Qwen-VL 模型，Qwen-VL 具有 grounding
功能和更高的分辨率（<span class="math inline">\(448 \times
448\)</span>）。在训练过程中，作者使用 LoRA 来微调视觉编码器和 LLM。</p>
<h1 id="android-control-on-the-effects-of-data-scale-on-ui-control-agents"><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2406.03679">(202411) (android control) On
the Effects of Data Scale on UI Control Agents</a></h1>
<p>近年来，能够自主控制用户界面以完成任务的智能体逐渐兴起，特别是利用大型语言模型（LLMs）驱动此类智能体的研究引发广泛关注。然而，如果不在人工收集的任务演示数据上进行微调，这些模型在真实界面控制中的表现仍然较弱。为此，研究者提出并发布了一个新数据集——ANDROID
CONTROL，包含 15,283 条 Android
应用中的日常任务演示。与现有数据集相比，ANDROID CONTROL
的每个任务实例都包含 high level 和 low level
的人类指令，支持对任务复杂度的系统研究。该数据集在多样性方面具有显著优势，涵盖
14,548 个独特任务、涉及 833 个 Android
应用，使得模型在域内和跨域的泛化能力得以深入评估。实验发现，在 in domain
场景下，经过微调的模型显著优于零样本和少样本方法，其性能随着数据规模的增加而持续提升，表明通过收集更多数据有望获得稳健表现。然而，在
out of domain 场景下，性能提升幅度明显放缓，尤其在处理 high level
任务时，单纯依赖微调和数据扩展可能不足以实现强泛化能力。</p>
<h1 id="autoglm-autonomous-foundation-agents-for-guis"><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2411.00820">（202410）AutoGLM: Autonomous
Foundation Agents for GUIs</a></h1>
<p>作者指出构建 GUI
基础智能体面临的核心挑战：<strong>现有预训练数据中缺乏决策类信息</strong>。尽管互联网上有大量人类知识，但这些信息大多是静态的，难以反映人类在动态环境中的决策行为和交互模式。为了解决这一问题，必须通过两种方式增强基础智能体的动态知识：1.
真实环境交互。 2.
学习合成的决策轨迹。此外，研究还强调了<strong>渐进式用户部署</strong>的重要性。基础智能体的目标不是替代人类，而是<strong>增强人类能力</strong>。通过与用户的真实互动，智能体能学习如何更好地协助人类，用户也能逐步适应智能体的帮助。</p>
<p>为应对基础智能体在图形用户界面场景中的挑战与机遇，研究团队提出了基于
ChatGLM 模型系列的 AUTOGLM
系列智能体，专注于网页浏览与安卓设备控制两个核心应用场景。针对决策类数据稀缺的问题，AUTOGLM
采用多种训练技术与用户部署基础设施，并提出两项关键创新：一是设计中间接口，将规划与执行行为解耦，前者强调灵活性与错误恢复，后者注重动作准确性，从而提升系统开发效率与性能；二是引入自我演化的在线课程强化学习框架，通过“从弱到强”的渐进式策略，使智能体在实际环境中持续学习与优化，弥补传统离线训练在错误恢复与数据缺乏方面的不足，推动智能体能力不断增强。</p>
<h2 id="insight-1-intermediate-interface-design">Insight 1: Intermediate
Interface Design</h2>
<p>研作者设计一个“中间接口”对于将“规划（Planning）”与“执行/落地（Grounding）”行为解耦。通过将这两个过程拆分为独立的模块，可以分别优化它们的性能——提升规划的灵活性，同时确保执行的准确性，彼此不会互相干扰。如下图所示，在解耦后的结构中：Planner
负责理解任务，生成自然语言描述（例如“点击右下角的提交按钮”）；Grounder
根据描述在图像中找到目标元素坐标（即 GUI
上的具体位置）。这样做的好处：规划与执行可以分别训练优化；错误定位更明确，可以知道是“指令理解错”还是“坐标找错”；更容易构造训练数据：通过环境中的自动观测，可以自动生成大量
Grounding 数据，提升 Grounder 的表现。</p>
<p><img src="/2025/04/01/GUIagent_papers/autoglm.png"></p>
<h2 id="insight-2-self-evolving-online-curriculum-rl">Insight 2:
Self-Evolving Online Curriculum RL</h2>
<p>尽管通过中间接口可以缓解执行（grounding）过程中的准确性问题，但在规划（planning）阶段，问题仍然突出。特别是目前很多智能体系统依赖于闭源的
LLM/LMM API，这些模型无法通过训练进一步优化其规划能力。因此，AUTOGLM
决定自行训练可持续优化的
Planner，采用强化学习（RL）方法构建智能体。研究团队开发了名为
<strong>WebRL</strong>
的在线课程强化学习框架，用于在真实任务环境中训练智能体。以 WebArena
环境为例，他们采用了经典的 <strong>Actor-Critic
架构</strong>，并重点解决了两个关键挑战： ### 任务数据稀缺（Task Data
Scarcity） - 开始时仅依赖约 1000 条来自 VisualAgentBench
的行为克隆（BC）数据，GLM-4-9B 模型起始成功率为 22.4%。</p>
<ul>
<li>数据不足后，采用<strong>自我演化技术</strong>来生成新任务：自动修改失败的任务指令，使其更复杂或更简单；使用
Critic 模块筛选有效的自生成任务，再用于下一轮训练；</li>
</ul>
<p>这种方式实现了<strong>边训练边扩充数据集</strong>，解决了缺少专家示范的困境。</p>
<p>VAB-WebArena-Lite
是一个用于评估人工智能（AI）智能体在网页浏览任务中表现的基准测试环境。它是原始
WebArena 的精简版本，包含 165
个经过人工验证的任务，旨在加速评估过程并确保评判的准确性。</p>
<h3 id="策略分布漂移policy-distribution-drift">策略分布漂移（Policy
Distribution Drift）</h3>
<p>课程学习过程中，策略会逐步强化，但也可能导致模型偏离早期数据分布，影响泛化能力。为此，研究引入：
- KL 约束的策略更新（限制策略变化范围）； -
基于信心度的经验重放机制（只用可信数据做回放）；</p>
<p>消融实验表明：这些机制是保证训练效果持续提升的关键。</p>
<h1 id="iclr-2025os-atlas-a-foundation-action-model-for-generalist-gui-agents"><a target="_blank" rel="noopener" href="https://osatlas.github.io/">（ICLR 2025）OS-ATLAS: A Foundation
Action Model for Generalist GUI Agents</a></h1>
<p>当前构建 GUI
智能体的工作在很大程度上依赖于商业视觉语言模型（VLM），如 GPT-4o 和
GeminiProVision。然而，由于开源 VLM 在 GUI grounding 和 OOD
任务上的性能明显落后，实践者往往不愿使用它们。为推动该领域的研究发展，作者提出了
OS-Atlas —— 一个专注于 GUI grounding 和 OOD
智能行为任务的基础模型，结合了数据构建和模型设计上的创新。该团队开发出一个支持
Windows、Linux、macOS、Android 和 Web 等多个平台的开源 GUI grounding
数据合成工具包，并据此发布了开源跨平台 GUI grounding 数据集，涵盖超过
1300 万个 GUI 元素。结合先进的模型训练方法，OS-Atlas 能够有效理解 GUI
截图，并具备良好的泛化能力，适应此前未见的界面。通过在移动端、桌面端和网页端的六项基准测试中广泛评估，OS-Atlas
在多个任务上显著超越现有最先进模型。</p>
<p><img src="/2025/04/01/GUIagent_papers/osatlas.png"></p>
<p><img src="/2025/04/01/GUIagent_papers/osatlas2.png"></p>
<p>研究的训练流程分为两个阶段：</p>
<ul>
<li><p>（1）GUI grounding 预训练，旨在使视觉语言模型具备理解 GUI
截图并识别屏幕元素的能力；</p></li>
<li><p>（2）动作微调阶段，将自然语言指令转化为可执行的 GUI
操作。</p></li>
</ul>
<p>第一阶段的 GUI grounding 预训练依赖于大量高质量、跨平台的三元组数据
&lt;截图、元素指代表达或指令、元素坐标&gt;，坐标可表示为点或 bounding
boxes。模型需根据截图与指令预测对应元素的位置。为支持大规模预训练，研究者构建了迄今为止最大的多平台
GUI 参考语料库，并使用 VLM 合成了一批指令 grounding
数据，涵盖五大平台，包含超过 230 万个截图和 1300 多万个 GUI
元素。该阶段训练后的模型被称为 OS-Atlas-Base。
第二阶段为动作微调，为实现模型在操作系统任务中的执行能力，研究者整合现有多任务智能体模仿学习数据集，训练模型根据
&lt;截图，任务指令，<strong>动作历史</strong>&gt;
三元组预测下一步操作。每个动作进一步表示为
&lt;思考，动作类型，动作参数（如坐标）&gt;
的三元组。然而，<strong>初步实验发现多个数据集混合训练可能引发动作冲突，影响性能。为此，研究者提出在训练中引入统一动作空间以缓解此问题。</strong></p>
<p>在多任务微调中，研究者发现不同数据源的动作定义存在冲突，盲目混合使用会严重影响模型性能。例如，桌面环境中的“click”操作在逻辑上等同于移动设备中的“tap”，但若不加区分地训练，会导致模型混淆。为解决此问题，研究团队提出了统一动作空间（Unified
Action Space），用于规范所有数据集的动作格式。
统一动作空间包括两类：基础动作和自定义动作。基础动作在所有平台上通用，当前设计包含
click、type 和 scroll
三种，确保了训练过程中的一致性并有助于跨平台共享知识。自定义动作则用于支持各平台或设备上特有的操作，如
open app（打开指定应用）和
drag（拖动物体至另一位置）。自定义动作的设计对于 OS-Atlas
在分布外任务中的表现尤为关键，因为它们支持用户按需扩展新任务与操作能力，从而提升模型的泛化能力。</p>
<p><img src="/2025/04/01/GUIagent_papers/osatlas3.png"></p>
<h1 id="iclr-2025navigating-the-digital-world-as-humans-do-universal-visual-grounding-for-gui-agents"><a target="_blank" rel="noopener" href="https://github.com/OSU-NLP-Group/UGround">（ICLR 2025）Navigating
the Digital World as Humans Do: UNIVERSAL VISUAL GROUNDING FOR GUI
AGENTS</a></h1>
<p>大多数 GUI 智能体依赖文本形式的界面表示方式，如 HTML
或可访问性树，但这些方式往往带来噪声、不完整性以及计算开销的增加。本文提出一种更类人化的方案：GUI
智能体应完全依靠视觉感知环境，并直接在像素层面上与界面交互。实现这一目标的关键是建立能够准确将不同形式的表达映射到
GUI 元素具体位置的 visual grounding models。作者首先构建了包含了约 1000
万个 GUI 元素及其表达方式的 GUI 视觉 grounding
数据集，并据此训练出通用视觉 grounding 模型 —— UGround。</p>
<p>作者认为，尽管智能体研究仍处于早期阶段，单一的“巨型模型”都难以完全涵盖各种环境中复杂多变的语义和特性。因此，构建一个能够在不同场景中稳健泛化的通用智能体，需要采用<strong>模块化</strong>系统设计。这意味着要将基础模型（如
GPT-4o）与多个专门模块有机结合，每个模块针对特定功能进行优化。其中，grounding
尤其适合由独立模块来处理。通过单独构建 grounding
模块，可以更有效地捕捉领域特有的语义特征，同时便于在新领域中适配，仅需微调
grounding 模块而无需重训整个基础模型。这正是 SeeAct-V
模型架构及本文所提出工作的核心设计动机。</p>
<p>原始的 SeeAct 框架分为两个阶段：planning 与
grounding，这两个过程均由一个多模态大语言模型（MLLM）完成。在每一步中，MLLM
首先生成一个文本形式的行动计划，然后从一组候选项中选择 grounding
目标。相比之下，SeeAct-V 则完全依赖截图进行环境感知。在 grounding
阶段，SeeAct-V 引入了一个专门用于视觉 grounding 的独立模型，直接输出
agent 应在当前屏幕上执行操作的具体坐标。</p>
<p><img src="/2025/04/01/GUIagent_papers/seeactv.png"></p>
<p>用户在指代 GUI
元素时，使用了多种不同方式，这种表达的多样性在以往的视觉 grounding
研究中（如 Hong 等人 2024；Cheng 等人 2024）尚未被充分重视。作者将 GUI
元素的常见指代表达（Referring Expressions,
REs）归纳为三类：1）<strong>视觉类指代</strong>，即通过显著的视觉特征进行表达，如文字或图像内容、元素类型（例如按钮、输入框）、形状、颜色等；2）<strong>位置类指代</strong>，包括绝对位置（如“页面左上角”）和相对位置（如“在元素
X 右边”），这类指代有时还包含语境信息（如“属于商品 A 的”，“在 X
部分下方”），这类上下文关联更具挑战性，因为它要求理解元素之间的空间关系与语义关系（例如“点赞按钮”通常与某个商品有关联）；3）<strong>功能类指代</strong>，即通过元素的主要功能来表达（如“跳转到首页”，“前往购物车”）。此外，在用户表达需要更强消歧能力时，还常出现<strong>组合型表达</strong>，将上述两种或三种方式结合使用，例如：“点击
Pokemon T
恤下面的心形按钮添加收藏”，同时融合了视觉、位置和功能线索。</p>
<p><img src="/2025/04/01/GUIagent_papers/seeactv2.png"></p>
<p>基于构建的数据集，作者使用开源模型架构 7B LLaVA-NeXT 作为视觉
grounding 的 backbone 模型。作者使用 CLIP-ViT-L-14 (224px)
作为图像编码器，并在训练过程中保持其冻结状态。作者使用 Vicuna-1.5-7b-16k
作为 language backbone。</p>
<h1 id="iclr-2025指导动态调整-discriminator-guided-embodied-planning-for-llm-agent"><a target="_blank" rel="noopener" href="https://openreview.net/forum?id=TjP1d8PP8l">（ICLR
2025）(指导、动态调整) Discriminator-Guided Embodied Planning for LLM
Agent</a></h1>
<p>目前的方法通常只在整个行为轨迹完成后，才接收到一个总体性的反馈信号（trajectory-level
feedback），例如任务成功或失败的最终结果。这种反馈方式是非主动的（non-proactive），也就是说，模型在执行过程中无法及时获得逐步的指导或修正信号。因此，它难以及时调整策略，从而限制了模型在复杂、动态的具身环境中的效果和泛化能力。</p>
<p>当前几类用于训练或优化大型语言模型在具身任务中表现的方法，以及它们各自的局限性：</p>
<ul>
<li><p><strong>In-context learning 方法 （Reflection-based
methods）</strong>：通过内在独白（inner monologue）或物理反馈（physical
feedback）的方式，在执行失败后引入闭环反馈。这种方式让模型在失败后进行自我反思或从环境中获取反馈信息。</p></li>
<li><p><strong>Tree-of-Thought（ToT）方法（Search-based
methods）</strong>：通过生成多个可能的推理路径（轨迹）来代表不同的思考过程，并在这些路径间进行策略切换（trajectory-level
switching），从而优化最终结果。但这种方法需要大量的探索（即反复尝试多种路径），<strong>代价高</strong>。</p></li>
<li><p><strong>Demonstration-based
方法</strong>：依赖于大量高质量、覆盖广泛场景的示范轨迹来学习策略，才能实现良好的泛化。但这类数据难以获得、成本高。</p></li>
</ul>
<p>这些方法都面临<strong>反馈滞后、高探索成本或数据依赖强的问题</strong>，限制了它们在动态任务中的实用性。</p>
<p><img src="/2025/04/01/GUIagent_papers/DGAP1.png"></p>
<p>作者提出一种融合现有方法优点的新方案，使用判别器（discriminator）实现对
LLM
动作的细粒度评价与优化。判别器基于任务目标与环境信息，结合历史动作和当前
LLM 输出，评估其与专家策略之间的对齐程度： <span class="math display">\[
D_{\phi} : (l, h_t, a^{π_{llm}}(l, s_t)) \to Q
\]</span> 其中，<span class="math inline">\(D_{\phi}\)</span>
是判别器，<span class="math inline">\(l\)</span>：任务目标（task
objective）,$ <span class="math inline">\(s_t\)</span>：时间步<span class="math inline">\(t\)</span>的环境状态，<span class="math inline">\(h_t\)</span>：前十步动作的历史轨迹，<span class="math inline">\(a^{\pi_{llm}}\)</span>：LLM
在当前状态下生成的动作，<span class="math inline">\(Q\)</span>：评分（0
到 10 之间），值越高表示越接近专家策略。作者将高质量的 demonstrations
数据信息转换为数值信息，通过对每一步生成动作进行评分，进而去优化 LLM
planner。</p>
<p>判别器的设计目标是对动作进行数值上的区分（即对每个动作给予一个评分）。嵌入回归（embedding
regression）被认为是一个有效的方法。但是存在一个关键问题：LLM
生成的动作与专家动作在嵌入空间中的表示非常相似，这种高度相似性导致判别器很难区分两者，从而难以生成具有良好泛化能力的数值评分。结合已有方案，作者对专家数据进行了改造，比如引入随机数据、增加语言模型自动生成的数据等增强数据的多样性。</p>
<p><img src="/2025/04/01/GUIagent_papers/DGAP2.png"></p>
<p>在提示（prompt）中会加入先前动作及其判别器评分，引导 LLM
具备“预见性”，即能够根据每一步的评分判断该动作是否有助于任务成功。然后利用判别器评分形成一个闭环过程：当某一步的动作得分低于设定阈值时，LLM
需重新调整其策略（即重采样或修改动作），在评分驱动下引导其优化输出策略。</p>
<h1 id="iclr-2025-任务分解轨迹反思经验积累奖励机制-agent-s-an-open-agentic-framework-that-uses-computers-like-a-human"><a target="_blank" rel="noopener" href="https://github.com/simular-ai/Agent-S">(ICLR 2025)
（任务分解、轨迹反思、经验积累、奖励机制） Agent S: An Open Agentic
Framework that Uses Computers Like a Human</a></h1>
<p>论文介绍了 Agent
S，这是一个开放的代理框架，通过图形用户界面（GUI）实现与计算机的自主交互，旨在通过自动化复杂的多步骤任务来改变人机交互。Agent
S
旨在解决自动化计算机任务的三个关键挑战：获取特定领域的知识，规划长期任务，以及处理动态的、非统一的界面。为此，Agent
S
引入了经验增强的分层规划，它从外部知识搜索和内部经验检索中学习多层次的知识，从而促进了任务规划和子任务执行的高效性。此外，它采用了基于多模式大语言模型（MLLMs）的代理计算机界面（ACI），以更好地引出
GUI 代理的推理和控制能力。在 OSWorld 基准测试中的评估表明，Agent S
在成功率上比基准测试高出 9.37%（相对改进
83.6%），并实现了新的最先进技术。全面的分析突出了各个组件的有效性，并为未来的改进提供了见解。此外，Agent
S 在新发布的 WindowsAgentArena 基准测试中展示了广泛的普适性。</p>
<p><img src="/2025/04/01/GUIagent_papers/agents1.png"></p>
<h2 id="related-work">Related work</h2>
<p><strong>GUI Agents</strong>. MLLM agents have been applied to execute
natural language instructions in both web and OS environments. Early
research concentrated on web navigation tasks, utilizing MLLMs to
interact with web interfaces (Gur et al., 2024; He et al., 2024; Kim et
al., 2023; Shaw et al., 2023; Putta et al., 2024). Recently, the focus
has shifted to OS-level environments, leading to the development of
benchmarks and frameworks such as OSWorld Xie et al. (2024) and
WindowsAgentArena Bonatti et al. (2024) for desktop control, and DiGIRL
(Bai et al., 2024) and AndroidWorld (Rawles et al., 2024) for mobile
environments. These OS-level tasks offer broader control capabilities
beyond the limitations of single-browser contexts in web navigation.
<strong>Methodologically, earlier GUI agents employed behavioral cloning
with reinforcement learning (Humphreys et al., 2022), in-context
trajectory examples (Zheng et al., 2024b), state-dependent offline
experience (Fu et al., 2024b), and reusable skill generation (Wang et
al., 2024). Contemporaneous work on GUI agents for video games and OS
(Wu et al., 2024; Song et al., 2024; Tan et al., 2024) propose varying
instances of cognitive architectures (Sumers et al., 2024). Our work
contributes unique modules such as experience-augmented hierarchical
planning and ACI for GUI control, integrated with a novel continual
memory update framework.</strong></p>
<p><strong>Retrieval-Augmented Generation (RAG) for AI Agents</strong>.
RAG (Fan et al., 2024) improves the reliability of MLLM inference by
augmenting the input with reliable and up-to-date external knowledge.
Similarly, <strong>MLLM agents benefit from retrieving task exemplars
(Kim et al., 2024), state-aware guidelines (Fu et al., 2024a), and past
experiences (Kagaya et al., 2024). Our use of experience for
augmentation differs in three ways: 1) our hierarchical planning
leverages both full task experience and subtask experience; 2) the full
task experience is summarized into an abstractive textual reward for
subtask planning; 3) the subtask experience is assessed and annotated by
a self-evaluator before being stored in memory.</strong></p>
<h2 id="agent-s">Agent S</h2>
<p><img src="/2025/04/01/GUIagent_papers/agents2.png"></p>
<p>Agent S
通过经验增强的分层规划，将复杂任务拆解为可管理的子任务，结合外部经验库与内部经验库，实现
high level 规划与 low level
执行协同推进；同时，它不断将自我评估的经验存储在 narrative 和 episodic
记忆库中，并在后续任务中检索利用，从而随着时间推移不断优化表现并适应开放式桌面环境的变化；此外，借助视觉增强的可访问性树观察机制，代理–计算机接口（ACI）为
agent 提供所有有效 GUI
元素的结构化视图，并将其动作限定在受约束的离散有效动作空间内，确保对图形界面的精准感知与操作。</p>
<h3 id="experience-augmented-hierarchical-planning">EXPERIENCE-AUGMENTED
HIERARCHICAL PLANNING</h3>
<h4 id="manager-fusing-external-knowledge-and-internal-experience-for-planning">MANAGER:
FUSING EXTERNAL KNOWLEDGE AND INTERNAL EXPERIENCE FOR PLANNING</h4>
<p>Manager G 是本系统中的主要计划生成模块。它接收来自用户的任务 <span class="math inline">\(T_u\)</span> 以及由代理-计算机接口（Agent-Computer
Interface, ACI）提供的初始环境观测 <span class="math inline">\(O_0\)</span>（包含带注释的可访问性树和屏幕截图）作为输入。</p>
<p>管理器根据用户指令和环境观测，生成一个具备观测感知能力的查询 <span class="math inline">\(Q\)</span>，格式为“How to do
X”。该查询被用于两类检索操作：</p>
<ul>
<li><p>在线检索：通过搜索引擎进行在线网页搜索，以获取外部通用知识 <span class="math inline">\(K_{web}\)</span>。</p></li>
<li><p>记忆检索：在自身的叙事记忆模块 <span class="math inline">\(M_n\)</span> 中检索与当前任务相似的任务经验摘要
<span class="math inline">\(E_{nu}\)</span>。该过程基于查询的向量嵌入相似性完成。</p></li>
</ul>
<p>Narrative Memory
包含来自以往任务的摘要信息，涵盖成功与失败的任务轨迹（其中去除了具体动作细节，仅保留抽象任务经历），该信息由
Self-Evaluator S 或真实标签进行成功/失败判定。</p>
<p>这两类知识随后通过经验上下文融合子模块（Experience Context
Fusion）进行整合，生成融合知识 <span class="math inline">\(K_{fused}\)</span>，具体流程如下： <span class="math display">\[
Q = \text{LLM}(T_u, O_0) \\
K_{web} = \text{Retrieve}(\text{Web}, Q) \\
E_{nu} = \text{Retrieve}(M_n, Q) \\
K_{fused} = \text{LLM}(E_{nu}, K_{web})
\]</span></p>
<p>最终，融合后的知识 <span class="math inline">\(K_{fused}\)</span>
被用于子任务规划子模块（Subtask Planner），以构建一个排序的子任务队列
<span class="math inline">\({s_0, s_1, \ldots, s_n
}\)</span>，用于实现用户任务指令。同时，为每个子任务 <span class="math inline">\(s_i\)</span> 生成对应的上下文信息 <span class="math inline">\(C_{s_i}\)</span>，以提供完成该子任务所需的辅助信息。</p>
<h4 id="worker-learning-from-subtask-experience-and-trajectory-reflection">WORKER:
LEARNING FROM SUBTASK EXPERIENCE AND TRAJECTORY REFLECTION</h4>
<p>由 Manager G 生成的子任务序列 <span class="math inline">\(&lt;s_0,
s_1, \ldots, s_n &gt;\)</span> 将被对应的工作模块 <span class="math inline">\(&lt;w_0, w_1, \ldots, w_n &gt;\)</span>
依次执行。每个工作模块（Worker）在一个子任务 <span class="math inline">\(s_i\)</span>
的执行过程中，可以跨越多个时间步进行交互与推理。</p>
<p>首先，用户任务 <span class="math inline">\(T_u\)</span>、当前子任务
<span class="math inline">\(s_i\)</span> 以及与其关联的上下文信息 <span class="math inline">\(C_{s_i}\)</span>
将被联合构造为查询向量，用于从该工作模块的情节记忆（Episodic
Memory）中检索相似的子任务执行经验 <span class="math inline">\(E_{s_i}\)</span>。检索过程基于嵌入向量的相似性，并使用
<span class="math inline">\(&lt;T_u, s_i, C_{s_i} &gt;\)</span>
作为索引关键。不同于 Narrative Memory，Episodic Memory
中保存的是已标记为 <span class="math inline">\(\text{DONE}\)</span>
的<strong>成功子任务轨迹</strong>的完整规划，包含明确的环境动作绑定信息。</p>
<p>该过程表示为： <span class="math display">\[
E_{s_i} = \text{Retrieve}(M_e, &lt;T_u, s_i, C_{s_i} &gt;)
\]</span></p>
<p>此外，每个工作模块还包含一个<strong>轨迹反思子模块（Trajectory
Reflector）<span class="math inline">\(TR_i\)</span></strong>，该模块在子任务执行期间对整个
episode
进行实时观察，提供反思性建议，以帮助代理重新思考策略、避免重复无效动作，从而提升效率和鲁棒性。</p>
<p>检索到的子任务经验 <span class="math inline">\(E_{s_i}\)</span> 与
<span class="math inline">\(TR_i\)</span> 提供的反思性建议将被送入该
Worker 内部的动作生成子模块（Action
Generator），用于生成结构化响应。该响应包括以下组成部分：上一动作的执行状态检查，当前观测的语义分析，下一个语义动作的推理，与图形界面绑定的下一个有效动作。最终，该过程生成一个明确的绑定动作
<span class="math inline">\(a_j\)</span>，并由代理-计算机接口（ACI）在桌面环境中实际执行。一旦工作模块判断当前子任务已完成，它将生成一个特殊的绑定动作
<span class="math inline">\(\text{DONE}\)</span>，作为子任务成功结束的信号。若子任务不可完成，也可生成
<span class="math inline">\(\text{FAIL}\)</span>
信号，触发整个分层流程的重置，此时 Manager G
将基于当前环境配置重新规划新的子任务序列。</p>
<h4 id="self-evaluator-summarizing-experiences-as-textual-rewards">SELF-EVALUATOR:
SUMMARIZING EXPERIENCES AS TEXTUAL REWARDS</h4>
<p>自我评估模块 <span class="math inline">\(S\)</span> 负责为 Manager
和工作模块（Worker）生成用于学习的经验总结，作为文本形式的奖励信号 <span class="math inline">\(r\)</span>。该模块贯穿于整个层级任务执行流程，并基于子任务或完整任务的执行结果，动态生成反馈经验以更新系统的内部记忆模块。</p>
<h5 id="子任务级奖励更新-episodic-memory">1. 子任务级奖励：更新 Episodic
Memory</h5>
<p>当某个子任务 <span class="math inline">\(s_i\)</span>
被对应的工作模块成功执行完成，并通过特殊动作 <span class="math inline">\(\text{DONE}\)</span>
发出完成信号时，评估器将观察该 episode
的完整轨迹，并对该子任务的完成策略进行总结。该总结结果作为学习信号反馈至工作模块的
Episodic Memory <span class="math inline">\(M_e\)</span>
中，供后续相似子任务的经验检索与执行规划使用。</p>
<h5 id="全任务级奖励更新-narrative-memory">2. 全任务级奖励：更新
Narrative Memory</h5>
<p>当用户提供的完整任务 <span class="math inline">\(T_u\)</span>
被成功完成（即所有子任务 <span class="math inline">\(\text{DONE}\)</span>），或达到预设的最大步数限制而被终止，评估器将对整个任务过程进行观察与总结，生成对全流程的策略反思。该任务级的经验总结将被存入管理器的叙事记忆
<span class="math inline">\(M_n\)</span>，用于未来相似任务的计划指导。</p>
<p>自我评估模块完成的过程可类比为经典的层级强化学习（Hierarchical
Reinforcement Learning, HRL）中的奖励生成机制。其关键特点在于：</p>
<ul>
<li><p>系统在每个层级（子任务/完整任务）均可获得总结性奖励。</p></li>
<li><p>奖励不是标量形式的即时反馈，而是结构化的文本经验摘要。</p></li>
<li><p>学习过程依赖于 检索机制（Retrieval as Learning
Strategy），即通过从记忆中提取过去经验而非参数更新来实现策略改进。</p></li>
</ul>
<h3 id="初始记忆构建与持续学习机制">初始记忆构建与持续学习机制</h3>
<h4 id="初始记忆构建自监督探索">1. 初始记忆构建：自监督探索</h4>
<p>为初始化 Narrative Memory <span class="math inline">\(M_n\)</span> 与
Episodic Memory <span class="math inline">\(M_e\)</span>，Agent S
通过在一组合成的探索任务上进行自监督探索。论文设计了两类探索任务：环境无关型任务与环境感知型任务。</p>
<h5 id="环境无关型任务">1.1 环境无关型任务</h5>
<p>通过任务生成器（Task Generator）从 OSWorld 与 WindowsAgentArena
中所涉及的多种常见应用中自动生成排名前 50
的通用任务。这些任务与具体桌面环境无关，适用于大范围泛化训练。</p>
<h5 id="环境感知型任务">1.2 环境感知型任务</h5>
<p>从 OSWorld 与 WindowsAgentArena
提取任务的初始环境观测，并基于该环境提示任务生成器构造与当前 GUI
状态相关但目标不同的新任务。这类任务更贴近真实桌面环境中可能遇到的变化情况，有助于提高模型在实际环境中的适应能力。</p>
<p>这两类任务统称为探索任务（Exploration Tasks），用于 Agent S
的初始训练过程。</p>
<h5 id="自监督运行与记忆收集">1.3 自监督运行与记忆收集</h5>
<p>在上述任务中，Agent S 仅依赖网页知识库 <span class="math inline">\(K_{web}\)</span>
进行任务执行，系统在运行过程中自动记录完整任务经验与子任务经验：</p>
<ul>
<li><p>Narrative Memory <span class="math inline">\(M_n\)</span>
中存储的 key 为任务级查询 <span class="math inline">\(Q\)</span>，value
为整个任务轨迹的摘要 <span class="math inline">\(E_n\)</span>（Narrative
Experience）。</p></li>
<li><p>Episodic Memory <span class="math inline">\(M_e\)</span> 中存储的
key 为三元组 <span class="math inline">\(&lt; Q, s_i, C_{s_i}
&gt;\)</span>，value 为子任务轨迹摘要 <span class="math inline">\(E_e\)</span>（Episodic Experience）。</p></li>
</ul>
<p>通过这一过程，Agent S
构建了初始化的可检索记忆库，为后续任务执行提供知识基础。</p>
<h4 id="持续记忆更新">2. 持续记忆更新</h4>
<p>在 Agent S 后续与实际任务交互过程中，Narrative Memory <span class="math inline">\(M_n\)</span> 与 Episodic Memory <span class="math inline">\(M_e\)</span>
将持续更新。对于每个新任务，无论成功或失败，系统都将自动总结经验并存入相应的记忆模块。这种机制使得
Agent S
不仅在训练阶段学习，也能在推理（inference）阶段持续积累经验。</p>
<h3 id="代理-计算机接口">代理-计算机接口</h3>
<p>Agent S 利用图像输入理解环境状态，通过增强后的辅助访问树实现对具体 UI
元素的精准锚定，并依赖唯一标签引用元素。所有操作限制在离散、可控的基础动作原语（如点击、输入、快捷键）中，确保每一步都有明确反馈，从而在保持安全性和解释性的同时，实现高效、稳健的桌面自动化能力。</p>
<h1 id="任务分解动态规划经验回溯多个-grounding-专家-agent-s2-a-compositional-generalist-specialist-framework-for-computer-use-agents"><a target="_blank" rel="noopener" href="https://www.simular.ai/articles/agent-s2-technical-review">(202504)
（任务分解、动态规划、经验回溯、多个 grounding 专家） Agent S2: A
Compositional Generalist-Specialist Framework for Computer Use
Agents</a></h1>
<p>本研究提出了 Agent
S2，一个面向<strong>计算机</strong>使用任务的创新型组合式智能体框架。为了提升模型对
GUI 元素的 grounding 能力，论文采用 Mixture-of-Grounding
技术，允许智能体围绕子目标进行推理，并将动作路由至特定的 grounding
专家。Agent S2 结合<strong>主动式层次规划（Proactive Hierarchical
Planning）</strong>，可根据环境变化动态调整多尺度下的行动计划。此外，Agent
S2 整合了来自 Agent S 的一些框架，比如外部知识库、经验
memory，反思机制等。</p>
<h2 id="mixture-of-grounding">Mixture of Grounding</h2>
<p><img src="/2025/04/01/GUIagent_papers/agents3.png"></p>
<p>在每一个执行步骤中，Agent S2 的 Worker 模块（W）
会接收一个当前的子目标 <span class="math inline">\(g_i\)</span>，以及来自环境的最新观测 <span class="math inline">\(o_t\)</span>。随后，其策略模块 <span class="math inline">\(\pi_W\)</span> 会基于此生成一个动作 <span class="math inline">\(a_t\)</span>，该动作配有一段自然语言描述，用于标明目标位置。生成动作后，Worker
会将 grounding 任务委托给以下三类专门的 grounding
专家之一，具体选择依据当前动作的需求：</p>
<h3 id="视觉-grounding-专家visual-grounding-expert">1. 视觉 grounding
专家（Visual Grounding Expert）</h3>
<p>视觉锚定专家接收一张截图 <span class="math inline">\(o\)</span>
与一段自然语言描述 <span class="math inline">\(d\)</span>，该描述指向图像中某一特定点，输出该描述所对应的二维坐标
$ &lt;x, y &gt;$。这种基于描述的视觉 grounding 方式允许 Agent S2
仅依赖截图即可执行任务，无需额外的可访问性树或 HTML 信息。</p>
<h3 id="文本-grounding-专家textual-grounding-expert">2. 文本 grounding
专家（Textual Grounding Expert）</h3>
<p>尽管像 UGround (Gou et al., 2024) 和 UI-TARS (Qin et al., 2025)
等视觉 grounding 模型在整体精度上表现出色，但在处理细粒度文本
grounding（时仍存在挑战。为应对该问题，Agent S2 引入传统的 OCR
技术。</p>
<p>该专家模块接受截图 <span class="math inline">\(o\)</span>，以及两段文字短语 <span class="math inline">\(p_1\)</span> 和 <span class="math inline">\(p_2\)</span>
作为输入，分别表示目标文本片段的起始与结束。随后，文本 grounding
专家利用 OCR 返回精确的坐标范围 <span class="math inline">\(&lt;x_{\text{start}}, y_{\text{start}}
&gt;\)</span> 与 <span class="math inline">\(&lt; x_{\text{end}},
y_{\text{end}} &gt;\)</span>，用于高精度的文本选择与交互。</p>
<h3 id="结构化-grounding-专家structural-grounding-expert">3. 结构化
grounding 专家（Structural Grounding Expert）</h3>
<p>由于单元格尺寸可变，且表格的位移可能改变行列的起始坐标，传统
grounding 方法难以实现精确对齐。为此，Agent S2 设计了结构化 grounding
专家，专门用于处理电子表格和表格 UI 中的结构化数据。</p>
<h2 id="proactive-hierarchical-planning">Proactive Hierarchical
Planning</h2>
<p><img src="/2025/04/01/GUIagent_papers/agents4.png"></p>
<p>复杂电脑任务的<strong>初始状态通常只包含部分与用户请求相关的信息。同时，后台程序和弹窗也会引入大量噪声</strong>，进一步加剧多模态大语言模型（MLLM）在图形界面中处理能力的挑战。</p>
<p>为此，Agent S2 引入了<strong>动态层级规划（Proactive Hierarchical
Planning）</strong>，在两个时间尺度（高层 Manager 与底层
Worker）上实现动态重规划和推理更新。这种方式不同于传统的被动规划（Reactive
Planning），后者通常在任务失败之后才更新计划。动态层级规划策略允许 Agent
S2
在完成每一个子目标后就根据最新观测进行重新规划，从而更好地适应环境的变化，并保持原始任务上下文，减少对噪声的敏感性。</p>
<p>具体而言，在每一个高层时间步 <span class="math inline">\(T\)</span>，系统会接收用户指令 <span class="math inline">\(I\)</span> 与初始观测 <span class="math inline">\(o_0\)</span>。随后，Manager 模块 <span class="math inline">\(M\)</span> 会生成一个子目标序列：<span class="math inline">\({g'_1, g'_2, g'_3, \ldots,
g'_n}\)</span>。然后，Worker 模块 $ W $ 选取第一个子目标 <span class="math inline">\(g_1 =
g'_1\)</span>，并开始执行。在每一个低层时间步 <span class="math inline">\(t\)</span>，Worker 根据策略 <span class="math inline">\(\pi_W\)</span> 选择动作 <span class="math inline">\(a_t\)</span>，并将其分配给合适的 grounding
专家模块。 经过多个 low level 动作后，Worker 会将当前子目标 <span class="math inline">\(g'_1\)</span> 执行结束，状态为 <span class="math inline">\(\text{SUCCESS}\)</span> 或 <span class="math inline">\(\text{FAILURE}\)</span>，此时控制权重新回到
Manager。Manager 会接收原始指令 <span class="math inline">\(I\)</span>、当前观测 <span class="math inline">\(o_t\)</span>、以及先前的子目标序列，进行上下文整合并生成更新后的子目标：<span class="math inline">\({g''_2, g''_3, \ldots,
g''_n}\)</span>。接着，Worker 继续执行新的子目标 <span class="math inline">\(g_2 =
g''_2\)</span>，重复上述流程，直到整个用户请求 <span class="math inline">\(I\)</span> 被完成。</p>
<p>通过这种方式，Agent S2
实现了任务执行过程中的持续上下文维护与动态规划，既能适应环境噪声变化，也提升了处理复杂、长时任务的鲁棒性与灵活性。</p>
<h1 id="经验回溯-rap-retrieval-augmented-planning-with-contextual-memory-for-multimodal-llm-agents"><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2402.03610">(202402) (经验回溯) RAP:
Retrieval-Augmented Planning with Contextual Memory for Multimodal LLM
Agents</a></h1>
<p>RAP（检索增强规划）旨在增强大语言模型（LLM）智能体的规划能力。它通过存储过往经验，并根据当前情境与历史任务的相似性进行智能检索，从而优化决策过程。</p>
<p><img src="/2025/04/01/GUIagent_papers/rap.png"></p>
<p><img src="/2025/04/01/GUIagent_papers/rap2.png"></p>
<h1 id="autoguide-automated-generation-and-selection-of-context-aware-guidelines-for-large-language-model-agents"><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2403.08978">(202412) AutoGuide: Automated
Generation and Selection of Context-Aware Guidelines for Large Language
Model Agents</a></h1>
<p>传统的基于演示的上下文学习（in-context
learning）方式难以有效指导模型做出准确决策。为解决这一挑战，本文提出了
AUTOGUIDE，能够自动从离线经验中生成上下文感知的自然语言指南，提升智能体的泛化能力与任务表现。AUTOGUIDE
利用离线交互数据自动构建上下文-条件对，提取任务中的关键决策要素。生成的指南采用简洁自然语言表达，具有明确的条件结构（if-context-then-action），能精确描述指南适用的情境。</p>
<p><img src="/2025/04/01/GUIagent_papers/autoguide.png"></p>
<p><img src="/2025/04/01/GUIagent_papers/autoguide2.png"></p>
<h1 id="历史动作回溯-scaletrack-scaling-and-back-tracking-automated-gui-agents"><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2505.00416">(202503) （历史动作回溯）
ScaleTrack: Scaling and back-tracking Automated GUI Agents</a></h1>
<p>自动 GUI
代理致力于在网页、移动或桌面等数字环境中自动完成复杂任务。它通常接收文本指令和
GUI 描述，逐步生成可执行操作（如点击、输入等）。传统 GUI
代理的训练面临两个关键问题：一是 GUI
锚定数据稀缺，难以准确定位执行坐标；二是任务规划阶段缺乏对历史行为的回溯，难以建模界面状态随操作演化的过程。</p>
<p>为解决这些挑战，ScaleTrack
提出了一种训练框架，通过<strong>扩展锚定数据规模</strong>与<strong>引入回溯式规划策略</strong>提升代理性能。具体来说，ScaleTrack
收集并统一了来自多个来源的大量 GUI 样本，用于训练 GUI grounding
模型；同时，在训练中不仅预测当前图像下的下一步操作，还<strong>回溯导致当前界面的历史动作</strong>，从而显式建模
GUI 状态的变化轨迹。实验表明，ScaleTrack
在多个任务上显著提升了性能，展示了该方法在数据利用与行为建模上的强大潜力。</p>
<p><img src="/2025/04/01/GUIagent_papers/scaletrack.png"></p>
<h1 id="任务分解反思纠正奖励机制-infigui-r1-advancing-multimodal-gui-agents-from-reactive-actors-to-deliberative-reasoners"><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2504.14239">(202504)
(任务分解、反思纠正、奖励机制) InfiGUI-R1: Advancing Multimodal GUI
Agents from Reactive Actors to Deliberative Reasoners</a></h1>
<p>近年来，多模态大模型（MLLMs）在自动化图形用户界面（GUI）任务中展现出强大潜力。然而，现有方法大多依赖手工设计的推理模板或基于隐式逻辑的反应式执行方式，难以应对复杂
GUI 环境中对计划与容错能力的需求。为此，InfiGUI-R1
引入了一种全新的训练框架 —— Actor2Reasoner，旨在将 GUI
代理从“反应型执行者”演化为“推理型行动者”。该框架分为两个阶段：推理注入（Reasoning
Injection） 和 推理增强（Deliberation Enhancement）。</p>
<p><img src="/2025/04/01/GUIagent_papers/actor2reasoner1.png"></p>
<h2 id="阶段1推理注入">阶段1:推理注入</h2>
<p>第 1 阶段的主要目标是让 agent 从反应型行动者（Perception <span class="math inline">\(\rightarrow\)</span> Action）到基础推理者
（Perception <span class="math inline">\(\rightarrow\)</span> Reasoning
<span class="math inline">\(\rightarrow\)</span> Action）过渡。Stage 1
引入了“空间推理蒸馏（Spatial Reasoning
Distillation）”机制，将教师模型生成的高质量推理轨迹用于训练学生模型，使其掌握中间推理步骤，尤其是空间推理逻辑。</p>
<h3 id="推理瓶颈样本筛选">推理瓶颈样本筛选</h3>
<p>为了提高蒸馏效率，首先筛选出模型失败主要源于推理能力不足的交互步骤，称为推理瓶颈样本。具体识别过程如下：
对于轨迹中的每个交互步骤 <span class="math inline">\(s\)</span>，应用如下两步准则：</p>
<ul>
<li><p>基础模型 <span class="math inline">\(M\)</span> 在仅给定 GUI 截图
<span class="math inline">\(I_s\)</span> 和整体任务目标 <span class="math inline">\(G\)</span> 的情况下，无法预测正确动作：<span class="math inline">\(a_{\text{high}} = M(I_s, G)\)</span>，且 <span class="math inline">\(a_{\text{high}}\)</span> 错误；</p></li>
<li><p>当进一步提供该步骤对应的子目标 <span class="math inline">\(g_s\)</span> 后，模型 <span class="math inline">\(M\)</span> 能够正确预测动作：<span class="math inline">\(a_{\text{low}} = M(I_s, G, g_s)\)</span>，且 <span class="math inline">\(a_{\text{low}}\)</span> 正确。</p></li>
</ul>
<p>由此定义推理瓶颈步骤集合为： <span class="math display">\[
S_{\text{bottleneck}} = \{ s \mid \text{IsCorrect}(a_{\text{high}}) =
\text{False} \land \text{IsCorrect}(a_{\text{low}}) = \text{True} \}
\]</span></p>
<p>这些步骤主要困难在于：模型需要从整体目标 <span class="math inline">\(G\)</span> 和视觉上下文 <span class="math inline">\(I_s\)</span> 中推理出当前子目标 <span class="math inline">\(g_s\)</span>，非常适合作为推理能力注入的训练样本。论文使用如
Qwen2.5-VL-3B-Instruct 等基础 MLLM 完成筛选。</p>
<h3 id="生成空间推理轨迹">生成空间推理轨迹</h3>
<p>对于每个 <span class="math inline">\(s \in
S_{\text{bottleneck}}\)</span>，使用高性能教师模型生成细致的推理轨迹，包括以下步骤：</p>
<ul>
<li><p>从对应 GUI 截图 <span class="math inline">\(I_s\)</span>
的可访问性树（a11y
tree）中提取结构化空间信息（如元素类型、文本内容、坐标、层级关系等），并过滤无关元素。之后，使用强大的多模态模型（如
Qwen2.5-VL-32B-Instruct）将该信息压缩为精炼的文本描述 <span class="math inline">\(D_{\text{spatial}}\)</span>，准确反映 GUI
页面中的空间布局与关键元素特征。</p></li>
<li><p>将 <span class="math inline">\(D_{\text{spatial}}\)</span>、可用动作空间的描述以及整体目标
<span class="math inline">\(G\)</span>
一并输入具有强推理能力的语言模型（如 QwQ-32B），生成显式推理文本 <span class="math inline">\(R_{\text{teacher}}\)</span> 与对应动作 <span class="math inline">\(a_{\text{teacher}}\)</span>。该推理过程要求教师模型详细阐述逻辑步骤，特别是如何利用
<span class="math inline">\(D_{\text{spatial}}\)</span>
进行元素定位、关系判断与动作选择。</p></li>
</ul>
<h3 id="通过监督微调注入推理能力sft">通过监督微调注入推理能力（SFT）</h3>
<p>生成的 <span class="math inline">\((R_{\text{teacher}},
a_{\text{teacher}})\)</span>
样本需经由动作正确性校验过滤，确保数据质量。筛选后的高质量样本用于微调学生模型，监督目标为：
<span class="math display">\[
(I_s, G) \rightarrow (R_{\text{teacher}}, a_{\text{teacher}})
\]</span></p>
<p>通过学习显式生成或隐式模拟这些推理步骤，学生模型逐步内化“感知 <span class="math inline">\(\rightarrow\)</span> 推理 <span class="math inline">\(\rightarrow\)</span>
动作”的处理流程，摆脱以往直接从感知到动作的反应式模式。</p>
<h2 id="阶段2推理增强">阶段2:推理增强</h2>
<p>阶段2以基于规则的奖励机制强化学习（Reinforcement Learning with
Rule-Based
Rewards）作为主要优化手段，系统性地增强代理在复杂任务中的推理能力。本阶段在强化学习过程中引入了两项关键机制：</p>
<ul>
<li><p>子目标引导（Sub-goal
Guidance）：通过评估和激励模型在推理过程中隐含的中间目标设定质量，提升其任务分解与计划能力；</p></li>
<li><p>错误恢复场景构建（Error Recovery Scenario
Construction）：通过构造失败—恢复情境，系统性训练模型的反思与纠错能力，增强鲁棒性。</p></li>
</ul>
<h1 id="does-chain-of-thought-reasoning-help-mobile-gui-agent-an-empirical-study"><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2503.16788">(202503) Does Chain-of-Thought
Reasoning Help Mobile GUI Agent? An Empirical Study</a></h1>
<p>这篇论文系统地评估了具备推理能力的多模态视觉语言模型（VLMs）在移动图形用户界面（GUI）代理任务中的实际效果。作者选取了两个商业模型组合进行对比实验：
Gemini 2.0 Flash（基础版 vs 推理版）和 Claude 3.7 Sonnet（基础版 vs
推理版），在三个评估环境中测试：ScreenSpot（静态基准）、AndroidControl（静态基准）和AndroidWorld（交互式环境）。通过一系列静态与交互式基准测试，本文提出了以下关键观察结论：</p>
<h3 id="静态任务中推理模型提升有限甚至表现下降">静态任务中推理模型提升有限，甚至表现下降</h3>
<ul>
<li><p>在<strong>AndroidControl</strong>基准中，推理模型 <strong>Gemini
Thinking</strong> 的动作预测准确率为
<strong>54.4%</strong>，仅比非推理版本高出
<strong>0.8%</strong>，提升极为有限。</p></li>
<li><p>在 <strong>ScreenSpot</strong> 的定位任务中，仅在 <strong>Claude
Thinking（带中心点归一化输出）</strong>
的设置下观察到性能提升，其它设置下精度下降幅度高达
<strong>29.7%</strong>。</p></li>
<li><p>结论：推理能力在静态任务中并非总能带来有效提升，某些场景甚至严重拖累模型性能。</p></li>
</ul>
<h3 id="交互式任务中推理模型展现更大潜力">交互式任务中推理模型展现更大潜力</h3>
<ul>
<li><p>在交互式测试平台 <strong>AndroidWorld</strong> 中，<strong>Claude
Thinking</strong> 采用 “set-of-mark” 提示方式时，任务完成率达到
<strong>64.7%</strong>，<strong>刷新了当前最佳水平（SOTA）</strong>，比其非推理版本高出
<strong>6.3%</strong>。</p></li>
<li><p>相比之下，<strong>Gemini Thinking</strong>
在该环境下表现略有下降，说明性能提升存在
<strong>模型依赖性</strong>。</p></li>
</ul>
<h3 id="推理与非推理模型失败样本几乎无重合影响相互抵消">推理与非推理模型失败样本几乎无重合，影响相互抵消</h3>
<ul>
<li><p>以 <strong>Gemini Thinking</strong> 为例，其在
ScreenSpot、AndroidControl 和 AndroidWorld 中分别失败了
<strong>36%、9%、12%</strong>
的任务，而这些任务是其基础版本能成功完成的。</p></li>
<li><p>同样地，Gemini Thinking 也能完成多达 <strong>10%</strong>
的基础版本无法完成的任务。</p></li>
<li><p>说明推理<strong>并非无效</strong>，但其正面与负面效应在整体评估指标中互相抵消，掩盖了其实际潜力。</p></li>
</ul>
<h3 id="推理过程受限于数据集与模型虽具人类类比能力但无法转化为性能收益">推理过程受限于数据集与模型，虽具人类类比能力但无法转化为性能收益</h3>
<p>推理VLM在操作过程中展现出类似人类的理解能力和思考流程，但由于以下问题，未能转化为实际性能收益：</p>
<ul>
<li><p><strong>基准缺陷</strong>：如任务描述模糊、静态数据集无法支持多动作路径评估；</p></li>
<li><p><strong>模型局限</strong>：如对界面细节理解能力不足，导致推理过程与最终输出不一致。</p></li>
</ul>
<h3 id="推理模型显著增加输出长度带来成本与延迟问题">推理模型显著增加输出长度，带来成本与延迟问题</h3>
<ul>
<li><p>推理模型输出 token 数量比非推理版本增加 <strong>3.11× 至
14.78×</strong>，例如在 <strong>ScreenSpot</strong> 中从平均
<strong>37.6</strong> 增加到 <strong>238.5</strong>。</p></li>
<li><p>这导致响应延迟提升、调用成本升高，且不一定带来性能提升。</p></li>
<li><p>推理模型常输出冗余内容（如思考总结），对任务无实际帮助，暴露出<strong>无差别使用推理模型的实用性问题</strong>。</p></li>
</ul>
<h1 id="规划反思-screenagent-a-vision-language-model-driven-computer-control-agent"><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2402.07945">(202402) （规划、反思）
ScreenAgent: A Vision Language Model-driven Computer Control
Agent</a></h1>
<p>在本研究中，我们提出了一个新的环境，供视觉语言模型（VLM）智能体与真实计算机屏幕进行交互。在这个环境中，智能体能够通过观察屏幕截图并输出鼠标和键盘操作来操控图形用户界面（GUI）。此外，我们设计了一个自动化控制流程，包含规划、行动和反思阶段，帮助智能体不断与环境交互，完成多步骤任务。</p>
<h1 id="aguvis-unified-pure-vision-agents-for-autonomous-gui-interaction"><a target="_blank" rel="noopener" href="https://aguvis-project.github.io/">(202412) Aguvis: Unified Pure
Vision Agents for Autonomous GUI Interaction</a></h1>
<p>研究团队提出 Aguvis，一个统一的、基于视觉的 GUI
智能体框架，具备以下三大核心创新：</p>
<p>图像驱动执行方式：直接处理屏幕截图，绕过传统 GUI
元数据依赖，具有极强的跨平台适应能力；不依赖于平台 API 或 DOM
结构，提升部署灵活性。</p>
<p>统一的跨平台操作空间建模：所有 GUI
操作（如点击、滑动、输入）均通过图像区域识别与动作映射完成；解决平台异构动作空间问题，实现一次训练、多平台泛化。</p>
<p>结构化推理机制（Inner
Monologue）：在执行过程中引入内在推理流程，模拟“思考过程”（如目标识别、子任务分解、操作验证）；通过语言内省促进策略透明度和可解释性，弥补传统
LLM Agent 推理能力薄弱的问题。</p>
<h1 id="轨迹数据生成-os-genesis-automating-gui-agent-trajectory-construction-via-reverse-task-synthesis"><a target="_blank" rel="noopener" href="https://qiushisun.github.io/OS-Genesis-Home/">(202412)
（轨迹数据生成） OS-Genesis: Automating GUI Agent Trajectory
Construction via Reverse Task Synthesis</a></h1>
<p>图形用户界面（GUI）智能体通过视觉语言模型（VLMs）为数字化自动化提供了类似人类的计算机控制能力。然而，在推动这一领域发展的过程中，存在一个关键瓶颈：高质量轨迹数据的收集。当前数据收集的常用方法主要依赖人工监督或通过执行预定义任务合成数据，这些方法要么资源密集，要么无法保证数据质量。此外，这些方法通常存在数据多样性不足和合成数据与现实世界环境之间存在显著差距的问题。</p>
<p>为了解决这些挑战，论文提出了
OS-Genesis，一种图形用户界面数据合成方法。与以往依赖预定义任务的做法不同，OS-Genesis
通过以下步骤重新定义了数据合成方式：</p>
<ul>
<li><p>环境感知与互动：智能体首先通过感知环境并执行逐步交互，探索图形用户界面元素。通过这些交互，智能体逐步了解界面布局、元素之间的关系以及潜在的任务目标。</p></li>
<li><p>任务回溯推导：与传统的合成方法不同，OS-Genesis
允许智能体基于其交互生成任务，而不是依赖于预设的任务。这样，智能体能够根据实际的交互过程回溯并推导出高质量的任务。</p></li>
<li><p>轨迹奖励模型：为了确保生成的轨迹质量，OS-Genesis
使用轨迹奖励模型对生成的任务进行评估和优化，确保数据的多样性和质量符合训练要求。</p></li>
</ul>
<h1 id="google-research-任务目标生成-identifying-user-goals-from-ui-trajectories"><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2406.14314">(202503, Google Research)
（任务目标生成） Identifying User Goals From UI Trajectories</a></h1>
<p>本研究提出了一个新任务：从用户在 UI
中的交互轨迹中识别其任务目标。具体来说，给定一系列用户的 UI
操作序列（即点击、滑动、输入等轨迹），系统需自动生成该用户执行任务的明确意图描述（goal
intent），从而更好地理解其真实目标和行为动机。</p>
<p><img src="/2025/04/01/GUIagent_papers/gs.png"></p>
<h1 id="iclr-2025多智能体-inverse-attention-agents-for-multi-agent-systems"><a target="_blank" rel="noopener" href="https://openreview.net/forum?id=OaoDVZntGe">（ICLR
2025）（多智能体） Inverse Attention Agents for Multi-Agent
Systems</a></h1>
<p>多智能体强化学习（MARL）目前存在的一个局限性为：虽然通过一起训练后的多智能体展现出熟练的协调能力，但在与不熟悉的
agent 合作时，它们的性能会明显下降。传统的 Theory of Mind (ToM)
研究关注的是智能体对他人“信念、欲望”等心理状态的推理，该研究将其“注意力”机制引入
MARL
中，通过端到端的注意力识别网络，提升多智能体系统中的认知建模能力与协作效果。</p>
<h2 id="马尔可夫博弈">马尔可夫博弈</h2>
<p>多智能体马尔可夫决策过程（Multi-Agent MDPs，Littman,
1994）的状态转移与奖励依赖于所有智能体的联合动作。一个包含 <span class="math inline">\(N\)</span>
个智能体的马尔可夫博弈形式上定义为：</p>
<ul>
<li><p>状态集合：<span class="math inline">\(\mathcal{S}\)</span></p></li>
<li><p>每个智能体 <span class="math inline">\(i\)</span>
的动作集合：<span class="math inline">\(\mathcal{A}_i\)</span></p></li>
<li><p>状态转移函数： <span class="math display">\[
T: \mathcal{S} \times \mathcal{A}_1 \times \cdots \times \mathcal{A}_N
\rightarrow \Delta(\mathcal{S}).
\]</span> 其中，<span class="math inline">\(\Delta(\mathcal{S})\)</span>
表示在状态集合上的概率分布。</p></li>
<li><p>每个智能体 <span class="math inline">\(i\)</span> 的奖励函数：
<span class="math display">\[
R_i: \mathcal{S} \times \mathcal{A}_1 \times \cdots \times \mathcal{A}_N
\rightarrow \mathbb{R}.
\]</span> 每个智能体的目标是通过最大化其期望的累积折扣奖励： <span class="math display">\[
\mathbb{E} [ \sum_{t=0}^{\infty} \gamma^t R_i(s_t, a_{1,t}, \ldots,
a_{N,t}) ].
\]</span> 来学习一个策略： <span class="math display">\[
\pi_i : \mathcal{S} \rightarrow \Delta(\mathcal{A}_i).
\]</span>
该策略定义了在当前状态下，智能体采取各个动作的概率分布，旨在优化其在当前环境下的长期收益。</p></li>
</ul>
<h2 id="方法介绍">方法介绍</h2>
<p><img src="/2025/04/01/GUIagent_papers/inverse.png"></p>
<ul>
<li><p>阶段一：自注意力策略建模（Self-Attention Policy）。使用
Transformer 中的 <strong>self-attention</strong>
机制构建策略函数；每个智能体通过计算自身对多个目标的注意力权重（attention
weights）决定采取的动作；目的是让智能体能够在内部根据对不同任务或目标的关注度进行行为决策。在这一步，通过优化历史行动策略，来获取每个智能体对多个目标的注意力权重数据。</p></li>
<li><p>阶段二：推理他人注意力（Inverse Attention Inference）。使用
<strong>逆注意力网络（Inverse Attention Network）</strong>
推理其它智能体的注意力：通过换位思考，智能体设想自己处于其他同种类型智能体的位置，依据观察到的行为和环境状态，反推出它们对不同目标的注意力权重；这是模仿“心智理论”（Theory
of
Mind）的关键步骤。在这一步，根据上一步获得的（目标、注意力权重）数据，去训练一个
attention 网络。然后基于训练后的 attention
网络，代入其他智能体的观测结果/环境状态，得到它们对不同目标的注意力权重。</p></li>
<li><p>阶段三：更新自身注意力权重。将第二阶段推理出的其他智能体对不同目标的注意力权重作为输入；智能体据此
<strong>更新自身的注意力权重</strong>，从而调整其对各个目标的重视程度；更新后的注意力权重将影响最终动作选择，实现更高层次的协作或对抗行为。</p></li>
</ul>
<p><img src="/2025/04/01/GUIagent_papers/inverse1.png"></p>
<h1 id="多智能体-agentstore-scalable-integration-of-heterogeneous-agents-as-specialized-generalist-computer-assistant"><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2410.18603">(202412) （多智能体）
AgentStore: Scalable Integration of Heterogeneous Agents As Specialized
Generalist Computer Assistant</a></h1>
 
      <!-- reward -->
      
    </div>
    

    <!-- copyright -->
    
    <div class="declare">
      <ul class="post-copyright">
        <li>
          <i class="ri-copyright-line"></i>
          <strong>Copyright： </strong>
          
          Copyright is owned by the author. For commercial reprints, please contact the author for authorization. For non-commercial reprints, please indicate the source.
          
        </li>
      </ul>
    </div>
    
    <footer class="article-footer">
       
<div class="share-btn">
      <span class="share-sns share-outer">
        <i class="ri-share-forward-line"></i>
        分享
      </span>
      <div class="share-wrap">
        <i class="arrow"></i>
        <div class="share-icons">
          
          <a class="weibo share-sns" href="javascript:;" data-type="weibo">
            <i class="ri-weibo-fill"></i>
          </a>
          <a class="weixin share-sns wxFab" href="javascript:;" data-type="weixin">
            <i class="ri-wechat-fill"></i>
          </a>
          <a class="qq share-sns" href="javascript:;" data-type="qq">
            <i class="ri-qq-fill"></i>
          </a>
          <a class="douban share-sns" href="javascript:;" data-type="douban">
            <i class="ri-douban-line"></i>
          </a>
          <!-- <a class="qzone share-sns" href="javascript:;" data-type="qzone">
            <i class="icon icon-qzone"></i>
          </a> -->
          
          <a class="facebook share-sns" href="javascript:;" data-type="facebook">
            <i class="ri-facebook-circle-fill"></i>
          </a>
          <a class="twitter share-sns" href="javascript:;" data-type="twitter">
            <i class="ri-twitter-fill"></i>
          </a>
          <a class="google share-sns" href="javascript:;" data-type="google">
            <i class="ri-google-fill"></i>
          </a>
        </div>
      </div>
</div>

<div class="wx-share-modal">
    <a class="modal-close" href="javascript:;"><i class="ri-close-circle-line"></i></a>
    <p>扫一扫，分享到微信</p>
    <div class="wx-qrcode">
      <img src="//api.qrserver.com/v1/create-qr-code/?size=150x150&data=https://xueyu-ubc.github.io/2025/04/01/GUIagent_papers/" alt="微信分享二维码">
    </div>
</div>

<div id="share-mask"></div>  
    </footer>
  </div>

   
  <nav class="article-nav">
    
      <a href="/2025/04/18/UI_TARS/" class="article-nav-link">
        <strong class="article-nav-caption">上一篇</strong>
        <div class="article-nav-title">
          
            UI-TARS
          
        </div>
      </a>
    
    
      <a href="/2025/03/21/The_State_of_LLM_Reasoning_Models/" class="article-nav-link">
        <strong class="article-nav-caption">下一篇</strong>
        <div class="article-nav-title">The State of LLM Reasoning Models (Part 1)</div>
      </a>
    
  </nav>

   
<!-- valine评论 -->
<div id="vcomments-box">
  <div id="vcomments"></div>
</div>
<script src="//cdn1.lncld.net/static/js/3.0.4/av-min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/valine@1.4.14/dist/Valine.min.js"></script>
<script>
  new Valine({
    el: "#vcomments",
    app_id: "",
    app_key: "",
    path: window.location.pathname,
    avatar: "monsterid",
    placeholder: "给我的文章加点评论吧~",
    recordIP: true,
  });
  const infoEle = document.querySelector("#vcomments .info");
  if (infoEle && infoEle.childNodes && infoEle.childNodes.length > 0) {
    infoEle.childNodes.forEach(function (item) {
      item.parentNode.removeChild(item);
    });
  }
</script>
<style>
  #vcomments-box {
    padding: 5px 30px;
  }

  @media screen and (max-width: 800px) {
    #vcomments-box {
      padding: 5px 0px;
    }
  }

  #vcomments-box #vcomments {
    background-color: #fff;
  }

  .v .vlist .vcard .vh {
    padding-right: 20px;
  }

  .v .vlist .vcard {
    padding-left: 10px;
  }
</style>

 
   
     
</article>

</section>
      <footer class="footer">
  <div class="outer">
    <ul>
      <li>
        Copyrights &copy;
        2021-2025
        <i class="ri-heart-fill heart_icon"></i> Xue Yu
      </li>
    </ul>
    <ul>
      <li>
        
        
        
        Powered by <a href="https://hexo.io" target="_blank">Hexo</a>
        <span class="division">|</span>
        Theme - <a href="https://github.com/Shen-Yu/hexo-theme-ayer" target="_blank">Ayer</a>
        
      </li>
    </ul>
    <ul>
      <li>
        
        
        <span>
  <span><i class="ri-user-3-fill"></i>Visitors:<span id="busuanzi_value_site_uv"></span></s>
  <span class="division">|</span>
  <span><i class="ri-eye-fill"></i>Views:<span id="busuanzi_value_page_pv"></span></span>
</span>
        
      </li>
    </ul>
    <ul>
      
    </ul>
    <ul>
      
    </ul>
    <ul>
      <li>
        <!-- cnzz统计 -->
        
        <script type="text/javascript" src='https://s9.cnzz.com/z_stat.php?id=1278069914&amp;web_id=1278069914'></script>
        
      </li>
    </ul>
  </div>
</footer>
      <div class="float_btns">
        <div class="totop" id="totop">
  <i class="ri-arrow-up-line"></i>
</div>

<div class="todark" id="todark">
  <i class="ri-moon-line"></i>
</div>

      </div>
    </main>
    <aside class="sidebar on">
      <button class="navbar-toggle"></button>
<nav class="navbar">
  
  <div class="logo">
    <a href="/"><img src="/images/ayer-side.svg" alt="Welcome to XueYu&#39;s Blog"></a>
  </div>
  
  <ul class="nav nav-main">
    
    <li class="nav-item">
      <a class="nav-item-link" href="/">主页</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/archives">归档</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/categories">分类</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/tags">标签</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/tags/%E6%97%85%E8%A1%8C/">旅行</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/">摄影</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/friends">友链</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/2021/about">关于我</a>
    </li>
    
  </ul>
</nav>
<nav class="navbar navbar-bottom">
  <ul class="nav">
    <li class="nav-item">
      
      <a class="nav-item-link nav-item-search"  title="Search">
        <i class="ri-search-line"></i>
      </a>
      
      
      <a class="nav-item-link" target="_blank" href="/atom.xml" title="RSS Feed">
        <i class="ri-rss-line"></i>
      </a>
      
    </li>
  </ul>
</nav>
<div class="search-form-wrap">
  <div class="local-search local-search-plugin">
  <input type="search" id="local-search-input" class="local-search-input" placeholder="Search...">
  <div id="local-search-result" class="local-search-result"></div>
</div>
</div>
    </aside>
    <script>
      if (window.matchMedia("(max-width: 768px)").matches) {
        document.querySelector('.content').classList.remove('on');
        document.querySelector('.sidebar').classList.remove('on');
      }
    </script>
    <div id="mask"></div>

<!-- #reward -->
<div id="reward">
  <span class="close"><i class="ri-close-line"></i></span>
  <p class="reward-p"><i class="ri-cup-line"></i></p>
  <div class="reward-box">
    
    
  </div>
</div>
    
<script src="/js/jquery-2.0.3.min.js"></script>


<script src="/js/lazyload.min.js"></script>

<!-- Tocbot -->


<script src="/js/tocbot.min.js"></script>

<script>
  tocbot.init({
    tocSelector: '.tocbot',
    contentSelector: '.article-entry',
    headingSelector: 'h1, h2, h3, h4, h5, h6',
    hasInnerContainers: true,
    scrollSmooth: true,
    scrollContainer: 'main',
    positionFixedSelector: '.tocbot',
    positionFixedClass: 'is-position-fixed',
    fixedSidebarOffset: 'auto'
  });
</script>

<script src="https://cdn.jsdelivr.net/npm/jquery-modal@0.9.2/jquery.modal.min.js"></script>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/jquery-modal@0.9.2/jquery.modal.min.css">
<script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/js/jquery.justifiedGallery.min.js"></script>

<script src="/dist/main.js"></script>

<!-- ImageViewer -->

<!-- Root element of PhotoSwipe. Must have class pswp. -->
<div class="pswp" tabindex="-1" role="dialog" aria-hidden="true">

    <!-- Background of PhotoSwipe. 
         It's a separate element as animating opacity is faster than rgba(). -->
    <div class="pswp__bg"></div>

    <!-- Slides wrapper with overflow:hidden. -->
    <div class="pswp__scroll-wrap">

        <!-- Container that holds slides. 
            PhotoSwipe keeps only 3 of them in the DOM to save memory.
            Don't modify these 3 pswp__item elements, data is added later on. -->
        <div class="pswp__container">
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
        </div>

        <!-- Default (PhotoSwipeUI_Default) interface on top of sliding area. Can be changed. -->
        <div class="pswp__ui pswp__ui--hidden">

            <div class="pswp__top-bar">

                <!--  Controls are self-explanatory. Order can be changed. -->

                <div class="pswp__counter"></div>

                <button class="pswp__button pswp__button--close" title="Close (Esc)"></button>

                <button class="pswp__button pswp__button--share" style="display:none" title="Share"></button>

                <button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>

                <button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button>

                <!-- Preloader demo http://codepen.io/dimsemenov/pen/yyBWoR -->
                <!-- element will get class pswp__preloader--active when preloader is running -->
                <div class="pswp__preloader">
                    <div class="pswp__preloader__icn">
                        <div class="pswp__preloader__cut">
                            <div class="pswp__preloader__donut"></div>
                        </div>
                    </div>
                </div>
            </div>

            <div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap">
                <div class="pswp__share-tooltip"></div>
            </div>

            <button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
            </button>

            <button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)">
            </button>

            <div class="pswp__caption">
                <div class="pswp__caption__center"></div>
            </div>

        </div>

    </div>

</div>

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.css">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/default-skin/default-skin.min.css">
<script src="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe-ui-default.min.js"></script>

<script>
    function viewer_init() {
        let pswpElement = document.querySelectorAll('.pswp')[0];
        let $imgArr = document.querySelectorAll(('.article-entry img:not(.reward-img)'))

        $imgArr.forEach(($em, i) => {
            $em.onclick = () => {
                // slider展开状态
                // todo: 这样不好，后面改成状态
                if (document.querySelector('.left-col.show')) return
                let items = []
                $imgArr.forEach(($em2, i2) => {
                    let img = $em2.getAttribute('data-idx', i2)
                    let src = $em2.getAttribute('data-target') || $em2.getAttribute('src')
                    let title = $em2.getAttribute('alt')
                    // 获得原图尺寸
                    const image = new Image()
                    image.src = src
                    items.push({
                        src: src,
                        w: image.width || $em2.width,
                        h: image.height || $em2.height,
                        title: title
                    })
                })
                var gallery = new PhotoSwipe(pswpElement, PhotoSwipeUI_Default, items, {
                    index: parseInt(i)
                });
                gallery.init()
            }
        })
    }
    viewer_init()
</script>

<!-- MathJax -->

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
      tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      }
  });

  MathJax.Hub.Queue(function() {
      var all = MathJax.Hub.getAllJax(), i;
      for(i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
      }
  });
</script>

<script src="https://cdn.jsdelivr.net/npm/mathjax@2.7.6/unpacked/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
<script>
  var ayerConfig = {
    mathjax: true
  }
</script>

<!-- Katex -->

<!-- busuanzi  -->


<script src="/js/busuanzi-2.3.pure.min.js"></script>


<!-- ClickLove -->

<!-- ClickBoom1 -->

<!-- ClickBoom2 -->

<!-- CodeCopy -->


<link rel="stylesheet" href="/css/clipboard.css">

<script src="https://cdn.jsdelivr.net/npm/clipboard@2/dist/clipboard.min.js"></script>
<script>
  function wait(callback, seconds) {
    var timelag = null;
    timelag = window.setTimeout(callback, seconds);
  }
  !function (e, t, a) {
    var initCopyCode = function(){
      var copyHtml = '';
      copyHtml += '<button class="btn-copy" data-clipboard-snippet="">';
      copyHtml += '<i class="ri-file-copy-2-line"></i><span>COPY</span>';
      copyHtml += '</button>';
      $(".highlight .code pre").before(copyHtml);
      $(".article pre code").before(copyHtml);
      var clipboard = new ClipboardJS('.btn-copy', {
        target: function(trigger) {
          return trigger.nextElementSibling;
        }
      });
      clipboard.on('success', function(e) {
        let $btn = $(e.trigger);
        $btn.addClass('copied');
        let $icon = $($btn.find('i'));
        $icon.removeClass('ri-file-copy-2-line');
        $icon.addClass('ri-checkbox-circle-line');
        let $span = $($btn.find('span'));
        $span[0].innerText = 'COPIED';
        
        wait(function () { // 等待两秒钟后恢复
          $icon.removeClass('ri-checkbox-circle-line');
          $icon.addClass('ri-file-copy-2-line');
          $span[0].innerText = 'COPY';
        }, 2000);
      });
      clipboard.on('error', function(e) {
        e.clearSelection();
        let $btn = $(e.trigger);
        $btn.addClass('copy-failed');
        let $icon = $($btn.find('i'));
        $icon.removeClass('ri-file-copy-2-line');
        $icon.addClass('ri-time-line');
        let $span = $($btn.find('span'));
        $span[0].innerText = 'COPY FAILED';
        
        wait(function () { // 等待两秒钟后恢复
          $icon.removeClass('ri-time-line');
          $icon.addClass('ri-file-copy-2-line');
          $span[0].innerText = 'COPY';
        }, 2000);
      });
    }
    initCopyCode();
  }(window, document);
</script>


<!-- CanvasBackground -->


    
  </div>
</body>

</html>
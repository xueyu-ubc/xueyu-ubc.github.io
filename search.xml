<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>Gaussian processes</title>
      <link href="/2023/08/06/gaussianprocess/"/>
      <url>/2023/08/06/gaussianprocess/</url>
      
        <content type="html"><![CDATA[<h1 id="jointly-gaussian-random-variables">Jointly Gaussian randomvariables</h1><p>Definition: Random variables (RV) <span class="math inline">\(X_1,..., X_n\)</span> are jointly Gaussian if any linear combination of themis Gaussian.</p><p>RV <span class="math inline">\(X = [X_1, ..., X_n]^{T}\)</span> isGaussian <span class="math inline">\(\leftrightarrows\)</span> Given anyscalars <span class="math inline">\(a_1,... a_n\)</span>, the RV <span class="math inline">\(Y = a_1 X_1 + a_2 X_2 + ... + a_n X_n\)</span> isGaussian distributed.</p><h2 id="pdf-of-jointly-gaussian-rvs-in-n-dimensions">Pdf of jointlyGaussian RVs in n dimensions</h2><p>Let <span class="math inline">\(X \in \mathbb{R}^n\)</span>, <span class="math inline">\(\mu = \mathbb{E}[X]\)</span>,</p>covariance matrix $C := [(X - )(X - )^T] =<span class="math display">\[\begin{pmatrix}    \sigma_{11}^2 &amp; \sigma_{12}^2 &amp; \cdots &amp; \sigma_{1n}^2\\    \sigma_{21}^2 &amp; \sigma_{22}^2 &amp; \cdots &amp; \sigma_{2n}^2\\    \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\    \sigma_{n1}^2 &amp; \sigma_{n2}^2 &amp; \cdots &amp; \sigma_{nn}^2\end{pmatrix}\]</span><p>$. Then, the pdf of RV <span class="math inline">\(X\)</span> can bedefined as <span class="math display">\[p(x) = \frac{1}{(2\pi)^{n/2} \text{det}^{1/2}(C)} \exp(-\frac{1}{2}(x -\mu)^T C^{-1} (x - \mu)).\]</span></p><ul><li><span class="math inline">\(C\)</span> is invertible</li><li>We can verify all linear combinations is Gaussian.</li><li>To fully specify the probability distribution of a Gaussian vector<span class="math inline">\(X\)</span>, the mean vector <span class="math inline">\(\mu\)</span> and covariance matrix <span class="math inline">\(C\)</span> suffice.</li></ul><h1 id="gaussian-processes">Gaussian processes</h1><p>Gaussian processes (GP) generalize Gaussian vectors to<strong>infinite</strong> dimensions.</p><p>Definition. <span class="math inline">\(X(t)\)</span> is a GP if anylinear combination of values <span class="math inline">\(X(t)\)</span>is Gaussian. That is, for arbitrary <span class="math inline">\(n &gt;0\)</span>, times <span class="math inline">\(t_1, ..., t_n\)</span> andconstants <span class="math inline">\(a_1, ..., a_n\)</span>, <span class="math inline">\(Y = a_1 X(t_1) + a_2 X(t_2) + ... + a_nX(t_n)\)</span> is Gaussian distributed.</p><ul><li><p>Time index <span class="math inline">\(t\)</span> can becontinuous or discrete.</p></li><li><p>Any linear functional of <span class="math inline">\(X(t)\)</span> is Gaussian distributed. Forexample, the integral <span class="math inline">\(Y = \int_{t_1}^{t_2}X(t) \text{d}t\)</span> is Gaussian distributed.</p></li></ul><h2 id="joint-pdf-in-a-gaussian-process">Joint pdf in a Gaussianprocess</h2><p>Consider times <span class="math inline">\(t_1,..., t_n\)</span>, themean value <span class="math inline">\(\mu(t_i)\)</span> is <span class="math display">\[\mu(t_i) = \mathbb{E}[X(t_i)].\]</span></p><p>The covariance between values at time <span class="math inline">\(t_i\)</span> and <span class="math inline">\(t_j\)</span> is <span class="math inline">\(C(t_i,t_j) := \mathbb{E}[(X(t_i) - \mu(t_i))(X(t_j) -\mu(t_j))^T]\)</span>.</p><p>The covariance matrix for values <span class="math inline">\(X(t_1),..., X(t_n)\)</span> is <span class="math display">\[C(t_1,..., t_n) =\begin{pmatrix}    C_{t_1, t_1} &amp; C_{t_1, t_2} &amp; \cdots &amp; C_{t_1, t_n}\\    C_{t_2, t_1} &amp; C_{t_2, t_2} &amp; \cdots &amp; C_{t_2, t_n} \\    \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\    C_{t_n, t_1} &amp; C_{t_n, t_2} &amp; \cdots &amp; C_{t_n, t_n}\end{pmatrix}\]</span>.</p><p>The joint pdf of <span class="math inline">\(X(t_1),...,X(t_n)\)</span> is <span class="math inline">\(N([\mu(t_1), ...,\mu(t_n)]^T, C(t_1,..., t_n))\)</span>.</p><h2 id="mean-value-and-autocorrelation-functions">Mean value andautocorrelation functions</h2><p>To specify a Gaussian process, we only need to specify: - Mean valuefunction: <span class="math inline">\(\mu(t) =\mathbb{E}[x(t)]\)</span>. - Autocorrelation function (symmetric): <span class="math inline">\(R(t_1, t_2) = \mathbb{E}[X(t_1)X(t_2)]\)</span>.</p><p>The autocovariance <span class="math inline">\(C(t_1, t_2) = R(t_1,t_2) - \mu(t_1) \mu (t_2)\)</span>.</p><p>More general, we consider GP with <span class="math inline">\(\mu(t)= 0\)</span>. [define new process <span class="math inline">\(Y(t) =X(t) - \mu(t)\)</span>]. In this case, <span class="math inline">\(C(t_1, t_2) = R(t_1, t_2)\)</span>.</p><p>All probs. in a GP can be expressed in terms of <span class="math inline">\(\mu(t)\)</span> and <span class="math inline">\(R(t, t)\)</span>. <span class="math display">\[p(x_t) = \frac{1}{\sqrt{2\pi (R(t,t) - \mu^2(t))}} \exp(- \frac{(x_t -\mu(t))^2}{2(R(t,t) - \mu^2(t))}).\]</span></p><h2 id="conditional-probabilities-in-a-gp">Conditional probabilities ina GP</h2><p>Consider a zero-mean GP <span class="math inline">\(X(t)\)</span>,two times <span class="math inline">\(t_1\)</span> and <span class="math inline">\(t_2\)</span>. The covariance matrix is <span class="math display">\[C = \begin{pmatrix}  R(t_1, t_1) &amp; R(t_1, t_2) \\  R(t_1, t_2) &amp; R(t_2, t_2)\end{pmatrix}\]</span></p><p>The joint pdf of <span class="math inline">\(X(t_1)\)</span> and<span class="math inline">\(X(t_2)\)</span> is <span class="math display">\[p(x_{t_1}, x_{t_2}) = \frac{1}{2\pi \text{det}^{1/2} C}\exp(-\frac{1}{2}[x_{t_1}, x_{t_2}]^T C^{-1} [x_{t_1}, x_{t_2}])\]</span></p><p>The conditional pdf of <span class="math inline">\(X(t_1)\)</span>given <span class="math inline">\(X(t_2)\)</span> is <span class="math display">\[p_{X(t_1)| X(t_2)}(x_{t_1} | x_{t_2}) = \frac{p(x_{t_1},x_{t_2})}{p(x_{t_2})}. \qquad (1)\]</span></p><h1 id="brownian-motion-process-a.k.a-wiener-process">Brownian motionprocess (a.k.a Wiener process)</h1><p>Definition. A Brownian motion process (a.k.a Wiener process)satisfies</p><ol type="1"><li><p><span class="math inline">\(X(t)\)</span> is normally distributedwith zero mean and variance <span class="math inline">\(\sigma^2t\)</span>, <span class="math display">\[X(t) \sim N(0, \sigma^2t).\]</span></p></li><li><p>Independent increments. For all times <span class="math inline">\(0 &lt; t_1 &lt; t_2 &lt; \cdots &lt; t_n\)</span>,the random variables <span class="math inline">\(X(t_1), X(t_2) -X(t_1), ..., X(t_n) - X(t_{n-1})\)</span> are independent.</p></li><li><p>Stationary increments. Probability distribution of increment<span class="math inline">\(X(t+s) - X(s)\)</span> is the same asprobability distribution of <span class="math inline">\(X(t)\)</span>.<span class="math display">\[[X(t+s) - X(s)]  \sim N(0,t).\]</span></p></li></ol><ul><li>Brownian motion is a Markov process.</li><li>Brownian motion is a Gaussian process.</li></ul><h2 id="mean-and-autocorrelation-of-brownian-motion">Mean andautocorrelation of Brownian motion</h2><p>1, Mean funtion <span class="math inline">\(\mu(t) = \mathbb{E}[X(t)]= 0\)</span>.</p><p>2, Autocorrelation of Brownian motion <span class="math inline">\(R(t_1, t_2) = \sigma^2 \min{t_1,t_2}\)</span>.</p><p>Proof. Assume <span class="math inline">\(t_1 &lt; t_2\)</span>, thenautocorrelation <span class="math inline">\(R(t_1, t_2) =\mathbb{E}[X(t_1)X(t_2)] = \sigma^2 t_1\)</span>.</p><p>If <span class="math inline">\(t_1 &lt; t_2\)</span>, according toconditional expectations, we have <span class="math display">\[\begin{aligned}  R(t_1, t_2) = \mathbb{E}[X(t_1)X(t_2)] &amp;=\mathbb{E}_{X(t_1)}[\mathbb{E}_{X(t_2)}[X(t_1)X(t_2) | X(t_1)]] \\  &amp;= \mathbb{E}_{X(t_1)}[X(t_1)\mathbb{E}_{X(t_2)}[X(t_2) | X(t_1)]]\end{aligned}\]</span> According to equation (1), the condition distribution of <span class="math inline">\(X(t_2)\)</span> given <span class="math inline">\(X(t_1)\)</span> is <span class="math display">\[[X(t_2) | X(t_1)] \sim N(X(t_1), \sigma^2 (t_2 - t_1)),\]</span> thus, <span class="math inline">\(\mathbb{E}_{X(t_2)}[X(t_2) |X(t_1)] = X(t_1)\)</span>.</p><p><span class="math display">\[\begin{aligned}  R(t_1, t_2) = \mathbb{E}[X(t_1)X(t_2)] &amp;=\mathbb{E}_{X(t_1)}[\mathbb{E}_{X(t_2)}[X(t_1)X(t_2) | X(t_1)]] \\  &amp;= \mathbb{E}_{X(t_1)}[X(t_1) X(t_1)] \\  &amp;= \mathbb{E}_{X(t_1)}[X^2(t_1)] = \sigma^2 t_1.\end{aligned}\]</span></p><p>Similarly, if <span class="math inline">\(t_2 &lt; t_1\)</span>,<span class="math inline">\(R(t_1, t_2) = \sigma^2 t_2\)</span>.</p><h2 id="brownian-motion-with-drift-bmd">Brownian motion with drift(BMD)</h2><p>For Brownian motion, it is a unbiased random walk. Walker steps rightor left with the same probability <span class="math inline">\(1/2\)</span> for each direction (onedimension).</p><p>For BMD, it is a biased random walk. Walker steps right or left withdifferent probs.</p><p>For example, consider time interval <span class="math inline">\(h\)</span>, step size <span class="math inline">\(\sigma \sqrt{h}\)</span>, <span class="math display">\[p(X(t+h) = x + \sigma \sqrt{h} | X(t) = x) = \frac{1}{2} (1 +\frac{\mu}{\sigma} \sqrt{h}).\]</span> <span class="math display">\[p(X(t+h) = x - \sigma \sqrt{h} | X(t) = x) = \frac{1}{2} (1 -\frac{\mu}{\sigma} \sqrt{h}).\]</span></p><ul><li><p><span class="math inline">\(\mu &gt; 0\)</span>, biased to theright. <span class="math inline">\(\mu &lt; 0\)</span>, biased to theleft.</p></li><li><p><span class="math inline">\(h\)</span> needs to be small enoughto make <span class="math inline">\(|\frac{\mu}{\sigma} \sqrt{h} | \leq1\)</span>.</p></li></ul><p>In this BMD case, <span class="math inline">\(x(t) \sim N(\mu t,\sigma^2 t)\)</span>.</p><ul><li>Independent and stationary increments.</li></ul><p>(We omit the proof. More details can be found at <a href="https://www.hajim.rochester.edu/ece/sites/gmateos/ECE440/Slides/block_5_stationary_processes_part_a.pdf">Gaussianprocess</a> ).</p><h2 id="geometric-brownian-motion-gbm">Geometric Brownian motion(GBM)</h2><p>Definition. Suppose that <span class="math inline">\(Z(t)\)</span> isa standard Brownian motion <span class="math inline">\(Z(t) \sim N(0,t)\)</span>. Parameters <span class="math inline">\(\mu \in\mathbb{R}\)</span> and <span class="math inline">\(\sigma \in (0,\infty)\)</span>. Let <span class="math display">\[X(t) = \exp[(\mu - \frac{\sigma^2}{2})t + \sigma Z(t)], \qquad t \geq 0.\qquad (2)\]</span> The stochastic process <span class="math inline">\(\{X(t): t\geq 0\}\)</span> is geometric Brownian motion with drift parameter<span class="math inline">\(\mu\)</span> and volatility parameter <span class="math inline">\(\sigma\)</span>.</p><ul><li>The process is always positive, one of the reasons that geometricBrownian motion is used to model financial and other processes that<strong>cannot be negative</strong>.</li><li>For the stochastic process</li></ul><p><span class="math display">\[(\mu - \frac{\sigma^2}{2})t + \sigma Z(t) \sim N((\mu -\frac{\sigma^2}{2})t , \sigma^2 t),  \]</span></p><p>it is a BMD with drift parameter <span class="math inline">\(\mu -\sigma^2/2\)</span> and scale parameter <span class="math inline">\(\sigma\)</span>. Thus, the geometric Brownianmotion is just the exponential of this BMD process.</p><ul><li><p>Here <span class="math inline">\(X(0) = 1\)</span>, the processstarts at 1. For GBM starting at <span class="math inline">\(X(0) =x_0\)</span>, the process is <span class="math display">\[X(t) = x_0 \exp[(\mu - \frac{\sigma^2}{2})t + \sigma Z(t)], \qquad t\geq 0.\]</span></p></li><li><p>GBM is not a Gaussian process.</p></li></ul><p>From the definietion of GBM (2), we can have the followingdifferential equation: <span class="math display">\[\begin{aligned}  \frac{\text{d}X}{\text{d} t} &amp;= \exp[(\mu - \frac{\sigma^2}{2})t +\sigma Z(t)][(\mu - \frac{\sigma^2}{2}) + \sigma\frac{\text{d}Z}{\text{d} t}] \\  &amp;= X [(\mu - \frac{\sigma^2}{2}) + \sigma\frac{\text{d}Z}{\text{d} t}] \\  &amp;= X \tilde{\mu} + \sigma X \frac{\text{d}Z}{\text{d} t},  \qquad(\tilde{\mu} := \mu - \frac{\sigma^2}{2})\end{aligned}\]</span> thus, Geometric Brownian motion <span class="math inline">\(X(t)\)</span> satisfies the stochasticdifferential equation <span class="math display">\[\begin{aligned}\frac{\text{d}X}{\text{d} t}  &amp;= X \tilde{\mu} + \sigma X\frac{\text{d}Z}{\text{d} t},  \\  \text{d}X &amp; = X \tilde{\mu} {\text{d} t} + \sigma X \text{d}Z.\end{aligned}\]</span></p><p>The second equation is the Black–Scholes model. In the Black–Scholesmodel, <span class="math inline">\(X(t)\)</span> is the stock price.</p><ul><li><a href="https://stats.libretexts.org/Bookshelves/Probability_Theory/Probability_Mathematical_Statistics_and_Stochastic_Processes_(Siegrist)/18%3A_Brownian_Motion/18.04%3A_Geometric_Brownian_Motion">GeometricBrownian motion</a></li></ul><h1 id="white-gaussian-processes">White Gaussian processes</h1><p>Definition. A white Gaussian noise (WGN) process <span class="math inline">\(W(t)\)</span> is a GP with</p><ol type="1"><li><p>zero mean: <span class="math inline">\(\mu(t) = \mathbb{E}[W(t)]= 0\)</span> for all <span class="math inline">\(t\)</span>.</p></li><li><p>Delta function antocorrelation: <span class="math inline">\(R(t_1, t_2) = \sigma^2 \delta(t_1 -t_2)\)</span>.</p></li></ol><p>Here the Dirac delta is often thought as a function that is 0everywhere and infinite at 0. <span class="math display">\[\delta(t) =\begin{cases}\infty, &amp; t=0 \\0, &amp; t\ne 0\end{cases}.\]</span> The Dirac delta is actually a distribution, a generalizationof functions, and it is defined through the integral of its product withan arbitrary function <span class="math inline">\(f(t)\)</span>. <span class="math display">\[\int_{a}^{b} f(t)\delta(t)=\begin{cases}f(0), &amp; a &lt; 0 &lt; b \\0, &amp; \text{otherwise}\end{cases}.\]</span></p><!-- since the autocorrelation function of W(t) is not really a function (it involves theDirac delta), WGN cannot model any real physical phenomena. Nonetheless, it is a convenientabstraction to generate processes that can model real physical phenomena. --><p>Properties of white Gaussian noise:</p><ol type="1"><li><p>For <span class="math inline">\(t_1 \ne t_2\)</span>, <span class="math inline">\(W(t_1)\)</span> and <span class="math inline">\(W(t_2)\)</span> are uncorrelated. <span class="math display">\[\mathbb{E}[W(t_1)W(t_2)] = R(t_1, t_2) = 0, \qquad t_1 \ne t_2.\]</span> This means <span class="math inline">\(W(t)\)</span> atdifferent times are independent.</p></li><li><p>WGN has infinite variance (large power). <span class="math display">\[\mathbb{E}[W^2(t)] = R(t, t) = \sigma^2 \delta(0) = \infty.\]</span></p></li></ol><ul><li>WGN is discontinuous almost everywhere.</li><li>WGN is unbounded and it takes arbitrarily large positive andnegative values at any finite interval.</li></ul><h2 id="white-gaussian-noise-and-brownian-motion">White Gaussian noiseand Brownian motion</h2><p>Remember that the Brownian motion is a solution to the differentialequation: <span class="math display">\[\frac{\text{d} X(t)}{\text{d}t} = W(t).\]</span> <strong>Why <span class="math inline">\(\frac{\text{d}X(t)}{\text{d}t}\)</span> is called white noise ?</strong></p><p>Proof. Assume <span class="math inline">\(X(t)\)</span> is theintegral of a WGN process <span class="math inline">\(W(t)\)</span>,i.e., <span class="math inline">\(X(t) = \int_{0}^{t} W(u) \text{d}u\)</span>.</p><p>Since integration is linear functional and <span class="math inline">\(W(t)\)</span> is a GP, <span class="math inline">\(X(t)\)</span> is also a GP.</p><p>A Gaussian process can be uniquely specified by its Mean valuefunction and Autocorrelation function.</p><ol type="1"><li>The mean function: <span class="math display">\[\mu(t) = \mathbb{E}[\int_{0}^{t} W(u) \text{d} u] = \int_{0}^{t}\mathbb{E} [W(u)] \text{d} u = 0.\]</span><br></li><li>The autocorrelation <span class="math inline">\(R_{X}(t_1,t_2)\)</span> with <span class="math inline">\(t_1 &lt; t_2\)</span>:<span class="math display">\[\begin{aligned}  R_{X}(t_1, t_2) &amp;= \mathbb{E}[(\int_{0}^{t_1} W(u_1) \text{d}u_1)(\int_{0}^{t_2} W(u_2) \text{d} u_2)] \\  &amp;= \mathbb{E}[\int_{0}^{t_1} \int_{0}^{t_2} W(u_1)  W(u_2)\text{d} u_1 \text{d} u_2] \\  &amp;= \int_{0}^{t_1} \int_{0}^{t_2} \mathbb{E}[W(u_1)  W(u_2)\text{d} u_1 \text{d} u_2] \\  &amp;= \int_{0}^{t_1} \int_{0}^{t_2} \sigma^2 \delta(u_1 - u_2)\text{d} u_1 \text{d} u_2 \\  &amp;= \int_{0}^{t_1} \int_{0}^{t_1} \sigma^2 \delta(u_1 - u_2)\text{d} u_1 \text{d} u_2 + \int_{0}^{t_1} \int_{t_1}^{t_2} \sigma^2\delta(u_1 - u_2) \text{d} u_1 \text{d} u_2 \\  &amp;= \int_{0}^{t_1} \int_{0}^{t_1} \sigma^2 \delta(u_1 - u_2)\text{d} u_1 \text{d} u_2 + 0\\  &amp;= \int_{0}^{t_1} \sigma^2 \delta(u_1 - u_1) \text{d} u_1  \\  &amp;= \int_{0}^{t_1} \sigma^2 \text{d} u_1 \\  &amp;= \sigma^2 t_1.\end{aligned}\]</span> If <span class="math inline">\(t_2 &lt; t_1\)</span>, we canobtain <span class="math inline">\(R_{X}(t_1, t_2) = \sigma^2t_2\)</span>. Thus, <span class="math inline">\(R_{X}(t_1, t_2) =\sigma^2 \min \{t_1, t_2\}\)</span>.</li></ol><p>The mean function and autocorrelation function are the same asBrownian motion!</p><p>Because a Gaussian process can be uniquely determined by its meanvalue function and autocorrelation function. We can conclude</p><ul><li>The integral of WGN is a Brownian motion process.</li><li>The derivative of Brownian motion is WGN.</li></ul><h1 id="reference">Reference</h1><p><a href="https://www.hajim.rochester.edu/ece/sites/gmateos/ECE440/Slides/block_5_stationary_processes_part_a.pdf">Gaussianprocess</a></p><p><a href="https://stats.libretexts.org/Bookshelves/Probability_Theory/Probability_Mathematical_Statistics_and_Stochastic_Processes_(Siegrist)/18%3A_Brownian_Motion/18.04%3A_Geometric_Brownian_Motion">GeometricBrownian motion</a></p><p><a href="http://www.columbia.edu/~ks20/FE-Notes/4700-07-Notes-GBM.pdf">GeometricBrownian motion</a></p><p><a href="https://www.seas.upenn.edu/~ese3030/homework/week_11/week_11_white_gaussian_noise.pdf">WhiteGaussian noise</a></p>]]></content>
      
      
      <categories>
          
          <category> Reading Group </category>
          
      </categories>
      
      
        <tags>
            
            <tag> SDE </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Ordinary Differential Equation</title>
      <link href="/2023/07/19/ODE/"/>
      <url>/2023/07/19/ODE/</url>
      
        <content type="html"><![CDATA[<h1 id="ordinary-differential-equation-ode">Ordinary DifferentialEquation (ODE)</h1><h2 id="the-defination-of-ode">The defination of ODE</h2><p>An ODE is an equation in which the unknown quantity is a function,and it also involves derivatives of the unknown function.</p><p>For example, the forced spring-mass system: <span class="math display">\[\frac{d^2 x(t)}{d t^2} + \gamma \frac{d x(t)}{dt} + v^2 x(t) =w(t).  \qquad (1)\]</span> In this equation:</p><ul><li><p><span class="math inline">\(v\)</span> and <span class="math inline">\(\gamma\)</span> are constants that determine theresonant angular velocity and damping of the spring.</p></li><li><p><span class="math inline">\(w(t)\)</span> is a given functionthat may or may not depend on time.</p></li><li><p>position variable <span class="math inline">\(x\)</span> iscalled dependent variable.</p></li><li><p>time <span class="math inline">\(t\)</span> is called independentvariable.</p></li><li><p>This equation is <strong>second order</strong>. It contains thesecond derivative and doesn't have higher-order terms.</p></li><li><p>This equation is <strong>linear</strong>. <span class="math inline">\(x(t)\)</span> is linearly. There is no terms like<span class="math inline">\(x^2(t), \log x(t)\)</span>...</p></li><li><p>This equation is <strong>inhomogeneous</strong>. Because itcontains forcing term <span class="math inline">\(w(t)\)</span>.</p></li></ul><h2 id="the-solution-to-ode">The solution to ODE</h2><p>It can be divided to two categories: - particular solution: afunction that satisfies the differential equation and does not containany arbitrary constants.</p><ul><li>general solution: a function that satisfies the differentialequation and contains free constants.</li></ul><p>To exactly solve the differential equation, it is necessary tocombine the general solution with some initial conditions (i.e., <span class="math inline">\(x(t_0)\)</span>, <span class="math inline">\(\frac{d x(t)}{dt} |_{t_0}\)</span>) or some other(boundary) conditions of the differential equation.</p><h2 id="different-formulation-of-ode">Different formulation of ODE</h2><p>It is common to omit the time <span class="math inline">\(t\)</span>,so equation (1) can also be writen is this form: <span class="math display">\[\frac{d^2 x}{d t^2} + \gamma \frac{d x}{dt} + v^2 x = w.  \]</span></p><p>Sometimes, time derivatives are also denoted with dots over thevariable, for example:</p><p><span class="math display">\[\ddot{x} + \gamma \dot x + v^2 x = w.  \qquad (2)\]</span></p><h2 id="state-space-form-of-the-differential-equation-first-order-vector-differential-equation">State-spaceform of the differential equation (first-order vector differentialequation)</h2><ul><li><p><strong>order</strong>: the order of a differential equation isthe order of the highest derivative that appears in theequation.</p></li><li><p>Order <span class="math inline">\(N\)</span> ODE can convert toOrder 1 vector ODE i.e., if we define a state variable <span class="math inline">\(\vec{x} = (x_1 = x, x_2 = \dot{x})\)</span>, thenwe can convert equation（2） to <span class="math display">\[\begin{pmatrix}\frac{d x_1 (t)}{dt} \\\frac{d x_2 (t)}{dt}\end{pmatrix}=\begin{pmatrix}0 &amp; 1 \\-v^2 &amp; -\gamma\end{pmatrix}\begin{pmatrix}x_1 (t) \\x_2 (t)\end{pmatrix} +\begin{pmatrix}0 \\1\end{pmatrix}w(t) \qquad (3)\]</span></p></li></ul>Define <span class="math inline">\(\frac{d \vec{x}(t)}{dt} =\begin{pmatrix} \frac{d x_1 (t)}{dt} \\ \frac{d x_2 (t)}{dt}\end{pmatrix},\)</span> <span class="math inline">\(f(\vec{x}(t)) =\begin{pmatrix} 0 &amp; 1 \\ -v^2 &amp; -\gamma \end{pmatrix}\begin{pmatrix} x_1 (t) \\ x_2 (t) \end{pmatrix}\)</span> and $ =<span class="math display">\[\begin{pmatrix}0 \\1\end{pmatrix}\]</span><p>. $</p><p>Equation (3) can be seen to be a special case of this form: <span class="math display">\[\frac{d \vec{x}(t)}{dt} = f(\vec{x}(t), t) + \mathbf{L}(\vec{x}(t),t)\vec{w}(t)\]</span> where the vector-valued function <span class="math inline">\(\vec{x}(t) \in R^D\)</span> is the state of thesystem. <span class="math inline">\(f(\cdot, \cdot)\)</span> and <span class="math inline">\(\mathbf{L}(\cdot, \cdot)\)</span> are arbitraryfunctions. <span class="math inline">\(\vec{w}(t)\)</span> is some(vector-valued) forcing function, driving function, or input to thesystem.</p><ul><li><p>The first-order vector differential equation representation of an<span class="math inline">\(n\)</span>th-order differential equation isoften called the state-space form of the differential equation.</p></li><li><p>The theory and solution methods for first-order vectordifferential equations are easier to analyze.</p></li><li><p>And, <span class="math inline">\(n\)</span> th order differentialequations can (almost) always be converted into equivalent <span class="math inline">\(n\)</span>-dimensional vector-valued first-orderdifferential equations.</p></li></ul><h2 id="linear-odes">Linear ODEs</h2><p>Equation (3) is also a special case of the <strong>lineardifferential equations</strong>: <span class="math display">\[\frac{d \vec{x}(t)}{dt} = \mathbf{F}(t) \vec{x}(t) + \mathbf{L}\vec{w}(t).\]</span> where <span class="math inline">\(\mathbf{F}(t)\)</span> is amatrix-valued function of time. <span class="math inline">\(\mathbf{L}\)</span> is a matrix. <span class="math inline">\(\vec{w}(t)\)</span> is a vector-valued function oftime.</p><ul><li><strong>homogeneous</strong>: the equation is homogeneous if theforcing function <span class="math inline">\(\vec{w}(t)\)</span> is zerofor all <span class="math inline">\(t\)</span>.</li><li><strong>time-invariant</strong>: the equation is time-invariant if<span class="math inline">\(\mathbf{F}(t)\)</span> is constant for all<span class="math inline">\(t\)</span>.</li></ul><p>In the following sections, we first start with the simple scalarlinear time-invariant homogeneous differential equation with fixedinitial condition at <span class="math inline">\(t = 0\)</span>. Then,we will consider the multidimensional generalization of this equation.Besides, we also consider the linear time-invariant inhomogeneousdifferential equations. Finally, we will consider more generaldifferential equations.</p><h4 id="solutions-of-linear-time-invariant-differential-equations">Solutionsof Linear Time-Invariant Differential Equations</h4><p>Consider the <strong>scalar</strong> linear<strong>homogeneous</strong> differential equation with fixed initialcondition at <span class="math inline">\(t = 0\)</span>: <span class="math display">\[\frac{d x(t)}{dt} = F x(t),  x(0) = \text{given}, \qquad (4)\]</span> where <span class="math inline">\(F\)</span> is aconstant.</p><p>This equation can be solved by separation of variables: <span class="math display">\[\frac{d x(t)}{x(t)} = F dt.\]</span> Integrating the left-hand side from <span class="math inline">\(x(0)\)</span> to <span class="math inline">\(x(t)\)</span>, and right-hand side from <span class="math inline">\(0\)</span> to <span class="math inline">\(t\)</span>, we get <span class="math display">\[\log \frac{x(t)}{x(0)} = F t.\]</span> Combining the initial condition, we get <span class="math display">\[x(t) = x(0) e^{F t}.\]</span></p><p>Another way to solve this equation is to by intergrating both sidesof equation (4) from <span class="math inline">\(0\)</span> to <span class="math inline">\(t\)</span>. We get <span class="math display">\[x(t) = x(0) + \int_0^t F x(\tau) d \tau.\]</span> The we can substitute the solution back into the right-handside of the equation, and we get <span class="math display">\[\begin{aligned}x(t) &amp;= x(0) + \int_0^t F x(\tau) d \tau \\&amp;= x(0) + \int_0^t F [x(0) + \int_0^{\tau} F x(\tau') d\tau'] d \tau \\&amp;= x(0) + F x(0) t +  \int_0^t \int_0^{\tau}F^2 x(\tau') d\tau' d \tau \\&amp;= x(0) + F x(0) t +  F^2 x(0) \frac{t^2}{2} + \int_0^t\int_0^{\tau} \int_0^{\tau'}F^3 x(\tau'')  d \tau''d \tau' d \tau \\&amp;= ...\\&amp;= x(0) + F x(0) t +  F^2 x(0) \frac{t^2}{2} + F^3 x(0)\frac{t^3}{6} + \cdots\\&amp;=  (1 + Ft + F^2 t^2 /2 + F^3 t^3 /3! + \cdots) x(0)\\&amp;=  e^{F t} x(0).  \qquad (\text{Taylor expansion})\end{aligned}\]</span></p><table style="width:26%;"><colgroup><col style="width: 26%"></colgroup><thead><tr class="header"><th style="text-align: left;">For the <strong>multidimensional</strong>linear <strong>homogeneous</strong> differential equation with fixedinitial condition at <span class="math inline">\(t = 0\)</span>: <span class="math display">\[\frac{d \vec{x}(t)}{dt} = \mathbf{F} \vec{x}(t),  \vec{x}(0) =\text{given}, \qquad (5)\]</span> where <span class="math inline">\(\mathbf{F}\)</span> is aconstant matrix.</th></tr></thead><tbody><tr class="odd"><td style="text-align: left;">Next, let's move to the linear<strong>time-invariant</strong> <strong>inhomogeneous</strong>differential equations: <span class="math display">\[\frac{d \vec{x}(t)}{dt} = \mathbf{F} \vec{x}(t) + \mathbf{L}\vec{w}(t),  \vec{x}(t_0) = \text{given}, \qquad (7)\]</span> where <span class="math inline">\(\mathbf{F}\)</span> is aconstant matrix, <span class="math inline">\(\mathbf{L}\)</span> is aconstant matrix, and <span class="math inline">\(\vec{w}(t)\)</span> isa vector-valued function of time.</td></tr><tr class="even"><td style="text-align: left;">First, we can move the <span class="math inline">\(\mathbf{F} \vec{x}(t)\)</span> to the left-handside of the equation and multify both sides with a integrating factor<span class="math inline">\(\exp(- \mathbf{F}t)\)</span>, and get <span class="math display">\[\begin{aligned}\exp(- \mathbf{F}t) \frac{d \vec{x}(t)}{dt} - \exp(-\mathbf{F}t)\mathbf{F} \vec{x}(t) = \exp(- \mathbf{F}t) \mathbf{L}\vec{w}(t),\end{aligned}\]</span> Since <span class="math inline">\(\frac{d}{dt} \exp(-\mathbf{F}t) \vec{x}(t) = \exp(- \mathbf{F}t) \frac{d \vec{x}(t)}{dt} -\exp(- \mathbf{F}t)\mathbf{F} \vec{x}(t)\)</span>, we can rewrite theabove equation as <span class="math display">\[\frac{d}{dt} \exp(- \mathbf{F}t) \vec{x}(t) = \exp(- \mathbf{F}t)\mathbf{L} \vec{w}(t).\]</span> Integrating both sides from <span class="math inline">\(t_0\)</span> to <span class="math inline">\(t\)</span>, we get <span class="math display">\[\exp(- \mathbf{F}t) \vec{x}(t) - \exp(- \mathbf{F}t_0) \vec{x}(t_0) =\int_{t_0}^t \exp(- \mathbf{F}\tau) \mathbf{L} \vec{w}(\tau) d \tau.\]</span> Then, we can get the solution of equation (7): <span class="math display">\[\vec{x}(t) = \exp(\mathbf{F}(t - t_0)) \vec{x}(t_0) + \int_{t_0}^t\exp(\mathbf{F}(t - \tau)) \mathbf{L} \vec{w}(\tau) d \tau. \qquad (8)\]</span></td></tr><tr class="odd"><td style="text-align: left;">### Solutions of General Linear ODEs Theprevious section only consider the linear time-invariant differentialequations(<span class="math inline">\(F\)</span> is a constant). In thissection, we will consider the general linear differential equations withtime-varying coefficients.</td></tr><tr class="even"><td style="text-align: left;">For the linear<strong>homogeneous</strong> <strong>time-varying</strong> differentialequation with fixed initial condition at <span class="math inline">\(t =t_0\)</span>: <span class="math display">\[\frac{d \vec{x}(t)}{dt} = \mathbf{F}(t) \vec{x}(t), \quad \vec{x}(t_0) =\text{given}, \qquad (9)\]</span> where <span class="math inline">\(\mathbf{F}(t)\)</span> is amatrix-valued function of time. We can not use the exponential matrix tosolve this equation, because the <span class="math inline">\(\mathbf{F}(t)\)</span> is a time-varying matrix.But the solution of this equation has a general form: <span class="math display">\[\vec{x}(t) = \mathbf{\Psi}(t, t_0) \vec{x}(t_0), \qquad (10)\]</span> where <span class="math inline">\(\mathbf{\Psi}(t,t_0)\)</span> is a matrix-valued function of time, and it is called the<strong>transition matrix</strong>. The transition matrix <span class="math inline">\(\mathbf{\Psi}(t, t_0)\)</span> satisfies thefollowing differential properties: <span class="math display">\[\begin{aligned}\frac{\partial \mathbf{\Psi}(\tau, t)}{\partial \tau} &amp;=\mathbf{F}(\tau) \mathbf{\Psi}(\tau, t), \\\frac{\partial \mathbf{\Psi}(\tau, t)}{\partial t} &amp;= -\mathbf{\Psi}(\tau, t) \mathbf{F}(t), \\\mathbf{\Psi}(\tau, t) &amp;= \mathbf{\Psi}(\tau, s) \mathbf{\Psi}(s, t)\qquad (\text{11}) \\\mathbf{\Psi}(t, \tau) &amp;= \mathbf{\Psi}^{-1}(\tau, t)  \\\mathbf{\Psi}(t, t) &amp;= \mathbf{I}.\end{aligned}\]</span> In most cases, the transition matrix <span class="math inline">\(\mathbf{\Psi}(t, t_0)\)</span> does not have aclosed-form solution.</td></tr></tbody></table><p>For the linear <strong>inhomogeneous</strong><strong>time-varying</strong> differential equation with fixed initialcondition at <span class="math inline">\(t = t_0\)</span>: <span class="math display">\[\frac{d \vec{x}(t)}{dt} = \mathbf{F}(t) \vec{x}(t) + \mathbf{L}(t)\vec{w}(t), \quad \vec{x}(t_0) = \text{given}, \qquad (12)\]</span> where <span class="math inline">\(\mathbf{F}(t)\)</span> is amatrix-valued function of time, <span class="math inline">\(\mathbf{L}(t)\)</span> is a matrix-valued functionof time, and <span class="math inline">\(\vec{w}(t)\)</span> is avector-valued function of time. The solution of this equation has ageneral form: <span class="math display">\[\vec{x}(t) = \mathbf{\Psi}(t, t_0) \vec{x}(t_0) + \int_{t_0}^t\mathbf{\Psi}(t, \tau) \mathbf{L}(\tau) \vec{w}(\tau) d \tau. \qquad(13)\]</span></p><p>When <span class="math inline">\(\mathbf{F}(t)\)</span> and <span class="math inline">\(\mathbf{L}(t)\)</span> is constant, the solutionof equation (7) is a special case of (13) and we can verfiy that the<span class="math inline">\(\Psi(t, t_0) = \exp(\mathbf{F}(t -t_0))\)</span> satisfies properties (10).</p><hr><p>Fourier tansforms and Laplace transforms are two useful methods tosolve inhomogeneous linear time-invariant ODE (Note that<strong>time-invariant</strong>).</p><h2 id="fourier-transforms">Fourier Transforms</h2><p>The Fourier transform of a function <span class="math inline">\(x(t)\)</span> is defined as: <span class="math display">\[X(i w) = \mathcal{F}[x(t)] = \int_{-\infty}^{\infty} x(t) \exp(-iwt)\text{d}t, \qquad (14)\]</span> where <span class="math inline">\(i\)</span> is the imaginaryunit.</p><p>The inverse Fourier transform of (14) is: <span class="math display">\[x(t) = \mathcal{F}^{-1}[X(i w)] = \frac{1}{2\pi} \int_{-\infty}^{\infty}X(iw) \exp(i w t) \text{d}t.\]</span></p><p>Some useful properties:</p><ul><li><p>differention: <span class="math inline">\(\mathcal{F}[\frac{\text{d}^n x(t)}{\text{d} t^n}]= (iw)^{n} \mathcal{F}[x(t)]\)</span>.</p></li><li><p>convolution: <span class="math inline">\(\mathcal{F}[x(t) \asty(t)] = \mathcal{F}[x(t)] \ast \mathcal{F}[y(t)]\)</span>, where theconvolution <span class="math inline">\(\ast\)</span> is defined as<span class="math display">\[x(t) \ast y(t) = \int_{-\infty}^{\infty} x(t - \tau)y(\tau) \text{d}\tau\]</span></p></li></ul><p><strong>If we want to use Fourier transform to solve ODEs, theinitial condition must be 0.</strong></p><p>Now, let's use Fourier transform to solve the linear<strong>time-invariant</strong> <strong>inhomogeneous</strong>differential equations: <span class="math display">\[\frac{d \vec{x}(t)}{dt} = \mathbf{F} \vec{x}(t) + \mathbf{L}\vec{w}(t),  \quad \vec{x}(t_0) = 0,\]</span> where <span class="math inline">\(\mathbf{F}\)</span> is aconstant matrix, <span class="math inline">\(\mathbf{L}\)</span> is aconstant matrix, and <span class="math inline">\(\vec{w}(t)\)</span> isa vector-valued function of time.</p><p>Take Fourier tranforms componentwise give <span class="math display">\[(iw)\vec{X}(iw) = \mathbf{F} \vec{X}(iw) + \mathbf{L} \vec{W}(iw),\]</span> thus, we can get <span class="math display">\[\vec{X}(iw) = [(iw)\mathbf{I} - \mathbf{F}]^{-1} \ast \mathbf{L}\vec{W}(iw),\]</span> The solution is the inverse Fourier transform <span class="math display">\[x(t) = \mathcal{F}^{-1}[[(iw)\mathbf{I} - \mathbf{F}]^{-1} \ast\mathbf{L} \vec{W}(iw)] = \mathcal{F}^{-1}[((iw)\mathbf{I} -\mathbf{F})^{-1}]\ast \mathbf{L} \vec{w}(t), \qquad (15)\]</span></p><p>Since <span class="math inline">\(\vec{x}(t_0) = 0\)</span>, comparethe solution (15) with solution (8), we can obtain <span class="math display">\[\mathcal{F}^{-1}[((iw)\mathbf{I} - \mathbf{F})^{-1}] = \exp(\mathbf{F}t)u(t),\]</span> where <span class="math inline">\(u(t)\)</span> is theHeaviside step function, which is 0 for <span class="math inline">\(t&lt;0\)</span> and 1 for <span class="math inline">\(t \geq 0\)</span>.</p><h2 id="laplace-transforms">Laplace Transforms</h2><p>The Laplace transform of a function <span class="math inline">\(f(t)\)</span> is defined on space <span class="math inline">\(\{t | t\geq 0\}\)</span>: <span class="math display">\[F(s) = \mathcal{L}[f(t)] = \int_{0}^{\infty} f(t)\exp(-st) \text{d}t,\qquad (16)\]</span> where <span class="math inline">\(s = \sigma +iw\)</span>.</p><p>The inverse transform is <span class="math inline">\(f(t) =\mathcal{L}^{-1}[F(s)]\)</span>.</p><p>Remember that the Fourier transform needs the initial condition <span class="math inline">\(x(0) = 0\)</span>. But Laplace transform can takethe initial conditions into account. If <span class="math inline">\(x(0)= \text{given}\)</span>, then <span class="math display">\[\mathcal{L}[\frac{\text{d} x(t)}{\text{d} t}] = s \mathcal{L}[x(t)] -x(0) = s X(s) - x(0).\]</span></p><p><span class="math display">\[\mathcal{L}[\frac{\text{d}^n x(t)}{\text{d} t^n}] = s^n X(s) - s^{n-1}x(0) - \cdots - \frac{\text{d} x^{n-1}}{\text{d} t^{n-1}}(0).\]</span></p><p>Now, let's use Laplace transform to solve the linear<strong>time-invariant</strong> <strong>inhomogeneous</strong>differential equations: <span class="math display">\[\frac{d \vec{x}(t)}{dt} = \mathbf{F} \vec{x}(t) + \mathbf{L}\vec{w}(t),  \quad \vec{x}(t_0) = \text{given} \ne 0,\]</span> where <span class="math inline">\(\mathbf{F}\)</span> is aconstant matrix, <span class="math inline">\(\mathbf{L}\)</span> is aconstant matrix, and <span class="math inline">\(\vec{w}(t)\)</span> isa vector-valued function of time.</p><p>Take Laplace tranforms componentwise give <span class="math display">\[s X(s) - x(0) = \mathbf{F} X(s) + \mathbf{L} W(s).\]</span> Then, <span class="math display">\[X(s) = [s\mathbf{I}-\mathbf{F}]^{-1} x(0) +[s\mathbf{I}-\mathbf{F}]^{-1} \ast  \mathbf{L} W(s).  \qquad (17)\]</span></p><p>Compare the solution (17) with solution (8), we can obtain <span class="math display">\[\mathcal{L}^{-1}[(s\mathbf{I}-\mathbf{F})^{-1}] = \exp(\mathbf{F}t)\]</span> for <span class="math inline">\(t \geq 0\)</span>.</p><h1 id="reference">Reference</h1><p><a href="https://users.aalto.fi/~asolin/sde-book/sde-book.pdf">SimoSärkkä and Arno Solin (2019). Applied Stochastic Differential Equations.Cambridge University Press.</a></p>]]></content>
      
      
      <categories>
          
          <category> Reading Group </category>
          
      </categories>
      
      
        <tags>
            
            <tag> diffusion model </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Convergence of Markov Chain</title>
      <link href="/2023/07/15/Convergence%20of%20MC/"/>
      <url>/2023/07/15/Convergence%20of%20MC/</url>
      
        <content type="html"><![CDATA[<h1 id="total-variation-distance">Total Variation Distance</h1><p>Define Total Variation Distance: <span class="math display">\[d_{TV}(\mu(x), v(x)) = \frac{1}{2} \sum_{x \in \Omega} |\mu(x) - v(x)|\]</span> This is equivalent to the following: <span class="math display">\[d_{TV}(\mu(x), v(x)) = \sum_{x \in \Omega^{-}} (v(x) - \mu(x))= \sum_{x \in \Omega^{+}} (\mu(x) - v(x))\]</span> where <span class="math inline">\(\Omega^{+} = \{x \in \Omega:\mu(x) \geq v(x)\}\)</span>, <span class="math inline">\(\Omega^{-} =\{x \in \Omega: \mu(x) &lt; v(x)\}\)</span>, and <span class="math display">\[d_{TV}(\mu(x), v(x)) = \max_{S \subset \Omega} |\mu(S) - v(S)|\]</span> where <span class="math inline">\(\mu(S) = \sum_{x \in S}\mu(x)\)</span>, <span class="math inline">\(v(S) = \sum_{x \in S}v(x)\)</span>.</p><p>Proof: Define <span class="math inline">\(\Omega^{+} = \{x \in\Omega: \mu(x) \geq v(x)\}\)</span>, <span class="math inline">\(\Omega^{-} = \{x \in \Omega: \mu(x) &lt;v(x)\}\)</span>, then <span class="math display">\[\begin{aligned}d_{TV}(\mu(x), v(x)) &amp;= \frac{1}{2} \sum_{x \in \Omega} |\mu(x) -v(x)| \\&amp;= \frac{1}{2} \sum_{x \in \Omega^{+}} (\mu(x) - v(x)) + \frac{1}{2}\sum_{x \in \Omega^{-}} (v(x) - \mu(x))\end{aligned}\]</span> Since <span class="math inline">\(\sum_{x \in \Omega} \mu(x) =1 = \sum_{x \in \Omega^{+}} \mu(x) + \sum_{x \in \Omega^{-}}\mu(x)\)</span>, we have <span class="math display">\[\begin{aligned}d_{TV}(\mu(x), v(x)) &amp;= \frac{1}{2} \sum_{x \in \Omega^{+}} (\mu(x)- v(x)) + \frac{1}{2} \sum_{x \in \Omega^{-}} (v(x) - \mu(x)) \\&amp;= \frac{1}{2} \sum_{x \in \Omega^{-}} (v(x) - \mu(x)) + \frac{1}{2}\sum_{x \in \Omega^{-}} (v(x) - \mu(x)) \\&amp;= \sum_{x \in \Omega^{-}} (v(x) - \mu(x)) \\&amp;= \sum_{x \in \Omega^{+}} (\mu(x) - v(x))\end{aligned}\]</span> If <span class="math inline">\(S = \Omega^{+}\)</span> or<span class="math inline">\(\Omega^{-}\)</span>, then <span class="math display">\[\begin{aligned}d_{TV}(\mu(x), v(x)) &amp;= \frac{1}{2} \sum_{x \in \Omega} |\mu(x) -v(x)| \\&amp;=  \max_{S \in \Omega} \sum_{x \in S} |\mu(x) - v(x)| \\\end{aligned}\]</span></p><p>If <span class="math inline">\(S\)</span> contains any elements <span class="math inline">\(x \in \Omega^{-}\)</span>, then <span class="math inline">\(d_{TV}(\mu(x), v(x)) = |\sum_{x \in S} (\mu(x) -v(x))| \leq \sum_{x \in \Omega^{+}} (\mu(x) - v(x))\)</span> when <span class="math inline">\(S = \Omega^{+}\)</span>.</p><h1 id="convergence-of-markov-chain">Convergence of Markov Chain</h1><p>Assume we start from state <span class="math inline">\(x\)</span>,run Markov chain for <span class="math inline">\(t\)</span> steps, thenwe get the distribution <span class="math inline">\(P_{x}^{t}\)</span>.If we want to prove <span class="math inline">\(P_{x}^{t}\)</span>converges to stationary distribution <span class="math inline">\(\pi\)</span>, we need to prove <span class="math inline">\(d_{TV}(P_{x}^{t}, \pi) \rightarrow 0\)</span> as<span class="math inline">\(t \rightarrow \infty\)</span>. Thus, we needto bound <span class="math inline">\(d_{TV}(P_{x}^{t},\pi)\)</span>.</p><p>Define <span class="math inline">\(d_{x}(t) := d_{TV}(P_{x}^{t},\pi)\)</span>.</p><p>Consider all possible initial states <span class="math inline">\(x\in \Omega\)</span>, we define <span class="math inline">\(d(t) :=\max_{x \in \Omega} d_{TV} (P_{x}^{t}, \pi)\)</span>.</p><p>Assume there are two Markov chains <span class="math inline">\(x_{t}\sim P_{x}^{t}\)</span>, <span class="math inline">\(y_{t} \simP_{y}^{t}\)</span>. <span class="math inline">\(x_{t}\)</span> and <span class="math inline">\(y_{t}\)</span> share the same transitionprobability matrix <span class="math inline">\(P\)</span> but start fromdifferent states. Then we define <span class="math display">\[\bar{d}(t):= \max_{x, y \in \Omega} d_{TV}(P_{x}^{t}, P_{y}^{t})\]</span>.</p><p>Next, we will prove this Lemma: &gt; Lemma 1. &gt; <span class="math display">\[d(t) \leq \bar{d}(t) \leq 2 d(t).\]</span></p><p>Proof: Let's prove the second inequality <span class="math inline">\(\bar{d}(t) \leq 2 d(t)\)</span>. <span class="math display">\[\begin{aligned}\bar{d}(t) &amp;= \max_{x, y \in \Omega} d_{TV}(P_{x}^{t}, P_{y}^{t}) \\&amp;= \max_{x, y \in \Omega} \frac{1}{2} \sum_{z \in \Omega}|P_{x}^{t}(z) - P_{y}^{t}(z)| \\&amp; = \max_{x, y \in \Omega} \frac{1}{2} \sum_{z \in \Omega}|P_{x}^{t}(z) - \pi(z) + \pi(z) - P_{y}^{t}(z)| \\&amp;\leq \max_{x, y \in \Omega} [\frac{1}{2} \sum_{z \in \Omega}|P_{x}^{t}(z) - \pi(z)| + \frac{1}{2} \sum_{z \in \Omega} |\pi(z) -P_{y}^{t}(z)| ]\\&amp;= \max_{x \in \Omega} d_{TV}(P_{x}^{t}, \pi) + \max_{y \in \Omega}d_{TV}(P_{y}^{t}, \pi) \\&amp;= d(t) + d(t) \\&amp;= 2 d(t)\end{aligned}\]</span></p><p>For the first inequality <span class="math inline">\(\bar{d}(t) \leq2 d(t)\)</span>, we need to prove <span class="math inline">\(d(t) \leq\bar{d}(t)\)</span>.</p><p>Define <span class="math display">\[S_{x,y}^{*} = \arg\max_{S \subset \Omega} （P_{x}^{t}(S) -P_{y}^{t}(S)）\]</span></p><p><span class="math display">\[S_{x}^{**} = \arg\max_{S \subset \Omega, y \in \Omega} （P_{x}^{t}(S) -P_{y}^{t}(S)）\]</span></p><p><span class="math display">\[S_{x}^{*} = \arg\max_{S \subset \Omega} \sum_{y \in \Omega}\pi(y)（P_{x}^{t}(S) - P_{y}^{t}(S)）\]</span></p><p><span class="math display">\[\begin{aligned}d(t) &amp;= \max_{x \in \Omega} d_{TV}(P_{x}^{t}, \pi) \\&amp;= \max_{x \in \Omega} \max_{S \subset \Omega}|P_{x}^{t}(S) -\pi(S)| \\&amp;= \max_{x \in \Omega} \max_{S \subset \Omega^{+}}(P_{x}^{t}(S) -\pi(S))  \\&amp;= \max_{x \in \Omega} \max_{S \subset \Omega^{+}}(P_{x}^{t}(S) -\sum_{y \in \Omega} \pi(y) P_{y}^{t}(S))  \qquad  (\pi(y) = \sum_{x \in\Omega} \pi(x) P_{x,y}) \\&amp;= \max_{x \in \Omega} \max_{S \subset \Omega^{+}} \sum_{y \in\Omega} \pi(y) (P_{x}^{t}(S) - P_{y}^{t}(S)).\end{aligned}\]</span> Since for all <span class="math inline">\(S\)</span>: <span class="math inline">\(（P_{x}^{t}(S_{x,y}^{*}) -P_{y}^{t}(S_{x,y}^{*})）\geq （P_{x}^{t}(S) -P_{y}^{t}(S)）\)</span></p><p>we have, <span class="math display">\[\begin{aligned}d(t) &amp;= \max_{x \in \Omega} \max_{S \subset \Omega^{+}} \sum_{y \in\Omega} \pi(y) (P_{x}^{t}(S) - P_{y}^{t}(S)) \\&amp; \leq \max_{x \in \Omega} \sum_{y \in \Omega} \pi(y)(P_{x}^{t}(S_{x,y}^{*}) - P_{y}^{t}(S_{x,y}^{*})) \\&amp;= \max_{x \in \Omega} \sum_{y \in \Omega} \pi(y)\max_{S}  (P_{x}^{t}(S) - P_{y}^{t}(S)) \\&amp; \leq  \max_{x \in \Omega} \sum_{y \in \Omega}\pi(y)   (P_{x}^{t}(S_{x}^{**}) - P_{y}^{t}(S_{x}^{**})) \\&amp; = \max_{x \in \Omega} \sum_{y \in \Omega} \pi(y) \max_{y,S}(P_{x}^{t}(S) - P_{y}^{t}(S)) \\&amp;= \max_{x, y \in \Omega}\max_{ S \subset \Omega}(P_{x}^{t}(S) -P_{y}^{t}(S))   \qquad (\sum_{y \in \Omega} \pi(y) = 1) \\&amp;= \max_{x, y \in \Omega} d_{TV}(P_{x}^{t}, P_{y}^{t}) \\&amp;= \bar{d}(t).\end{aligned}\]</span> that is, <span class="math inline">\(d(t) \leq \bar{d}(t) \leq2d(t)\)</span>.</p><p><strong>What we have proved is that, <span class="math inline">\(d(t)\)</span> is bounded by <span class="math inline">\(\bar{d}(t)\)</span>, and <span class="math inline">\(\bar{d}(t)\)</span> is controlled by <span class="math inline">\(\max_{x,y \in \Omega} d_{TV}(P_{x}^{t},P_{y}^{t})\)</span>. Let's find a way to bound <span class="math inline">\(d_{TV}(P_{x}^{t},P_{y}^{t})\)</span>!</strong></p><blockquote><p>Define 1. (Coupling) Assume <span class="math inline">\(x , y \in\Omega\)</span>, <span class="math inline">\(x \sim \mu, y \simv\)</span>, <span class="math inline">\(\mu, v\)</span> are twodistributions. A joint distribution <span class="math inline">\(w(x,y)\)</span> on <span class="math inline">\(\Omega \times \Omega\)</span>is called <strong>couping</strong> if <span class="math inline">\(\forall x \in \Omega\)</span>, <span class="math inline">\(\sum_{y}w(x, y) = \mu(x)\)</span>, <span class="math inline">\(\forall y \in \Omega\)</span>, <span class="math inline">\(\sum_{x} w(x, y) = v(y)\)</span>.</p></blockquote><blockquote><p>Lemma 2. Consider <span class="math inline">\(\mu, v\)</span> definedon <span class="math inline">\(\Omega\)</span>,<br>(a) For any coupling <span class="math inline">\(w(x, y)\)</span> of<span class="math inline">\(\mu, v\)</span>, <span class="math inline">\(d_{TV}(\mu, v) \leq P(x \ne y)\)</span>.<br>(b) There always exists a coupling <span class="math inline">\(w(x,y)\)</span> of <span class="math inline">\(\mu, v\)</span> such that<span class="math inline">\(d_{TV}(\mu, v) = P(x \ne y)\)</span>.</p></blockquote><p>Proof: (a) <span class="math inline">\(\forall z\)</span>, <span class="math inline">\(w(z, z) \leq \sum_{y \in \Omega} w(z, y) =\mu(z)\)</span>. Similarly, <span class="math inline">\(w(z, z) \leqv(z)\)</span>. Thus, <span class="math inline">\(w(z, z) \leq\min(\mu(z), v(z))\)</span>.</p><p><span class="math display">\[\begin{aligned}P(x \ne y) &amp;= 1 - P(x = y) \\&amp;= 1 - \sum_{z \in \Omega} w(z, z) \\&amp;\geq 1 - \sum_{z \in \Omega} \min(\mu(z), v(z)) \\&amp;= \sum_{z \in \Omega} \mu(z) - \sum_{z \in \Omega} \min(\mu(z),v(z)) \\&amp;= \sum_{z \in \Omega^{+}} (\mu(z) - v(z)) \\&amp;= d_{TV}(\mu, v)\end{aligned}\]</span></p><ol start="2" type="a"><li>We can construct a coupling <span class="math inline">\(w(x,y)\)</span>: <span class="math display">\[w(x, y) = \left\{\begin{aligned}&amp; \min \{\mu(x), v(y)\}, \quad x = y \\&amp;\frac{(\mu(x) - w(x, x))(v(y) - w(y,y))}{1 - \sum_{z \in\Omega}w(z,z)}, \quad x \ne y\end{aligned}\right.\]</span> For this joint distribution, we have <span class="math display">\[\begin{aligned}P(x \ne y) &amp;= 1 - P(x = y) \\&amp;= 1 - \sum_{z \in \Omega} w(z, z) \\&amp;= 1 - \sum_{z \in \Omega} \min(\mu(z), v(z)) \\&amp;= \sum_{z \in \Omega} \mu(z) - \sum_{z \in \Omega} \min(\mu(z),v(z)) \\&amp;= \sum_{z \in \Omega^{+}} (\mu(z) - v(z)) \\&amp;= d_{TV}(\mu, v)\end{aligned}\]</span> Thus, this joint distribution <span class="math inline">\(w(x,y)\)</span> satisfies <span class="math inline">\(d_{TV}(\mu, v) = P(x\ne y)\)</span>. Now, we need to prove that this joint distribution<span class="math inline">\(w(x, y)\)</span> is a coupling of <span class="math inline">\(\mu, v\)</span>.</li></ol><p><span class="math inline">\(\forall x \in \Omega\)</span>, <span class="math display">\[\begin{aligned}\sum_{y \in \Omega} w(x, y) &amp;= \sum_{y=x} \min \{\mu(x), v(y)\} +\sum_{y \in \Omega, y \ne x} \frac{(\mu(x) - w(x, x))(v(y) - w(y,y))}{1- \sum_{z \in \Omega}w(z,z)} \\&amp;= w(x, x) + \frac{(\mu(x) - w(x, x))}{1 - \sum_{z \in\Omega}w(z,z)} \sum_{y \ne x} (v(y) - w(y,y)) \\&amp;= w(x,x) + \frac{(\mu(x) - w(x, x))}{1 - \sum_{z \in \Omega}w(z,z)}(1 - v(x) - (\sum_{z} w(z,z) - w(x,x)))  \qquad (\sum_{y \ne x} v(y) = 1- v(x))\\&amp;= w(x,x) + \frac{(\mu(x) - w(x, x))}{1 - \sum_{z \in \Omega}w(z,z)}(1 - \sum_{z} w(z,z) + w(x,x) - v(x) ) \qquad (\sum_{y \ne x} w(y,y) =\sum_{z} w(z,z) - w(x, x))\end{aligned}\]</span> If $ x ^{+} = {x | (x) v(x)}$, then <span class="math inline">\(w(x,x) = v(x)\)</span>, <span class="math inline">\(\sum_{y \in \Omega} w(x, y) = v(x) + (\mu(x) -v(x)) = \mu(x)\)</span>.</p><p>If $ x ^{-} = {x | (x) &lt; v(x)}$, then <span class="math inline">\(w(x,x) = \mu(x)\)</span>, <span class="math inline">\(\sum_{y \in \Omega} w(x, y) = \mu(x) + 0 =\mu(x)\)</span>. Thus, <span class="math inline">\(w(x, y)\)</span> is acoupling of <span class="math inline">\(\mu, v\)</span>.</p><p>Last but not least, let's begin to prove the nonincreasing propertyof <span class="math inline">\(d(t)\)</span>!! <strong>Almost close tothe end!! ^o/</strong></p><blockquote><p>Lemma 3. Consider two Markov chains <span class="math inline">\(x^{t}\sim P_{x}^{t}\)</span>, <span class="math inline">\(y^{t} \simP_{y}^{t}\)</span>, <span class="math inline">\(x^{t}\)</span> and <span class="math inline">\(y^{t}\)</span> share the same transitionprobability matrix <span class="math inline">\(P\)</span> but start fromdifferent states. If <span class="math inline">\(x^t = y^t\)</span>,then <span class="math inline">\(x^{t+1} = y^{t+1}\)</span>, elif <span class="math inline">\(x^t \ne y^t\)</span>, then <span class="math inline">\(x^{t+1} \ne y^{t+1}\)</span>, <span class="math inline">\(x^{t+1}\)</span> and <span class="math inline">\(y^{t+1}\)</span> are independent.</p></blockquote><blockquote><p>Define 2. (Coupling of Markov chains) Consider two Markov chains<span class="math inline">\(x^{t} \sim P_{x}^{t}\)</span>, <span class="math inline">\(y^{t} \sim P_{y}^{t}\)</span>, <span class="math inline">\(x^{t}\)</span> and <span class="math inline">\(y^{t}\)</span> share the same transitionprobability matrix <span class="math inline">\(P\)</span> but start fromdifferent states. If <span class="math display">\[P(y^{t+1} | x^{t}, y^{t}) = P(y^{t+1} | y^{t})\]</span> and <span class="math display">\[P(x^{t+1} | x^{t}, y^{t}) = P(x^{t+1} | x^{t})\]</span> then we say <span class="math inline">\(x^{t}\)</span> and<span class="math inline">\(y^{t}\)</span> are coupled.</p></blockquote><p>Define <span class="math inline">\(w^{t} := w^{t}(x^t, y^t)\)</span>is a coupling of <span class="math inline">\(P_{x}^{t},P_{y}^{t}\)</span>, <span class="math inline">\(x^t \sim P_x^t, y \simP_{y}^{t}\)</span>, and <span class="math inline">\(w^{t}\)</span>satisfies Lemma 2(b).</p><p><span class="math display">\[P_{w^{t}}(x^{t+1} \ne y^{t+1}) = P_{w^{t}}(x^{t+1} \ne y^{t+1} | x^t \ney^t) p(x^t \ne y^t) + P_{w^{t}}(x^{t+1} \ne y^{t+1} | x^t = y^t) p(x^t =y^t)\]</span> If <span class="math inline">\(x^t = y^t\)</span>, accordingto Lemma 3, <span class="math inline">\(P_{w^{t}}(x^{t+1} \ne y^{t+1}) =P_{w^{t}}(x^{t+1} = y^{t+1} | x^t = y^t) p(x^t = y^t) \leqP_{w^{t}}(x^{t} \ne y^t)\)</span>;</p><p>If <span class="math inline">\(x^t \ne y^t\)</span>, <span class="math inline">\(P_{w^{t}}(x^{t+1} \ne y^{t+1}) = P_{w^{t}}(x^{t+1}\ne y^{t+1} | x^t \ne y^t) p(x^t \ne y^t) \leq P_{w^{t}}(x^{t} \ney^t)\)</span>.</p><p><span class="math inline">\(d_x(t+1) = d_{TV}(P_{x}^{t+1},P_y^{t+1}） \leq P_{w^{t}}(x^{t+1} \ne y^{t+1}) \leq P_{w^{t}}(x^{t} \ney^t) = d_{TV}(P_x^t, P_y^{t}) = d_x(t)\)</span></p><p><span class="math inline">\(d_{x}(t) := d_{TV}(P_{x}^{t},\pi)\)</span>.</p><p><span class="math inline">\(d(t) = \max_{x \in \Omega}d_{x}(t)\)</span>.</p><p><span class="math inline">\(d(t) \leq \bar{d}(t) \leq2d(t)\)</span>.</p><p><span class="math inline">\(\bar{d}(t) := \max_{x, y \in \Omega}d_{TV}(P_{x}^{t}, P_{y}^{t})\)</span></p><p>???????????? Q: if we consider <span class="math inline">\(d_{TV}(P_x^t, \pi)\)</span>, then the coupling<span class="math inline">\(w^{t} := w^{t}(x^t, y)\)</span> should be acoupling of <span class="math inline">\(P_{x}^{t}, \pi\)</span>, <span class="math inline">\(x^t \sim P_x^t, y \sim \pi\)</span>.</p><p><span class="math inline">\(\pi\)</span> is different from <span class="math inline">\(P_y^t\)</span> at first.</p><p><span class="math display">\[d_x(t) = d_{TV}(P_{x}^{t}, \pi) \leq d(t) \leq \bar{d}(t) = \max_{x, y\in \Omega} d_{TV}(P_{x}^{t}, P_{y}^{t}) = P_{w^t}(x^t \ne y^t)\]</span></p><p>Now, we have already proved that <span class="math inline">\(d(t)\)</span> is nonincreasing. Next, we willprove that <span class="math inline">\(d(t)\)</span> converges to 0.</p><h1 id="useful-lectures">Useful Lectures:</h1><ul><li><a href="https://people.eecs.berkeley.edu/~sinclair/cs294/n7.pdf">L1</a></li><li><a href="https://courses.cs.duke.edu/spring13/compsci590.2/slides/lec5.pdf">MarkovChains and Coupling</a></li><li><a href="https://faculty.cc.gatech.edu/~vigoda/MCMC_Course/MC-basics.pdf">MarkovChains, Coupling, Stationary Distribution</a></li><li><a href="https://mpaldridge.github.io/math2750/S10-stationary-distributions.html">Stationarydistributions</a></li><li><a href="https://mpaldridge.github.io/math2750/S11-long-term-chains.html">Long-termbehaviour of Markov chains</a></li></ul>]]></content>
      
      
      <categories>
          
          <category> Reading Group </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Markov Chains </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Metropolis-Hastings</title>
      <link href="/2023/07/15/Metropolis-Hastings/"/>
      <url>/2023/07/15/Metropolis-Hastings/</url>
      
        <content type="html"><![CDATA[<h1 id="introduction">Introduction</h1><p>The first MCMC algorithm is the Metropolis algorithm, published byMetropolis et al. (1953). It was generalized by Hastings (1970) andPeskun (1973, 1981) towards statistical applications. After a long time,it was rediscovered by Geman and Geman (1984), Tanner and Wong (1987),and Gelfand and Smith (1990).</p><p>Assume the probability density <span class="math inline">\(\pi\)</span> is the target distribution and <span class="math inline">\(q(\cdot | \cdot)\)</span> is the proposaltransition distribution. From an initial state <span class="math inline">\(X_0\)</span>, the Metropolis-Hastings algorithmaims to generate a Markov chain <span class="math inline">\(\{X_1, X_2,...\}\)</span>, such that <span class="math inline">\(X_t\)</span>converges to distribution <span class="math inline">\(\pi\)</span>.</p><h1 id="metropolis-hastings-algorithm">Metropolis-Hastingsalgorithm</h1><p>Input: initial value <span class="math inline">\(X_0\)</span>,transition <span class="math inline">\(q(\cdot | \cdot)\)</span>, numberof iterations <span class="math inline">\(T\)</span>.</p><p>Output: Markov chain <span class="math inline">\(\{X_1, X_2, ...,X_T\}\)</span>.</p><p>For <span class="math inline">\(t = 1, 2, ..., T\)</span>:</p><ol type="1"><li>Generate <span class="math inline">\(u\)</span> from uniformdistribution <span class="math inline">\(U(0, I)\)</span>.</li><li>Generate <span class="math inline">\(X\)</span> from <span class="math inline">\(q(X | X_{t-1})\)</span>.</li><li>Compute the acceptance probability <span class="math inline">\(A(X,X_{t-1}) = \min\{1, \frac{\pi(X)q(X_{t-1} | X)}{\pi(X_{t-1})q(X |X_{t-1})}\}\)</span>.</li><li>if <span class="math inline">\(u \leq A(X, X_{t-1})\)</span>, then<span class="math inline">\(X_t = X\)</span>; else <span class="math inline">\(X_t = X_{t-1}\)</span>.</li></ol><p>Now, we prove that <span class="math inline">\(\pi\)</span> is one ofthe stationary distribution of the generated Markov chain.</p><p>Proof: Recall the detailed balance equation, for any <span class="math inline">\(i, j \in \Omega\)</span>, we have <span class="math display">\[\pi_i P_{i,j} = \pi_j P_{j,i},\]</span> then <span class="math inline">\(\pi\)</span> is a stationarydistribution of the Markov chain.</p><p>For the Metropolis-Hastings algorithm, we have the transitionprobability <span class="math display">\[K(X_t | X_{t-1}) = q(X_t| X_{t-1}) A(X_t, X_{t-1}) + \delta(X_t =X_{t-1}) (1 - \sum_{X \in \Omega} q(X|X_{t-1}) A(X, X_{t-1})).\]</span></p><p>Now we need to prove <span class="math display">\[\pi(X_{t-1})K(X_t | X_{t-1}) = \pi(X_t)K(X_{t-1} | X_t).\]</span></p><p>If <span class="math inline">\(X_t = X_{t-1}\)</span>, then theequation holds. If <span class="math inline">\(X_t \neqX_{t-1}\)</span>, then <span class="math display">\[\pi(X_{t-1})K(X_t | X_{t-1}) = \pi(X_{t-1})q(X_t| X_{t-1}) A(X_t,X_{t-1}) = \min\{\pi(X_{t-1})q(X_t| X_{t-1}), \pi(X_t)q(X_{t-1} |X_t)\}.\]</span> and <span class="math display">\[\pi(X_{t})K(X_{t-1} | X_{t}) = \pi(X_{t})q(X_{t-1}| X_{t}) A(X_{t-1},X_{t}) = \min\{\pi(X_{t})q(X_{t-1}| X_{t}), \pi(X_{t-1})q(X_{t} |X_{t-1})\}.\]</span> So, <span class="math inline">\(\pi(X_{t-1})K(X_t | X_{t-1}) =\pi(X_{t})K(X_{t-1} | X_{t})\)</span>. <span class="math inline">\(\pi\)</span> is a stationary distribution of theMarkov chain.</p><hr><p>Remark 1. The detailed balance condition is a sufficient but notnecessary condition for <span class="math inline">\(\pi\)</span> to be astationary distribution of the Markov chain. If we want to prove <span class="math inline">\(\pi\)</span> is the unique stationary distributionof the Markov chain, we need to prove the Markov chain is<strong>irreducible and positive recurrent</strong>. 【？？？】</p><p>Remark 2. The first initial state <span class="math inline">\(X_0\)</span> is randomly generated and usuallyremoved from the sample as burn-in or warm-up.</p><p>Q: <strong>Since it is recurrent, it must return to the initialvalues. Will this initial be rejected with a highprobability?</strong></p><p>Remark 3. In practice, the performances of the algorithm areobviously highly dependent on the choice of the transition <span class="math inline">\(q(\cdot | \cdot)\)</span>, since some choices seethe chain unable to converge in a manageable time.</p><p>Remark 4. We need to able to evaluate a function <span class="math inline">\(p(x) \propto \pi(x)\)</span>. Since we only needto compute the ratio <span class="math inline">\(\pi(y)/\pi(x)\)</span>,the proportionality constant is irrelevant. Similarly, we only careabout <span class="math inline">\(q(\cdot | \cdot)\)</span> up to aconstant</p><h1 id="reference">Reference</h1><ul><li>C.P. Robert. (2016). The Metropolis-Hastings algorithm.</li></ul>]]></content>
      
      
      <categories>
          
          <category> Reading Group </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Markov Chains </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Introduction about diffusion model</title>
      <link href="/2023/07/12/diffusion-model/"/>
      <url>/2023/07/12/diffusion-model/</url>
      
        <content type="html"><![CDATA[<h1 id="introduction-about-diffusion-model">Introduction about diffusionmodel</h1><p>For detailed information, please refer to <a href="https://lilianweng.github.io/posts/2021-07-11-diffusion-models/">Blog</a></p><h2 id="ddpm-denoising-diffusion-probabilistic-models">DDPM: DenoisingDiffusion Probabilistic Models</h2><p>diffusion model is a generative model <img src="/2023/07/12/diffusion-model/image.png"></p><ul><li>diffusion process: add noise to a real image, finally we get a noiseimage.</li><li>reverse process: from noise image to generate real image.</li></ul><ol type="1"><li><p>training phase from a real image datasets ---&gt; throughdiffusion process ---&gt; noise images ---&gt; through reverse process---&gt; real images</p></li><li><p>inference phase</p></li></ol><p>sampling noise images from a gaussian distribution, then use thepre-trained reverse process to generate images.</p><h2 id="compare-with-baselines">Compare with baselines</h2><p>GAN: need the balance between generator and discriminator, unstableand it's difficult to train.</p><h2 id="diffusion-process">Diffusion process</h2><p>add noise to a clean image <span class="math inline">\(X_0\)</span>and then we get noisy image <span class="math inline">\(X_1, X_2, ...,X_T\)</span>.</p><p>Now, we focus on the process from image <span class="math inline">\(X_{t-1}\)</span> to <span class="math inline">\(X_t\)</span>. <span class="math display">\[X_t = \sqrt {1 - \beta_t}X_{t-1} + \sqrt {\beta_t} Z_{t}, \quad Z_t \simN(0, I)\]</span> &gt;Remark: the noise scale <span class="math inline">\(\beta_t\)</span> will be increased gradually. $_t$ increases from <span class="math inline">\(10^{-4}\)</span> to <span class="math inline">\(2*10^{-2}\)</span> linearly. <span class="math inline">\(T = 2000\)</span>.</p><p>Let <span class="math inline">\(1 - \beta_t = \alpha_t\)</span>, thenwe have <span class="math display">\[X_t = \sqrt{\alpha_t}X_{t-1} + \sqrt{1 - \alpha_t} Z_t \\X_{t-1} = \sqrt{\alpha_{t-1}}X_{t-2} + \sqrt{1 - \alpha_{t-1}} Z_{t-1}\\\]</span> Combine these two equlities, we have <span class="math display">\[\begin{aligned}X_t &amp;= \sqrt{\alpha_t}X_{t-1} + \sqrt{1 - \alpha_t} Z_t \\&amp;= \sqrt{\alpha_t}(\sqrt{\alpha_{t-1}}X_{t-2} + \sqrt{1 -\alpha_{t-1}} Z_{t-1})+ \sqrt{1 - \alpha_t} Z_t \\&amp;= \sqrt{\alpha_t \alpha_{t-1}}X_{t-2} + \sqrt{\alpha_t(1 -\alpha_{t-1})} Z_{t-1}+ \sqrt{1 - \alpha_t} Z_t \\&amp; = \sqrt{\alpha_t \alpha_{t-1}}X_{t-2} + + \sqrt{1 - \alpha_t\alpha_{t-1}} Z,   \quad Z \sim N(0, I) \\&amp;= ... \\&amp;= \sqrt{\alpha_t \alpha_{t-1}...\alpha_1}X_{0} + + \sqrt{1 -\alpha_t \alpha_{t-1}... \alpha_{1}} Z \\&amp;= \sqrt{\bar{\alpha}_t}X_{0} + + \sqrt{1 - \bar{\alpha}_t}Z,   \qquad \bar{\alpha}_t = \prod_{i = 1}^{t}\alpha_i.\end{aligned}\]</span></p><h2 id="reverse-process">Reverse process</h2><h1 id="reference">Reference</h1><ul><li>Jonathan et al. Denoising Diffusion Probabilistic Models.NeurIPS2020.</li></ul>]]></content>
      
      
      <categories>
          
          <category> Reading Group </category>
          
      </categories>
      
      
        <tags>
            
            <tag> diffusion model </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Discrete Markov Chains</title>
      <link href="/2023/07/12/Markov-Chains/"/>
      <url>/2023/07/12/Markov-Chains/</url>
      
        <content type="html"><![CDATA[<h1 id="markov-chains-definitions-and-representations">Markov Chains:Definitions and Representations</h1><p>A stochastic process <span class="math inline">\(X = \{ x(t): t\inT\}\)</span> is a collection of random variables.</p><p>There are two elements:</p><ul><li>Time <span class="math inline">\(t\)</span>:<ul><li>discrete time (<span class="math inline">\(T\)</span> is a countablyinfinite set; under this case, we call 'Markov chain')</li><li>continuous time (under this case, we call 'Markov process')</li></ul></li><li>Space <span class="math inline">\(\Omega\)</span>:<ul><li>discrete space (<span class="math inline">\(X_{t}\)</span> comesfrom a countably infinite set)</li><li>continuous space.</li></ul></li></ul><p>Markov chain is a <strong>discrete-time</strong> process for whichthe future behaviour, given the past and the present, only depends onthe present and not on the past.</p><p>Markov process is the <strong>continuous-time</strong> version of aMarkov chain.</p><blockquote><p>Definition 1.[Markov chain] A discrete time stochastic process $ X_0,X_1, X_2, $. . . is a Markov chain if <span class="math display">\[P(X_{t} = a_t | X_{t-1} = a_{t-1}, X_{t-2} = a_{t-2}, ..., X_0 = a_0) =P(X_{t} = a_t | X_{t-1} = a_{t-1}) = P_{a_{t-1},  a_{t}}\]</span></p></blockquote><p>Remark 1: This is time-homogeneous markov chain, for <span class="math inline">\(\forall t\)</span>, for <span class="math inline">\(\forall a_{t-1}, a_{t} \in \Omega\)</span>, thetransition probability <span class="math inline">\(P_{a_{t-1}, a_{t}}\)</span> is the same.</p><p>Remark 2: In DDPM, it is not a time-homogeneous chain, as thetransition probability at t is obtained by a network(t).</p><p>The state <span class="math inline">\(X_{t}\)</span> depends on theprevious state <span class="math inline">\(X_{t-1}\)</span> but isindependent of the particular history <span class="math inline">\(X_{t-2}, X_{t-3},...\)</span>. This is called the<strong>Markov property</strong> or <strong>memorylessproperty</strong>.</p><p>The Markov property does not imply that <span class="math inline">\(X_{t}\)</span> is independent of the randomvariables <span class="math inline">\(X_{0}\)</span>, <span class="math inline">\(X_{1}\)</span>,..., <span class="math inline">\(X_{t-2}\)</span>; it just implies that <strong>anydependency of <span class="math inline">\(X_{t}\)</span> on the past iscaptured in the value of <span class="math inline">\(X_{t-1}\)</span></strong>.</p><p>The Markov chain is <strong>uniquely</strong> defined by the one-steptransition probability matrix P: <span class="math display">\[P =\begin{pmatrix}P_{0,0} &amp; P_{0, 1} &amp; \cdots &amp; P_{0, j} &amp; \cdots\\\vdots &amp; \vdots &amp; \ddots &amp; \vdots&amp; \vdots\\P_{i,0} &amp; P_{i, 1} &amp; \cdots &amp; P_{i, j} &amp; \cdots\\\vdots &amp; \vdots &amp; \ddots &amp; \vdots&amp; \vdots\\\end{pmatrix}\]</span> where <span class="math inline">\(P_{i,j}\)</span> is theprobability of transition from state <span class="math inline">\(i\)</span> to state <span class="math inline">\(j\)</span>. <span class="math inline">\(P_{i,j} =P(X_{t} = j| X_{t-1} = i), i,j \in \Omega\)</span>. For <span class="math inline">\(i\)</span>, <span class="math inline">\(\sum_{j\geq 0} P_{i,j} = 1\)</span>.</p><h1 id="classification-of-states">Classification of States</h1><p>For simplicity, we assume that the state space <span class="math inline">\(\Omega\)</span> is finite. ## Communicatingclass</p><blockquote><p>Definition 2. [Communicating class] A state <span class="math inline">\(j\)</span> is reachable from state <span class="math inline">\(i\)</span> if there exists a positive integer<span class="math inline">\(n\)</span> such that <span class="math inline">\(P_{i,j}^{(n)} &gt; 0\)</span>. We write <span class="math inline">\(i \rightarrow j\)</span>. If <span class="math inline">\(j\)</span> is reachable from <span class="math inline">\(i\)</span>, and <span class="math inline">\(i\)</span> is reachable from <span class="math inline">\(j\)</span>, then the states <span class="math inline">\(i\)</span> and <span class="math inline">\(j\)</span> are said to<strong>communicate</strong>, denoted by <span class="math inline">\(i\leftrightarrow j\)</span>. A communicating class <span class="math inline">\(C\)</span> is a <strong>maximal</strong> set ofstates that communicate with each other. <strong>No state in <span class="math inline">\(C\)</span> communicates with any state not in<span class="math inline">\(C\)</span>.</strong></p></blockquote><h2 id="irreducible">Irreducible</h2><blockquote><p>Definition 3: A Markov chain is <strong>irreducible</strong> if allstates belong to <strong>one</strong> communicating class.</p></blockquote><p>This means that <strong>any state can be reached from any otherstate</strong>. For <span class="math inline">\(\forall i, j \in\Omega\)</span>, <span class="math inline">\(P_{i,j} &gt;0\)</span>.</p><blockquote><p>Lemma 1. A finite Markov chain is irreducible if and only if itsgraph representation is a strongly connected graph.</p></blockquote><h3 id="transient-vs-recurrent-states">Transient vs Recurrentstates</h3><p>Let <span class="math inline">\(r_{i,j}^{t}\)</span> denote theprobability that the chain, starting at state <span class="math inline">\(i\)</span>, <strong>the first time</strong>transition to state <span class="math inline">\(j\)</span> occurs attime <span class="math inline">\(t\)</span>. That is, <span class="math display">\[r_{i,j}^{t} = P(X_{t} = j, X_{s} \neq j, \forall 1 \leq s \leq t-1 |X_{0} = i)\]</span></p><blockquote><p>Definition 4. A state is <strong>recurrent</strong> if <span class="math inline">\(\sum_{t \geq 1} r_{i,i}^{t} = 1\)</span> and it is<strong>transient</strong> if <span class="math inline">\(\sum_{t \geq1} r_{i,i}^{t} &lt; 1\)</span>. A Markov chain is recurrent if everystate in the chain is recurrent.</p></blockquote><ul><li><p>If state i is recurrent then, once the chain visits that state,it will (with probability 1) eventually return to that state. Hence thechain will visit state <span class="math inline">\(i\)</span> over andover again, <strong>infinitely</strong> often.</p></li><li><p>A transient state has the property that a Markov chain startingat this state returns to this state only <strong>finitelyoften</strong>, with probability 1.</p></li><li><p>If one state in a communicating class is transient (respectively,recurrent) then all states in that class are transient (respectively,recurrent).</p></li></ul><blockquote><p>Definition 5. An irreducible Markov chain is called recurrent if atleast one (equivalently, every) state in this chain is recurrent. Anirreducible Markov chain is called transient if at least one(equivalently, every) state in this chain is transient.</p></blockquote><p>Let <span class="math inline">\(\mu_{i} = \sum_{t \geq 1} t \cdotr_{i,i}^{t}\)</span> denote the expected time to return to state <span class="math inline">\(i\)</span> when starting at state <span class="math inline">\(i\)</span>.</p><blockquote><p>Definition 6. A state <span class="math inline">\(i\)</span> is<strong>positive recurrent</strong> if <span class="math inline">\(\mu_{i} &lt; \infty\)</span> and <strong>nullrecurrent</strong> if <span class="math inline">\(\mu_{i} =\infty\)</span>.</p></blockquote><p>Here we give an example of a Markov chain that has null recurrentstates. Consider the following markov chain whose states are thepositive integers.</p><figure><img src="/2023/07/12/Markov-Chains/image.png" alt="Fig. 1. An example of a Markov chain that has null recurrent states"><figcaption aria-hidden="true">Fig. 1. An example of a Markov chain thathas null recurrent states</figcaption></figure><p>Starting at state 1, the probability of not having returned to state1 within the first <span class="math inline">\(t\)</span> steps is <span class="math display">\[\prod_{j=1}^{t} \frac{j}{j+1} = \frac{1}{t+1}.\]</span> The probability of never returning to state 1 from state 1 is0, and state 1 is recurrent. Thus, the probability of the first timetransition to state <span class="math inline">\(1\)</span> occurs attime <span class="math inline">\(t\)</span> is <span class="math display">\[r_{1,1}^{t} = \frac{1}{t} \cdot \frac{1}{t+1} = \frac{1}{t(t+1)}.\]</span> The expected number of steps until the first return to state 1when starting at state 1 is <span class="math display">\[\mu_{1} = \sum_{t = 1}^{\infty} t \cdot r_{1,1}^{t} = \sum_{t =1}^{\infty} \frac{1}{t+1} = \infty.\]</span> State 1 is recurrent but null recurrent.</p><blockquote><p>Lemma 2. In a finite Markov chain: 1. at least one state isrecurrent; and 2. all recurrent states are positive recurrent.</p></blockquote><p>Thus, all states of a finite, irreducible Markov chain are positiverecurrent.</p><h3 id="periodic-vs-aperiodic-states">Periodic vs Aperiodic states</h3><blockquote><p>Definition 7. A state <span class="math inline">\(j\)</span> in adiscrete time Markov chain is <strong>periodic</strong> if there existsan integer <span class="math inline">\(k&gt;1\)</span> such that <span class="math inline">\(P(X_{t+s}= j | X_t = j) = 0\)</span> unless <span class="math inline">\(s\)</span> is divisible by <span class="math inline">\(k\)</span>. A discrete time Markov chain isperiodic if any state in the chain is periodic. A state or chain that isnot periodic is <strong>aperiodic</strong>.</p></blockquote><p>A state <span class="math inline">\(i\)</span> is periodic means thatfor <span class="math inline">\(s = k, 2k, 3k,...\)</span>, <span class="math inline">\(P(X_{t+s}= j | X_t = j) &gt; 0\)</span>.</p><p><strong>NB: k &gt; 1</strong></p><h3 id="ergodic">Ergodic</h3><blockquote><p>Definition 8. An <strong>aperiodic</strong>, <strong>positiverecurrent</strong> state is an <strong>ergodic</strong> state. A Markovchain is ergodic if all its states are ergodic.</p></blockquote><blockquote><p>Corollary 1. Any finite, irreducible, and aperiodic Markov chain isan ergodic chain.</p></blockquote><h3 id="stationary-distribution">Stationary distribution</h3><p>Consider the two-state “broken printer” Markov chain:</p><figure><img src="/2023/07/12/Markov-Chains/2023-07-22-11-00-52.png" alt="Transition diagram for the two-state broken printer chain"><figcaption aria-hidden="true">Transition diagram for the two-statebroken printer chain</figcaption></figure><p>There are two state (0 and 1) in this Markov chain, and assume thatthe initial distribution is <span class="math display">\[P(X_0 = 0) = \frac{\beta}{\alpha+\beta}, \qquad P(X_0 = 1) =\frac{\alpha}{\alpha+\beta}.\]</span> Then, according to the transition probability matrix <span class="math inline">\(P\)</span>, after one step, the distribution is<span class="math display">\[\begin{align*}P(X_1 = 0) &amp;= P(X_0 = 0)P(X_1 = 0 | X_0 = 0) + P(X_0 = 1)P(X_1 = 0 |X_0 = 1) \\&amp;= \frac{\beta}{\alpha+\beta} \cdot (1-\alpha) +\frac{\alpha}{\alpha+\beta} \cdot \beta = \frac{\beta}{\alpha+\beta}, \\P(X_1 = 1) &amp;= P(X_0 = 0)P(X_1 = 1 | X_0 = 0) + P(X_0 = 1)P(X_1 = 1 |X_0 = 1) \\&amp;= \frac{\beta}{\alpha+\beta} \cdot \alpha +\frac{\alpha}{\alpha+\beta} \cdot (1-\beta) =\frac{\alpha}{\alpha+\beta}.\end{align*}\]</span> Apparently, the distribution of <span class="math inline">\(X_1\)</span> is the same as the initialdistribution. Similarly, we can prove that the distribution of <span class="math inline">\(X_t\)</span> is the same as the initialdistribution for any <span class="math inline">\(t\)</span>. Here, <span class="math inline">\(\pi = (\frac{\beta}{\alpha+\beta},\frac{\alpha}{\alpha+\beta})\)</span> is called <strong>stationarydistribution</strong>.</p><blockquote><p>Definition 9. A probability distribution <span class="math inline">\(\pi = (\pi_i)\)</span>, <span class="math inline">\(\sum_{i \in \Omega} \pi_i = 1\)</span>(<strong>rowvector</strong>) on the state space <span class="math inline">\(\Omega\)</span> is called a <strong>stationarydistribution</strong> (or an equilibrium distribution) for the Markovchain with transition probability matrix <span class="math inline">\(P\)</span> if <span class="math inline">\(\pi =\pi P\)</span>, equivalently, <span class="math inline">\(\pi_j =\sum_{i \in \Omega}\pi_i P_{i,j}\)</span> for all <span class="math inline">\(j \in \Omega\)</span>.</p></blockquote><ul><li><p>One interpretation of the stationary distribution: if we startedoff a <strong>thousand</strong> Markov chains, choosing each startingposition to be state <span class="math inline">\(i\)</span> withprobability <span class="math inline">\(\pi_i\)</span>, then(roughly)<strong><span class="math inline">\(1000 \pi_j\)</span></strong> of themwould be in state <span class="math inline">\(j\)</span> at any time inthe future – but not necessarily the same ones each time.</p></li><li><p>If a chain ever reaches a stationary distribution then itmaintains that distribution for all future time, and thus a stationarydistribution represents a steady state or an equilibrium in the chain’sbehavior.</p></li></ul><h4 id="finding-a-stationary-distribution">Finding a stationarydistribution</h4><p>Consider the following no-claims discount Markov chain with statespace <span class="math inline">\(\Omega = \{1,2,3\}\)</span> andtransition matrix <span class="math display">\[P =\begin{pmatrix}\frac{1}{4} &amp; \frac{3}{4} &amp; 0\\\frac{1}{4} &amp; 0 &amp; \frac{3}{4}\\0 &amp; \frac{1}{4} &amp; \frac{3}{4}\end{pmatrix}\]</span></p><ul><li><p>Step 1: Assume $= {_1, _2, _3} $ is a stationary distribution.According to the definition 9 of stationary distribution, we need tosolve the following equations: <span class="math display">\[\begin{align*}\pi_1 &amp;= \frac{1}{4}\pi_1 + \frac{1}{4}\pi_2, \\\pi_2 &amp;= \frac{3}{4}\pi_1 + \frac{1}{4}\pi_3, \\\pi_3 &amp;= \frac{3}{4}\pi_2 + \frac{3}{4}\pi_3.\end{align*}\]</span> Adding the normalising condition <span class="math inline">\(\pi_1 + \pi_2 + \pi_3 = 1\)</span>, we get fourequations in three unknown parameters.</p></li><li><p>Step 2: Choose one of the parameters, say <span class="math inline">\(\pi_1\)</span>, and solve for the other twoparameters in terms of <span class="math inline">\(\pi_1\)</span>. Weget <span class="math display">\[\pi_1 = \frac{1}{4}\pi_1 + \frac{1}{4}\pi_2 \Rightarrow \pi_2 = 3\pi_1,\qquad \pi_3 = 3\pi_2 = 9\pi_1.\]</span></p></li><li><p>Step 3: Combining with the normalising condition, we get <span class="math display">\[\pi_1 + 3\pi_1 + 9\pi_1 = 1 \Rightarrow \pi_1 = \frac{1}{13}, \qquad\pi_2 = \frac{3}{13}, \qquad \pi_3 = \frac{9}{13}.\]</span> Finally, we get the stationary distribution <span class="math inline">\(\pi = (\frac{1}{13}, \frac{3}{13},\frac{9}{13})\)</span>.</p></li></ul><h4 id="existence-and-uniqueness">Existence and uniqueness</h4><p>Given a Markov chaine, how can we know whether it has a stationarydistribution? If it has, is it unique? At this part, we will answerthese questions.</p><p>Some notations: - Hitting time to hit the state <span class="math inline">\(j\)</span>: <span class="math inline">\(H_{j} =\min \{ t \in \{0, 1, 2,...\}: X_t = j\}\)</span>. Note that here weinclude time <span class="math inline">\(t = 0\)</span>.</p><ul><li>Hitting probability to hit the state <span class="math inline">\(j\)</span> staring from state <span class="math inline">\(i\)</span>: <span class="math inline">\(h_{i,j} =P(X_t = j, \text{for some} \ t \geq 0 | X_0 = i) = P(H_{j} &lt; \infty |X_0 = i) = \sum_{t \geq 0} r_{i,j}^{t}\)</span>.</li></ul><p>Note that this is different from <span class="math inline">\(r_{i,j}^{t}\)</span>, which denotes theprobability that the chain, starting at state <span class="math inline">\(i\)</span>, the <strong>first</strong> timetransition to state <span class="math inline">\(j\)</span><strong>occurs at time <span class="math inline">\(t\)</span></strong>.</p><p>We also have <span class="math display">\[h_{i,j} =\begin{cases}\sum_{k \in \Omega}P_{i,k}h_{k,j} &amp; , &amp; \text{if} \quad j \ne i,\\1 &amp; , &amp; \text{if} \quad  j = i.\end{cases}\]</span> - Expected hitting time: <span class="math inline">\(\eta_{i,j} = E(H_{j} | X_0 = i) = \sum_{t \geq 0}t \cdot r_{i,j}^{t}\)</span>. The expected time until we hit state <span class="math inline">\(j\)</span> starting from state <span class="math inline">\(i\)</span>. We also have <span class="math display">\[\eta_{i,j} =\begin{cases}1 + \sum_{k \in \Omega}P_{i,k}\eta_{k,j} &amp; , &amp; if j \ne i, \\0 &amp; , &amp; if j = i.\end{cases}\]</span> (For the first case, we add 1 because we need to consider thefirst step from state <span class="math inline">\(i\)</span> to state<span class="math inline">\(k\)</span>.)</p><ul><li>Return time: <span class="math inline">\(M_i = \min \{ t \in \{1,2,...\}: X_t = i\}\)</span>. It is different from <span class="math inline">\(H_{i}\)</span>, as we exclude time <span class="math inline">\(t = 0\)</span>. It is the first time that thechain returns to state <span class="math inline">\(i\)</span> after<span class="math inline">\(t = 0\)</span>.</li><li>Return probability: <span class="math inline">\(m_{i} = P(X_t = i \\text{for some} \ n \geq 1 | X_0 = i) = P(M_i &lt; \infty | X_0 = i) =\sum_{t&gt;1}r_{i,i}^{t}.\)</span></li><li>Expected return time: <span class="math inline">\(\mu_{i} = E(M_i |X_0 = i) = \sum_{t \geq 1} t \cdot r_{i,i}^{t}\)</span>. The expectedtime until we return to state <span class="math inline">\(i\)</span>starting from state <span class="math inline">\(i\)</span>. <span class="math display">\[m_{i} = \sum_{j \in \Omega} P_{i,j}h_{j,i},  \qquad \mu_{i} = 1 +\sum_{j \in \Omega} P_{i,j}\eta_{j,i}.\]</span></li></ul><table style="width:24%;"><colgroup><col style="width: 23%"></colgroup><tbody><tr class="odd"><td>&gt; Theorem 1. Consider an irreducible Markov chain (<strong>finiteor infinite</strong>), &gt; (1) if it is <strong>positiverecurrent</strong>, <span class="math inline">\(\exists\)</span> anunique stationary distribution <span class="math inline">\(\pi\)</span>,such that <span class="math inline">\(\pi_i =\frac{1}{\mu_{i}}\)</span>. &gt; (2) if it is <strong>nullrecurrent</strong> or <strong>transient</strong>, no stationarydistribution exists.</td></tr><tr class="even"><td>Remark: If the chain is <strong>finite</strong> irreducible, it mustbe positive recurrent, thus it has an unique stationarydistribution.</td></tr><tr class="odd"><td>Remark: If the Markov chain is not irreducible, we can decompose thestate space into several communicating classes. Then, we can considereach communicating class separately. - If none of the classes arepositive recurrent, then no stationary distribution exists. - If exactlyone of the classes is positive recurrent (and therefore closed), thenthere exists a unique stationary distribution, supported only on thatclosed class. - If more the one of the classes are positive recurrent,then many stationary distributions will exist.</td></tr><tr class="even"><td>Now, we give the proof of Theorem 1. We first prove that if a Markovchain is irreducible and positive recurrent, then there<strong>exists</strong> a stationary distribution. Next, we will provethe stationary distribution is <strong>unique</strong>. Since the secondpart with the null recurrent or transitive Markov chains is lessimportant and more complicated, we will omit it. If you are interestedin it, you can refer to the book <a href="https://www.statslab.cam.ac.uk/~james/Markov/">Markov Chains</a>by James Norris.</td></tr><tr class="odd"><td>Proof. (1) Suppose that <span class="math inline">\((X_0, X_1...)\)</span> a recurrent Markov chain, which can be positive recurrentor null recurrent. Then we can desigh a stationary distribution asfollows. (If we can desigh a stationary distribution, then it must beexisted.)</td></tr><tr class="even"><td>Let <span class="math inline">\(\nu_i\)</span> be the expectednumber of visits to <span class="math inline">\(i\)</span> before wereturn back to <span class="math inline">\(k\)</span>, <span class="math display">\[\begin{align*}\nu_i &amp;= \mathbb{E}(\# \text{visits to $i$ before returning to } k |X_0 = k) \\&amp;= \mathbb{E}\sum_{t=1}^{M_k} P(X_t = i | X_0 = k) \\&amp;= \mathbb{E}\sum_{t = 0}^{M_k - 1} P(X_t = i | X_0 = k)\end{align*}\]</span> The last equation holds because of $ P(X_0 = i | X_0 = k) = 0$and $ P(X_{M_k} = i | X_0 = k) = 0$.</td></tr><tr class="odd"><td>If we want design a stationary distribution, it must statisfy <span class="math inline">\(\pi P = \pi\)</span> and <span class="math inline">\(\sum_{i \in \Omega}\pi_i = 1\)</span>.</td></tr><tr class="even"><td>(a) We first prove that <span class="math inline">\(\nu P =\nu\)</span>. <span class="math display">\[\begin{align*}\sum_{i \in \Omega} \nu_i P_{i,j} &amp;= \mathbb{E}\sum_{i \in \Omega}\sum_{t = 0}^{M_k - 1} P(X_t = i, X_{t+1} = j | X_0 = k) \\&amp;= \mathbb{E}\sum_{t = 0}^{M_k - 1}  \sum_{i \in \Omega}  P(X_t = i,X_{t+1} = j | X_0 = k) \\&amp;=  \mathbb{E} \sum_{t = 0}^{M_k - 1} P(X_{t+1} = j | X_0 = k) \\&amp;= \mathbb{E} \sum_{t = 1}^{M_k } P(X_{t} = j | X_0 = k) \\&amp;= \mathbb{E} \sum_{t = 0}^{M_k - 1} \nu_i \\&amp;= \nu_j.\end{align*}\]</span> (b) Next, what we need to do is to normalize <span class="math inline">\(\nu\)</span> to get a stationary distribution. Wehave <span class="math display">\[\sum_{i \in \Omega} \nu_i = \sum_{i \in \Omega} \mathbb{E} \sum_{t =0}^{M_k - 1} P(X_t = i | X_0 = k) =\mathbb{E} \sum_{t = 0}^{M_k -1}  \sum_{i \in \Omega}  P(X_t = i | X_0 = k) = E(M_k | X_0 = i) =\mu_k.\]</span> Thus, we can define <span class="math inline">\(\pi_i =\nu_i/\mu_k\)</span>, <span class="math inline">\(\pi = \{\pi_i, i \in\Omega\}\)</span> is one of the stationary distribution.</td></tr><tr class="odd"><td>(2) Next, we prove that if a Markov chain is irreducible andpositive recurrent, then the stationary distribution is<strong>unique</strong> and is given by <span class="math inline">\(\pi_j = \frac{1}{\mu_j}\)</span>.</td></tr><tr class="even"><td>Given a stationary distribution <span class="math inline">\(\pi\)</span>, if we prove that for all <span class="math inline">\(i\)</span>, <span class="math inline">\(\pi_j ==\frac{1}{\mu_j}\)</span>, then we prove that the stationary distributionis unique.</td></tr><tr class="odd"><td>Remember that the expected hitting time: <span class="math display">\[\eta_{i,j} = 1 + \sum_{k \in \Omega}P_{i,k}\eta_{k,j},  j \ne i  \qquad(eq:1)\]</span> We multiply both sides of (eq:1) by <span class="math inline">\(\pi_i\)</span> and sum over <span class="math inline">\(i (i \ne j)\)</span> to get <span class="math display">\[\sum_{i \ne j} \pi_i \eta_{i,j} = \sum_{i \ne j} \pi_i + \sum_{i \ne j}\sum_{k \in \Omega} \pi_i P_{i,k}\eta_{k,j}\]</span> Since <span class="math inline">\(\eta_{j,j} = 0\)</span>, wecan rewrite the above equation as <span class="math display">\[\sum_{i \in \Omega} \pi_i \eta_{i,j} = \sum_{i \ne j} \pi_i + \sum_{i\ne j} \sum_{k \in \Omega} \pi_i P_{i,k}\eta_{k,j}. \qquad (eq:2)\]</span></td></tr><tr class="even"><td>(The above equality lacks <span class="math inline">\(j\)</span>,and we also want to design <span class="math inline">\(\pi_j =1/\mu_j\)</span>.) Remember that the expected return time: <span class="math display">\[ \mu_{j} = 1 + \sum_{i \in \Omega}P_{j,i}\eta_{i,j}. \qquad (eq:3) \]</span> We multiply both sides of(eq:2) by <span class="math inline">\(\pi_j\)</span> to get <span class="math display">\[\pi_j \mu_{j} =\pi_j +  \sum_{k \in \Omega} \pi_j P_{j,k}\eta_{k,j}\qquad (eq:4)\]</span> Adding (eq:2) and (eq:4), we get <span class="math display">\[\begin{align*}\sum_{i \in \Omega} \pi_i \eta_{i,j} + \pi_j \mu_{j} &amp;= \sum_{i \in\Omega} \pi_i + \sum_{i \in \Omega} \sum_{k \in \Omega} \pi_iP_{i,k}\eta_{k,j} \\&amp;= 1 + \sum_{k \in \Omega} \sum_{i \in \Omega}  \pi_iP_{i,k}\eta_{k,j} \\&amp;= 1 +  \sum_{k \in \Omega} \pi_k \eta_{k,j}  \qquad (\text{since}\sum_{i \in \Omega} \pi_i P_{i,k} = \pi_k) \\\end{align*}\]</span> <strong>Since the Markov chain is irreducible and positiverecurrent, that means all states belong to a communication class and theexpected return time of each state is finite. Thus, the space <span class="math inline">\(\Omega\)</span> is a finite dimensionalspace.</strong> We can substract <span class="math inline">\(\sum_{k \in\Omega} \pi_k \eta_{k,j}\)</span> and $_{i } <em>i </em>{i,j} $ (equal)from both sides of the above equation to get <span class="math display">\[\pi_j \mu_{j}=1,\]</span> which means <span class="math inline">\(\pi_j =1/\mu_j\)</span>. Similarly, we can prove that <span class="math inline">\(\pi_i = 1/\mu_i\)</span> for all <span class="math inline">\(i \in \Omega\)</span>.</td></tr></tbody></table><blockquote><p>Theorem 2 (Limit theorem) Consider an irreducible, aperiodic Markovchain (maybe infinite), we have <span class="math inline">\(\lim\limits_{t \to \infty} P_{i,j}^{t} =\frac{1}{\mu_{j}}\)</span>. Spectially, (1) Suppose the Markov chain ispositive recurrent. Then <span class="math inline">\(\lim\limits_{t \to\infty} P_{i,j}^{t} = \pi_j = \frac{1}{\mu_{j}}\)</span>. (2) Supposethe Markov chain is null recurrent or transient. Then there is no limiteprobability.</p></blockquote><ul><li>Three conditions for convergence to an equilibrium probabilitydistribution: irreducibility, aperiodicity, and positive recurrence. Thelimit probability <span class="math display">\[P =\begin{pmatrix}\pi_1 &amp; \pi_2 &amp; \cdots &amp; \pi_N\\\pi_1 &amp; \pi_2 &amp; \cdots &amp; \pi_N\\\vdots &amp; \vdots &amp; \ddots &amp; \vdots\\\pi_1 &amp; \pi_2 &amp; \cdots &amp; \pi_N\\\end{pmatrix}\]</span> where each row is identical.</li></ul><table style="width:7%;"><colgroup><col style="width: 6%"></colgroup><tbody><tr class="odd"><td>Define <span class="math inline">\(V_{i,j}^{t} = |\{ n &lt; t | X_n= j\}|\)</span>. <span class="math inline">\(V_{i,j}^{t}\)</span> is thenumber of visits to state <span class="math inline">\(j\)</span> beforetime <span class="math inline">\(t\)</span> starting from state <span class="math inline">\(i\)</span>. Then we can interpret <span class="math inline">\(V_{i,j}^{t}/t\)</span> as the proportion of timeup to time <span class="math inline">\(t\)</span> spent in state <span class="math inline">\(j\)</span>.</td></tr><tr class="even"><td>&gt; Theorem 3 [Ergodic theorem] Consider an irreducible Markovchain, we have <span class="math inline">\(\lim\limits_{t \to \infty}V_{i,j}^{t}/t = \frac{1}{\mu_{j}}\)</span> <strong>almostsurely</strong>. Spectially, &gt; (1) Suppose the Markov chain ispositive recurrent. Then <span class="math inline">\(\lim\limits_{t \to\infty} V_{i,j}^{t}/t = \pi_j = \frac{1}{\mu_{j}}\)</span><strong>almost surely</strong>. &gt; (2) Suppose the Markov chain isnull recurrent or transient. Then $ V_{i,j}^{t}/t $ <strong>almostsurely</strong> for all <span class="math inline">\(j\)</span>.</td></tr><tr class="odd"><td><strong>almost surely</strong> means that the convergenceprobability of the event is 1.</td></tr></tbody></table><blockquote><p>Theorem 4[Detailed balance condition]. Consider a finite,irreducible, and ergodic Markov chain with transition matrix <span class="math inline">\(P\)</span>. If there are nonnegative numbers <span class="math inline">\(\bar{\pi} = (\pi_0, \pi_1, ..., \pi_n)\)</span>such that <span class="math inline">\(\sum_{i=0}^{n} \pi_i = 1\)</span>and if, for any pair of states <span class="math inline">\(i,j\)</span>, <span class="math display">\[\pi_i P_{i,j} = \pi_{j} P_{j,i},\]</span> then <span class="math inline">\(\bar{\pi}\)</span> is thestationary distribution corresponding to <span class="math inline">\(P\)</span>.</p></blockquote><p>Proof. <span class="math display">\[\sum_{i} \pi_i P_{i,j} = \sum_{i}\pi_{j} P_{j,i} = \pi_{j}\]</span> Thus, <span class="math inline">\(\bar{\pi} =\bar{\pi}P\)</span>. Since this is a finite, irreducible, and ergodicMarkov chain, <span class="math inline">\(\bar{\pi}\)</span> must be theunique stationary distribution of the Markov chain.</p><p>Remark: Theorem 2 is a sufficient but not necessary condition.</p><h2 id="reference">Reference</h2><ul><li>Mitzenmacher, M., &amp; Upfal, E. (2005). Probability and Computing.Cambridge University Press.</li><li><a href="https://mpaldridge.github.io/math2750/S09-recurrence-transience.html">Recurrenceand transience</a></li><li><a href="https://mpaldridge.github.io/math2750/S07-classes.html">Classstructure</a></li><li><a href="https://mpaldridge.github.io/math2750/S10-stationary-distributions.html">Stationarydistributions</a></li><li>Stirzaker, David. <a href="https://www.ctanujit.org/uploads/2/5/3/9/25393293/_elementary_probability.pdf">ElementaryProbability</a></li></ul>]]></content>
      
      
      <categories>
          
          <category> Reading Group </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Markov Chains </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Gradient Descent, Stochastic Gradient Descent, Variance Reduction</title>
      <link href="/2021/02/27/GD/"/>
      <url>/2021/02/27/GD/</url>
      
        <content type="html"><![CDATA[<h1 id="svrg2013-accelerating-stochastic-gradient-descent-using-predictive-variance-reduction">[SVRG2013]Accelerating Stochastic Gradient Descent using Predictive VarianceReduction</h1><h2 id="introduction">Introduction</h2><p>考虑如下优化问题 <span class="math display">\[\min P(\omega) = \frac{1}{n}\psi_{i}(\omega)\]</span></p><ul><li>如果采用平方损失，最小二乘回归；</li><li>考虑正则项，令 <span class="math inline">\(\psi_{i}(\omega) = \ln(1+ \exp(-\omega^{T}x_{i}y_{i})) + 0.5\lambda\omega^{T}\omega, y_{i} \in\{-1,1\}\)</span>，regularized logistic regression。</li></ul><p>梯度下降算法更新过程： <span class="math display">\[\omega^{(t)} = \omega^{(t-1)} - \eta_{t}\nabla P(\omega^{(t-1)}) =\omega^{(t-1)} - \frac{\eta_{t}}{n}\sum_{i=1}^{n}\nabla\psi_{i}(\omega^{(t-1)})\]</span></p><p>但是，在每一步，GD都需要计算 <span class="math inline">\(n\)</span>个一阶偏导，计算量大。所以，一个改进就是随机梯度下降SGD：在每一步迭代时，随机从<span class="math inline">\(\{1,...,n\}\)</span> 中抽取 <span class="math inline">\(i_{t}\)</span>，然后 <span class="math display">\[\omega^{(t)} = \omega^{(t-1)} - \eta_{t}\nabla\psi_{i_{t}}(\omega^{(t-1)})\]</span></p><p>期望 <span class="math inline">\(E[\omega^{(t)}|\omega^{(t-1)}]\)</span>同梯度更新的结果一致。 SGD的更一般表达形式为 <span class="math display">\[\omega^{(t)} = \omega^{(t-1)} - \eta_{t}g_{t}(\omega^{(t-1)},  \xi_{t})\]</span></p><p><span class="math inline">\(\xi_{t}\)</span> 为一个依赖于 <span class="math inline">\(\omega^{(t-1)}\)</span> 的随机变量， 并且期望<span class="math inline">\(E[g_{t}(\omega^{(t-1)},\xi_{t})|\omega^{(t-1)}] = \nabla P(\omega^{(t-1)})\)</span>。</p><p>SGD的优势就是每步迭代只需要计算一个梯度，因此计算成本是GD的 <span class="math inline">\(\frac{1}{n}\)</span>。但是，SGD的一个缺点就是随机性引入了方差：虽然<span class="math inline">\(g_{t}(\omega^{(t-1)}, \xi_{t})\)</span>的期望等于梯度 <span class="math inline">\(\nablaP(\omega^{(t-1)})\)</span>，但是每个<span class="math inline">\(g_{t}(\omega^{(t-1)}, \xi_{t})\)</span>是不同的。方差的出现导致收敛速度变慢。对于SGD，由于随机取样带来的方差，一般都会要求其步长 <span class="math inline">\(\eta_{t} = O(1/t)\)</span>，从而得到一个sub-linear 收敛率 <span class="math inline">\(O(1/t)\)</span>。</p><ul><li>GD: 每步迭代计算慢，收敛快。</li><li>SGD：每步迭代计算快，收敛慢。</li></ul><p>为了改进SGD，一些学者开始设计算法以减少方差，从而可以使用较大的步长<span class="math inline">\(\eta_{t}\)</span>。有一些算法被提出来，比如：SAG(stochasticaveragegradient)，SDCA。但是这两个算法需要存储所有的梯度，一些情况下不太实际。因此作者提出了一个新的算法，该算法不需要存储所有的梯度信息，并且有较快的收敛速度，可以应用于非凸优化问题。</p><h2 id="stochastic-variance-reduced-gradient-svrg">Stochastic VarianceReduced Gradient (SVRG)</h2><p>为了保证收敛，SGD的步长必须衰减到0，从而导致收敛率变慢。需要较小步长的原因就是SGD的方差。作者提出一个解决方案。每进行<span class="math inline">\(m\)</span> 次SGD迭代后，记录当前参数 <span class="math inline">\(\tilde{\omega}\)</span> 以及平均梯度： <span class="math display">\[\tilde{\mu} = \nabla P(\tilde{\omega}) = \frac{1}{n}\sum_{i=1}^{n}\nabla \psi_{i}(\tilde{\omega}).\]</span></p><p>然后接下来的更新为：</p><p><span class="math display">\[\omega^{(t)} = \omega^{(t-1)} - \eta_{t}(\nabla\psi_{i_{t}}(\omega^{(t-1)}) - \nabla \psi_{i_{t}}(\tilde{\omega}) +\tilde{\mu})\]</span></p><p>注意到： <span class="math display">\[E[\omega^{(t)}|\omega^{(t-1)}] = \omega^{(t-1)} - \eta_{t}\nablaP(\omega^{(t-1)})\]</span></p><p>算法如下：</p><p><img src="/2021/02/27/GD/SVRG.png"></p><p>更新步骤中梯度的方差是减小的。当 <span class="math inline">\(\tilde{\omega}\)</span> 和 <span class="math inline">\(\omega^{(t)}\)</span> 收敛到最优参数 <span class="math inline">\(\omega_{*}\)</span>，<span class="math inline">\(\tilde{\mu} \to 0\)</span>， <span class="math inline">\(\nabla\psi_{i}(\tilde{\omega}) \to \nabla\psi_{i}(\omega_{*})\)</span>，有 <span class="math display">\[\nabla\psi_{i}(\omega^{(t-1)}) - \nabla\psi_{i}(\tilde{\omega}) + \tilde{\mu} \to \nabla\psi_{i}(\omega^{(t-1)}) -\nabla\psi_{i}(\omega_{*}) \to 0\]</span></p><p>SVRG的学习率不需要衰减，因此能有较快的收敛速度。作者提到，参数 <span class="math inline">\(m\)</span> 应该 <span class="math inline">\(O(n)\)</span>， 比如对于凸问题：<span class="math inline">\(m = 2n\)</span>，非凸问题：<span class="math inline">\(m = 2n\)</span>。</p><h1 id="saga2015-saga-a-fast-incremental-gradient-method-with-support-for-non-strongly-convex-composite-objectives">[SAGA2015]SAGA: A Fast Incremental Gradient Method With Support for Non-StronglyConvex Composite Objectives</h1><p>考虑最小化函数： <span class="math display">\[f(x) = \frac{1}{n}\sum_{i=1}^{n}f_{i}(x)\]</span></p><p>作者提出了一个叫做SAGA的算法，在目标函数为强凸函数时，SAGA的收敛速度优于SAG和SVRG。算法如下：</p><p><img src="/2021/02/27/GD/SAGA.png"></p><p>本文给出了variance reduction算法的一个解释： <img src="/2021/02/27/GD/SAGA1.png"></p><p>几种算法比较： <img src="/2021/02/27/GD/SAGA2.png"></p><h1 id="variance-reduced-stochastic-gradient-descent-with-neighbors-2015">[VarianceReduced Stochastic Gradient Descent with Neighbors 2015]</h1><h1 id="katyusha-2017-katyusha-the-first-direct-acceleration-of-stochastic-gradient-methods">[Katyusha2017] Katyusha: The First Direct Acceleration of Stochastic GradientMethods</h1><p>Nesterov's momentum通常用于加速梯度下降算法，但是，对于随机梯度下降，Nesterov's momentum可能无法对算法进行加速，即使优化目标为凸函数。因此，针对SGD，作者提出Katyusha 算法，借助于动量Katyusha momentum实现加速SGD。</p><p>考虑如下优化问题：</p><p><span class="math display">\[\min_{x \in \mathbb{R}^{d}} \{F(x) = f(x) + \psi(x) =\frac{1}{n}\sum_{i=1}^{n} f_{i}(x) + \psi(x)\}\]</span> 其中 <span class="math inline">\(f(x) =\frac{1}{n}\sum_{i=1}^{n} f_{i}(x)\)</span> 为凸函数，并且是 <span class="math inline">\(n\)</span> 个凸函数的有限平均。<span class="math inline">\(\psi(x)\)</span>为凸函数，可为近端函数。大多数假设 <span class="math inline">\(\psi(x)\)</span> 为 <span class="math inline">\(\sigma\)</span>-strongly，并且 <span class="math inline">\(f_{i}(x)\)</span> L-smooth。</p><p>作者提出一个可以求解上述优化问题的加速随机梯度下降算法-Katyusha：</p><p><img src="/2021/02/27/GD/ka.png"></p><p>其中，<span class="math inline">\(\tilde{x}\)</span> 为snapshotpoint，每经过 <span class="math inline">\(n\)</span>次迭代更新一次。<span class="math inline">\(\tilde{\nabla}_{k+1}\)</span> 为variance reduction中的梯度形式。<span class="math inline">\(\tau_{1}\)</span>，<span class="math inline">\(\tau_{2} \in[0,1]\)</span> 为两个动量参数，<span class="math inline">\(\alpha = \frac{1}{3\tau_{1}L}\)</span>。</p><h2 id="our-new-technique-katyusha-momentum">Our New Technique –Katyusha Momentum</h2><p>最novel的部分是 <span class="math inline">\(x_{k+1}\)</span>的更新步骤，是 <span class="math inline">\(y_{k}\)</span>, <span class="math inline">\(z_{k}\)</span> 以及 <span class="math inline">\(\tilde{x}\)</span> 的凸组合。理论建议参数 <span class="math inline">\(\tau_{2} = 0.5\)</span>，<span class="math inline">\(\tau_{1} = \min\{\sqrt{n\sigma/L},0.5\}\)</span>。</p><p>对于传统的加速梯度下降算法，<span class="math inline">\(x_{k+1}\)</span> 仅仅是 <span class="math inline">\(y_{k}\)</span> 和 <span class="math inline">\(z_{k}\)</span> 的凸组合，<span class="math inline">\(z_{k}\)</span>起到了一个“动量”的作用，即将历史加权的梯度信息添加到 <span class="math inline">\(y_{k+1}\)</span> 上。比如假设 <span class="math inline">\(\tau_{2} = 0, \tau_{1} = \tau\)</span>, <span class="math inline">\(x_{0} = y_{0} = z_{0}\)</span>，我们可以得到：</p><p><img src="/2021/02/27/GD/ka1.png"></p><p>由于 <span class="math inline">\(\alpha\)</span> 通常大于 <span class="math inline">\(1/3L\)</span>，上述递推过程意味着随着迭代进行，梯度<span class="math inline">\(\tilde{\nabla}_{t}\)</span>的贡献越高。比如，<span class="math inline">\(\tilde{\nabla}_{1}\)</span> 的权重不断增大 (<span class="math inline">\(\frac{1}{3L} &lt; ((1-\tau)\frac{1}{3L} +\tau\alpha) &lt; ((1 - \tau)^{2}\frac{1}{3L} + (1 - (1 -\tau)^{2})\alpha)\)</span>)。这就是一阶加速方法的核心思想。</p><p>在Katyusha算法中，作者认为 <span class="math inline">\(\tilde{x}\)</span> 同等重要，它能保证 <span class="math inline">\(x_{k+1}\)</span> 不要太远离 <span class="math inline">\(\tilde{x}\)</span>。<span class="math inline">\(\tilde{x}\)</span> 的添加可以看作是一个 “negativemomentum”，使 <span class="math inline">\(x_{k+1}\)</span> back to <span class="math inline">\(\tilde{x}\)</span>，抵消一部分前面迭代时的positive momentum”。</p><p>当 <span class="math inline">\(\tau_{1} = \tau_{2} = 0.5\)</span>时，Katyusha同SVRG几乎一致。</p><h1 id="l-svrg-l-katyusha-2019-dont-jump-through-hoops-and-remove-those-loops-svrg-and-katyusha-are-betterwithout-the-outer-loop">[L-SVRG,L-Katyusha 2019] Don’t Jump Through Hoops and Remove Those Loops: SVRGand Katyusha are BetterWithout the Outer Loop</h1><p>SVRG和Katyusha算法的共同关键结构就是两者都包含一个外层循环 (outerloop)。最初先在outerloop上使用所有样本计算梯度，然后计算出来的结果再用于内层循环 (innerloop)，结合新的随机梯度信息，构造variance-reduced梯度估计量。作者指出，由于SVRG和Katyusha算法都包括一个outerloop，所以存在一些问题，比如：算法很难分析；人们需要决定内部循环的次数。对于SVRG，理论上内部循环的最优次数取决于<span class="math inline">\(L\)</span>和 <span class="math inline">\(\mu\)</span>，但是 <span class="math inline">\(\mu\)</span>通常未知。由于这些问题存在，人们只能选择次优的inner loopsize，通常设置内部循环次数为 <span class="math inline">\(O(n)\)</span>或者 <span class="math inline">\(n\)</span>。</p><p>在这篇论文中，作者将外层循环 (outer loop)丢弃，在每次迭代时采用掷硬币技巧决定是否计算梯度，从而解决了上述问题。作者证明，新提出的算法和原始两个算法具有同样的理论性质。</p><p><img src="/2021/02/27/GD/loopless1.jpg"> <img src="/2021/02/27/GD/loopless1.jpg"></p><h1 id="l-svrg-l-katyusha-2019-l-svrg-and-l-katyusha-with-arbitrary-sampling">[L-SVRG,L-Katyusha 2019] L-SVRG and L-Katyusha with Arbitrary Sampling</h1><h1 id="参考文献">参考文献</h1><ul><li>Johnson, R. and Zhang, T. Accelerating stochastic gradient descentusing predictive variance reduction. In Advances in Neural InformationProcessing Systems 26, pp. 315–323, 2013a.<br></li><li>Defazio, A., Bach, F., and Lacoste-Julien, S. SAGA: a fastincremental gradient method with support for non-strongly convexcomposite objectives. In Advances in Neural Information ProcessingSystems, pp. 1646–1654, 2014.<br></li><li>Hofmann, T., Lucchi, A., Lacoste-Julien, S., and McWilliams, B.Variance reduced stochastic gradient descent with neighbors. In Advancesin Neural Information Processing Systems, pp.2305–2313, 2015.<br></li><li>Allen-Zhu, Z. Katyusha: The first direct acceleration of stochasticgradient methods. In Proceedings of the 49th Annual ACM SIGACT Symposiumon Theory of Computing, pp.1200–1205. ACM,2017.</li><li>Kovalev, D., Horváth, S., and Richtárik, P. Don’t jump through hoopsand remove those loops: SVRG and Katyusha are better without the outerloop. In Proceedings of the 31st International Conference on AlgorithmicLearning Theory, 2020.</li><li>Qian, X., Qu, Z., and Richtárik, P. L-SVRG and L-Katyusha witharbitrary sampling. arXiv preprint arXiv:1906.01481, 2019a.</li></ul>]]></content>
      
      
      <categories>
          
          <category> paper </category>
          
      </categories>
      
      
        <tags>
            
            <tag> GD </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>ADMMdecentralized</title>
      <link href="/2021/02/03/ADMMdecentralized/"/>
      <url>/2021/02/03/ADMMdecentralized/</url>
      
        <content type="html"><![CDATA[<ul><li><a href="#abstract">Abstract</a></li><li><a href="#introduction">Introduction</a></li><li><a href="#preliminaries">Preliminaries</a><ul><li><a href="#notations-and-problem-setting">Notations and ProblemSetting</a></li></ul></li><li><a href="#model-propagation">Model Propagation</a><ul><li><a href="#asynchronous-gossip-algorithm">Asynchronous GossipAlgorithm</a></li></ul></li><li><a href="#collaborative-learning">Collaborative Learning</a><ul><li><a href="#problem-formulation">Problem Formulation</a></li><li><a href="#asynchronous-gossip-algorithm-1">Asynchronous GossipAlgorithm</a></li></ul></li><li><a href="#experiments">Experiments</a><ul><li><a href="#collaborative-linear-classification">Collaborative LinearClassification</a></li></ul></li><li><a href="#参考文献">参考文献</a></li></ul><!-- /TOC --><h1 id="abstract">Abstract</h1><p>考虑点对点的协作网络。本文解决的问题是：每个节点如何与具有相似目标的其他节点进行通信来改善本地模型？作者介绍了两种完全去中心化的算法，一种是受标签传播的启发，旨在平滑预先训练好的局部模型；第二种方法，节点基于本地数据核相邻节点进行迭代更新来共同学习和传播。</p><h1 id="introduction">Introduction</h1><p>数据不断产生，当前从数据中提取信息的主要方式是收集所有用户的个人数据于一个服务器上，然后进行数据挖掘。但是，中心化的方式存在一些问题，比如说一些用户拒绝提供个人数据，带宽和设备花费问题。即使一些算法允许数据分布在用户设备上，通常需要中心端来进行聚合和协调。</p><p>在本文中，作者考虑完全去中心化的点对点网络。不同于那些求解全局模型的算法，本文关注于每个节点可以根据自身目标函数学习一个个性化模型。作者假设网络结构已知，该网络结构能够反映出不同节点的相似度（如果两个节点具有相似的目标函数，那么这两个节点在网络中是邻居），每个节点只知道与其直接相邻的节点。一个节点不仅可以根据自身数据学习模型，还可以结合它的邻居。假设每个节点只知道相邻节点的信息，不知道整个网络结构。</p><p>作者提出两个算法。第一个是 modelpropagation：首先，每个节点先基于自己的局部数据学习到模型参数，然后，结合整个网络结构，平滑这些参数。第二个是collaborativelearning，这个算法更加灵活，它通过优化一个模型参数正则化（平滑）和局部模型准确性上的折中问题。作者基于分布式的ADMM算法提出一个异步gossip算法。</p><h1 id="preliminaries">Preliminaries</h1><h2 id="notations-and-problem-setting">Notations and ProblemSetting</h2><p>考虑 <span class="math inline">\(n\)</span> 个节点 <span class="math inline">\(V = [n] = \{1,...,n\}\)</span>。凸的损失函数 <span class="math inline">\(l: \mathbb{R}^{p} \times \mathcal{X} \times\mathcal{Y}\)</span>，节点 <span class="math inline">\(i\)</span>的目标是学习模型参数 <span class="math inline">\(\theta_{i} \in\mathbb{R}^{p}\)</span>，使得关于未知分布 <span class="math inline">\(\mu_{i}\)</span> 的期望损失 <span class="math inline">\(E_{(x_{i}, y_{i})\sim \mu_{i}}l(\theta_{i}; x_{i},y_{i})\)</span> 很小。节点 <span class="math inline">\(i\)</span> 具有<span class="math inline">\(m_{i}\)</span> 个来自分布 <span class="math inline">\(\mu_{i}\)</span> 的 i.i.d 的训练样本 <span class="math inline">\(S_{i} = \{(x_{i}^{j},y_{i}^{j})\}_{j=1}^{m_{i}}\)</span>。允许不同节点的样本量相差很大。每个节点可以最小化局部损失函数得到<span class="math inline">\(\theta_{i}^{sol}\)</span>:</p><p><span class="math display">\[\theta_{i}^{sol} \in \argmin_{\theta \in \mathbb{R}^{p}} L_{i}(\theta) =\sum_{j=1}^{m_{i}} l(\theta;x_{i}^{j}, y_{i}^{j}).\]</span></p><p>我们目标是通过结合其他节点信息，进一步改善上述模型。考虑一个加权网络结构<span class="math inline">\(G = (V, E)\)</span>，具有 <span class="math inline">\(V\)</span> 个节点，<span class="math inline">\(E\subseteq V \times V\)</span> 为无向边。定义 <span class="math inline">\(W \in \mathbb{R}^{n \times n}\)</span> 为由 <span class="math inline">\(G\)</span> 得到的对称非负加权矩阵，如果 <span class="math inline">\((i,j) \ne E\)</span> or <span class="math inline">\(i = j\)</span>， <span class="math inline">\(W_{ij} =0\)</span>。本文假设权重矩阵已知。定义对角阵 <span class="math inline">\(D\in \mathbb{R}^{n \times n}\)</span>，<span class="math inline">\(D_{ii} = \sum_{j=1}^{n} W_{ij}\)</span>。节点<span class="math inline">\(i\)</span> 的邻域 ：<span class="math inline">\(\mathcal{N}_{i} = \{j \ne i: W_{ij} &gt;0\}\)</span>。</p><h1 id="model-propagation">Model Propagation</h1><p>假设每个节点通过最小化局部损失函数得到各自的模型 <span class="math inline">\(\theta_{i}^{sol}\)</span>。由于每个节点上的模型都是在不同大小数据集上考虑得到，作者使用<span class="math inline">\(c_{i} \in (0,1]\)</span>定义每个节点模型的可信度。 <span class="math inline">\(c_{i}\)</span>的值应该和节点 <span class="math inline">\(i\)</span>的样本量大小呈正相关，可以设置为 <span class="math inline">\(c_{i} =\frac{m_{i}}{\max_{j} m_{j}}\)</span>。如果 <span class="math inline">\(m_{i}=0\)</span>，可以设置为一个小量。</p><p>定义 <span class="math inline">\(\Theta = [\theta_{1};\theta_{2};...;\theta_{n}] \in \mathbb{R}^{n \timesp}\)</span>，我们要优化的目标函数为：</p><figure><img src="https://cdn.jsdelivr.net/gh/sugar-xue/BlogImages@main/Picbed1612018569.jpg" alt="1612018569"><figcaption aria-hidden="true">1612018569</figcaption></figure><p>第一项二次函数用来平滑相邻节点的参数，当两个节点间权重越大时，节点间参数越相近；第二项的目的是使具有较高置信度的模型的参数不要太远离各自模型上的参数。具有较低置信度的模型的参数被允许具有较大的偏差，容易被相邻节点影响。<span class="math inline">\(D_{ii}\)</span> 的目的是为了normalization。</p><p><img src="https://cdn.jsdelivr.net/gh/sugar-xue/BlogImages@main/Picbed1612018994(1).png" alt="1612018994(1)"><br><img src="https://cdn.jsdelivr.net/gh/sugar-xue/BlogImages@main/Picbed1612019022(1).png" alt="1612019022(1)"></p><p>计算 (4)需要知道整个网络的信息以及所有节点的独立模型信息，这对于节点而言是未知的，因为每个节点只知道相邻节点的信息。因此，作者提出下面的迭代形式：</p><figure><img src="https://cdn.jsdelivr.net/gh/sugar-xue/BlogImages@main/Picbed1612019342(1).png" alt="1612019342(1)"><figcaption aria-hidden="true">1612019342(1)</figcaption></figure><p>作者证明，无论初始值 <span class="math inline">\(\Theta(0)\)</span>取何值，上述迭代序列收敛到 (4)。(5) 式可以进一步分解为</p><figure><img src="https://cdn.jsdelivr.net/gh/sugar-xue/BlogImages@main/Picbed1612019480(1).png" alt="1612019480(1)"><figcaption aria-hidden="true">1612019480(1)</figcaption></figure><p>考虑一个同步计算：在每一步，每个节点都和其所有相邻节点进行通信，收集它们当前参数，然后使用它们的参数更新上式。同步更新会导致很大的延迟，因为任何节点都必须等剩余节点更新完后才能进行下一步更新。并且，每一步，所有节点都需要和其邻居节点进行通信，降低了算法的效率。所以作者提出一个异步算法。</p><h2 id="asynchronous-gossip-algorithm">Asynchronous GossipAlgorithm</h2><p>在异步设置中，每个节点都有一个局部clock ticking at times of rate 1Poisson process.由于节点都是独立同分布的，所以相当于在每一步时等概率激活每个节点。</p><p>在时间 <span class="math inline">\(t\)</span>时，每个节点都存有相邻节点的信息。以数学形式表示，考虑矩阵 <span class="math inline">\(\tilde{\Theta}_{i}(t) \in \mathbb{R}^{n \timesp}\)</span>，第 <span class="math inline">\(i\)</span> 行 <span class="math inline">\(\tilde{\Theta}_{i}^{i}(t) \in\mathbb{R}^{p}\)</span> 为节点 <span class="math inline">\(i\)</span>在时刻 <span class="math inline">\(t\)</span> 的模型参数，<span class="math inline">\(\tilde{\Theta}_{i}^{j}(t) \in \mathbb{R}^{p} (j\ne i)\)</span> 为节点 <span class="math inline">\(i\)</span>储存的关于邻居节点 <span class="math inline">\(j\)</span> 的lastknowledge. 对于 <span class="math inline">\(j \notin \mathcal{N}_{i}\bigcup \{i\}\)</span>，<span class="math inline">\(\forall t &gt;0\)</span>，<span class="math inline">\(\tilde{\Theta}_{i}^{j}(t) =0\)</span>。令<span class="math inline">\(\tilde{\Theta} =[\tilde{\Theta}_{1}^{T}, ...,\tilde{\Theta}_{n}^{T}] \in\mathbb{R}^{n^{2} \times p}\)</span>。</p><p>如果在时间 <span class="math inline">\(t\)</span> 时，节点 <span class="math inline">\(i\)</span> wakes up，执行如下步骤：</p><ul><li><p>communication: 节点 <span class="math inline">\(i\)</span>随机选择一个邻居节点 <span class="math inline">\(j \in\mathcal{N}_{i}\)</span>，(先验概率 <span class="math inline">\(\pi_{i}^{j}\)</span>)，节点 <span class="math inline">\(i\)</span> 和节点 <span class="math inline">\(j\)</span> 同时更新它们的参数： <span class="math display">\[\tilde{\Theta}_{i}^{j}(t+1) = \tilde{\Theta}_{j}^{j}(t) \qquad\tilde{\Theta}_{j}^{i}(t+1) = \tilde{\Theta}_{i}^{i}(t),\]</span></p></li><li><p>update: 基于当前信息，节点 <span class="math inline">\(i\)</span>和节点 <span class="math inline">\(j\)</span> 更新自己的模型参数： <span class="math display">\[\tilde{\Theta}_{l}^{l}(t+1) = (\alpha +\bar{\alpha}c_{l})^{-1}(\alpha\sum_{k \in \mathcal{N}_{l}}\frac{W_{lk}}{D_{ll}}\tilde{\Theta}_{l}^{k}(t+1) +\bar{\alpha}c_{l}\theta^{sol}_{l}) \quad(l \in \{i,j\}).\]</span></p><p>网络中的其他变量保持不变。作者提出的算法属于 gossipalgorithms，每个节点每次最多只和一个邻居节点通信。</p><p>作者证明，上述算法可以收敛到使每个节点具有最优参数。</p><figure><img src="https://cdn.jsdelivr.net/gh/sugar-xue/BlogImages@main/Picbed1612340701.jpg" alt="1612340701"><figcaption aria-hidden="true">1612340701</figcaption></figure></li></ul><h1 id="collaborative-learning">Collaborative Learning</h1><p>上述算法先在局部节点上进行学习，然后在进行网络通信。在这部分，作者提出了一个使节点可以同时进行基于局部数据和邻居节点信息更新模型参数的算法。相较于前面的算法，该算法通信成本较高，但是估计精度高于前者。</p><h2 id="problem-formulation">Problem Formulation</h2><p>优化目标：</p><figure><img src="https://cdn.jsdelivr.net/gh/sugar-xue/BlogImages@main/Picbed1612341031(1).png" alt="1612341031(1)"><figcaption aria-hidden="true">1612341031(1)</figcaption></figure><p>注意到，这里的置信度通过 <span class="math inline">\(\mathcal{L}_{i}\)</span> 体现，因为 <span class="math inline">\(\mathcal{L}_{i}\)</span> 为局部节点 <span class="math inline">\(i\)</span> 上所有观测的损失函数和。</p><p>一般情况下，上述问题没有解析解，作者提出一个分散式迭代算法进行求解。</p><h2 id="asynchronous-gossip-algorithm-1">Asynchronous GossipAlgorithm</h2><p>作者基于ADMM提出了一个异步分散式算法。本文的目的不是寻找一个consensus解，因为我们的目标是为了学习到每个节点的personalized model.作者通过将问题 (7)进行变换为一个部分consensus问题，使用ADMMD进行求解。</p><p>令 <span class="math inline">\(\Theta_{i} \in\mathbb{R}^{(|\mathcal{N_{i}}|+1)\times p}\)</span> 为变量 <span class="math inline">\(\theta_{j} \in \mathbb{R}^{p}(j \in\mathcal{N_{i}} \bigcup \{i\})\)</span> 的集合。定义 <span class="math inline">\(\theta_{j}\)</span> 为 <span class="math inline">\(\Theta_{i}^{j}\)</span>。优化问题(7)重新写为：</p><figure><img src="https://cdn.jsdelivr.net/gh/sugar-xue/BlogImages@main/Picbed1612341938(1).png" alt="1612341938(1)"><figcaption aria-hidden="true">1612341938(1)</figcaption></figure><p>在这个目标函数中，所有的节点相互依赖，因为它们共享一个优化变量 <span class="math inline">\(\in\Theta\)</span>。为了使用ADMM，需要将各个节点的优化变量独立，对于每个节点<span class="math inline">\(i\)</span>，定义一个local copy <span class="math inline">\(\tilde{\Theta}_{i} \in\mathbb{R}^{(|\mathcal{N_{i}}|+1)\times p}\)</span>，添加等式约束：<span class="math inline">\(\tilde{\Theta}_{i}^{i} =\tilde{\Theta}_{j}^{i}\)</span>，对于所有的 <span class="math inline">\(i \in [n], j \in \mathcal{N}_{i}\)</span>。</p><figure><img src="https://cdn.jsdelivr.net/gh/sugar-xue/BlogImages@main/Picbed1612342502(1).png" alt="1612342502(1)"><figcaption aria-hidden="true">1612342502(1)</figcaption></figure><p>增广拉格朗日乘子：</p><figure><img src="https://cdn.jsdelivr.net/gh/sugar-xue/BlogImages@main/Picbed1612342580(1).png" alt="1612342580(1)"><figcaption aria-hidden="true">1612342580(1)</figcaption></figure><p>算法如下，假设时刻 <span class="math inline">\(t\)</span> 时节点<span class="math inline">\(i\)</span> wakes up，选取邻居节点 <span class="math inline">\(j \in \mathcal{N}_{i}\)</span>，定义 <span class="math inline">\(e = (i,j)\)</span>，</p><figure><img src="https://cdn.jsdelivr.net/gh/sugar-xue/BlogImages@main/Picbed1612342728(1).png" alt="1612342728(1)"><figcaption aria-hidden="true">1612342728(1)</figcaption></figure><figure><img src="https://cdn.jsdelivr.net/gh/sugar-xue/BlogImages@main/Picbed1612342765(1).png" alt="1612342765(1)"><figcaption aria-hidden="true">1612342765(1)</figcaption></figure><h1 id="experiments">Experiments</h1><h2 id="collaborative-linear-classification">Collaborative LinearClassification</h2><p>考虑100个节点，每个节点的目标是建立一个线性分类模型 in <span class="math inline">\(\mathbb{R}^{p}\)</span>。为了方便可视化，每个节点的真实参数位于2维子空间：将其参数看作是<span class="math inline">\(\mathbb{R}^{p}\)</span>空间中的向量，前两项从正态分布中随机产生，剩余项为0。两个节点 <span class="math inline">\(i\)</span> 和节点 <span class="math inline">\(j\)</span> 的相似度通过参数距离的高斯核定义，定义<span class="math inline">\(\phi_{ij}\)</span>为两个真实参数在单位圆上投影的夹角，<span class="math inline">\(W_{ij} =\exp(\cos \phi_{ij} - 1)/\sigma\)</span>，<span class="math inline">\(\sigma =0.1\)</span>。权重为负值的将被忽略。每个节点具有随机的训练样本，样本的标签为二元标签，由线性分类模型产生。以概率0.05随机使标签反转，以产生噪音数据。每个节点的损失函数为hinge损失：<span class="math inline">\(l(\theta;(x_{i}, y_{i})) = \max(0,1-y_{i}\theta^{T}x_{i})\)</span>。作者评估了模型在100个测试样本上的预测精度。</p><figure><img src="https://cdn.jsdelivr.net/gh/sugar-xue/BlogImages@main/Picbed1612343843(1).png" alt="1612343843(1)"><figcaption aria-hidden="true">1612343843(1)</figcaption></figure><h1 id="参考文献">参考文献</h1><ul><li>Vanhaesebrouck, P., Bellet, A. &amp; Tommasi, M.. (2017).Decentralized Collaborative Learning of Personalized Models overNetworks. Proceedings of the 20th International Conference on ArtificialIntelligence and Statistics, in PMLR 54:509-517</li></ul>]]></content>
      
      
      <categories>
          
          <category> paper </category>
          
      </categories>
      
      
        <tags>
            
            <tag> FL </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Lower Bounds and Optimal Algorithms for Personalized Federated Learning</title>
      <link href="/2021/01/26/AL2SGD/"/>
      <url>/2021/01/26/AL2SGD/</url>
      
        <content type="html"><![CDATA[<ul><li><a href="#introduction">Introduction</a></li><li><a href="#contributions">Contributions</a></li><li><a href="#lower-complexity-bounds">Lower complexity bounds</a><ul><li><a href="#lower-complexity-bounds-on-the-communication">Lowercomplexity bounds on the communication</a></li><li><a href="#lower-complexity-bounds-on-the-local-computation">Lowercomplexity bounds on the local computation</a></li></ul></li><li><a href="#优化算法">优化算法</a><ul><li><a href="#accelerated-proximal-gradient-descent-apgd-for-federated-learning">AcceleratedProximal Gradient Descent (APGD) for Federated Learning</a></li><li><a href="#beyond-proximal-oracle-inexact-apgd-iapgd">Beyond proximaloracle: Inexact APGD (IAPGD)</a></li><li><a href="#accelerated-l2sgd">Accelerated L2SGD+</a></li></ul></li><li><a href="#experiments">Experiments</a></li><li><a href="#参考文献">参考文献</a></li></ul><!-- /TOC --><h1 id="introduction">Introduction</h1><p>作者在前文考虑了一个新的优化问题： <span class="math display">\[\min_{\mathbf{x_{1},...,x_{n}} \in \mathbb{R}^{d}} \{ F(\mathbf{x}) :=f(\mathbf{x}) + \lambda \psi(\mathbf{x})\}\]</span> <span class="math display">\[f(\mathbf{x}):= \frac{1}{n}\sum_{i=1}^{n}f_{i}(\mathbf{x_{i}}), \quad\psi(\mathbf{x}):= \frac{1}{2n}\sum_{i=1}^{n}\|\mathbf{x_{i}} -\mathbf{\bar{x}}\|^{2}\]</span></p><p>Remark：该问题的最优解 <span class="math inline">\(\mathbf{x}^{*} =[\mathbf{x}_{1}^{*},..., \mathbf{x}_{n}^{*}] \in\mathbb{R}^{nd}\)</span> 可以被表示为 <span class="math inline">\(\mathbf{x}^{*}_{i} = \mathbf{\bar{x}}^{*} -\frac{1}{\lambda}\nabla f_{i}(\mathbf{x}_{i}^{*})\)</span>，其中 <span class="math inline">\(\mathbf{\bar{x}}^{*} = \frac{1}{n}\sum_{i=1}^{n}\mathbf{x}_{i}^{*}\)</span>，该形式与MAML相似。</p><h1 id="contributions">Contributions</h1><p>在这篇论文中，作者给出了求解上述优化问题的通信和局部计算复杂度（迭代次数）的最低界限，并且给出了几种能够达到最低界限的算法。</p><ul><li><p>lower bound on the communication complexity.作者证明对于任意一个满足一定假设条件的算法，会有一个L-smooth, <span class="math inline">\(\mu\)</span>-strongly convex 局部目标函数 <span class="math inline">\(f_{i}\)</span> 至少需要通信 <span class="math inline">\(O(\sqrt{\frac{\min\{L,\lambda\}}{\mu}}\log\frac{1}{\epsilon})\)</span> 轮才能得到最优解 <span class="math inline">\(\epsilon\)</span>邻域内的解。</p></li><li><p>lower complexity bound on the number of local oracle calls.作者证明对于局部近端梯度下降，至少需要迭代<span class="math inline">\(O(\sqrt{\frac{\min\{L,\lambda\}}{\mu}}\log\frac{1}{\epsilon})\)</span>次；对于局部梯度下降，至少需要进行 <span class="math inline">\(O(\sqrt{\frac{L}{\mu}}\log\frac{1}{\epsilon})\)</span>次迭代；若每个目标函数为 <span class="math inline">\(m\)</span>个有限和形式（<span class="math inline">\(\tilde{L}\)</span>-smooth)，至少需要 <span class="math inline">\(O((m +\sqrt{\frac{m\tilde{L}}{\mu}})\log\frac{1}{\epsilon})\)</span>次。</p></li><li><p>作者讨论了不同的用于求解上述优化问题的算法，这些算法在不同设定下可以达到最优通信复杂度和最优局部梯度复杂度。。首先是加速近端梯度下降算法(APGD)，作者考虑两种不同的应用方式，第一种是：函数<span class="math inline">\(f\)</span> 采用梯度下降，<span class="math inline">\(\lambda \psi\)</span>采用近端梯度下降，第二种是反过来。对于第一种情况，当 <span class="math inline">\(L \leq \lambda\)</span>时，我们可以实现最优通信复杂度和局部梯度复杂度 <span class="math inline">\(O(\sqrt{\frac{L}{\mu}}\log\frac{1}{\epsilon})\)</span>；对于第二种情况，当<span class="math inline">\(L \geq \lambda\)</span>时，我们可以得到最优通信复杂度和局部近端复杂度 <span class="math inline">\(O(\sqrt{\frac{\lambda}{\mu}}\log\frac{1}{\epsilon})\)</span>。受一篇论文启发，作者提到局部近端可以由局部加速梯度下降 (Local AGD) 近似(inexactly) 得到，当目标函数为有限和形式，还可以采用Katyusha算法近似得到。Local AGD 可以得到 <span class="math inline">\(O(\sqrt{\frac{\lambda}{\mu}}\log\frac{1}{\epsilon})\)</span>的通信复杂度，以及 <span class="math inline">\(\tilde{O}(\sqrt{\frac{L+\lambda}{\mu}})\)</span>的局部梯度复杂度，当 <span class="math inline">\(L \geq\lambda\)</span>（取决于对数因子）时，两者都能达到最优。同样，当局部采用 Katyusha时，我们可以得到通信复杂度 <span class="math inline">\(O(\sqrt{\frac{\lambda}{\mu}}\log\frac{1}{\epsilon})\)</span>和局部梯度复杂度 <span class="math inline">\(\tilde{O}(m\sqrt{\frac{\lambda}{\mu}} + \sqrt{m\frac{\tilde{L}}{\mu}})\)</span>，前者当 <span class="math inline">\(L\geq \Lambda\)</span> 时能达到最优，后者当 <span class="math inline">\(m\lambda \leq \tilde{L}\)</span>（取决于对数因子）时达到最优。</p></li><li><p>作者提出了加速的L2SGD+算法-AL2SGD+，该算法可以实现最优通信复杂 度<span class="math inline">\(O(\sqrt{\frac{\min\{\tilde{L},\lambda\}}{\mu}}\log\frac{1}{\epsilon})\)</span>，以及局部梯度复杂度<span class="math inline">\(O((m + \sqrt{\frac{m(\tilde{L} +\lambda)}{\mu}})\log\frac{1}{\epsilon})\)</span>，当 <span class="math inline">\(\lambda \leq \tilde{L}\)</span>时最优。但是，两者无法同时实现最优。</p><p><img src="/2021/01/26/AL2SGD/t1.jpg"></p></li></ul><h1 id="lower-complexity-bounds">Lower complexity bounds</h1><h2 id="lower-complexity-bounds-on-the-communication">Lower complexitybounds on the communication</h2><p><img src="/2021/01/26/AL2SGD/3.1.png"> <img src="/2021/01/26/AL2SGD/3.11.png"></p><h2 id="lower-complexity-bounds-on-the-local-computation">Lowercomplexity bounds on the local computation</h2><p><img src="/2021/01/26/AL2SGD/3.2.png"></p><h1 id="优化算法">优化算法</h1><h2 id="accelerated-proximal-gradient-descent-apgd-for-federated-learning">AcceleratedProximal Gradient Descent (APGD) for Federated Learning</h2><p>首先介绍非加速版本的近端梯度下降算法(PGD): <img src="/2021/01/26/AL2SGD/1.png"></p><p>根据另一篇论文，有两种不同的方式可以将梯度下降算法应用到上述优化问题上。最直接的方式是令<span class="math inline">\(h = f\)</span>，<span class="math inline">\(\phi =\lambda\psi\)</span>，那么可以得到如下更新步骤： <img src="/2021/01/26/AL2SGD/2.png"></p><p>另一种方式是令 <span class="math inline">\(h(\mathbf{x}) = \lambda\phi(\mathbf{x}) + \frac{\mu}{2n}\|\mathbf{x}\|^{2}\)</span>， <span class="math inline">\(\phi(\mathbf{x}) = f(\mathbf{x}) -\frac{\mu}{2n}\|\mathbf{x}\|^{2}\)</span>。由此得到的更新过程如下： <img src="/2021/01/26/AL2SGD/3.png"></p><p>同FedProx算法一致。</p><p>由于上述两种情况下，每次迭代都需要进行一轮通信，相应的通信复杂度次优。但是可以结合动量算法，程序(6)可以结合Nesterov'smomentum，能够得到最优通信复杂度，以及最优局部近端复杂度（当 <span class="math inline">\(\lambda \leqL)\)</span>，该算法定义为APGD1，具体如下：</p><p><img src="/2021/01/26/AL2SGD/a2.png"></p><p>将更新过程(5)和动量结合，可以得到最优通信复杂度以及最优局部近端复杂度（当<span class="math inline">\(\lambda \geqL）\)</span>。将该算法定义为APGD2，具体如下：</p><p><img src="/2021/01/26/AL2SGD/a3.png"></p><h2 id="beyond-proximal-oracle-inexact-apgd-iapgd">Beyond proximaloracle: Inexact APGD (IAPGD)</h2><p>在多数情况下，如果采用局部近端操作，每一步迭代时都需要得到子问题的精确解，这是不实际的。因此，作者提出了一个针对(6)的加速非精确的算法，每个节点只需要进行局部梯度运算（AGD, Katyusha)：</p><p><img src="/2021/01/26/AL2SGD/a1.png"></p><h2 id="accelerated-l2sgd">Accelerated L2SGD+</h2><p>作者给出L2SGD+算法的一个加速版本-AL2SGD+。作者指出AL2SGD+算法不过是L-Katyusha算法与非均匀抽样的结合。</p><p><img src="/2021/01/26/AL2SGD/a4.png"></p><h1 id="experiments">Experiments</h1><p>在第一个实验中，作者比较了当局部损失为有限和形式时，算法IAPGD+Katyusha、AL2SGD+以及L2SGD+的收敛速度。结果如下图：</p><p><img src="/2021/01/26/AL2SGD/5.jpg"></p><p>对于通信轮数，IAPGD+Katyusha和AL2SGD+都显著优于L2SGD+；对于局部计算次数，AL2SGD+表现最优，IAPGD+Katyusha不如L2SGD+。</p><p>第二个实验中，作者研究了数据异质性对算法的影响，结果如下图所示。可以看出，数据异质性不影响算法的收敛速度，各个算法的表现同第一个实验相似。</p><p><img src="/2021/01/26/AL2SGD/6.png"></p><p>在第三个实验中，作者比较了APGD算法的两种变形：APGD1和APGD2。作者不断改变参数<span class="math inline">\(\lambda\)</span>的取值，其余参数保持不变。在理论上，APGD2算法应该不受参数 <span class="math inline">\(\lambda\)</span> 影响，而APGD1 算法的收敛率会随着<span class="math inline">\(\lambda\)</span> 而增加 (<span class="math inline">\(\sqrt{\lambda}\)</span>)。 当 <span class="math inline">\(\lambda \leq L = 1\)</span>时，APGD1是最优选择；当<span class="math inline">\(\lambda &gt; L = 1\)</span> 时，APGD2应该是最优选择。实验结果如下图所示，结果与理论一致。</p><p><img src="/2021/01/26/AL2SGD/7.png"></p><h1 id="参考文献">参考文献</h1><ul><li>Filip Hanzely (KAUST) · Slavomír Hanzely (KAUST) · Samuel Horváth(King Abdullah University of Science and Technology)· Peter Richtarik(KAUST). Lower Bounds and Optimal Algorithms for Personalized FederatedLearning.arXiv e-prints.https://ui.adsabs.harvard.edu/abs/2020arXiv201002372H</li></ul>]]></content>
      
      
      <categories>
          
          <category> paper </category>
          
      </categories>
      
      
        <tags>
            
            <tag> FL </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Federated Learning of a Mixture of Global and Local Models</title>
      <link href="/2021/01/26/L2SGD/"/>
      <url>/2021/01/26/L2SGD/</url>
      
        <content type="html"><![CDATA[<ul><li><a href="#introduction">Introduction</a><ul><li><a href="#11-federated-learning">1.1 Federated learning</a></li></ul></li><li><a href="#contributions">Contributions</a></li><li><a href="#新的优化问题">新的优化问题</a></li><li><a href="#l2gd-loopless-local-gd">L2GD: Loopless Local GD</a><ul><li><a href="#收敛理论">收敛理论</a></li><li><a href="#收敛率优化">收敛率优化</a></li></ul></li><li><a href="#loopless-local-sgd-with-variance-reduction">Loopless LocalSGD with Variance Reduction</a><ul><li><a href="#问题设置">问题设置</a></li><li><a href="#理论">理论</a></li></ul></li><li><a href="#experiments">Experiments</a></li><li><a href="#附录">附录</a><ul><li><a href="#experimental-setup-and-further-experiments">ExperimentalSetup and further experiments</a></li><li><a href="#其余算法">其余算法</a></li></ul></li><li><a href="#参考文献">参考文献</a></li></ul><!-- /TOC --><h1 id="introduction">Introduction</h1><h2 id="federated-learning">1.1 Federated learning</h2><p>联邦学习的目标函数： <span class="math display">\[  \min_{\mathbf{x} \in \mathbb{R}^{d}}\frac{1}{n}\sum_{i=1}^{n}f_{i}(\mathbf{x})  \]</span> 其中 <span class="math inline">\(n\)</span>表示参与训练的节点个数，<span class="math inline">\(\mathbf{x} \in\mathbb{R}^{d}\)</span>为全模型优化变量。 <span class="math inline">\(f_{i}(\mathbf{x})\)</span>为节点<span class="math inline">\(i\)</span>上的损失函数。</p><h1 id="contributions">Contributions</h1><ul><li>提出了新的FL优化形式，尝试学习全局模型和局部模型的混合。</li><li>给出了新的优化形式的理论性质。作者证明了最优局部模型以<span class="math inline">\(O(1/\lambda)\)</span>收敛到传统的全局模型；作者证明了在局部模型上得到的损失不高于全局模型上的损失(定理3.1)；作者指出局部模型的最优解等于所有局部模型最优解的平均值减去对应局部模型上损失函数的一阶梯度，这一点和MAML一致。</li><li>Loopless LGD：作者提出了一个随机梯度算法 — Loopless Local GradientDescent(L2GD)（算法1）来解决提出的优化问题。该算法不是一个标准的SGD，它可以看作是一个关于损失函数和惩罚项的不均匀抽样。当抽到损失函数部分时，每个节点执行一次随机梯度下降；当抽到惩罚项时，进行信息聚合。</li><li>收敛理论。假设函数 <span class="math inline">\(f_{i}\)</span> 为<span class="math inline">\(L-smooth\)</span>，并且为 <span class="math inline">\(\mu-strong \, convex\)</span>，可以得到抽样概率<span class="math inline">\(p^{*} = \frac{\lambda}{\lambda +L}\)</span>，固定期望局部更新次数为 <span class="math inline">\(1 +\frac{L}{\lambda}\)</span>，作者证明通信 (communication)复杂度为（通信次数上界）为 <span class="math inline">\(\frac{2\lambda}{\lambda +L}\frac{L}{\mu}\log\frac{1}{\epsilon}\)</span>。当 <span class="math inline">\(\lambda \to 0\)</span>时，通信次数非常小；当$$时，根据新优化问题得到的解收敛到全局模型最优解，并且L2GD算法的通信上界为 <span class="math inline">\(O(\frac{L}{\mu}\log\frac{1}{\epsilon})\)</span>。</li><li>推广。部分连接，局部SGD，variancereduction（variance来自三部分：非均匀抽样，部分连接，从节点样本随机抽样）。</li><li>可用于异质数据。</li><li>经验表现不错。</li></ul><h1 id="新的优化问题">新的优化问题</h1><p><span class="math display">\[\min_{\mathbf{x_{1},...,x_{n}} \in \mathbb{R}^{d}} \{ F(\mathbf{x}) :=f(\mathbf{x}) + \lambda \psi(\mathbf{x})\}\]</span> <span class="math display">\[f(\mathbf{x}):= \frac{1}{n}\sum_{i=1}^{n}f_{i}(\mathbf{x_{i}}), \quad\psi(\mathbf{x}):= \frac{1}{2n}\sum_{i=1}^{n}\|\mathbf{x_{i}} -\mathbf{\bar{x}}\|^{2}\]</span></p><ul><li>Local model (<span class="math inline">\(\lambda = 0\)</span>)</li><li>Mixed model (<span class="math inline">\(\lambda \in (0,\infty)\)</span>)</li><li>Global model (<span class="math inline">\(\lambda =\infty\)</span>)</li></ul><h1 id="l2gd-loopless-local-gd">L2GD: Loopless Local GD</h1><p>在这一部分中，作者给出一个算法求解上述优化问题，该算法可以看作是一个非均匀SGD，要么抽取<span class="math inline">\(\nabla f\)</span>，要么抽取 <span class="math inline">\(\nabla \psi\)</span> 估计 <span class="math inline">\(\nabla F\)</span>。令 <span class="math inline">\(0 &lt; p &lt; 1\)</span>，定义一个随机梯度如下：<span class="math display">\[G(\mathbf{x}):= \begin{cases} \frac{\nabla f(\mathbf{x})}{1-p}, &amp;\text {概率 $1-p$} \\ \frac{\lambda \nabla \psi(\mathbf{x})}{p}, &amp;\text{概率 $p$ } \end{cases}\]</span> 显然，<span class="math inline">\(G(\mathbf{x})\)</span>为<span class="math inline">\(\nabla F(\mathbf{x})\)</span>的无偏估计量。每步的更新为: <span class="math display">\[\mathbf{x}^{k+1} = \mathbf{x}^{k} - \alpha G(\mathbf{x}).\]</span></p><figure><img src="/2021/01/26/L2SGD/l2gd.png" alt="Algorithm 1"><figcaption aria-hidden="true">Algorithm 1</figcaption></figure><p><span class="math inline">\(\textbf{Lemma 4.2}\)</span> 经过 <span class="math inline">\(k\)</span> 步迭代后，期望的通信次数为 <span class="math inline">\(p(1-p)k\)</span>。</p><h2 id="收敛理论">收敛理论</h2><p>作者首先证明梯度估计量<span class="math inline">\(G(\mathbf{x})\)</span>的期望具有光滑性质，然后证明了算法L2GD的收敛性质。（<span class="math inline">\(\mathbf{x(\lambda)}\)</span>为最优解，定理4.4表明，L2GD算法只能收敛到最优解邻域。） <img src="/2021/01/26/L2SGD/4.3.png"></p><h2 id="收敛率优化">收敛率优化</h2><p>作者给出最优抽样概率 <span class="math inline">\(p^{*} =\frac{\lambda}{L + \lambda}\)</span>，步长 <span class="math inline">\(\alpha\)</span> 要满足 <span class="math inline">\(\frac{\alpha\lambda}{np} \leq\frac{1}{2}\)</span>. <img src="/2021/01/26/L2SGD/4.4.png"></p><h1 id="loopless-local-sgd-with-variance-reduction">Loopless Local SGDwith Variance Reduction</h1><p>L2GD算法仅线性收敛到最优解的邻域，无法收敛到最优解。假设每个子目标函数具有有限和形式，作者提出了一个算法L2SGD+，在每个节点上进行随机梯度下降，并且具有线性收敛速度。L2SGD是一个具有variancereduction 的局部SGD算法，关于SGD的variance reduction，见另一篇博客：SGDwith variance reduction.</p><h2 id="问题设置">问题设置</h2><p>假设 <span class="math inline">\(f_{i}\)</span> 具有有限和结构：<span class="math display">\[f_{i} = \frac{1}{m}\sum_{j=1}^{m}f_{i,j}(\mathbf{x}_{i})\]</span></p><p>那么目标函数变为： <span class="math display">\[F(\mathbf{x}) =\frac{1}{n}\sum_{i=1}^{n}(\frac{1}{m}\sum_{i=1}^{m}f_{i,j}(\mathbf{x}_{i}))+ \lambda\frac{1}{2n}\sum_{i=1}^{n}\|\mathbf{x}_{i} -\mathbf{\bar{x}}\|^{2}\]</span></p><p><img src="/2021/01/26/L2SGD/l2sgd+.png"></p><p>L2SGD算法仅在两次抽样不同时才会发生通信，经过 <span class="math inline">\(k\)</span> 次迭代后，需要进行 <span class="math inline">\(p(1-p)k\)</span>次聚合平均。但是，L2SGD算法还需要通信控制变量 <span class="math inline">\(\mathbf{J_{i}I,\Psi_{i}}\)</span>，因此通信次数变为原来的3倍。在附录中，作者给出了一个高效的L2SGD+，不需要通信控制变量。</p><h2 id="理论">理论</h2><p>作者给出了L2SGD算法的理论性质，并且给出最优抽样概率 <span class="math inline">\(p^{*}\)</span>。</p><p><img src="/2021/01/26/L2SGD/5.1.jpg"> <img src="/2021/01/26/L2SGD/5.2.png"> <img src="/2021/01/26/L2SGD/5.3.png"></p><h1 id="experiments">Experiments</h1><p>作者考虑Logistic回归问题，数据为LibSVM data(Chank &amp; Lin,2011)。数据首先进行normalized，以使得 <span class="math inline">\(f_{ij}\)</span>为1-smooth。步长根据定理5.2确定。每个数据集被划分为不同个数的节点，具体参数设置如下：<img src="/2021/01/26/L2SGD/table1.png"></p><p>作者考虑三种算法：L2SGD+, L2SGD(L2GD with local SGD), L2SGD2(L2GDwith local subsampling and control variates constructed for <span class="math inline">\(\Psi\)</span>)。根据理论分析，L2SGD+线性收敛到最优解，而L2SGD和L2SGD2收敛到最优解邻域。</p><p>作者考虑了两种数据分割方式。对于homogeneous data,首先将观测样本随机打乱，然后按照打乱后的数据划分到不同节点上；对于heterogeneousdata,首先根据观测样本的标签将样本排序，然后将排序后的数据依次划分到不同节点上(the worst-case heterogeneity)。</p><p><img src="/2021/01/26/L2SGD/figure3.png"></p><p>结果表明 - L2SGD+ (Full variance reduction)可以收敛到最优解，而L2SGD(without variance reduction)和 L2SGD2(with partial variancereduction) 只收敛到最优解邻域。 - 进行variancereduction是非常有必要的。它可以保证较快的全局收敛。 -数据异质性对算法收敛性没有影响。</p><h1 id="附录">附录</h1><h2 id="experimental-setup-and-further-experiments">Experimental Setupand further experiments</h2><ul><li>参数 <span class="math inline">\(p\)</span>如何影响算法L2SGD+的收敛速度</li><li>参数 <span class="math inline">\(\lambda\)</span>如何影响算法L2SGD+的收敛速度</li></ul><h2 id="其余算法">其余算法</h2><ul><li><p>Local GD with variance reduction</p><p>当每个节点采用梯度下降算法，且考虑variance reduction时，</p><p><img src="/2021/01/26/L2SGD/b1.png"> <img src="/2021/01/26/L2SGD/a3.png"></p></li><li><p>Efficient implementation of L2SGD+考虑到L2SGD+需要通信控制变量，增加了通信次数。作者给出了一个高效的版本，不需要通信控制变量，<span class="math inline">\(k\)</span>次迭代只需要通信 <span class="math inline">\(p(1-p)k\)</span>次。</p><p><img src="/2021/01/26/L2SGD/a4.png"></p></li><li><p>Local SGD with variance reduction – general method在这部分中，作者给出了一个使用性更广的版本。每个节点上目标函数可以包含一个非光滑正则项：</p><p><img src="/2021/01/26/L2SGD/b3.png"></p><p>另外，该版本算法允许从所有节点中任意抽样，允许节点结构任意（比如节点数据集大小，目标函数光滑程度，每个节点抽样方式任意）。</p><p><img src="/2021/01/26/L2SGD/a5.png"></p></li><li><p>Local stochastic algorithms</p><p>在这部分中，作者给出两个简单算法，不考虑variance reduction的LocalSGD(算法6)以及只考虑部分variance reduction的Local SGD (算法7)。</p><p><img src="/2021/01/26/L2SGD/a6.png"></p><p><img src="/2021/01/26/L2SGD/a7.png"></p></li></ul><h1 id="参考文献">参考文献</h1><ul><li>Hanzely, F. , &amp; Richtárik, Peter. (2020). Federated learning ofa mixture of global and local models.</li></ul>]]></content>
      
      
      <categories>
          
          <category> paper </category>
          
      </categories>
      
      
        <tags>
            
            <tag> FL </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>FLreview</title>
      <link href="/2021/01/21/FLreview/"/>
      <url>/2021/01/21/FLreview/</url>
      
        <content type="html"><![CDATA[<h1 id="aaai21-personalized-cross-silo-federated-learning-on-non-iid-data">[AAAI21]Personalized Cross-Silo Federated Learning on Non-IID Data</h1><p>该算法的目标函数为： <img src="https://cdn.jsdelivr.net/gh/sugar-xue/BlogImages@main/Picbed1613893223.png" alt="1613893223"></p><p>第二项<span class="math inline">\(A(\|\omega_{i} -\omega_{j}\|^{2})\)</span>的作用是使不同节点进行信息交流。该函数的定义如下：</p><figure><img src="https://cdn.jsdelivr.net/gh/sugar-xue/BlogImages@main/Picbed1614603445(1).png" alt="1614603445(1)"><figcaption aria-hidden="true">1614603445(1)</figcaption></figure><p>作者提出了一个求解上述目标函数的算法-FedAMP，具体如下：</p><figure><img src="https://cdn.jsdelivr.net/gh/sugar-xue/BlogImages@main/Picbed1613893256(1).png" alt="1613893256(1)"><figcaption aria-hidden="true">1613893256(1)</figcaption></figure><p>注意到，函数<span class="math inline">\(A(\cdot)\)</span>中的变量为<span class="math inline">\(\|\omega_{i} -\omega_{j}\|^{2}\)</span>，由于是子模型参数距离二范数的平方，在式(3)进行求导时，会出现<span class="math inline">\((\omega_{i} -\omega_{j})\)</span>项，进而式(3)可以表示为模型参数 <span class="math inline">\(\omega_{1}^{k-1},...,\omega_{m}^{k-1}\)</span>的线性组合：<img src="https://cdn.jsdelivr.net/gh/sugar-xue/BlogImages@main/Picbed1613893464(1).png" alt="1613893464(1)"></p><p>我们可以将 <span class="math inline">\(u_{i}\)</span> 看作是节点<span class="math inline">\(i\)</span>在云端子模型的参数，可以聚合各个节点的参数<span class="math inline">\(\omega_{1}^{k-1},...,\omega_{m}^{k-1}\)</span>信息。计算得到 <span class="math inline">\(u_{i}^{k}\)</span> 后，我们可以根据公式（4）在节点<span class="math inline">\(i\)</span> 上更新 <span class="math inline">\(\omega_{i}^{k}\)</span>:</p><figure><img src="https://cdn.jsdelivr.net/gh/sugar-xue/BlogImages@main/Picbed1613893769(1).png" alt="1613893769(1)"><figcaption aria-hidden="true">1613893769(1)</figcaption></figure><p>借助于<span class="math inline">\(u_{i}\)</span>聚合其他节点的参数，节点<span class="math inline">\(i\)</span>可以获取其他节点的信息。在云端优化完<span class="math inline">\(A(W)\)</span>后，对于每个节点，再利用式(6)优化损失函数<span class="math inline">\(F_{i}(w)\)</span>。</p><figure><img src="https://cdn.jsdelivr.net/gh/sugar-xue/BlogImages@main/Picbed1613893796(1).png" alt="1613893796(1)"><figcaption aria-hidden="true">1613893796(1)</figcaption></figure><figure><img src="https://cdn.jsdelivr.net/gh/sugar-xue/BlogImages@main/Picbed1613893828(1).png" alt="1613893828(1)"><figcaption aria-hidden="true">1613893828(1)</figcaption></figure><p>在聚合其他节点参数时，式(5)中不同节点参数的权重为 <span class="math display">\[\xi_{i,j} = \alpha_{k}A'(\|\omega_{i}^{k-1} -\omega_{j}^{k-1}\|^{2}), \quad (i \ne j)\]</span> 根据定义1，在<span class="math inline">\([0,\infty)\)</span>上，<span class="math inline">\(A\)</span>是一个increasing and concave函数，函数A的导数<span class="math inline">\(A'\)</span>在<span class="math inline">\((0, \infty)\)</span>上为non-negative andnon-increasing 函数，所以<span class="math inline">\(A'(\|\omega_{i}^{k-1} -\omega_{j}^{k-1}\|^{2})\)</span>相当于一个相似度函数，如果两个节点的参数<span class="math inline">\(w_{i}^{k-1}\)</span>和<span class="math inline">\(w_{j}^{k-1}\)</span>的欧氏距离小，那么这两个节点的相似度要高，对应到<span class="math inline">\(u_{i}^{k}\)</span>和<span class="math inline">\(u_{j}^{k}\)</span>中，它们的权重更高，因而<span class="math inline">\(u_{i}^{k}\)</span>和<span class="math inline">\(u_{j}^{k}\)</span>更接近，进一步，<span class="math inline">\(w_{i}^{k}\)</span>和<span class="math inline">\(w_{j}^{k}\)</span>更接近。</p><h1 id="aaai21-tornadoaggregate-accurate-and-scalable-federated-learning-via-the-ring-based-architecture">[AAAI21]TornadoAggregate: Accurate and Scalable Federated Learning via theRing-Based Architecture</h1><p>在这篇文章中，作者提出一种可以提高精度和稳定性的聚合方式，并且讨论了当前已有的各种聚合方式。</p><figure><img src="https://cdn.jsdelivr.net/gh/sugar-xue/BlogImages@main/Picbed1613894143.png" alt="1613894143"><figcaption aria-hidden="true">1613894143</figcaption></figure><p>作者指出STAR 这种全局聚合结构的稳定性差，相较而言，RING结构通过移除全局聚合，解决了STAR稳定性差的问题。但是，RING结构在FL中不切实际，假设共 <span class="math inline">\(|N|\)</span>个节点，RING需要进行的通信轮数是 STAR结构的 <span class="math inline">\(|N|\)</span> 倍数。除此之外，作者也总结讨论了其他已有聚合结构：STAR-stars, STAR-rings,RING-stars, RING-rings。</p><p>作者基于RING结构提出了两种新的聚合结构，通过减少RING结构带来的方差，提高了稳定性和精度。</p><h1 id="icml20-fedboost-communication-efficient-algorithms-for-federated-learning">[ICML20]FedBoost: Communication-Efficient Algorithms for Federated Learning</h1><p>作者借助集成的思想以减少FL中的通信成本。一些预先训练好的弱模型可以通过可获得的公共数据集训练。假设我们有<span class="math inline">\(q\)</span> 个已经训练好的弱模型 <span class="math inline">\(H =(h_{1},...,h_{q})\)</span>，本文的目标是学习组合权重 <span class="math inline">\(\alpha = \{ \alpha_{1}, ...,\alpha_{q}\}\)</span>，从而得到 <span class="math inline">\(\sum_{k=1}^{q} \alpha_{k}h_{k}\)</span>使得损失最小化。</p><figure><img src="https://cdn.jsdelivr.net/gh/sugar-xue/BlogImages@main/Picbed1613895221(1).png" alt="1613895221(1)"><figcaption aria-hidden="true">1613895221(1)</figcaption></figure><h1 id="icml20-fetchsgd-communication-efficient-federated-learning-with-sketching">[ICML20]FetchSGD: Communication-Efficient Federated Learning with Sketching</h1><p>作者提出一个新的算法，算法思想为：在每一轮，每个节点基于自己的局部信息计算得到一个梯度，然后在进行聚合前，作者使用一种叫做CountSketch的数据结构对梯度进行压缩。中心端保留momentum和error accumulationCount Sketches，每轮更新的权重参数根据error accumulationsketch得到。</p><h1 id="icml20-federated-learning-with-only-positive-labels">[ICML20]Federated Learning with Only Positive Labels</h1><h1 id="icml20-from-local-sgd-to-local-fixed-point-methods-for-federated-learning">[ICML20]From Local SGD to Local Fixed-Point Methods for Federated Learning</h1><h1 id="nips20-lower-bounds-and-optimal-algorithms-for-personalized-federated-learning">[NIPS20]Lower Bounds and Optimal Algorithms for Personalized Federatedlearning</h1><p>L2SGD # [NIPS20] Federated Bayesian Optimization # [NIPS20] FederatedMulti-Task Learning MOCHA # [NIPS20] FedSplit: An algorithmic frameworkfor fast federated optimization作者首先讨论了两种已有算法FedSGD和FedProx算法，作者证明这两种算法都不具有可行的收敛理论保证，因为它们得到的稳定点都不是它们预先要求解的目标函数的解。因此，作者提出FedSplit算法，该算法得到的稳定点是优化问题的最优解。</p><figure><img src="https://cdn.jsdelivr.net/gh/sugar-xue/BlogImages@main/Picbed1613897029.png" alt="1613897029"><figcaption aria-hidden="true">1613897029</figcaption></figure><h1 id="nips20-an-efficient-framework-for-clustered-federated-learning">[NIPS20]An Efficient Framework for Clustered Federated Learning</h1><p>作者提出一个迭代的聚类算法，论文假设所有的节点都能够被划分为若干类。由于每个节点所属类别未知，该算法可以交替估计每个节点所属的类别，并且通过梯度下降优化模型参数。论文中的算法可以解决数据分布的异质性问题。但是需要预先给定聚类个数<span class="math inline">\(k\)</span>。 <img src="https://cdn.jsdelivr.net/gh/sugar-xue/BlogImages@main/Picbed1613897504(1).png" alt="1613897504(1)"></p><h1 id="nips20-group-knowledge-transfer-federated-learning-of-large-cnns-at-the-edge">[NIPS20]Group Knowledge Transfer: Federated Learning of Large CNNs at theEdge</h1><p>作者提出一种新的交替最小化算法，该算法在每个节点上先训练较小的CNN网络，然后通过信息迁移训练一个较大的中心端CNN网络。<img src="https://cdn.jsdelivr.net/gh/sugar-xue/BlogImages@main/Picbed1613898317(1).jpg" alt="1613898317(1)"></p><p>上图展示了每个节点有一个特征提取器和分类器，可以在单个节点上进行模型训练。进行局部训练后，每个节点生成同样的张量，将其特征输出到中心端进行训练，然后借助于最小化预测标签和真实标签的KD损失函数训练参数。为了提升节点模型的表现，中心端会将其预测的标签发送给每个节点，然后每个节点可以基于其预测标签和中心端预测结果的损失函数训练子模型。</p><p><img src="https://cdn.jsdelivr.net/gh/sugar-xue/BlogImages@main/Picbed1613898899(1).png" alt="1613898899(1)"> # [NIPS20] Personalized Federated Learning withMoreau Envelopes为了解决异质性问题，作者考虑给每个节点的损失函数添加正则项： <span class="math display">\[f_{i}(\theta_{i}) + \frac{\lambda}{2}\|\theta_{i} - w\|^{2}，\]</span></p><p>优化问题表示为： <img src="https://cdn.jsdelivr.net/gh/sugar-xue/BlogImages@main/Picbed1613899301(1).jpg" alt="1613899301(1)"></p><h1 id="nips20-tackling-the-objective-inconsistency-problem-in-heterogeneous-federated-optimization">[NIPS20]Tackling the Objective Inconsistency Problem in Heterogeneous FederatedOptimization</h1><p>大多数论文在分析算法的收敛性时，往往会假设每个节点上进行局部更新的次数相同，它们的工作表明算法能够达到全局目标函数的稳定点。事实上，论文指出当不同节点局部更新次数不一致时，算法收敛到的稳定点不是原始目标函数的最优解，而是另一个目标函数。</p><p>解决这个问题的最简单想法就是固定每个节点的局部更新次数，在进行新的一轮迭代前，要等所有节点进行迭代完才能开始。这种方法能够保证目标函数的一致性，但是会带来训练成本。一些算法比如FedProx,VRLSGD以及SCAFFOLD用于处理non-IID问题，可以减少目标函数的不一致问题，但是要么有较慢的收敛速度，要么需要额外的通信成本和内存。</p><p>本文作者提出FedNova算法，可以保证目标函数的一致性问题。</p><h1 id="nips20-throughput-optimal-topology-design-for-cross-silo-federated-learning">[NIPS20]Throughput-Optimal Topology Design for Cross-Silo FederatedLearning</h1><h1 id="nips20-federated-principal-component-analysis">[NIPS20]Federated Principal Component Analysis</h1><h1 id="nips20-ensemble-distillation-for-robust-model-fusion-in-federated-learning">[NIPS20]Ensemble Distillation for Robust Model Fusion in Federated Learning</h1><h1 id="nips20-differentially-private-federated-linear-bandits">[NIPS20]Differentially-Private Federated Linear Bandits</h1><h1 id="nips20-inverting-gradients---how-easy-is-it-to-break-privacy-in-federated-learning">[NIPS20]Inverting Gradients - How easy is it to break privacy in federatedlearning?</h1><h1 id="nips20-distributionally-robust-federated-averaging">[NIPS20]Distributionally Robust Federated Averaging</h1><h1 id="iclr20-fair-resource-allocation-in-federated-learning">[ICLR20]FAIR RESOURCE ALLOCATION IN FEDERATED LEARNING</h1><p>作者提出 q-FFL算法，目的是解决FL中的公平问题：不同节点上的精度均匀。通过最小化一个加权的损失函数，具有较高损失的节点具有较高的权重。</p><figure><img src="https://cdn.jsdelivr.net/gh/sugar-xue/BlogImages@main/Picbed1613901146(1).png" alt="1613901146(1)"><figcaption aria-hidden="true">1613901146(1)</figcaption></figure><p>目标函数：</p><figure><img src="https://cdn.jsdelivr.net/gh/sugar-xue/BlogImages@main/Picbed1613901254(1).png" alt="1613901254(1)"><figcaption aria-hidden="true">1613901254(1)</figcaption></figure><p>具体算法略。 # [ICLR20] DIFFERENTIALLY PRIVATE META-LEARNING</p><h1 id="iclr20-dba-distributed-backdoor-attacks-against-federated-learning">[ICLR20]DBA: DISTRIBUTED BACKDOOR ATTACKS AGAINST FEDERATED LEARNING</h1><h1 id="iclr20-generative-models-for-effective-ml-on-private-decentralized-datasets">[ICLR20]GENERATIVE MODELS FOR EFFECTIVE ML ON PRIVATE, DECENTRALIZEDDATASETS</h1><h1 id="iclr20-attack-resistant-federated-learning-with-residual-based-reweighting">[ICLR20]ATTACK-RESISTANT FEDERATED LEARNING WITH RESIDUAL-BASED REWEIGHTING</h1>]]></content>
      
      
      <categories>
          
          <category> paper </category>
          
      </categories>
      
      
        <tags>
            
            <tag> FL </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>

<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>Machine Learning</title>
      <link href="/2024/10/08/statistic/"/>
      <url>/2024/10/08/statistic/</url>
      
        <content type="html"><![CDATA[<h1 id="一机器学习相关">一、机器学习相关</h1><h2 id="基本概念">1、基本概念</h2><h3 id="排序"><a href="">1.1. 排序</a></h3><p><a href="">1. Why does sorting a collection have (at least) <span class="math inline">\(O(n log n)\)</span> complexity? n is the length ofthe collection.</a></p><ul><li><span class="math inline">\(O(n log n)\)</span> is the optimal valuefor a comparison sort.[https://theartofmachinery.com/2019/01/05/sorting_is_nlogn.html].</li></ul><p><a href="">2. 几种排序算法介绍以及复杂度分析。</a></p><ul><li><p>排序有内部排序和外部排序，内部排序是数据记录在内存中进行排序，而外部排序是因排序的数据很大，一次不能容纳全部的排序记录，在排序过程中需要访问外存。</p></li><li><p>排序的稳定性是指：若待排序的序列中，存在多个具有相同元素，经过排序，这些元素的相对次序保持不变，则称该算法是稳定的；若经排序后，元素的相对次序发生了改变，则称该算法是不稳定的。</p></li><li><p>常见的八种排序方法都属于内部排序，交换排序（冒泡排序、快速排序）、选择排序（简单选择排序、堆排序）、插入排序（直接插入排序、希尔排序）、归并排序、基数排序。<img src="/2024/10/08/statistic/sort.jpg"></p></li><li><p>冒泡排序：两两比较待排序数据元素的大小，如果两个数据元素的次序相反时则进行交换，直到没有反序的数据元素未知。<strong>平均时间复杂度为<span class="math inline">\(O(n^2)\)</span>，空间复杂度为 <span class="math inline">\(O(1)\)</span>，属于稳定排序。</strong></p></li><li><p>快速排序：在当前无序区任取一个元素作为待比较的基准，用此基准将当前无序区划分为左右两个较小的无序区，其中左边无序子区的数据元素均小于等于基准元素，右边无序子区中的元素均大于等于基准元素，而基准则始终位于最终排序位置上。依次再对左右两个无序子区进行上述划分过程，直到所有无序子区中的元素都已排序为止。<strong>平均时间复杂度为<span class="math inline">\(O(n \log_2 n)\)</span>，空间复杂度为 <span class="math inline">\(O(\log_2 n) - O(n)\)</span>（可能退化为冒泡排序），属于不稳定算法。</strong></p></li><li><p>简单选择排序：每一趟从待排序的数据元素中选出最小（或最大）的一个元素，顺序放在已排好序的数列最后，直到全部待排序的数据元素排完。<strong>平均时间复杂度为<span class="math inline">\(O(n^2)\)</span>，空间复杂度为<span class="math inline">\(O(1)\)</span>，属于稳定排序。</strong></p></li><li><p>堆排序：将数组看成是一棵完全二叉树的顺序存储结构，利用完全二叉树中的双亲结点和孩子结点之间的内在关系来选择最小的元素。小根堆：树中任一非叶子结点的值均小于等于其孩子结点的值（堆顶最小）；大根堆：树中任一非叶子结点的值均大于等于其孩子结点的值（堆顶最大）。<strong>平均时间复杂度为<span class="math inline">\(O(n \log_2 n)\)</span>，空间复杂度为 <span class="math inline">\(O(1)\)</span>，属于不稳定排序。</strong></p></li><li><p>直接插入排序：每次将一个待排序的数据元素，插入到前面已经排好序的数列中的适当位置，使数列依然有序；直到待排序数据元素全部插入完为止。<strong>平均时间复杂度为<span class="math inline">\(O(n^2)\)</span>，空间复杂度为 <span class="math inline">\(O(1)\)</span>，属于稳定排序。</strong></p></li><li><p>希尔排序：先取一个小于 n 的整数 <span class="math inline">\(d_1\)</span>作为第一个增量，把所有元素分为若干组，所有下标距离为 <span class="math inline">\(d_1\)</span>的倍数的元素放在同一组中。先在各组内进行直接插入排序。然后取第二个增量<span class="math inline">\(d_2 &lt;d_1\)</span>，按照原始位置下标，重复上述分组和排序，直到所取的增量 <span class="math inline">\(d_t =1\)</span>，即所有元素都放在同一组中进行直接插入排序为止。<strong>平均时间复杂度为<span class="math inline">\(O(n^{1.3})\)</span>，空间复杂度为O(1)，不稳排序。</strong></p></li><li><p>归并排序：归并排序最核心的部分是合并（merge）过程，将两个有序的数组a 和 b 合并为一个有序数组 c。从左往右枚举 a[i] 和b[j]，找出最小的值并放入数组 c[k]；重复上述过程直到 a 和 b有一个为空时，将另一个数组剩下的元素放入c。为保证排序的稳定性，前段首元素小于或等于后段首元素时（a[i] &lt;=b[j]）而非小于时（a[i] &lt; b[j]）就要作为最小值放入c[k]。<strong>平均时间复杂度为 <span class="math inline">\(O(n \logn)\)</span>，空间复杂度为 <span class="math inline">\(O(1)\)</span>，稳定排序。</strong></p></li><li><p>基数排序：将待排序的元素拆分为 <span class="math inline">\(k\)</span> 个关键字，先对第 <span class="math inline">\(1\)</span>关键字进行稳定排序，然后对于每组具有相同关键字的元素 再对第 <span class="math inline">\(2\)</span>关键字进行稳定排序（递归执行）。最后对于每组 具有相同关键字的元素 再对第<span class="math inline">\(k\)</span>关键字进行稳定排序。如果对自然数进行比较，将自然数按个位对齐后往高位补齐<span class="math inline">\(0\)</span>，则一个数字从左往右数第 <span class="math inline">\(i\)</span> 位数就可以作为第 <span class="math inline">\(i\)</span> 关键字。<strong>设数据量为<span class="math inline">\(n\)</span>, 数据为<span class="math inline">\(d\)</span>进制, 最大位数为 <span class="math inline">\(k\)</span>, 则对某一位执行计数排序使用<span class="math inline">\(O(n+d)\)</span>时间，排序所有 <span class="math inline">\(k\)</span> 位使用<span class="math inline">\(O((n+ d) \times k)\)</span> 时间。空间复杂度为 <span class="math inline">\(O(n + d)\)</span>,非原地排序。如果对内层关键字的排序是稳定的，则基数排序是稳定的排序算法。</strong></p></li></ul><h3 id="二分查找"><a href="">1.2. 二分查找</a></h3><p>以在一个升序数组中查找一个数为例。它每次考察数组当前部分的中间元素，如果中间元素刚好是要找的，就结束搜索过程；如果中间元素小于所查找的值，那么左侧的只会更小，不会有所查找的元素，只需到右侧查找；如果中间元素大于所查找的值同理，只需到左侧查找。</p><p>二分查找的最优时间复杂度为 <span class="math inline">\(O(1)\)</span>。二分查找的平均时间复杂度和最坏时间复杂度均为<span class="math inline">\(O(\logn)\)</span>。因为在二分搜索过程中，算法每次都把查询的区间减半，所以对于一个长度为<span class="math inline">\(n\)</span> 的数组，至多会进行 <span class="math inline">\(O(\log n)\)</span> 次查找。</p><p>迭代版本的二分查找的空间复杂度为 <span class="math inline">\(O(1)\)</span>。</p><h3 id="回归模型和分类模型常用损失函数有哪些"><a href="">1.3.回归模型和分类模型常用损失函数有哪些？</a></h3><p><strong>回归模型常用的损失函数</strong></p><ol type="1"><li><p>0-1损失函数： <span class="math display">\[L(\hat{y},y) =\begin{cases}  1,&amp;y \neq \hat{y} \\0,&amp;y = \hat{y}\end{cases}  \]</span></p></li><li><p>平均绝对误差MAE：异常点多的情况下鲁棒性更强，不像MSE那样过度惩罚大误差；但不方便求导<span class="math display">\[L(\hat{y}, y) = \frac{1}{n} \sum_{i=1}^{n}|y_i - \hat{y}_i|.\]</span></p></li><li><p>均方误差MSE：求导方便，能够用梯度下降法优化；对异常值敏感，异常值的存在会导致MSE值急剧增大，从而影响模型效果。<span class="math display">\[L(\hat{y},y) = \frac{1}{n} \sum_{i=1}^{n}(y_i - \hat{y}_i)^2.\]</span></p></li><li><p>对数损失函数/对数似然损失函数： <span class="math display">\[L(P(Y|X),Y) = -{\rm log} P(Y|X)\]</span></p></li><li><p>Huber损失函数：结合了MAE和MSE的优点，对异常值更加鲁棒，比MSE对大的误差的惩罚更加温和；在误差较小时仍然能够提供与MSE类似的梯度更新；缺点是需要调整超参数<span class="math inline">\(\delta\)</span> <span class="math display">\[L_{Huber}(\hat{y}, y) =\begin{cases}(\hat{y}-y)^2&amp;|\hat{y}-y| \leq \delta\\2 \delta |\hat{y}-y| - \delta^2&amp;|\hat{y}-y| &gt; \delta\end{cases}\]</span></p></li><li><p>对数余弦 Log-Cosh损失函数：近似于MSE，但对大误差的惩罚更温和，兼顾了MSE和MAE的优点。同时二阶处处可微（牛顿法要求二阶可微），更加平滑且容易优化。但是，不如MSE那样简单直观，在某些应用中表现不如MSE。<span class="math display">\[L(\hat{y},y) = \log \cosh(\hat{y}-y)\]</span></p></li></ol><p><strong>分类模型中常用的损失函数</strong> 1.交叉熵损失。对于二元分类，公式为如下。其中，<span class="math inline">\(y\)</span>是真实标签，<span class="math inline">\(p\)</span>是模型预测的概率。 <span class="math display">\[  L(p,y) = -[y \log p + (1-y) \log (1-p)].  \]</span></p><ul><li>优点：直接衡量模型预测的概率与真实分类标签之间的差异，适合分类任务。对概率差异敏感，能够较好地区分高概率和低概率的预测。</li><li>缺点：当预测的概率非常接近0或1时，交叉熵的梯度可能变得极端，导致训练不稳定。对于不平衡数据，模型倾向于偏向多数类，需要配合其他技术如加权损失函数或欠采样来处理。</li></ul><ol start="2" type="1"><li>Hinge loss。通常用于支持向量机，公式如下。<span class="math inline">\(y \in {-1,1}\)</span>是真实标签，<span class="math inline">\(f(x)\)</span>是模型输出。 <span class="math display">\[  L(f(x),y) = \max(0, 1 - y \times f(x))  \]</span></li></ol><ul><li>优点：强调分类边界的最大化，适用于SVM；对小的误差不敏感，能够防止过拟合。</li><li>缺点：不适用于概率输出的分类模型。仅适用于二分类问题，多分类任务中扩展性较差。</li></ul><ol start="3" type="1"><li>Kullback-Leibler 散度。假设<span class="math inline">\(p(x)\)</span>是真实分布，<span class="math inline">\(q\)</span>是模型预测的概率分布，公式如下： <span class="math display">\[  D_{\text{KL}}(p||q) = \sum p(x) \log(\frac{p(x)}{q(x)}),  \]</span></li></ol><ul><li>优点：可以量化两个分布之间的差异，适合概率模型；</li><li>缺点：对极端概率值（接近于1或0）的预测非常敏感，可能导致数值不稳定。</li></ul><h3 id="什么是结构误差和经验误差"><a href="">1.4.什么是结构误差和经验误差？</a></h3><p>经验风险（经验损失）：模型 <span class="math inline">\(f(X)\)</span>关于训练数据集的平均损失 <span class="math display">\[R_{\rm emp}(f) = \frac{1}{N} \sum_{i=1}^N L(y_i,f(x_i)).\]</span></p><p>结构风险：是在经验风险上加上表示模型复杂度的正则化项 <span class="math display">\[R_{\rm srm}(f) = \frac{1}{N} \sum_{i=1}^{N}L(y_i,f(x_i))+\lambda J(f).\]</span></p><p>经验风险最小化的策略认为，经验风险最小的模型是最优的模型。</p><p>结构风险最小化是为了防止过拟合而提出的，结构风险最小化等价于正则化。结构风险最小化的策略认为结构风险最小的模型是最优的模型。</p><h3 id="如何选择合适的模型评估指标rocauc精准度召回率f1值"><a href="#1-6">1.5.如何选择合适的模型评估指标？ROC、AUC、精准度、召回率、F1值</a></h3><p>模型评估指标用于衡量机器学习模型在测试集或验证集上的表现，帮助评估其性能。</p><p>混淆矩阵，又称误差矩阵，就是分别统计分类模型归错类，归对类的观测值个数，然后把结果放在一个表里展示出来。这个表就是混淆矩阵。</p><p>混淆矩阵是ROC曲线绘制的基础，同时它也是衡量分类型模型准确度中最基本，最直观，计算最简单的方法。</p><table><thead><tr><th style="text-align: center;">混淆矩阵</th><th style="text-align: center;">预测结果</th><th style="text-align: center;">预测结果</th></tr></thead><tbody><tr><td style="text-align: center;">真实情况</td><td style="text-align: center;">反例</td><td style="text-align: center;">正例</td></tr><tr><td style="text-align: center;">反例</td><td style="text-align: center;">TN（真反例）</td><td style="text-align: center;">FP（假正例）</td></tr><tr><td style="text-align: center;">正例</td><td style="text-align: center;">FN（假反例）</td><td style="text-align: center;">TP（真正例）</td></tr></tbody></table><ul><li>TN：True Negative，将样本中的负类样本预测为负类的数量</li><li>FP：False Positive，将样本中的负类样本预测为正类的数量</li><li>FN：False Negative，将样本中的正类样本预测为负类的数量</li><li>TP：True Positive，将样本中的正类样本预测为正类的数量</li></ul><p><strong>分类任务指标</strong></p><p><strong>Accuracy</strong>（准确率）：分类正确的样本占总样本个数的比例<span class="math display">\[Accuracy = \frac{n_{correct}}{n_{total}}\]</span> -缺点：不同类别的样本比例非常不均衡时，占比大的类别往往成为影响准确率的最主要因素。比如，当负样本占99%时，分类器把所有样本都预测为负样本也可以获得99%的准确率。-解决：可以使用每个类别下的样本准确率的算术平均（平均准确率）作为模型评估的指标。</p><p><strong>Precision</strong>（精确率）：分类正确的正样本个数占分类器判定为正样本的样本个数的比例<span class="math display">\[Precision = \frac{TP}{TP+FP}\]</span><strong>Recall</strong>（召回率）：分类正确的正样本数占真正的正样本个数的比例<span class="math display">\[Recall = \frac{TP}{TP+FN}\]</span><strong>F1-score</strong>：precision和recall的调和平均值；当精确率和召回率都高时，F1值也会高；特别适用于类别不平衡的数据集。<span class="math display">\[{\rm F1} = \frac{2 \times precision \times recall}{precision + recall}\]</span></p><p>在排序问题中，通常没有一个确定的阈值把得到的结果直接判定为正样本或负样本，而是采用TopN返回结果的Precision和Recall值来衡量排序模型的性能。即认为模型返回的TopN结果就是模型判定的正样本，计算前N个位置的Precision@N和Recall<span class="citation" data-cites="N">@N</span>。为了综合评估一个排序模型的好坏，不仅要看模型在不同TopN下的Precision@N和Recall<span class="citation" data-cites="N">@N</span>，而且最好画出模型的P-R曲线。P-R曲线的横轴是Recall，纵轴是Precision。</p><p><strong>ROC</strong>：横坐标为假阳性率（False PositiveRate，FPR）；纵坐标为真阳性率（True Positive Rate，TPR） <span class="math display">\[FPR = \frac{FP}{N}, \quadTPR = \frac{TP}{P},\]</span>其中，P是真实的正样本的数量，N是真实的负样本的数量，TP是P个正样本中被分类器预测为正样本的个数，FP是N个负样本中被预测为正样本的个数。</p><p><strong>如何绘制ROC曲线</strong></p><p>通过不断移动分类器的“截断点”来生成曲线上的一组关键点。在二分类问题中，模型输出一般是预测样本为正例的概率，在输出最终的正例负例之前，我们需要制定一个阈值。大于该阈值的样本判定为正例，小于该阈值的样本判定为负例。通过动态调整截断点，绘制每个截断点对应位置，再连接所有点得到最终的ROC曲线。比如，阈值为0时，此时所有样本被预测为负例，TPR = 0, FPR =0；当阈值增加为1时，此时所有样本被预测为正例，TPR = 1, FPR = 1.</p><p><strong>一般情况下，PR曲线易受样本数量的影响，样本数量不均衡情况下PR曲线会有明显变化，故一般使用ROC曲线。</strong></p><p><strong>AUC</strong>：ROC曲线下的面积大小。计算AUC值只要沿着ROC横轴做积分就可以。AUC取值在0.0~1之间。AUC越大，分类性能越好。AUC表示预测的正例排在负例前面的概率。</p><p>指标想表达的含义，简单来说其实就是随机抽出一对样本（一个正样本，一个负样本），然后用训练得到的分类器来对这两个样本进行预测，预测得到正样本的概率大于负样本概率的概率。AUC为0.5表明对正例和负例没有区分能力，对于不论真实类别是1还是0，分类器预测为1的概率是相等的。</p><p>我们希望分类器达到的效果：对于真实类别为1的样本，分类器预测为1的概率（TPR）要大于真实类别为0而预测类别为1的概率（FPR），即y&gt;x</p><p>AUC的计算方法同时考虑了分类器对于正例和负例的分类能力，在样本不平衡的情况下，依然能够对分类器作出合理的评价。</p><p><strong>回归任务指标</strong></p><p><strong>均方根误差RMSE</strong>：计算预测值和实际值的平均误差 <span class="math display">\[{\rm RMSE} = \sqrt{\frac{\sum_{i=1}^n (y_i-\hat{y}_i)^2}{n}}\]</span> <strong>均方误差MSE</strong></p><p><strong>平均绝对误差MAE</strong></p><p><strong>决定系数 <span class="math inline">\(R^2\)</span></strong>：表示模型解释变量总方差的比例，反映了模型拟合的好坏。</p><p>SST: sum of squares total，总的偏差平方和，表示变量<span class="math inline">\(y\)</span>相对于中心<span class="math inline">\(\bar{y}\)</span>的异动。 <span class="math display">\[SST = \sum_{i=1}^n (y_i - \bar{y}_i)^2.\]</span></p><p>SSR: sum of squares regression, 回归平方和，表示估计值 <span class="math inline">\(\hat{y}\)</span>相对于中心 <span class="math inline">\(\bar{y}\)</span>的异动。</p><p><span class="math display">\[SSR = \sum_{i=1}^{n} (\hat{y}_i - \bar{y}_i)^2.\]</span></p><p>SSE: sum of squares error,残差平方和，表示拟合数据和原始数据之间的误差的平方和。 <span class="math display">\[SSE = \sum_{i=1}^n(y_i - \hat{y}_i)^2.\]</span></p><p><span class="math display">\[R^2 = 1 - \frac{\sum_{i=1}^n(y_i - \hat{y}_i)^2}{\sum_{i=1}^n(y_i -\bar{y})^2} = 1 - \frac{SSE}{SSR}.\]</span></p><p>决定系数<span class="math inline">\(R^2\)</span>的取值范围为<span class="math inline">\([0,1]\)</span>，0表示没有线性关系，1表示拟合模型可以解释所有变异y。</p><p><strong>调整的决定系数 <span class="math inline">\(\bar{R}^2\)</span></strong>：对于决定系数 <span class="math inline">\(R^2\)</span>，当解释变量个数增加（即模型复杂度提高，偏差降低）时，<span class="math inline">\(R^2\)</span>会不断增加。但是，随着模型复杂度的提高，方差可能会越来越大。因此，引入了调整的决定系数<span class="math inline">\(\bar{R}^2\)</span>： <span class="math display">\[\bar{R}^2 = 1 - \frac{SSE/df_{err}}{SSR/df_{tot}} = 1 - （1 - R^2)\times \frac{n-1}{n-p-1},\]</span> 其中，df_{err} 表示残差平方和的自由度，为<span class="math inline">\(n-p-1\)</span>，df_{tot}表示关于总平方和的自由度，为 <span class="math inline">\(n-1\)</span>。</p><p>调整后的 <span class="math inline">\(\bar{R}^2\)</span>可以是负值，其值总是小于或等于 <span class="math inline">\({R}^2\)</span>。当引入更多解释变量时，决定系数<span class="math inline">\({R}^2\)</span>会增加，导致<span class="math inline">\(\bar{R}^2\)</span>的增加。但是后面的分数项会降低<span class="math inline">\(\bar{R}^2\)</span>。只有当减少的偏差大于引入的方差时，<span class="math inline">\(\bar{R}^2\)</span>才会增加。因此，<span class="math inline">\(\bar{R}^2\)</span> 可以看作是对bias-variance间的tradeoff.</p><p><strong>平均绝对百分比误差</strong>：（Mean Absolute PercentageError, MAPE），MAPE表示预测误差相对于真实值的百分比。 <span class="math display">\[MAPE = \frac{1}{n}\sum_{1}^{n} |\frac{y_i - \hat{y}_i}{y_i}| \times100\%.\]</span>优点：易于解释，特别适用于需要对误差进行相对度量的场景。缺点：当真实值接近0时，MAPE会变得不稳定。</p><h3 id="bias-variance-trade-off模型过拟合欠拟合"><a href="">1.6.Bias-variance trade-off，模型过拟合、欠拟合</a></h3><p><img src="/2024/10/08/statistic/bias_variance.jpg"></p><p><strong>误差分析</strong>：通过训练误差和测试误差来分析模型是否存在高方差、高偏差。</p><ul><li>如果训练误差较高：说明模型的偏差较大，模型出现了欠拟合。</li><li>如果训练误差较低，而测试误差较高：说明模型的方差较大，出现了过拟合。</li><li>如果训练误差较低，测试误差也较低：说明模型的方差和偏差都适中，是一个比较理想的模型。</li><li>如果训练误差较高，且测试误差更高：说明模型的方差和偏差都较大。</li></ul><p>上述分析的前提是：训练集、测试集的数据来自于同一个分布，且最优误差较小。否则讨论更复杂。</p><p><strong>欠拟合</strong>：模型过于简单，没有很好地学习到数据间的关系，训练集效果差。<strong>模型复杂度低，此时模型预测的方差较小，表示预测较稳定。但是模型预测的偏差会较大，表示预测不准确。。</strong></p><p><strong>过拟合</strong>：指学习时选择的模型所包含的参数过多，以至出现这一模型对已知数据预测得很好，但对未知数据预测得很差的现象。训练集效果好，测试集效果差。<strong>模型复杂度高，此时模型预测的方差大，偏差小。</strong></p><p><strong>欠拟合解决方法：</strong> 1. 增加特征 2.提高模型复杂度：神经网络提高神经元数、增加层数；SVM使用核函数； 3.减小正则项的系数</p><p><strong>过拟合解决方法：</strong> 1. 提高样本数量。神经网络：DataAugmentation（数据增强） 2. 简化模型。神经网络使用 Dropout、EarlyStopping；决策树剪枝、限制树的深度。 3.加入正则化项（L1或L2）或提高惩罚系数 4. 使用集成学习 5.神经网络中使用dropout机制 6. early stopping</p><h3 id="奥卡姆剃刀定律是什么对机器学习模型优化有何启发"><a href="">1.7.奥卡姆剃刀定律是什么？对机器学习模型优化有何启发？</a></h3><p>奥卡姆剃刀定律：若有多个假设与观察一致，则选最简单的那个。</p><p>奥卡姆剃刀原理应用于模型选择时变为以下想法：在所有可能选择的模型中，能够很好地解释已知数据并且十分简单才是最好的，也就是应该选择的模型。</p><p>从贝叶斯估计的角度来看，正则化项对应于模型的先验概率。可以假设复杂的模型有较小的先验概率，简单的模型有较大的先验概率。</p><h3 id="线性模型-vs.-非线性模型-生成式模型-vs.-判别式模型-概率模型-vs.-非概率模型-参数化模型-vs.-非参数化模型"><a href="">1.8. 线性模型 vs. 非线性模型， 生成式模型 vs. 判别式模型，概率模型 vs. 非概率模型， 参数化模型 vs. 非参数化模型</a></h3><p><strong>线性模型 vs. 非线性模型</strong></p><p>非概率模型可以分为线性模型和非线性模型。如果函数 <span class="math inline">\(y=f(x)\)</span> 或 <span class="math inline">\(z =g(x)\)</span>是线性函数，则称模型是线性模型，否则成模型为非线性模型。</p><p>线性模型：感知机、线性支持向量机、k近邻、k均值、潜在语义分析</p><p>非线性模型：核函数支持向量机、AdaBoost，神经网络</p><p><strong>生成式模型 vs. 判别式模型</strong></p><p>监督学习方法分为生成方法（generativeapproach）和判别方法（discriminativeapproach）。所学习到的模型分别称为生成模型和判别模型。监督学习的模型一般形式为决策函数：<span class="math inline">\(Y = f(X)\)</span> 或者条件概率分布 <span class="math inline">\(P(Y|X)\)</span>。</p><p>生成方法：由数据学习联合概率分布 <span class="math inline">\(P(X,Y)\)</span>，然后求出条件概率分布 <span class="math inline">\(P(Y|X)\)</span>作为预测模型： <span class="math display">\[P(Y|X) = \frac{P(X,Y)}{P(X)}\]</span> 之所以叫做生成方法，是因为模型表示了给定输入 <span class="math inline">\(X\)</span> 产生输出 <span class="math inline">\(Y\)</span>的生成关系。典型的生成模型：朴素贝叶斯法、隐马尔可夫模型。</p><p>判别方法：由数据直接学习决策函数 <span class="math inline">\(f(X)\)</span> 或者条件概率分布 <span class="math inline">\(P(X,Y)\)</span>作为预测的模型，关心的是对给定的输入<span class="math inline">\(X\)</span>，应该预测什么样的输出 <span class="math inline">\(Y\)</span>。典型的判别模型：k近邻、感知机、决策树、逻辑斯蒂回归、最大熵模型、支持向量机、提升方法、条件随机场。</p><p><strong>概率模型 vs. 非概率模型</strong></p><p>概率模型与非概率模型的区别在于模型的内在结构。<strong>概率模型一定可以表示为联合概率分布的形式</strong>，其中的变量表示输入、输出、因变量甚至参数。而针对非概率模型则不一定存在这样的联合概率分布。</p><p>统计学习的模型可以分为概率模型（probabilisticmodel）和非概率模型（non-probabilisticmodel）或者确定性模型（deterministicmodel）。在监督学习中，概率模型取条件概率分布形式 <span class="math inline">\(p(y|x)\)</span>，非概率模型取函数形式 <span class="math inline">\(y=f(x)\)</span>，其中<span class="math inline">\(x\)</span>是输入，<span class="math inline">\(y\)</span>是输出。在无监督学习中，概率模型取条件概率分布形式<span class="math inline">\(p(z|x)\)</span>或 <span class="math inline">\(p(x|z)\)</span>，其中<span class="math inline">\(x\)</span>是输入，<span class="math inline">\(z\)</span>是输出。在监督学习中，概率模型是生成模型，非概率模型是判别模型。</p><p>概率模型：决策树、朴素贝叶斯、隐马尔可夫模型、条件随机场、概率潜在语义分析、潜在狄利克雷分布、高斯混合模型</p><p>非概率模型：感知机、支持向量机、k近邻、AdaBoost、k均值、潜在语义分析、神经网络</p><p>逻辑斯蒂回归即可看做概率模型，又可看做非概率模型。</p><p><strong>参数化模型 vs. 非参数化模型</strong></p><p>统计学习模型又可以分为参数化模型（parametricmodel）和非参数化模型（non-parametricmodel）。参数化模型假设模型参数的维度固定，模型可以由有限维参数完全刻画；非参数模型假设模型参数的维度不固定或者说无穷大，随着训练数据量的增加而不断增大。</p><p>参数化模型：感知机、朴素贝叶斯、逻辑斯蒂回归、k均值、高斯混合模型</p><p>非参数化模型：决策树、支持向量机、AdaBoost、k近邻、潜在语义分析、概率潜在语义分析、潜在狄利克雷分配</p><h3 id="缺失值如何处理"><a href="">1.9. 缺失值如何处理？</a></h3><ol type="1"><li>缺失值较多.直接将该特征舍弃掉，否则可能反倒会带入较大的噪声，对结果造成不良影响。</li><li>缺失值较少,其余的特征缺失值都在10%以内，我们可以采取很多的方式来处理:（1）把NaN直接作为一个特征，假设用0表示；（离散特征取值k维扩充到k+1维）（2）用均值填充；（连续特征-均值，离散特征-特征取值的众数）（3）用随机森林等算法预测填充。</li></ol><h3 id="标准化归一化介绍"><a href="#2-2">1.10.标准化、归一化介绍</a></h3><p>为了消除数据特征之间的量纲影响，我们需要对特征进行归一化/标准化处理，使得不同指标之间具有可比性。以梯度下降过程为例，如果不做归一化/标准化处理，在学习速率相同的情况下，大量纲变量的更新速度会大于小量纲，需要较多迭代才能找到最优解。如果将其变换到相同的数值区间后，更新速度变得更为一致，容易更快地通过梯度下降找到最优解。</p><p><strong>1. 归一化（Min-MaxScaling）</strong>：将数据缩放到一个特定的范围（通常是 [0, 1] 或 [-1,1]）。它的核心思想是通过线性变换，将数据映射到指定区间中。 <span class="math display">\[  X_{norm} = \frac{X-X_{min}}{X_{max}-X_{min}}\]</span>用于输入范围已知的模型（如神经网络），或者需要对距离敏感的算法（如KNN）。不适用于有异常值的数据，例如有异常大的数值，其他数据会被压缩到很小的数值。</p><p><strong>2. 标准化（Z-scoreNormalization）</strong>：是将数据进行中心化和缩放处理，使得数据的均值为0，标准差为1。这是通过减去数据的均值再除以数据的标准差来实现的，也叫Z-score标准化。假设原始特征的均值为 <span class="math inline">\(\mu\)</span>，方差为 <span class="math inline">\(\sigma\)</span> ，那么标准化公式定义为 <span class="math display">\[  z = \frac{x-\mu}{\sigma}.  \]</span>适用于数据服从高斯分布（正态分布）或在模型中需要假设数据是标准正态分布的情况，尤其在一些线性模型（如线性回归、逻辑回归、支持向量机）和PCA等算法中较为常用。</p><p><strong>3.对比分析</strong>：归一化可以保持原数据的形状和分布，仅改变其取值范围，因此不会改变数据的分布类型（例如正态分布仍是正态分布）。标准化会改变了数据的中心和尺度，将数据转化为标准正态分布，因此数据的分布会被改变。</p><h3 id="l1和l2正则为什么l1比l2更容易产生稀疏解"><a href="">1.11.L1和L2正则，为什么L1比L2更容易产生稀疏解</a></h3><p>L1和L2正则，都可以防止过拟合，增强模型的泛化能力；区别在于L1使参数更稀疏，达到特征选取的作用；L2使参数更接近于0.</p><p><img src="/2024/10/08/statistic/l1_l2.jpeg"></p><p><strong>从解空间的形状来看：</strong>L1正则项约束后的解空间是多边形，而L2正则项约束后的解空间是圆形。而多边形的解空间更容易在尖角处与等高线碰撞出稀疏解。图中红色等高线表示不同正则参数下的残差平方和，椭圆中心点为最小二乘估计。绿色区域分别表示使用<span class="math inline">\(l_1\)</span> 正则函数和<span class="math inline">\(l_2\)</span>正则函数对应的约束域。等高线与约束域的切点表示目标函数的最优解。从图中可以看出，当使用<span class="math inline">\(l_1\)</span>正则函数时，最优解有可能为稀疏解。</p><p><strong>从函数叠加的观点：</strong>L2正则化使得权重衰减，降低模型复杂度，避免模型过拟合问题。（1）通过限制权重的大小，L2正则化可以让模型更为“平滑”，即更关注输入特征的整体趋势而不是单个特征的微小变化。这降低了模型过度拟合训练数据中的噪声。（2）权重较小的模型通常更具泛化性，因为它们对数据中偶然的扰动（噪声）不太敏感。权重减小后，模型在验证集或测试集上也会有更好的表现。</p><h2 id="经典机器学习算法">2、经典机器学习算法</h2><h3 id="线性回归和逻辑回归">线性回归和逻辑回归</h3><h4 id="线性回归">线性回归</h4><p>线性回归的五个基本假设条件： 1.自变量（解释变量）与因变量（响应变量）之间为线性关系。 2.自变量之间相互独立，无多重共线性。如果存在多重共线性，就很难确定每个预测因子的单独影响。可以通过计算皮尔逊相关系数，或者方差膨胀因子系数（VIF)进行检验。3.误差项之间相互独立，不存在自相关性。尤其是对时间序列数据尤为重要。可以使用DW检验。如果存在自相关性，可以采取自回归模型。4.误差项与自变量之间相互独立，无内生性。这一假设可确保自变量真正独立于误差项，且不会产生遗漏变量偏差。可以使用工具变量法等进行检验和处理。5.误差项应该呈正态分布，期望为0，方差为定值。这两个假设是为了保证回归模型在小样本下能够顺利进行假设检验，在进行假设检验（如计算p值或置信区间）时尤为重要。可以使用Q-Q图可视化来检验是否满足正态分布，Q-Q图趋近于落在一条直线上，说明残差满足正态分布。如果误差项的方差不是恒值（可以通过可视化残差观察到），那么存在异方差性，可以使用加权回归、稳健回归等方法解决。</p><p><strong>关于内生变量和外生变量</strong>：与干扰项（误差项）相关的变量称为内生变量(endogenousvariable)；与干扰项不相关的变量称为外生变量(exogenousvariable)。对于线性回归模型，自变量会对因变量产生影响，干扰项也会对因变量产生影响，且干扰项与自变量假设无关。那么此时，自变量就是外生变量，因变量是内生变量。但是有时，可能由于某种原因，干扰项也会对因变量产生一定影响，此时干扰项和因变量相关，出现内生性。主要原因有遗漏变量、双向因果和测量误差等导致无法满足第四条假设。</p><p>线性回归模型： <span class="math display">\[y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + ...+ \beta_p x_p + \epsilon.\]</span> 通过最小化残差平方和SSE来进行求解。最小二乘解为： <span class="math display">\[\hat{\beta} = (X^T X)^{-1}X^T y.\]</span></p><p><strong>模型评估</strong>： MSE 和 决定系数 <span class="math inline">\(R^2\)</span>。二者的具体定义可见section 1.5。</p><ul><li>线性假设指的是模型参数的线性，而不一定是原始数据的线性。可以引入变换或非线性特征（如交叉项<span class="math inline">\(x_1 \times x_2\)</span>、多项式项 <span class="math inline">\(x_1^2\)</span>），只要因变量与参数之间的关系保持线性即可。此时加入非线性特征（如交叉特征或多项式项）并不违反线性回归的假设，因为模型的参数仍然是"线性的"。</li></ul><p><strong>多重共线性</strong></p><p>当线性回归模型中的两个或多个自变量高度相关，导致信息重叠或冗余时，就会产生多重共线性。在这种情况下，模型很难分离出每个自变量对因变量的单独影响，从而导致不可靠的系数估计、标准误差膨胀以及解释上的挑战。</p><p>如何检测和解决多重共线性问题？ 1. 方差膨胀因子 (VIF)：VIF是一种常见的诊断工具，用于衡量回归系数的方差因多重共线性而膨胀的程度。VIF超过 5 或 10 表示多重共线性很高。 2.相关矩阵：检查自变量的相关矩阵可以发现高度相关的变量对（相关性接近 1或-1）。这表明存在潜在的多重共线性。 3.放弃其中一个相关变量：如果两个或多个变量高度相关，可考虑放弃其中一个变量。这可以简化模型并减少多重共线性。4. 主成分分析（PCA）：PCA可以将相关变量转化为一组不相关的成分，用于回归分析。这可以降低数据维度，避免多重共线性。5.岭回归或lasso回归：这些正则化技术有助于减轻多重共线性的影响。岭回归会对系数的大小进行惩罚，从而降低系数对多重共线性的敏感性（不具备变量筛选的能力，无法完全解决）。lasso回归则更进一步，可以将某些系数缩减为零，从而有效地选择预测因子的子集。</p><h4 id="逻辑回归">逻辑回归</h4><p>逻辑回归是一种广泛用于二元分类任务的监督学习算法。在机器学习中，监督学习包括对输入输出对进行模型训练，以学习能够预测未见数据的模式。逻辑回归专门预测基于输入特征的分类结果的概率，其中结果属于两个类别之一。虽然逻辑回归可以扩展到多个类别，但其最常见的应用还是二元分类。</p><p>逻辑回归模型： <span class="math display">\[p(y = 1 | x) = sigmoid(z) = \frac{1}{1 + e^{-z}}, z = {\beta}_0 +{\beta}_1 x_1 + ...+ {\beta}_k x_k.\]</span></p><p><strong>模型评估</strong></p><p>逻辑回归模型使用损失函数进行评估，该函数用于衡量模型预测真实结果的能力。对于二元分类，合适的损失函数是Log-Loss（也称为二元交叉熵）。对于有 <span class="math inline">\(n\)</span> 个样本的给定数据集，Log-Loss 的定义为:<span class="math display">\[Log-loss = - \sum_{i=1}^{n} [y_i \log p_i + (1-y_i)\log (1-p_i)],\]</span> 其中，<span class="math inline">\(y_i\)</span> 为第<span class="math inline">\(i\)</span>个样本的真实标签，<span class="math inline">\(p_i\)</span> 为第<span class="math inline">\(i\)</span>个样本的输出概率。</p><p><strong>系数估计算法</strong></p><p>有两种主流的方式去估计模型的参数 <span class="math inline">\({\beta}\)</span>，（1）梯度下降；（2）最大似然估计（MLE).</p><p>最大似然估计：对于逻辑回归，定义函数 <span class="math inline">\(h_{\beta}(x) = \frac{1}{1 +e^{-z}}\)</span>，其似然函数为所有样本观测值出现概率的乘积，可以表示为：<span class="math display">\[L(\beta) = \prod_{i=1}^{n} [h_{\beta}(x_i)]^{y_i}[1 -h_{\beta}(x_i)]^{(1-y_i)}.\]</span>最大化该似然函数，通常会转化为最大化其对数似然，然后用梯度下降法求解。对数似然函数为：<span class="math display">\[\log L(\beta) = \sum_{i=1}^{n} [y_i \log h_{\beta}(x_i)+(1-y_i)\log(1 -h_{\beta}(x_i))].\]</span> 显然，最大化对数似然等价于最小化log-loss。</p><p><strong>多标签分类</strong>：假设每个样本属于不同标签的概率服从几何分布，可以使用softmaxregression进行分类： $$ h_= = </p><p></p><p> $$ 其中 <span class="math inline">\(\theta_1,\theta_2 \dots,\theta_k\in \mathbb{R}^n\)</span></p><p>如果存在样本可能属于多个标签的情况时，可以训练k个二分类的逻辑回归分类器。第i个分类器用以区分每个样本是否可以归为第i类。</p><h4 id="二者之间的联系">二者之间的联系</h4><p>如果把一个事件的几率（odds）定义为该事件发生的概率与不发生概率的比值<span class="math inline">\(\frac{p}{1-p}\)</span>，那么逻辑回归可以看做是对于"y=1|x"这一事件的对数几率的线性回归 <span class="math display">\[{\rm log} \frac{p}{1-p} = \theta^{T}x ，其中\ p  = P(y=1|x).\]</span></p><h3 id="knn">KNN</h3><p><strong>建模流程</strong></p><p>（1）根据给定的距离度量，在训练集 <span class="math inline">\(T\)</span> 中找出与 <span class="math inline">\(x\)</span> 最邻近的 <span class="math inline">\(k\)</span>个点，涵盖这 <span class="math inline">\(k\)</span> 个点的邻域记作 <span class="math inline">\(N_k(x)\)</span>；</p><p>（2）在<span class="math inline">\(N_k(x)\)</span>中根据分类决策规则（如多数表决）决定<span class="math inline">\(x\)</span> 的类别 <span class="math inline">\(y\)</span>： <span class="math display">\[y=\arg \max _{c_{j}} \sum_{x_{i} \in N_{k}(x)}I\left(y_{i}=c_{j}\right), \quad i=1,2, \cdots, N_{i} \quad j=1,2,\cdots, K\]</span> 在上式中，<span class="math inline">\(I\)</span>为指示函数，即当<span class="math inline">\(y_{i}=c_{j}\)</span>时为1，否则为0.</p><p><strong>knn优点：</strong></p><ol type="1"><li><p>理论成熟，思想简单，既可以用来做分类又可以做回归</p></li><li><p>KNN是一种在线技术，新数据可以直接加入数据集而不必进行重新训练</p></li><li><p>可用于非线性分类（数据集不要求线性可分）</p></li><li><p>和朴素贝叶斯之类的算法比，对数据没有假设，准确度高，对异常点不敏感</p></li></ol><p><strong>knn缺点：</strong></p><ol type="1"><li>计算量大，尤其是数据集非常大的时候</li><li>样本不平衡的时候，对稀有类别的预测准确率低</li><li>KD树，球树之类的模型建立需要大量的内存</li><li>k值大小的选择很重要</li></ol><p>[ ] <a href="#2-2-3">2-2-3 Knn适合什么样的场景和数据类型？</a></p><p>通常最近邻分类器使用于特征与目标类之间的关系为比较复杂的数字类型，或者说二者关系难以理解，但是相似类间特征总是相似。</p><p>数据要求归一化，统一各个特征的量纲。</p><ul class="task-list"><li><p><label><input type="checkbox"><a href="#2-2-4">2-2-4常用的距离衡量公式都有哪些？具体说明它们的计算流程，以及使用场景？</a></label></p><p>特征空间 <span class="math inline">\(\mathcal X\)</span>是n维实数向量空间 <span class="math inline">\(\mathbf{R}^n\)</span>，<span class="math inline">\(x_i,x_j\in \mathcal{X}, x_i = (x_i^{(1)},x_i^{(2)},\cdots x_i^{(n)}  ),  x_j = (x_j^{(1)}, x_j^{(2)}, \cdots,x_j^{(n)})\)</span> 。则 <span class="math inline">\(x_i,x_j\)</span>的<span class="math inline">\(L_p\)</span>距离（闵可夫斯基距离）定义为<span class="math display">\[L_p(x_i, x_j) = (\sum_{l=1}^n |x_i^{(l)}-x_j^{(l)}|)^{\frac{1}{p}}\]</span> 这里 $p $。</p><p>1.<strong>欧式距离</strong></p><p>当 <span class="math inline">\(p=2\)</span>时，称为欧氏距离，强调数值上的绝对误差</p><p>是严格定义的距离，满足正定性、对称性、三角不等式 <span class="math display">\[L_2(x_i, x_j) = (\sum_{l=1}^n |x_i^{(l)}-x_j^{(l)}|)^{\frac{1}{p}}\]</span> 2.<strong>曼哈顿距离</strong>（p=1） <span class="math display">\[L_1(x_i, x_j) = \sum_{l=1}^n |x_i^{(l)}-x_j^{(l)}|\]</span> 3.<strong>切比雪夫距离</strong>（<span class="math inline">\(p= \infty\)</span>），各个坐标距离数值差的绝对值的最大值 <span class="math display">\[L_{\infty}(x_i, x_j) = \mathop{\max}_{l} \  |x_i^{(l)}-x_j^{(l)}|\]</span> 4.<strong>马氏距离</strong></p></li></ul><p>考虑各个分量（特征）之间的相关性并与各个分量的尺度无关。给定一个样本集合<span class="math inline">\(X\)</span>，<span class="math inline">\(X=(x_{ij})_{m\timesn}\)</span>，其协方差矩阵记为<span class="math inline">\(S\)</span>。样本<span class="math inline">\(x_i\)</span>与样本<span class="math inline">\(x_j\)</span>之间的马氏距离<span class="math inline">\(d_{ij}\)</span>定义为 <span class="math display">\[d_{ij} = [(x_i - x_j)^TS^{-1}(x_i - x_j)]^{\frac{1}{2}}\]</span> 当<span class="math inline">\(S\)</span>为单位矩阵时，即样本数据的各个分量互相独立且各个分量的方差为1时，马氏距离就是欧氏距离。</p><p><strong>汉明距离</strong></p><p>两个等长字符串之间的汉明距离是两个字符串对应位置的不同字符的个数</p><p>1011101 与 1001001 之间的汉明距离是 2。</p><p>2143896 与 2233796 之间的汉明距离是 3。</p><p>"toned" 与 "roses" 之间的汉明距离是 3。</p><p>5.<strong>相关系数</strong>（correlation coefficient）</p><p>相关系数的绝对值越接近1，表示样本越相似；越接近0，表示样本越不相似。</p><p><span class="math inline">\(x_i\)</span>与<span class="math inline">\(x_j\)</span>之间的相关系数定义为 <span class="math display">\[r_{ij} =\frac{\sum_{k=1}^{m}\left(x_{k i}-\overline{x}_{i}\right)\left(x_{kj}-\overline{x}_{j}\right)}{\left[\sum_{k=1}^{m}\left(x_{ki}-\overline{x}_{i}\right)^{2} \sum_{k=1}^{m}\left(x_{kj}-\overline{x}_{j}\right)^{2}\right]^{\frac{1}{2}}}\]</span></p><p><span class="math display">\[\overline{x}_{i}=\frac{1}{m} \sum_{k=1}^{m} x_{k i}, \quad\overline{x}_{j}=\frac{1}{m} \sum_{k=1}^{m} x_{k j}\]</span></p><p>4.<strong>余弦相似度</strong></p><p>强调方向上的相对误差</p><p>不是严格定义的距离，满足正定性、对称性，不满足三角不等式 <span class="math display">\[  cos(A,B) = \frac{A \cdot B}{||A||_2 ||B||_2}\]</span> 5.<strong>KL散度</strong></p><p>计算两个分布的差异性</p><p>不是严格定义的距离，满足正定性，不满足对称性、三角不等式</p><p><strong>使用场景</strong></p><p>欧氏距离：适用于向量各分量的度量标准统一的情况；当某些特征比其他特征取值大很多时，精确度会变差，很多特征值为0，即稀疏矩阵，结果不准，数据点的分布是某个圆心的半径，用欧式距离就不能比较了。</p><p>曼哈顿距离：适用于计算类似街区距离这样的实际问题。异常值对分类结果影响比欧式距离小。量纲不同时使用曼哈顿距离比欧式距离好。</p><p><strong>总结</strong></p><p>用距离度量相似度时，距离越小样本越相似；用相关系数时，相关系数越大样本越相似。</p><ul class="task-list"><li><label><input type="checkbox"><a href="#2-2-5">2-2-5超参数K值过大或者过小对结果有什么影响，你是如何选择K值？</a></label></li></ul><p>如果选择较小的K值，就相当于用较小的领域中的训练实例进行预测，“学习”近似误差会减小，只有与输入实例较近或相似的训练实例才会对预测结果起作用，与此同时带来的问题是“学习”的估计误差会增大，换句话说，K值的减小就意味着整体模型变得复杂，容易发生过拟合；</p><p>如果选择较大的K值，就相当于用较大领域中的训练实例进行预测，其优点是可以减少学习的估计误差，但缺点是学习的近似误差会增大。这时候，与输入实例较远（不相似的）训练实例也会对预测器作用，使预测发生错误，且K值的增大就意味着整体的模型变得简单。</p><p>K=N，则完全不足取，因为此时无论输入实例是什么，都只是简单的预测它属于在训练实例中最多的累，模型过于简单，忽略了训练实例中大量有用信息。</p><p>在实际应用中，K值一般取一个比较小的数值，例如采用交叉验证法（简单来说，就是一部分样本做训练集，一部分做测试集）来选择最优的K值。</p><ul class="task-list"><li><p><label><input type="checkbox"><a href="#2-2-6">2-2-6介绍一下Kd树？如何建树，以及如何搜索最近节点？</a></label></p><p>kd树是一种对k维空间中的实例点进行存储，以便对其进行快速检索的树形数据结构。kd树是二叉树，表示对k维空间的一个划分。构造kd树相当于不断地用垂直于坐标轴的超平面将k维空间切分，构成一系列的k维超矩形区域。kd树的每个节点对应于一个k维超矩形区域。</p><p><strong>构造平衡kd树过程：</strong></p><p>输入：k维空间数据集 <span class="math inline">\(T=\left\{x_{1},x_{2}, \cdots, x_{N}\right\}\)</span>，其中<span class="math inline">\(x_{i}=\left(x_{i}^{(1)}, x_{i}^{(2)}, \cdots,x_{i}^{(k)}\right)^{\mathrm{T}}, \quad i=1,2, \cdots,N_{\mathrm{i}}\)</span></p><p>输出：kd树</p><p>（1）开始：构造根节点，根节点对应于包含T的k维空间的超矩形区域</p><p>​ 选择 $x^{(1)} $ 为坐标轴，以T中所有实例的 $x^{(1)} $坐标的中位数为切分点，将根节点对应的超矩形区域且分为两个子区域。切分由通过切分点并与坐标轴<span class="math inline">\(x^{(1)}\)</span>垂直的超平面实现。</p><p>​ 由根节点生成深度为1的左、右子节点：左子节点对应坐标 <span class="math inline">\(x^{(1)}\)</span>小于切分点的子区域，右子节点对应于坐标<span class="math inline">\(x^{(1)}\)</span> 大于切分点的子区域。</p><p>​ 将落在切分超平面上的实例点保存在根节点。</p><p>（2）重复：对深度为<span class="math inline">\(j\)</span>的节点，选择 <span class="math inline">\(x^{(l)}\)</span>为切分的坐标轴，<span class="math inline">\(l = j({\rm mod}\ k) +1\)</span>，以该节点的区域中所有实例的 <span class="math inline">\(x^{(l)}\)</span>坐标的中位数为切分点，将该节点对应的超矩形区域切分为两个子区域。切分由通过切分点并与坐标轴<span class="math inline">\(x^{(l)}\)</span>垂直的超平面实现。</p><p>​ 由该节点生成深度为 <span class="math inline">\(j+1\)</span>的左、右子节点：左子节点对应坐标 <span class="math inline">\(x^{(l)}\)</span>小于切分点的子区域，右子节点对应坐标<span class="math inline">\(x^{(l)}\)</span>大于切分点的子区域。</p><p>​ 将落在切分超平面上的实例点保存在根节点。</p><p>（3）直到两个子区域没有实例存在时停止，从而形成kd树的区域划分</p><p><img src="https://raw.githubusercontent.com/zhengjingwei/image-bed/master/kdtree-1.jpg"></p><p><img src="https://raw.githubusercontent.com/zhengjingwei/image-bed/master/kdtree-2.jpg"></p><p><strong>算法：用kd树的最近邻搜索</strong></p><p>输入：已构造的kd树：目标点<span class="math inline">\(x\)</span>；</p><p>输出：<span class="math inline">\(x\)</span>的最近邻</p><p>（1）在kd树中找出包含目标点<span class="math inline">\(x\)</span>的叶节点：从根节点出发，递归地向下访问kd树。若目标点<span class="math inline">\(x\)</span>当前维的坐标小于切分点的坐标，则移动到左子节点，否则移动到右子节点。直到子节点为叶节点为止。</p><p>（2）以此叶节点为“当前最近点”。</p><p>（3）递归地向上回退，在每个节点进行以下操作：</p><p>​（a）如果该节点保存的实例点比当前最近点距离目标点更近，则以该实例点作为“当前最近点”</p><p>​（b）当前最近点一定存在于该节点一个子节点对应的区域。检查该子节点的父节点的另一子节点（兄弟节点）对应区域是否有更近的点。具体地，检查另一子节点对应的区域是否与以目标点为球心、以目标点与“当前最近点”间的距离为半径的超球体相交。</p><p>​如果相交，可能在另一个子节点对应的区域内存在距目标点更近的点，移动到另一个子节点。接着递归地进行最近邻搜索；</p><p>​ 如果不相交，向上回退</p><p>（4）当回退到根节点时，搜索结束。最后的“当前最近点”即为<span class="math inline">\(x\)</span>的最近邻点。</p><p>如果实例点是随机分布的，kd树搜索的平均计算复杂度是 <span class="math inline">\(O({\rm log} N)\)</span>，这里 <span class="math inline">\(N\)</span>是训练实例数。<strong>kd树更适用与训练实例数远大于空间维数时的k近邻搜索。</strong>当空间维数接近训练实例数时，它的效率会迅速下降，几乎接近线性扫描。</p><p><img src="https://raw.githubusercontent.com/zhengjingwei/image-bed/master/kdtree-3.jpg"></p></li></ul><h3 id="支持向量机-svm">支持向量机 SVM</h3><p>hinge loss + kernel method</p><p><a href="#2-3-1">2-3-1 简单讲解SVM模型原理？</a></p><p>支持向量机是是一种二类分类模型，它的基本模型是是定义在特征空间的<strong>间隔最大的线性分类器</strong>，间隔最大，间隔最大使它有别于感知机；支持向量机还包括<strong>核技巧</strong>，这使它成为实质上的非线性分类器。支持向量机的学习策略是<strong>间隔最大化</strong>，可形式化为求解凸二次规划的问题，也<strong>等价于正则化的合页损失函数最小化问题</strong>。</p><p>线性可分支持向量机：当训练数据线性可分，通过硬间隔最大化，学习一个线性的分类器</p><p>线性支持向量机：当训练数据近似线性可分，通过软间隔最大化，学习一个线性的分类器</p><p>非线性支持向量机：当训练数据线性不可分，通过使用核技巧及软间隔最大化，学习非线性分类器</p><p><a href="#2-3-2">2-3-2SVM为什么会对缺失值敏感？实际应用时候你是如何处理？</a></p><p>涉及到距离度量(distancemeasurement)时，如计算两个点之间的距离，缺失数据就变得比较重要。如果缺失值处理不当就会导致效果很差，如SVM，KNN。</p><p>常用的缺失值处理方法：</p><p>（1）把数值型变量(numericalvariables)中的缺失值用其所对应的类别中(class)的中位数(median)替换。把描述型变量(categoricalvariables)缺失的部分用所对应类别中出现最多的数值替代(most frequentnon-missingvalue)。【快速简单但效果差】（平均数、中位数、众数、插值等）</p><p>（2）将缺失值当成新的数值，NaN</p><p>（3）忽略该项数据（当缺失少时）</p><p><a href="#2-3-3">2-3-3 SVM为什么可以分类非线性问题？</a></p><p>原输入空间是一个非线性可分问题，能用一个超曲面将正负例正确分开；</p><p>通过核技巧的非线性映射，将输入空间的超曲面转化为特征空间的超平面，原空间的非线性可分问题就变成了新空间的的线性可分问题。低维映射到高维。</p><p>在核函数 <span class="math inline">\(K(x,z)\)</span>给定的条件下，可以利用解线性分类问题的方法求解非线性分类问题的支持向量机。学习是隐式地在特征空间进行的，在学习和预测中只定义核函数<span class="math inline">\(K(x,z)\)</span>，而不需要显式地定义特征空间和映射函数<span class="math inline">\(\phi\)</span>，这样的技巧成为核技巧。通常直接计算<span class="math inline">\(K(x,z)\)</span>比较容易，而通过<span class="math inline">\(\phi(x)\)</span>和<span class="math inline">\(\phi(z)\)</span>计算<span class="math inline">\(K(x,z)\)</span>并不容易。</p><p>对于给定核 <span class="math inline">\(K(x,z)\)</span>，特征空间和映射函数的取法并不唯一。</p><p><a href="#2-3-4">2-3-4常用的核函数有哪些？你是如何选择不同的核函数的？</a></p><p><strong>核函数定义</strong>：设<span class="math inline">\(\mathcal{X}\)</span>是输入空间，又设<span class="math inline">\(\mathcal{H}\)</span>为特征空间，如果存在一个从<span class="math inline">\(\mathcal{X}\)</span>到<span class="math inline">\(\mathcal{H}\)</span>的映射 <span class="math display">\[\phi(x) : \mathcal{X} \rightarrow \mathcal{H}\]</span> 使得对所有<span class="math inline">\(x, z \in\mathcal{X}\)</span>，函数<span class="math inline">\(K(x,z)\)</span>满足条件 <span class="math display">\[K(x, z)=\phi(x) \cdot \phi(z)\]</span> 则称<span class="math inline">\(K(x,z)\)</span>为核函数，<span class="math inline">\(\phi(x)\)</span>为映射函数，式中<span class="math inline">\(\phi(x) \cdot \phi(z)\)</span><span class="math inline">\(为\phi(x)\)</span>和<span class="math inline">\(\phi(z)\)</span>的内积</p><p><strong>线性核函数</strong> <span class="math display">\[K(x,z) = x\cdot z\]</span>主要用于线性可分的情况。可以看到特征空间到输入空间的维度是一样的，其参数少速度快，对于线性可分数据，其分类效果很理想，因此我们通常首先尝试用线性核函数来做分类，看看效果如何，如果不行再换别的。</p><p><strong>多项式核函数</strong>（polynomial kernel function） <span class="math display">\[K(x, z)=(x \cdot z+1)^{p}\]</span> 对应的支持向量机是一个p次多项式分类器。分类决策函数为 <span class="math display">\[f(x)=\operatorname{sign}\left(\sum_{i=1}^{N_{s}} a_{i}^{*}y_{i}\left(x_{i} \cdot x+1\right)^{p}+b^{*}\right)\]</span>多项式核函数可以实现将低维的输入空间映射到高维的特征空间，但是多项式核函数的参数多，当多项式的阶数比较高的时候，核矩阵的元素值将趋于无穷大或者无穷小，计算复杂度会大到无法计算。</p><p><strong>高斯核函数</strong>（Gaussian kernel function） <span class="math display">\[K(x,z) = exp(-\frac{1}{2} \ ||x - z ||_2 ) = \phi(x) \cdot \phi(z)\]</span> 对应的支持向量机是高斯径向基函数（radial basisfunction）分类器，分类决策函数为 <span class="math display">\[f(x)=\operatorname{sign}\left(\sum_{i=1}^{N_{s}} a_{i}^{*} y_{i} \exp\left(-\frac{\|x-x_i\|^{2}}{2 \sigma^{2}}\right)+b^{*}\right)\]</span>高斯径向基函数是一种局部性强的核函数，其可以将一个样本映射到一个更高维的空间内，该核函数是应用最广的一个，无论大样本还是小样本都有比较好的性能，而且其相对于多项式核函数参数要少，因此大多数情况下在不知道用什么核函数的时候，优先使用高斯核函数。</p><p><strong>Sigmod核函数</strong> <span class="math display">\[K\left(x, z\right)=\tanh \left(\eta \ x \cdot z +\theta\right)\]</span> 总结</p><ul><li>如果特征的数量大到和样本数量差不多，则选用LR或者线性核的SVM；<ul><li>（特征维度高，往往线性可分，SVM解决非线性分类问题的思路就是将样本映射到更高维的特征空间中）</li></ul></li><li>如果样本数量很多，由于求解最优化问题的时候，目标函数涉及两两样本计算内积，使用高斯核明显计算量会大于线性核，所以手动添加一些特征，使得线性可分，然后可以用LR或者线性核的SVM</li><li>如果特征的数量小，样本的数量正常，则选用SVM+高斯核函数；</li></ul><p><a href="#2-3-5">2-3-5 RBF核函数一定线性可分么？为什么</a></p><p>根据Cover定理，从低维度映射到高维度后，线性可分的可能性比较大。</p><p>而RBF核函数将原始空间映射到无穷维的特征空间，基本线性可分（也有线性不可分的情况，如加入噪声：同一样本不同标签）。如果忽略噪声，同时允许一定限度的误差，可以说升到足够高的维度，几乎所有数据集都是线性可分了。</p><p>同时，维度特别高，几乎一定线性可分，也以为这模型特别复杂，几乎一定会碰到过拟合问题。</p><p><a href="#2-3-6">2-3-6SVM属于线性模型还是非线性模型？为什么？</a></p><p>基本模型是一个线性分类器；而通过使用核函数可以学习非线性支持向量机</p><p><a href="#2-3-7">2-3-7训练误差为0的SVM分类器一定存在吗？说明原因？</a></p><p>对于训练一个不加入松弛变量的SVM模型时，一定存在。百面p55</p><p>对于加入松弛变量的SVM的训练误差不一定能达到0</p><p><a href="#2-3-7">2-3-8当用支持向量机进行分类时，支持向量越多越好还是越少越好</a></p><p>结论：在n维特征空间中，线性SVM一般会产生n+1个支持向量（不考虑退化情况）</p><p>通常的SVM的使用会伴随着核技巧（kernel），这用于将低维空间映射到一个更高维的空间，使得原本不线性可分的数据点变得在高维空间中线性可分。虽然这种映射是隐式的，我们通常并不知道映射到的空间是什么样子。但是根据之前的结论，我们可以认为如果训练出来的SVM有d+1个支持向量，这个kernel在这个任务里就讲原来的数据映射到了一个d维的空间中，并使得其线性可分。</p><p>更高的维度通常意味着更高的模型复杂度，所以支持向量越多，表示着训练得到的模型越复杂。根据泛化理论，这意味着更有过拟合的风险。</p><p>如果在性能一致的情况下，更少的支持向量可能是更好的。但是这一点其实不绝对，因为泛化理论仅仅是误差的上界，实际的泛化情况的决定因素比较复杂，也可能取决于kernel的性质。所以还是自己做crossvalidation比较好。</p><p>[ ] <a href="">2-3-9 多类分类问题</a></p><ol type="1"><li><p>某些算法原生的支持多分类，如：决策树、最近邻算法等。但是有些算法只能求解二分类问题，如：支持向量机。</p></li><li><p>对于只能求解二分类问题的算法，一旦遇到问题是多类别的，那么可以将多分类问题拆解成二分类任务求解。</p><p>即：</p><ul><li>先对原问题进行拆分，然后为拆出的每个二分类任务训练一个分类器。</li><li>测试时，对这些二分类器的预测结果进行集成，从而获得最终的多分类结果。</li></ul></li><li><p>多分类问题有三种拆解方式：</p><ul><li><p>一对其余(<code>One-vs-rest:OvR</code>) 。</p><ul><li>为每一对类别训练一个分类器。</li></ul></li><li><p>一对一(<code>one-vs-one:OvO</code>) 。</p><ul><li>训练k(k-1)个分类器</li></ul></li><li><p>多对多(<code>many-vs-many:MvM</code>) 。</p></li></ul></li></ol><h3 id="朴素贝叶斯模型">朴素贝叶斯模型</h3><ul class="task-list"><li><label><input type="checkbox"><a href="2-4-1">2-4-1讲解贝叶斯定理？</a> <span class="math display">\[P\left(B_{i} | A\right)=\frac{P\left(B_{i}\right) P\left(A |B_{i}\right)}{\sum_{j=1}^{n} P\left(B_{j}\right) P\left(A |B_{j}\right)}\]</span></label></li></ul><p>[ ] <a href="#2-4-2">2-4-2什么是条件概率、边缘概率、联合概率？</a></p><p><strong>条件概率</strong>：条件概率表示在条件<span class="math inline">\(Y=b\)</span>成立的情况下，<span class="math inline">\(X=a\)</span>的概率，记作<span class="math inline">\(P(X=a|Y=b)\)</span>或<span class="math inline">\(P(a|b)\)</span>。它具有如下性质：“在条件Y=b下X的条件分布”也是一种“X的概率分布”，因此穷举X的可取值之后，所有这些值对应的概率之和为1即： <span class="math display">\[  \sum_{a} P(X=a | Y=b)=1  \]</span><strong>边缘概率</strong>：仅与单个随机变量有关的概率称为边缘概率，如<span class="math inline">\(P(X=a)\)</span> 或 <span class="math inline">\(P(Y=b)\)</span></p><p><strong>联合概率</strong>：联合概率指的是包含多个条件且<strong>所有条件同时成立</strong>的概率，记作<span class="math inline">\(P(X=a,Y=b)\)</span>或<span class="math inline">\(P(a,b)\)</span></p><p>联合概率、边缘概率与条件概率的关系： <span class="math display">\[  P(X=a | Y=b)=\frac{P(X=a, Y=b)}{P(Y=b)}  \]</span></p><p>[ ] <a href="#2-4-3">2-4-3 后验概率最大化的含义是什么？</a></p><p>朴素贝叶斯法将实例分到后验概率最大的类中。后验概率最大化这等价于期望风险最小化。</p><p>假设选择0-1损失函数： <span class="math display">\[  L(Y, f(X))=\left\{\begin{array}{ll}{1,} &amp; {Y \neq f(X)} \\ {0,}&amp; {Y=f(X)}\end{array}\right.  \]</span> 其中 <span class="math inline">\(f(X)\)</span>是分类决策函数。这是期望风险函数为<span class="math display">\[  R_{\operatorname{cap}}(f)=E[L(Y, f(X))]  \]</span> 期望是对联合分布 <span class="math inline">\(P(X,Y)\)</span>取的。由此取条件期望 <span class="math display">\[  R_{\mathrm{exp}}(f)=E_{X} \sum_{k=1}^{K}\left[L\left(c_{k},f(X)\right)\right] P\left(c_{k} | X\right)  \]</span> 为了使期望奉献最小化，只需对 <span class="math inline">\(X=x\)</span> 逐个最小化，由此得到 <span class="math display">\[  \begin{aligned} f(x) &amp;=\arg \min _{y \in \mathcal{Y}}\sum_{k=1}^{K} L\left(c_{k}, y\right) P\left(c_{k} | X=x\right) \\&amp;=\arg \min _{y \in \mathcal{Y}} \sum_{k=1}^{K} P\left(y \neq c_{k}| X=x\right) \\ &amp;=\arg \min _{y \in\mathcal{Y}}\left(1-P\left(y=c_{k} | X=x\right)\right) \\ &amp;=\arg\max _{y \in \mathcal{Y}} P\left(y=c_{k} | X=x\right) \end{aligned}  \]</span> 这样一来，根据期望风险最小化准则就得到了后延概率最大化准则：<span class="math display">\[  f(x)=\arg \max _{c_{k}} P\left(c_{k} | X=x\right)  \]</span> 即朴素贝叶斯法所采用原理</p><p>[ ] <a href="#2-4-4">2-4-4朴素贝叶斯模型如何学习的？训练过程是怎样？</a></p><p>对于给定的训练数据集，首先基于特征条件独立性假设学习输入输出的联合概率分布；</p><p>然后基于此模型，对给定的输入x，利用贝叶斯定理求出后验概率最大的输出y。</p><p><strong>训练过程</strong>：</p><p>（1）计算先验概率及条件概率 <span class="math display">\[  \begin{array}{l}{P\left(Y=c_{k}\right)=\frac{\sum_{i=1}^{N}I\left(y_{i}=c_{k}\right)}{N}, \quad k=1,2, \cdots, K} \\{P\left(X^{(j)}=a_{j l} | Y=c_{k}\right)=\frac{\sum_{i=1}^{N}I\left(x_{i}^{(j)}=a_{j}, y_{i}=c_{k}\right)}{\sum_{i=1}^{N}I\left(y_{i}=c_{k}\right)}} \\ {j=1,2, \cdots, n ; \quad l=1,2, \cdots,S_{j} ; \quad k=1,2, \cdots, K}\end{array}  \]</span> （2）对于给定实例 <span class="math inline">\(x=\left(x^{(1)}, x^{(2)}, \cdots,x^{(n)}\right)^{\mathrm{T}}\)</span>，计算 <span class="math display">\[  P\left(Y=c_{k}\right) \prod_{i=1}^{n} P\left(X^{(j)}=x^{(j)} |Y=c_{k}\right), \quad k=1,2, \cdots, K  \]</span> （3）确定实例x的类（最大后验概率） <span class="math display">\[  y = \arg\max_{c_k}\ P(Y=c_k)\prod_{j}P(X^{(j)}=x^{(j)}|Y=c_k)  \]</span></p><p>[ ] <a href="#2-4-5">2-4-5 你如何理解生成模型和判别模型？</a></p><p><strong>生成方法</strong>由数据学习联合概率分布 <span class="math inline">\(P(X,Y)\)</span>，然后求出条件概率分布 <span class="math inline">\(P(Y|X)\)</span>作为预测模型： <span class="math display">\[  P(Y|X) = \frac{P(X,Y)}{P(X)}  \]</span> 之所以成为生成方法，是因为模型表示了给定输入 <span class="math inline">\(X\)</span> 产生输出 <span class="math inline">\(Y\)</span>的生成关系。</p><p>典型的生成模型：朴素贝叶斯法、隐马尔可夫模型。</p><p><strong>判别方法</strong>由数据直接学习决策函数 <span class="math inline">\(f(X)\)</span> 或者条件概率分布 <span class="math inline">\(P(X,Y)\)</span>作为预测的模型，关心的是对给定的输入<span class="math inline">\(X\)</span>，应该预测什么样的输出 <span class="math inline">\(Y\)</span>。</p><p>典型的判别模型：k近邻、感知机、决策树、逻辑斯蒂回归、最大熵模型、支持向量机、提升方法、条件随机场。</p><p>[ ] <a href="#2-4-6">2-4-6朴素贝叶斯模型“朴素”体现在哪里？存在什么问题？有哪些优化方向？</a></p><p>"朴素"体现在朴素贝叶斯模型对条件概率分布作了<strong>条件独立性假设</strong>，这是一个较强的假设。<span class="math display">\[  \begin{aligned} P(X&amp;=x | Y=c_{k} )=P\left(X^{(1)}=x^{(1)}, \cdots,X^{(n)}=x^{(n)} | Y=c_{k}\right) \\ &amp;=\prod_{j=1}^{n}P\left(X^{(j)}=x^{(j)} | Y=c_{k}\right) \end{aligned}  \]</span>存在问题：当特征分布不满足条件独立性假设时，分类的性能不高</p><p>优化方法：贝叶斯网络</p><p>[ ] <a href="#2-4-7">2-4-7什么是贝叶斯网络？它能解决什么问题？</a></p><p>朴素贝叶斯法假设输入变量都是条件独立的，如果假设他们之间<strong>存在概率依存关系</strong>，模型就被成了贝叶斯网络。</p><p>贝叶斯网络也称为“信念网”，借助<strong>有向无环图</strong>来刻画属性之间的依赖关系，并使用<strong>条件概率表</strong>来描述属性的联合概率分布。贝叶斯网结构有效地表达了属性的条件独立性。</p><p>具体来说，一个贝叶斯网B由结构G和参数 <span class="math inline">\(\theta\)</span> 表示，即 <span class="math inline">\(B = &lt;G,\theta&gt;\)</span>。网络结构G是一个邮箱无环图，其每个节点对应于一个属性，若两个属性有直接依赖关系，则它们由一条边连接起来；参数<span class="math inline">\(\theta\)</span>定量描述这种依赖关系，假设属性 <span class="math inline">\(x_i\)</span>在G中的父节点集为 <span class="math inline">\(\pi_i\)</span>，则 <span class="math inline">\(\theta\)</span>包含了每个属性的条件概率 <span class="math inline">\(\theta_{x_i|\pi_i} = P_B(x_i|\pi_i)\)</span>。</p><p>给定父节点集，贝叶斯网假设每个属性与它的非后裔属性独立，于是将属性的联合概率分布定义为</p><p><span class="math display">\[P_{B}\left(x_{1}, x_{2}, \ldots, x_{d}\right)=\prod_{i=1}^{d}P_{B}\left(x_{i} | \pi_{i}\right)=\prod_{i=1}^{d} \theta_{x_{i} |\pi_{i}}以上图为例，联合概率分布定义为\]</span></p><p>​<br><span class="math display">\[P\left(x_{1}, x_{2}, x_{3}, x_{4}, x_{5}\right)=P\left(x_{1}\right)P\left(x_{2}\right) P\left(x_{3} | x_{1}\right) P\left(x_{4} | x_{1},x_{2}\right) P\left(x_{5} | x_{2}\right)\]</span> ​ <strong>贝叶斯网学习过程</strong></p><p>​精确求解NP难，所以（1）贪心法：从某个网络结构出发，每次调整一条边，直到评分函数值不再降低（2）给网络结构施加约束条件：如限定为树形结构等。</p><p>​ <strong>贝叶斯网推断</strong></p><p>​理想情况根据贝叶斯网定义的联合概率分布来精确计算后验概率，但这样的精确推断时NP难的。近似推断采用吉布斯采样法，通过随机游走，使马尔可夫链趋于平稳分布。</p><p>[ ] <a href="#2-4-8">2-4-8为什么说朴素贝叶斯也是线性模型而不是非线性模型呢？</a></p><p>线性分类器是通过特征的线性组合来做出分类决定的分类器。本质上，朴素贝叶斯分类器是一种线性分类器。</p><p>朴素贝叶斯分类器是建立在属性变量相互独立的基础上，后验概率为判定准则的分类器。不等式1成立，则样例x=[x_1,...,x_n]为正类。否则，样例为负类。</p><ol type="1"><li></li></ol><figure><img src="https:////upload-images.jianshu.io/upload_images/1385318-0c8849c769c7cae3?imageMogr2/auto-orient/strip%7CimageView2/2/w/291/format/webp" alt="img"><figcaption aria-hidden="true">img</figcaption></figure><p>线性分类器直观地来说，是在高维样本空间中找到一组超平面，将样本空间划分了两个区域。每个区域对应于不同的类别。数学上来说，线性分类器能找到权值向量w，使得判别公式可以写成特征值的线性加权组合。</p><ol start="2" type="1"><li></li></ol><figure><img src="https:////upload-images.jianshu.io/upload_images/1385318-2443a8f4fea70ee7?imageMogr2/auto-orient/strip%7CimageView2/2/w/137/format/webp" alt="img"><figcaption aria-hidden="true">img</figcaption></figure><p>如果公式2成立，则样本属于正类；反之，则样本属于负类。</p><hr><p><strong>离散特征的朴素贝叶斯分类器</strong></p><p>一般离散特征的取值范围有两种，{-1,1}或者{0,1}。这两种取值方式不会影响分析。不妨假设离散特征的取值范围为{-1,1}。下面的不等式成立，样例x=[x_1,...,x_n]为正类。(3)</p><figure><img src="https:////upload-images.jianshu.io/upload_images/1385318-fbf05fdf34f5fdd5?imageMogr2/auto-orient/strip%7CimageView2/2/w/385/format/webp" alt="img"><figcaption aria-hidden="true">img</figcaption></figure><p>对于某个特征x，我们很容易推导出下面的公式</p><ol start="4" type="1"><li></li></ol><figure><img src="https:////upload-images.jianshu.io/upload_images/1385318-f7280559286d0abc?imageMogr2/auto-orient/strip%7CimageView2/2/w/526/format/webp" alt="img"><figcaption aria-hidden="true">img</figcaption></figure><p>其中p(x|F)也有类似的结果，从而有 (5) <img src="https:////upload-images.jianshu.io/upload_images/1385318-101318b723db87f3?imageMogr2/auto-orient/strip%7CimageView2/2/w/490/format/webp" alt="img"></p><p>将公式5带入朴素贝叶斯分类器的公式3，得到下面的公式 (6)</p><figure><img src="https:////upload-images.jianshu.io/upload_images/1385318-d3e418e1bef50305?imageMogr2/auto-orient/strip%7CimageView2/2/w/529/format/webp" alt="img"><figcaption aria-hidden="true">img</figcaption></figure><p>根据公式6，离散特征的朴素贝叶斯分类器判别公式能够写成特征值的加权线性组合。也就是说，离散特征的朴素贝叶斯分类器本质上是线性分类器。</p><p>[ ] <a href="#2-4-9">2-4-9如何解决用极大似然法可能出现所要估计的概率为0的情况？</a></p><p>分子加1，分母加可能情况数</p><p>用极大似然法估计可能会出现所要估计的概率值为0的情况。这是会影响到后验概率的计算结果，使分类产生偏差。解决这一问题的方法是采用贝叶斯估计。具体地，条件概率的贝叶斯估计是<span class="math display">\[  P_{\lambda}\left(X^{(j)}=a_{j l} | Y=c_{k}\right)=\frac{\sum_{i=1}^{N}I\left(x_{i}^{(j)}=a_{j l}, y_{i}=c_{k}\right)+\lambda}{\sum_{i=1}^{N}I\left(y_{i}=c_{k}\right)+S_{j} \lambda}  \]</span> 式中 <span class="math inline">\(\lambda \geq0\)</span>。等价于在随机变量各个取值的频数上赋予一个正数 <span class="math inline">\(\lambda\)</span>。当<span class="math inline">\(\lambda=0\)</span>时就是极大似然估计。常取 <span class="math inline">\(\lambda=1\)</span>，这时称为拉普拉斯平滑（laplaciansmoothing）。显然，对任何<span class="math inline">\(l=1,2, \cdots,S_{j}, \quad k=1,2, \cdots, K\)</span> 有 <span class="math display">\[  \begin{array}{l}{P_{\lambda}\left(X^{(j)}=a_{j l} |Y=c_{k}\right)&gt;0} \\ {\sum_{l=1}^{s_{j}} P\left(X^{(j)}=a_{j l} |Y=c_{k}\right)=1}\end{array}  \]</span></p><p>表明上式确实为一种概率分布。同样，先验概率的贝叶斯估计是 <span class="math display">\[  P_{\lambda}\left(Y=c_{k}\right)=\frac{\sum_{i=1}^{N}I\left(y_{i}=c_{k}\right)+\lambda}{N+K \lambda}  \]</span></p><h3 id="fm模型">FM模型</h3><ul class="task-list"><li><p><label><input type="checkbox"><a href="#2-7-1">2-7-1FM模型与逻辑回归相比有什么优缺点？</a></label></p><p>优点：</p><ul><li>存储空间和计算复杂度减小</li><li>解决样本过于稀疏训练不充分的问题</li></ul><p>缺点：</p><ul><li>记忆能力不如LR</li></ul></li><li><p><label><input type="checkbox"><a href="#2-7-2">2-7-2为什么FM模型计算复杂度时O(kn)？</a></label></p></li><li><p><label><input type="checkbox"><a href="#2-7-3">2-7-3介绍FFM场感知分解机器（Field-aware FactorizationMachine），说说与FM异同？</a></label></p></li><li><p><label><input type="checkbox"><a href="#2-7-4">2-7-4使用FM进行模型训练时候，有哪些核心参数对模型效果影响大？</a></label></p></li><li><p><label><input type="checkbox"><a href="#2-7-5">2-7-5如何从神经网络的视角看待FM模型？</a></label></p></li></ul><h3 id="决策树">决策树</h3><ul class="task-list"><li><label><input type="checkbox" checked=""><a href="#2-8-1">2-8-1讲解完成的决策树的建树过程</a></label></li></ul><p>自上而下，对样本数据进行树形分类的过程。每个内部节点表示一个特征，叶节点表示一个类别。从顶部根节点开始，所有的样本聚在一起。经过根节点的划分，样本被分到不同的子节点，再根据子节点的特征进一步划分，直至所有样本被归到某一个类别（叶节点）中。</p><ul class="task-list"><li><label><input type="checkbox" checked=""><a href="#2-8-2">2-8-2你是如何理解熵？从数学原理上解释熵公式可以作为信息不确定性的度量？</a></label></li></ul><p>熵（entropy）是表示随机变量不确定性的度量， <span class="math inline">\(X\)</span>是一个取有限个值的离散随机变量，其概率分布为 <span class="math display">\[P(X = x_i) = p_i, \ i=1,2,\cdots,n\]</span> 则随机变量 <span class="math inline">\(X\)</span> 的熵定义为<span class="math display">\[H(X) = -\sum_{i=1}^{n} p_i {\rm log } \ p_i\]</span> 熵越大，随机变量的不确定性就越大。</p><p>而熵其实表示的是一个系统的平均信息量。<strong>自信息量</strong>是用来描述某一条信息的大小<span class="math display">\[I = - {\rm log} \ p_i\]</span>通常以2为底，单位是bit；含义是用多少位二进制可以表示衡量该信息的大小。而通常我们衡量整个系统的信息量，系统存在多个事件<span class="math inline">\(X=\{x_1,\cdots,x_n\}\)</span>，每个事件的概率分布<span class="math inline">\(P=\{p_1,\cdots,p_n\}\)</span>，<strong>熵是整个系统的平均信息量</strong> 。</p><ul class="task-list"><li><label><input type="checkbox"><a href="#2-8-3">2-8-3联合熵、条件熵、KL散度、信息增益、信息增益比、gini系数都是什么？如何计算？</a></label></li></ul><p><strong>联合熵</strong>：将一维随机变量分布推广到多维随机变量分布<span class="math display">\[H(X,Y) = -\sum\limits_{x,y} p(x,y)\ {\rm log}\ p(x,y)\]</span> <strong>条件熵</strong>：某个特征A对于数据集D的经验条件熵<span class="math inline">\(H(D|A)\)</span> 为 <span class="math display">\[H(D|A) = - \sum_{i=1}^{n} \frac{|D_i|}{|D|} H(D_i) \\ = - \sum_{i=1}^{n}\frac{|D_i|}{|D|} \lgroup \sum_{k=1}^{K} \frac{|D_{ik}|}{|D_i|} {\rm log} \frac{|D_{ik}|}{|D_i|} \rgroup\]</span> <strong>信息增益</strong>： <span class="math inline">\(g(D,A)\)</span> 定义为数据集D的经验熵 <span class="math inline">\(H(D)\)</span> 与特征A给定条件下D的经验条件熵 <span class="math inline">\(H(D|A)\)</span> 的差 <span class="math display">\[g(D,A) = H(D) - H(D|A)\]</span></p><p><strong>信息增益比</strong>：特征A对于数据集D 的信息增益比定义为<span class="math display">\[g_R(D|A) = \frac{g(D|A)}{H_A(D)}\]</span> 其中 <span class="math display">\[H_A{(D)} = - \sum_{i=1}^{n} \frac{|D_i|}{|D|} {\rm log }\frac{|D_i|}{|D|}\]</span> 为数据集D关于A的取值熵；n为特征A在D上的取值数目；</p><p><strong>Gini系数</strong>：描述数据的不确定性。数据集D的Gini系数为<span class="math display">\[{\rm Gini}(D) = 1 - \sum_{k=1}^{K}(\frac{|C_k|}{|D|})^2\]</span> 其中 <span class="math inline">\(C_k\)</span>是D中第k类的样本子集，K是类的个数。例如二分类问题，K=2。基尼系数越大，样本集合的不确定性也就越大，这一点与熵相似。基尼系数Gini(D,A)表示经A=a分割后集合D的不确定性。</p><p><strong>交叉熵</strong>：刻画两个概率分布之间的距离，通过q来表示p的交叉熵为；一般<strong>p(x)为真实分布</strong>，<strong>q(x)为预测分布</strong></p><p>交叉熵不对称。交叉熵越小，概率分布越接近 <span class="math display">\[H(p,q) = - \sum\limits_{x} p(x) {\rm log } \ q(x)\]</span> <strong>KL散度/相对熵</strong>： <span class="math display">\[D_{K L}(p \| q)=\sum_{i=1}^{n} p\left(x_{i}\right) \log\left(\frac{p\left(x_{i}\right)}{q\left(x_{i}\right)}\right)\]</span>n表示事件可能发生的情况总数，KL散度的值越小表示两个分布越接近。 <span class="math display">\[D_{KL}(p||q) = H(p,q) - H(p)\]</span></p><p>机器学习中，我们常常使用KL散度来评估predict和label之间的差别，但是由于KL散度的后半部分是一个常量，所以我们常常将前半部分的交叉熵作为损失函数，其实二者是一样的。</p><ul class="task-list"><li><label><input type="checkbox" checked=""><a href="#2-8-4">2-8-4常用的决策树有哪些？ID3、C4.5、CART有啥异同？</a></label></li></ul><table><colgroup><col style="width: 11%"><col style="width: 29%"><col style="width: 26%"><col style="width: 32%"></colgroup><thead><tr><th>不同点</th><th>ID3</th><th>C4.5</th><th>CART</th></tr></thead><tbody><tr><td>原则</td><td>信息增益最大</td><td>信息增益比最大</td><td>划分后集合基尼指数最小</td></tr><tr><td>用途</td><td>分类</td><td>分类</td><td>分类、回归</td></tr><tr><td>输入取值</td><td>离散</td><td>离散、连续</td><td>离散、连续</td></tr><tr><td>树结构</td><td>多叉树</td><td>多叉树</td><td>二叉树</td></tr><tr><td></td><td>特征在层级间不复用</td><td>特征在层级间不复用</td><td>每个特征可被重复利用</td></tr><tr><td></td><td>对样本特征缺失值敏感</td><td></td><td></td></tr></tbody></table><p><strong>ID3 最大信息增益</strong></p><p>信息增益 <span class="math inline">\(g(D,A)\)</span>定义为数据集D的经验熵 <span class="math inline">\(H(D)\)</span>与特征A给定条件下D的经验条件熵 <span class="math inline">\(H(D|A)\)</span> 的差 <span class="math display">\[g(D,A) = H(D) - H(D|A)\]</span> 选择 <span class="math inline">\(g(D,A)\)</span>最大的特征，所有样本根据此特征，划分到不同的节点上。在经验熵不为0的节点中继续生长。ID3算法只有树的生成，容易产生过拟合。</p><p><strong>C4.5 最大信息增益比</strong></p><p>因为信息增益对取值数目多的属性有所偏好，为了减少这种偏好带来的影响，使用信息增益比来选择最优划分属性。</p><p><strong>CART 基尼指数</strong></p><p>基尼系数Gini（D）用来表示集合D的不确定性。CART在每一次迭代中选择划分后<strong>基尼指数最小</strong>的特征及其对应的切分点进行分类。CART是一颗二叉树，每次将数据按特征A的区分分成两份，分别进入左右子树。</p><ul class="task-list"><li><label><input type="checkbox"><a href="#2-8-5">2-8-5决策树如何防止过拟合？前剪枝和后剪枝过程是怎样的？剪枝条件都是什么</a></label></li></ul><p>通过<strong>剪枝</strong>防止过拟合。</p><p><strong>预剪枝</strong>是指在决策树生成的过程中，对每个节点在划分前先进行估计，若当前节点的划分不能带来决策树泛化性能提升，则停止划分，并将当前节点标记为叶子节点；此时可能存在不同类别的样本同时存于同个节点中，按照多数投票的原则判断节点所属类别</p><p>预剪枝对于何时停止决策树的生长：</p><p>（1）当树达到一定深度</p><p>（2）当到达当前节点的样本数量小于某个阈值</p><p>（3）计算每次分裂对测试集的准确度提升，小于某个阈值时停止</p><p><strong>后剪枝</strong>则是先从训练集生成一棵完整的决策树，然后自底向上地对<strong>非叶子节点</strong>进行考察，若该节点对应的<strong>子树替换成叶子结点</strong>能带来泛化性能提升，则将该子树替换为叶子节点。</p><h3 id="随机森林rf">随机森林（RF）</h3><ul class="task-list"><li><label><input type="checkbox"><a href="#2-9-1">2-9-1介绍RF原理和思想</a></label></li><li><label><input type="checkbox"><a href="#2-9-2">2-9-2RF是如何处理缺失值？</a></label></li><li><label><input type="checkbox"><a href="#2-9-3">2-9-3RF如何衡量特征重要度？</a></label></li><li><label><input type="checkbox"><a href="#2-9-4">2-9-4RF“随机”主要体现在哪里？</a></label></li><li><label><input type="checkbox"><a href="#2-9-5">2-9-5RF有哪些优点和局限性？</a></label></li><li><label><input type="checkbox"><a href="#2-9-6">2-9-6为什么多个弱分类器组合效果会比单个要好？如何组合弱分类器可以获得更好的结果？原因是什么？</a></label></li><li><label><input type="checkbox"><a href="#2-9-7">2-9-7Bagging的思想是什么？它是降低偏差还是方差，为什么？</a></label></li></ul><p>Bagging的思想是通过对数据再抽样，然后在每组样本上训练出来的模型取平均。Bagging是降低方差，防止过拟合。可以这样理解，对n个独立不相关的模型的预测结果取平均，方差是原来单个模型的<span class="math inline">\(1/n\)</span> 。</p><ul class="task-list"><li><label><input type="checkbox"><a href="#2-9-8">2-9-8可否将RF的基分类模型由决策树改成线性模型或者knn？为什么？</a></label></li></ul><p>随机森林属于bagging类的集成学习方法，主要好处是减小集成后分类器的方差，比基分类器的方差小。所以Bagging所采用的的基分类器最好是本身对样本分布较为敏感（不稳定分类器），这样bagging才能体现效果。而线性分类器和KNN属于较为稳定的分类器，本身方差不大，所以将他们作为基分类器使用bagging不能再原基分类器的基础上获得更好的表现。相反地，可能因为bagging的采样而使得训练中难以收敛从而增大集成分类器的偏差。</p><h3 id="gbdt">GBDT</h3><p>梯度提升<strong>GradientBoosting</strong>的基本思想是根据当前模型损失函数<strong>负梯度</strong>信息来训练新加入的弱分类器，然后将训练好的弱分类器以<strong>累加</strong>的形式结合到现有的模型中。</p><p>GBDT（梯度提升决策树）的核心思想：每一棵树学的是之前所有树结果和的残差，这个残差就是加上预测之后能得到真实值的累加量。</p><ul class="task-list"><li><label><input type="checkbox"><a href="#2-10-1">2-10-1梯度提升和梯度下降有什么区别和联系？</a></label></li></ul><p>两者都是在每一轮迭代中，利用损失函数相对于模型的负梯度方向的信息来对当前模型进行更新，只不过在梯度下降中，模型是以参数化的形式表示，从而模型的更新等价于参数的更新。</p><p>而在梯度提升中，模型并不需要参数化表示，而是直接定义在函数空间中，从而大大扩展了可以使用的模型种类。</p><ul class="task-list"><li><label><input type="checkbox"><a href="#2-10-2">2-10-2你是如何理解Boosting和Bagging？他们有什么异同？</a></label></li></ul><p>Bagging通过模型集成降低方差，提高弱分类器的性能。</p><p>Boosting通过模型集成降低偏差，提高弱分类器的性能。</p><table><thead><tr><th></th><th>Bagging</th><th>Boosting</th></tr></thead><tbody><tr><td>降低</td><td>方差</td><td>偏差</td></tr><tr><td>训练</td><td>各个弱分类器可独立训练</td><td>弱分类器需要依次生成</td></tr><tr><td>典型方法</td><td>随机森林</td><td>Adaboost，GBDT，XGBoost</td></tr><tr><td></td><td></td><td></td></tr></tbody></table><ul class="task-list"><li><label><input type="checkbox"><a href="#2-10-3">2-10-3讲解GBDT的训练过程？</a></label></li></ul><p>用每个样本的残差训练下一棵树，直到残差收敛到某个阈值以下，或者树的总数达到某个上限为止。</p><ul class="task-list"><li><label><input type="checkbox"><a href="#2-10-4">2-10-4你觉得GBDT训练过程中哪些环节可以平行提升训练效率？</a></label></li></ul><p>决策树内部局部并行。</p><ul class="task-list"><li><label><input type="checkbox"><a href="#2-10-5">2-10-5GBDT的优点和局限性有哪些？</a></label></li></ul><p><strong>优点</strong></p><p>（1）预测阶段计算速度块，树与树之间可并行化计算</p><p>（2）在分布稠密的数据集上，泛化能力和表达能力都很好</p><p>（3）采用决策树作为弱分类器使得GBDT模型具有较好的可解释性和鲁棒性，能够自动发现特征间的高阶关系，并且不需要对数据进行特殊的预处理如归一化等</p><p><strong>缺点</strong></p><p>（1）GBDT在高维稀疏的数据集上，表现不如支持向量机或者神经网络</p><p>（2）GBDT在处理文本分类特征问题上，优势不如在处理数值特征时明显</p><p>（3）训练过程需要串行训练，只能在决策树内容采用一些局部并行手段提高训练速度</p><ul class="task-list"><li><label><input type="checkbox"><a href="#2-10-6">2-10-6GBDT是否对异常值敏感，为什么？</a></label></li></ul><p><strong>GDBT对异常值敏感</strong>。对于回归类的问题，如果采用平方损失函数。当出现异常值时，后续模型会对异常值关注过多。</p><ul class="task-list"><li><label><input type="checkbox"><a href="#2-10-7">2-10-7如何防止GBDT过拟合？</a></label></li></ul><ol type="1"><li><p>在工程应用中，通常利用下列公式来更新模型： <span class="math inline">\(f_{m}(\overrightarrow{\mathbf{x}})=f_{m-1}(\overrightarrow{\mathbf{x}})+\nuh_{m}\left(\overrightarrow{\mathbf{x}} ; \Theta_{m}\right), \quad0&lt;\nu \leq 1\)</span>。</p><p>其中 <span class="math inline">\(\nu\)</span>称作<strong>学习率</strong>。</p><p>学习率是正则化的一部分，它可以降低模型更新的速度（需要更多的迭代）。</p><ul><li>经验表明：一个小的学习率 (<span class="math inline">\(\nu&lt;0.1\)</span>)可以显著提高模型的泛化能力（相比较于<span class="math inline">\(\nu=1\)</span> ) 。</li><li>如果学习率较大会导致预测性能出现较大波动。</li></ul></li><li><p><code>Freidman</code> 从<code>bagging</code>策略受到启发，采用<strong>随机梯度提升</strong>来修改了原始的梯度提升树算法。</p></li></ol><ul><li><p>每一轮迭代中，新的决策树拟合的是原始训练集的一个子集（而并不是原始训练集）的残差。</p><p>这个子集是通过对原始训练集的无放回随机采样而来。</p></li><li><p>子集的占比 <span class="math inline">\(f\)</span>是一个超参数，并且在每轮迭代中保持不变。</p><ul><li>如果 <span class="math inline">\(f=1\)</span>，则与原始的梯度提升树算法相同。</li><li>较小的<span class="math inline">\(f\)</span>会引入随机性，有助于改善过拟合，因此可以视作一定程度上的正则化。</li><li>工程经验表明， <span class="math inline">\(0.5 \leq f \leq0.8\)</span>会带来一个较好的结果。</li></ul></li><li><p>这种方法除了改善过拟合之外，另一个好处是：未被采样的另一部分子集可以用来计算包外估计误差。</p><p>因此可以避免额外给出一个独立的验证集。</p></li></ul><ol start="3" type="1"><li><p>梯度提升树会<strong>限制每棵树的叶子结点包含的样本数量至少包含<span class="math inline">\(m\)</span> 个样本</strong>，其中 <span class="math inline">\(m\)</span>为超参数。在训练过程中，一旦划分结点会导致子结点的样本数少于<span class="math inline">\(m\)</span>，则终止划分。</p><p>这也是一种正则化策略，它会改善叶结点的预测方差。</p></li></ol><ul class="task-list"><li><p><label><input type="checkbox"><a href="#2-10-8">2-10-8在训练过程中哪些参数对模型效果影响比较大？这些参数造成影响是什么？</a></label></p><p>减小步长需要对应增加最大迭代次数</p><ol type="1"><li><p><strong>n_estimators</strong>:也就是弱学习器的最大迭代次数，或者说最大的弱学习器的个数。一般来说n_estimators太小，容易欠拟合，n_estimators太大，又容易过拟合，一般选择一个适中的数值。默认是100。在实际调参的过程中，我们常常将n_estimators和下面介绍的参数learning_rate一起考虑。</p></li><li><p><strong>learning_rate</strong>:即每个弱学习器的权重缩减系数νν，也称作步长，在原理篇的正则化章节我们也讲到了，加上了正则化项</p></li><li><p><strong>subsample</strong>:即我们在原理篇的正则化章节讲到的子采样，取值为(0,1]。注意这里的子采样和随机森林不一样，随机森林使用的是放回抽样，而这里是不放回抽样。如果取值为1，则全部样本都使用，等于没有使用子采样。如果取值小于1，则只有一部分样本会去做GBDT的决策树拟合。选择小于1的比例可以减少方差，即防止过拟合，但是会增加样本拟合的偏差，因此取值不能太低。推荐在[0.5,0.8]之间，默认是1.0，即不使用子采样。</p></li></ol></li></ul><h3 id="k-means">k-means</h3><p>kmean的总体特点</p><ul><li><p>基于划分的聚类方法</p></li><li><p>类别k事先指定</p></li><li><p>以欧式距离平方表示样本之间的距离</p></li><li><p>以中心或样本的均值表示类别</p></li><li><p>以样本和其所属类的中心之间的距离总和为最优化的目标函数</p></li><li><p>得到的类别是平坦的、非层次化的</p></li><li><p>算法是迭代算法，不能保证全局最优</p></li><li><p><label><input type="checkbox"><a href="#2-11-1">2-11-1简述kmeans建模过程？</a></label></p><p>kmeans聚类是基于样本集合划分的聚类算法</p><p>（1）首先随机选择k个样本点作为初始聚类中心</p><p>（2）计算每个样本到类中心的距离，将样本逐个指派到与其最近的类中，得到一个聚类结果</p><p>（3）更新每个类的样本的均值，作为新的中心</p><p>（4）重复以上步骤，知道划分不再改变，收敛为止</p><p>kmeans的算法复杂度是<span class="math inline">\(O(mnk)\)</span>，其中m是样本位数，n是样本个数，k是类别个数。比层次聚类复杂度低。</p></li><li><p><label><input type="checkbox"><a href="#2-11-2">2-11-2Kmeans损失函数是如何定义？</a></label></p><p>样本与所属类的中心之间的距离的总和为损失函数 <span class="math display">\[W(C) = \sum_{l=1}^k \sum_{C(i)=l} {||x_i - \overline{x}_l||}\]</span> 其中 <span class="math inline">\(\overline{x}_l\)</span> 是第<span class="math inline">\(l\)</span>个类的均值或中心。相似的样本被聚到同类时，损失函数值最小。但是这是一个组合优化问题，n个样本分到k个类，可能的分法数目是指数级的，NP难问题。采样迭代的方法求解。</p></li><li><p><label><input type="checkbox"><a href="#2-11-3">2-11-3你是如何选择初始类族的中心点？</a></label></p><p><strong>初始类中心点的选择</strong></p><p>（1）可以用层次聚类对样本进行聚类，得到k个类时停止。然后从每个类中选取一个与中心距离最近的点。</p><p><strong>类别数k的选择</strong></p><p>k值需要预先指定，而在实际应用中最优的k值是不知道的。解决这个问题的一个方法是尝试用不同的k值聚类，检验各自得到聚类结果的质量，推测最优的k值</p><p>（1）一般地，类别数变小时，平均直径会增加；类别数变大超过某个值后，平均直径会不变；而这个值是最优的k值。实验时可以采用二分查找，快速找到最优的k值。</p></li><li><p><label><input type="checkbox"><a href="#2-11-4">2-11-4如何提升kmeans效率？</a></label></p><p>kmeans时间复杂度 <span class="math inline">\(O(nkt)\)</span>n，k，t分别为样本数，聚类中心数，迭代轮次</p><p>（1）我们可以使用kd树以及ball树（数据结构）来提高k-means算法的效率。（KNN计算的优化）</p><p>（2）并行计算</p></li><li><p><label><input type="checkbox"><a href="#2-11-5">2-11-5常用的距离衡量方法有哪些？他们都适用什么类型问题？</a></label></p><p>见 2-2-3</p></li><li><p><label><input type="checkbox"><a href="#2-11-6">2-11-6Kmeans对异常值是否敏感？为什么？</a></label></p><p>敏感，因为需要计算距离，使用传统的欧式距离度量方式。所以需要预处理离群点、数据归一化。</p></li><li><p><label><input type="checkbox"><a href="#2-11-7">2-11-7如何评估聚类效果？</a></label></p><p>知乎：https://zhuanlan.zhihu.com/p/53840697</p><p>（1）<strong>纯度</strong>（<em>Purity</em>）</p><p>我们把每个簇中最多的类作为这个簇所代表的类，然后计算正确分配的类的数量，然后除以<span class="math inline">\(N\)</span> 。 <span class="math display">\[(\Omega, \mathbb{C})=\frac{1}{N} \sum_{k} \max _{j}\left|\omega_{k} \capc_{j}\right|\]</span> 其中 <span class="math inline">\(\Omega=\left\{\omega_{1},\omega_{2}, \ldots, \omega_{K}\right\}\)</span> 是聚类结果的集合 <span class="math inline">\(\omega_{k}\)</span>表示第k个聚类的集合；<span class="math inline">\(\mathbb{C}=\left\{c_{1}, c_{2}, \ldots,c_{J}\right\}\)</span> 是原始分类的集合，<span class="math inline">\(c_j\)</span>表示第j个分类的集合。</p><figure><img src="https://pic4.zhimg.com/v2-e19781ffa33cc60608fad3fa60d5769f_b.jpg" alt="img"><figcaption aria-hidden="true">img</figcaption></figure><p>purity优点：方便计算，值在0~1之间；</p><p>缺点：当簇的数量很多的时候，容易达到较高的纯度——特别是，如果每个文档都被分到独立的一个簇中，那么计算得到的纯度就会是1。因此，不能简单用纯度来衡量聚类质量与聚类数量之间的关系。</p><p>（2）<strong>归一化化互信息</strong>（NMI, <em>Normalized MutualInformation</em>）</p><p>NMI越大，聚类效果越好 <img src="https://www.zhihu.com/equation?tex=%5Ctext%7BNMI%7D%28%5COmega%2C+C%29+%3D+%5Cfrac%7BI%28%5COmega%3B+C%29%7D%7B%28H%28%5COmega%29%2BH%28C%29%2F2%29%7D%5C%5C" alt="[公式]"></p><p>其中, <img src="https://www.zhihu.com/equation?tex=I" alt="[公式]">表示互信息(Mutual Information), H 为熵，当 <img src="https://www.zhihu.com/equation?tex=+%5Clog" alt="[公式]"> 取 2为底时，单位为 bit，取 e 为底时单位为 nat。</p><figure><img src="https://www.zhihu.com/equation?tex=%5Cbegin%7Bsplit%7D+I%28%5COmega%3B+C%29+%26%3D%5Csum_%7Bk%7D%5Csum_%7Bj%7DP%28w_k+%5Ccap+c_j%29%5Clog+%5Cfrac%7BP%28w_k+%5Ccap+c_j%29%7D%7BP%28w_k%29P%28c_j%29%7D%5C%5C+%26%3D+%5Csum_%7Bk%7D%5Csum_%7Bj%7D%5Cfrac%7B%7Cw_k+%5Ccap+c_j%7C%7D%7BN%7D%5Clog+%5Cfrac%7BN%7Cw_k+%5Ccap+c_j%7C%7D%7B%7Cw_k%7C%7Cc_j%7C%7D+%5Cend%7Bsplit%7D%5C%5C" alt="[公式]"><figcaption aria-hidden="true">[公式]</figcaption></figure><p>其中, <img src="https://www.zhihu.com/equation?tex=P%28w_k%29%2CP%28c_j%29%2CP%28w_k+%5Ccap+c_j%29+" alt="[公式]"> 可以分别看作样本 (document) 属于聚类簇 <img src="https://www.zhihu.com/equation?tex=w_k" alt="[公式]"> , 属于类别<img src="https://www.zhihu.com/equation?tex=c_j" alt="[公式]"> ,同时属于两者的概率。第二个等价式子则是由概率的极大似然估计推导而来。</p><p><img src="https://www.zhihu.com/equation?tex=%5Cbegin%7Bsplit%7D+H%28%5COmega%29+%26%3D+-%5Csum_%7Bk%7DP%28w_k%29%5Clog+P%28w_k%29%5C%5C+%26%3D+-%5Csum_%7Bk%7D+%5Cfrac%7B%7Cw_k%7C%7D%7BN%7D%5Clog+%5Cfrac%7B%7Cw_k%7C%7D%7BN%7D+%5Cend%7Bsplit%7D%5C%5C" alt="[公式]">互信息 <img src="https://www.zhihu.com/equation?tex=I%28%5COmega%3B+C%29" alt="[公式]"> 表示给定类簇信息 <img src="https://www.zhihu.com/equation?tex=C" alt="[公式]">的前提条件下,类别信息 <img src="https://www.zhihu.com/equation?tex=%5COmega" alt="[公式]">的增加量,或者说其不确定度的减少量。直观地,互信息还可以写出如下形式：</p><figure><img src="https://www.zhihu.com/equation?tex=I%28%5COmega%3B+C%29+%3D+H%28%5COmega%29+-+H%28%5COmega+%7C+C%29%5C%5C" alt="[公式]"><figcaption aria-hidden="true">[公式]</figcaption></figure><p>（3）<strong>兰德指数</strong>（RI, <em>RandIndex</em>）能度量聚类过程中的假阳性和假阴性结果的惩罚</p><p>兰德指数 (Rand index, RI),将聚类看成是一系列的决策过程,即对文档集上所有 <img src="https://www.zhihu.com/equation?tex=N%28N-1%29%2F2" alt="[公式]">个文档 (documents)对进行决策。当且仅当两篇文档相似时,我们将它们归入同一簇中。</p><p>Positive:</p><ul><li>TP 将两篇相似文档归入一个簇 (同 - 同)</li><li>TN 将两篇不相似的文档归入不同的簇 (不同 - 不同)</li></ul><p>Negative:</p><ul><li>FP 将两篇不相似的文档归入同一簇 (不同 - 同)</li><li>FN 将两篇相似的文档归入不同簇 (同- 不同) (worse)</li></ul><p>RI 则是计算「正确决策」的比率(精确率, accuracy).</p><figure><img src="https://www.zhihu.com/equation?tex=RI+%3D+%5Cfrac%7BTP%2BTN%7D%7BTP%2BFP%2BTF%2BFN%7D%3D%5Cfrac%7BTP%2BTN%7D%7BC%5E%7B2%7D_%7BN%7D%7D%5C%5C" alt="[公式]"><figcaption aria-hidden="true">[公式]</figcaption></figure><p>（4）<strong>熵</strong>（Entropy）</p></li><li><p><label><input type="checkbox"><a href="#2-11-8">2-11-8超参数类的个数k如何选取？</a></label></p><p>（1）<strong>手肘法</strong></p><p>尝试不同的K值，并将不同K值所对应的损失函数化成折线，横轴为K的取值，纵轴为误差平方和所定义的损失函数。K值越大，距离和越小。当K=K'时，存在一个拐点，像人的肘部。当<span class="math inline">\(K\in(1,K')\)</span>，曲线急速下降；当<span class="math inline">\(K&gt;K'\)</span>，曲线趋于平稳。手肘法认为拐点就是K的最佳值。</p><p>（2）<strong>Gap Statistic</strong></p><p>手肘法是一个经验方法，缺点是不够自动化。GapStatistic方法的优点是，不需要肉眼判断，只需要找到最大Gapstatistic对应的K即可。因此该方法适用于批量化作业。 <span class="math display">\[{\rm Gap}(K) = E({\rm log}D_k) - {\rm log}D_k\]</span> 当分为K簇时，对应的损失函数记为 <span class="math inline">\(D_k\)</span>，<span class="math inline">\(E({\rmlog}D_k)\)</span>是 <span class="math inline">\({\rm log}D_k\)</span>的期望，通过蒙特卡洛模拟产生。$(K) $的物理含义是随机样本的损失与实际样本的损失之差。当$(K)$取最大时，是样本的损失应该相对较小。</p></li><li><p><label><input type="checkbox"><a href="#2-11-9">2-11-9Kmeans有哪些优缺点？是否有了解过改进的模型，举例说明？</a></label></p><p><strong>优点</strong></p><p>（1）对于大数据集，kmeans聚类算法相对是可伸缩和高效的，它的计算复杂度是<span class="math inline">\(O(NKt)\)</span>接近线性，其中N是样本数，K是聚类的簇数，t是迭代的轮数。</p><p>（2）尽管算法经常以局部最优解结束，但一般情况下达到的局部最优已经可以满足聚类的需求</p><p><strong>缺点</strong></p><p>（1）受初值和离群点的影响，每次的结果不稳定</p><p>（2）结果通常不是全局最优而是局部最优解</p><p>（3）无法很好地解决数据簇分布差别比较大的情况（比如一类是另一类样本数量的100倍）</p><p>（4）不太适用于离散分类</p><p>（5）K值需要人工预先确定，且该值和真实的数据分布未必吻合【最大缺点】</p><p>（6）样本点只能被划分到单一的类中</p><p><strong>如何对Kmeans进行调优</strong></p><p>（1）数据归一化和离群点处理</p><p>​Kmeans聚类本质上是一种基于欧式距离度量的数据划分方法，均值和方差大的维度对聚类结果产生决定性影响。未做归一化无法直接参与计算；离群点或少量噪声会对均值产生较大影响，导致中心偏移。</p><p>（2）合理选择K值</p><p>​ 手肘法、Gap Statistic（不需要肉眼判断，只需要找到最大Gapstatistic对应的K）</p><p>（3）采用核函数</p><p>​核聚类。通过非线性映射，将输入空间中的数据点映射到高维的特征空间中，并在新的特征空间中进行聚类。非线性映射增加了数据点线性可分的概率，从而在经典的聚类算法失效的情况下，通过引入核函数可以达到更为准确的聚类效果</p><p><strong>改进的模型</strong></p><p>（1）kmeans++ （对初始值选择的改进）</p><p>​原始的kmeans算法最开始随机选取数据集中K个点作为聚类中心，而kmeans++按照以下思想选取k个聚类中心：假设已经选取了n个初始聚类中心（0&lt;n&lt;k），则在选取第n+1个聚类中心时，距离当前n个聚类中心越远的点会有更高的概率被选为第n+1个聚类中心。在选取第一个聚类中心时同样通过随机的方法。这符合我们的直觉：聚类中心互相离得越远越好。后续步骤与kmeans一致。</p><p>（2）ISODATA（K值不确定时）</p><p>​ ISODATA全称是迭代自组织数据分析法。在kmeans算法中，聚类个数K值需要预先人为确定，并且在整个算法过程中无法更改。当遇到高维度、海量数据时，难以准确估计出K的大小。ISODATA的思想是：当属于某个类别的样本数过少时，把该类别去除（合并操作）；当属于某个类别的样本数过多、分散程度大时，把该类别分为两个子类别（分裂操作）。</p><p>​ 缺点是需要制定的参数比较多，4个：预期聚类中心数目 <span class="math inline">\(K_0\)</span>、每个类所要求的最少样本数目 <span class="math inline">\(N_{min}\)</span>、最大方差 <span class="math inline">\(\sigma\)</span> 、两个聚类中心之间所允许最小距离<span class="math inline">\(D_{min}\)</span>。</p><p>（3）模糊C均值，高斯混合模型</p><p>​ 样本不划分到单一的类中，即软聚类。</p><p>​<strong>高斯混合模型</strong>：假设不同簇中的样本各自服从不同的高斯分布，由此得到的聚类算法称为高斯混合模型。在该假设下，每个单独的分模型都是标准高斯模型，其均值<span class="math inline">\(u_i\)</span> 和方差 <span class="math inline">\(\Sigma_i\)</span>是待估计的参数。瓷王每个分模型还都有一个参数 <span class="math inline">\(\pi_i\)</span>，可以理解为权重或者生成数据的概率。<span class="math display">\[p(x)=\sum_{i=1}^{K}\pi_iN(x|u_i,\Sigma_i)\]</span> ​ 高斯混合模型是一个生成式模型。</p><p>相比Kmeans的优点：可以给出一个样本属于某类的概率是多少；不仅仅可以用于聚类，还可以用于概率密度的估计；并且可以用于生成新的样本点。</p></li><li><p><label><input type="checkbox"><a href="#2-11-10">2-11-10试试证明kmeans算法的收敛性</a></label></p><p>kmeans聚类属于启发式方法，不能保证收敛到全局最优，初始中心的选择会直接影响聚类结果。</p><p>类中心在聚类过程中会发生移动，但是往往不会移动太大，因为在每一步，样本被分到与其最近的中心的类中。</p></li><li><p><label><input type="checkbox"><a href="#2-11-11">2-11-11除了kmeans聚类算法之外，你还了解哪些聚类算法？简要说明原理</a></label></p><p><strong>层次聚类</strong>（hierarchicalclustering）假设类别之间存在层次结构，分为聚合（自下而上）和分裂（自上而下）两种方法。<strong>聚合法</strong>开始讲每个样本各自分到一个类；之后将相邻最近的两类合并（根据类间距离最小合并规则），建立一个新的类，重复此操作直到满足停止条件（类的个数达到阈值/类的直径超过阈值）；得到层次化的分类。<strong>分裂法</strong>开始讲所有样本分到一个类；之后将已有类中相距最远的样本分到两个新的类，重复此操作直到满足停止条件；得到层次化的分类。</p><p>层次聚类算法复杂度：<span class="math inline">\(O(n^3m)\)</span>其中n是样本个数，m是样本维数。</p><p><strong>k-means聚类</strong>是基于中心的聚类方法，通过迭代，将样本分到k个类别中，使得每个样本与其所属类的中心或均值最近；得到k个平坦的、非层次化的类别。</p><p>基于密度的方法DBSCAN</p><p>谱聚类</p></li><li><p><label><input type="checkbox"><a href="">kmeans初始化为什么要从数据中随机挑k个，可以生成k个随机点吗？</a></label></p><p>不可以；如果是用随机值，可能某个簇在第一轮就没有任何节点，无法继续计算</p></li></ul><h3 id="pca降维">PCA降维</h3><ul class="task-list"><li><p><label><input type="checkbox"><a href="#2-12-1">2-12-1为什么要对数据进行降维？它能解决什么问题？</a></label></p><p>高维（多变量）数据，很难观察变量的样本区分能力，也很难观察样本之间的关系。降维是将样本集合中的样本从高维空间转换到低维空间。假设样本原本存在于低维空间，或近似存在与低维空间，通过降维则可以更好地表示样本数据的结构，即更好地表示样本之间的关系。降维有线性降维和非线性降维。</p><p>维度灾难</p></li><li><p><label><input type="checkbox"><a href="#2-12-1">2-12-2你是如何理解维度灾难？</a></label></p><p>特征数量超过一定值的时候，分类器的效果反而下降。原因：特征数过多，过拟合</p></li><li><p><label><input type="checkbox"><a href="#2-12-1">2-12-3PCA主成分分析思想是什么？</a></label></p><p>变量之间可能存在相关性，以致增加了分析的难度。考虑用少数不想管的变量来替代相关的变量，用来表示数据，并且要求能保留数据中的大部分信息。</p><p>PCA利用正交变换把线性相关变量表示的观测数据转换为少数几个由线性无关变量表示的数据，线性无关的变量称为主成分。PCA属于降维方法。</p></li><li><p><label><input type="checkbox"><a href="#2-12-1">2-12-4如何定义主成分？</a></label></p><p>协方差矩阵的特征向量</p></li><li><p><label><input type="checkbox"><a href="#2-12-1">2-12-5如何设计目标函数使得降维达到提取主成分的目的？</a></label></p><p>PCA目标函数：最大化投影方差。因为方差表示新变量的信息量大小。</p></li><li><p><label><input type="checkbox"><a href="#2-12-1">2-12-6PCA有哪些局限性？如何优化</a></label></p><p>（1）无法进行非线性降维</p><p>​ 通过核映射对PCA进行扩展得到核主成分分析（KPCA）</p><p>​通过流形映射的降维方法，比如等距映射、局部线性嵌入（LLE）、拉普拉斯特征映射等</p><p>（2）无监督的，算法没有考虑数据的标签，只是把把元数据映射到方差比较大的方向</p><p>​ 有监督的降维方法：线性判别分析 LDA</p></li><li><p><label><input type="checkbox"><a href="#2-12-1">2-12-7线性判别分析和主成分分析在原理上有何异同？在目标函数上有何区别和联系？</a></label></p><p>PCA选择的是投影后数据方差最大的方向。由于PCA是无监督的，因此假设方差越大，信息量越多，用主成分来表示原始数据可以去除冗余的维度，达到降维。</p><p>LDA选择的是投影后类内方差最小，类间方差最大的方向。其用到了类别标签信息，为了找到数据中具有判别性的维度，使得原始数据在这些方向投影后，不同类别尽可能区分开。</p></li></ul><h2 id="深度学习">3、 深度学习</h2><h3 id="dnn">DNN</h3><ul class="task-list"><li><label><input type="checkbox"><a href="#3-1-1">3-1-1描述一下神经网络？推倒反向传播公式？</a></label></li><li><label><input type="checkbox"><a href="#3-1-2">3-1-2讲解一下dropout原理？</a></label></li></ul><p>在神经网络前向传播的时候，让某个神经元的激活值以一定的概率p停止工作，这样可以使模型泛化性更强，因为它不会太依赖某些局部的特征。</p><ul class="task-list"><li><p><label><input type="checkbox"><a href="#3-1-3">3-1-3梯度消失和梯度膨胀的原因是什么？有什么方法可以缓解？</a></label></p><p>（1）深度学习的网络层数太多，在进行反向传播时根据链式法则，要连乘每一层梯度值</p><p>（2）每一层的梯度值是由，非线性函数的导数以及本层的权重相乘得到的，这样非线性的导数的大小和初始化权重的大小会直接影响是否发生梯度弥散或者梯度爆炸</p><p>注：任何网络都有可能发生梯度弥散或者梯度爆炸，这是深度学习的基本性质决定的，无法避免。</p></li></ul><p>梯度消失（梯度弥散）的原因：</p><p>解决方法：</p><p>梯度爆炸的原因：</p><p>解决方法：</p><ul class="task-list"><li><label><input type="checkbox"><a href="#3-1-4">3-1-4什么时候该用浅层神经网络，什么时候该选择深层网络</a></label></li><li><label><input type="checkbox"><a href="#3-1-5">3-1-5Sigmoid、Relu、Tanh激活函数都有哪些优缺点？</a></label></li></ul><p><strong>Sigmoid</strong> <span class="math display">\[f(x) = \frac{1}{1+ exp(-x)} \\f'(x) = f(x)(1-f(x))\]</span> <span class="math display">\[\begin{aligned}f'(z) &amp;= (\frac{1}{1+e^{-z}})'\\&amp;= \frac{e^{-z}}{(1+e^{-z})^{2}}\\&amp;= \frac{1+e^{-z}-1}{(1+e^{-z})^{2}}  \\&amp;= \frac{1}{(1+e^{-z})}(1-\frac{1}{(1+e^{-z})})\\&amp;= f(z)(1-f(z))\\\end{aligned}\]</span></p><p>优点：</p><p>缺点：（1）需要计算指数，速度慢（2）会产生梯度消失问题</p><p><strong>Tanh</strong> $$ f(x) = tanh(x) = \</p><p>f'(x) = 1 - (f(x))^2 $$ 优点：</p><p>缺点：（1）需要计算指数，速度慢（2）会产生梯度消失问题</p><p><strong>Relu</strong> <span class="math display">\[f(x) = max(x,0) \\\begin{equation}f'(x)=\left\{\begin{aligned}    1,x&gt;0 \\    0,x\leq0 \\\end{aligned}\right.\end{equation}\]</span>优点：（1）从计算的角度上，sigmoid和tanh都需要计算指数，复杂度高，而ReLU只需要一个阈值就可以得到激活值</p><p>（2）ReLU的非饱和性可以有效解决梯度消失的问题，提供相对宽的激活边界</p><p>（3）ReLU的单侧抑制提供了网络的稀疏表达能力（防止过拟合）</p><p>缺点：（1）训练过程中会导致神经元死亡的问题</p><p>缺点：（1）训练过程中会导致神经元死亡的问题</p><p><strong>Leaky ReLU</strong> $$ <span class="math display">\[\begin{equation}f(x)=\left\{\begin{aligned}    x,x&gt;0 \\    ax,x\leq0 \\\end{aligned}\right.\end{equation}\]</span> \</p>f'(x)={<span class="math display">\[\begin{aligned}    1,x&gt;0 \\    a,x\leq0 \\\end{aligned}\]</span><p>. $$ 优点：实现单侧抑制，又保留了部分附体度信息以致不完全消失</p><p>缺点：a值需要人工选择</p><ul class="task-list"><li><label><input type="checkbox"><a href="#3-1-6">3-1-6写出常用激活函数的导数</a></label></li></ul><p><img src="https://cdn.mathpix.com/snip/images/lch_19BxC77kvBQ20p3no_Z71liDpIIAvdPnxoYWoYA.original.fullsize.png"></p><ul class="task-list"><li><label><input type="checkbox"><a href="#3-1-7">3-1-7训练模型的时候，是否可以把网络参数全部初始化为0？为什么</a></label></li></ul><p>不可以；参数全部为0时，网络不同神经元的输出必然相同，相同输出则导致梯度更新完全一样，会使得更新后的参数仍然保持完全相同。从而使得模型无法训练。</p><ul class="task-list"><li><label><input type="checkbox"><a href="#3-1-8">3-1-8Batchsize大小会如何影响收敛速度？</a></label></li></ul><h3 id="cnn">CNN</h3><ul class="task-list"><li><label><input type="checkbox"><a href="#3-2-1">3-2-1简述CNN的工作原理？</a></label></li></ul><p>CNN利用了图像的三个性质：</p><p>（1）图像的pattern通常比整张图像小</p><p>（2）通用的patterns会出现在图像的不同区域</p><p>（3）对图像进行子采样并不影响图像的识别</p><p>CNN通过卷积层+pooling层不断堆积，从小的pattern开始不断识别到大的pattern，从而识别整张图像。</p><blockquote><p>CNN适合处理什么问题</p></blockquote><p>具有以上三个特性的问题</p><ul class="task-list"><li><label><input type="checkbox"><a href="#3-2-2">3-2-2卷积核是什么？选择大卷积核和小卷积核有什么影响？</a></label></li><li><label><input type="checkbox"><a href="#3-2-3">3-2-3你在实际应用中如何设计卷积核？</a></label></li><li><label><input type="checkbox"><a href="#3-2-4">3-2-4为什么CNN具有平移不变性？</a></label></li></ul><p>参数共享的物理意义是使得卷积层具有平移等变性。在卷积神经网路中，卷积核中的每一个元素将作用于每一次局部输入的特定位置上。</p><p>假如图像中有一只猫，无论它出现在图像中的任何位置，我们都应该将它识别为猫，也就是说神经网络的输出对于平移变换来说应当是等变的。</p><ul class="task-list"><li><label><input type="checkbox"><a href="#3-2-5">3-2-5Pooling操作是什么？有几种？作用是什么？</a></label></li></ul><p>将Pooling核覆盖区域中所有值的平均值（最大值）作为汇合结果</p><p>average-pooling，Max-polling，stochastic-polling（对输入数据中的元素按照一定概率大小随机选择，元素值大的activattion被选中的概率大）</p><p>Pooling操作后的结果相比起输入减小了，是一种<strong>降采样</strong>操作。</p><p>pooling层的作用</p><ol type="1"><li><p><strong>特征不变性（featureinvariant）</strong>。pooling操作使模型更关注是否存在某些特征而不是特征具体的位置。</p></li><li><p><strong>数据降维</strong>。降采样的操作使模型可以抽取更广范围的特征，同时减小了下一层输入大小，进而减少计算量和参数个数</p></li></ol><ul class="task-list"><li><p><label><input type="checkbox"><a href="#3-2-6">3-2-6为什么CNN需要pooling操作？</a></label></p></li><li><p><label><input type="checkbox"><a href="#3-2-7">3-2-7什么是batchnormalization？它的原理是什么？在CNN中如何使用？</a></label></p><p>为了解决内部协方差偏移（ICS）问题提出的。高层网络需要不断适应底层输出的分布，导致</p><p>对于某一层某个神经元d维输入<span class="math inline">\(\mathrm{x}=\left(x^{(1)} \ldotsx^{(d)}\right)\)</span>，对每一个维度进行批标准化 <span class="math display">\[\widehat{x}^{(k)}=\frac{x^{(k)}-\mathrm{E}\left[x^{(k)}\right]}{\sqrt{\operatorname{Var}\left[x^{(k)}\right]}}\]</span> <img src="C:\Users\Kevin\AppData\Roaming\Typora\typora-user-images\1567511107747.png" alt="1567511107747"></p><figure><img src="https://cdn.mathpix.com/snip/images/3nV904o42ao1MH6C2EGrKbM59gU5lXNzzy5TFJ5Le_w.original.fullsize.png" alt="1567511156114"><figcaption aria-hidden="true">1567511156114</figcaption></figure><p><strong>BN训练和测试的区别</strong></p><p>先说结论：并不是测试时的mean,var的计算方式与训练时不同，而是测试时的mean,var在训练完成整个网络中就全部固定了。</p><p>由于在优化网络的时候，我们一般采用的是batch梯度下降。所以在训练过程中，只能计算当前batch样本上的mean和var。但是我们做的normalization是对于整个输入样本空间，因此需要对<strong>每个batch</strong>的mean,var做<strong>指数加权平均</strong>来将batch上的mean和var近似成<strong>整个样本空间</strong>上的mean和var.</p><p>而在测试Inference过程中，一般不必要也不合适去计算测试时的batch的mean和var，比如测试仅对单样本输入进行测试时，这时去计算单样本输入的mean和var是完全没有意义的。因此会直接拿训练过程中对整个样本空间估算的mean和var直接来用。此时对于inference来说，BN就是一个线性变换。</p></li><li><p><label><input type="checkbox"><a href="#3-2-8">3-2-8卷积操作的本质特性包括稀疏交互和参数共享，具体解释这两种特性以其作用？</a></label></p></li></ul><p><strong>稀疏交互</strong>：每个神经元的只跟上一层的某些神经元连接（vsDNN全连接），用到较少参数</p><p><strong>参数共享</strong>：同一层的不同神经元之间共享部分权重，用到比原来更少的参数</p><ul class="task-list"><li><p><label><input type="checkbox"><a href="#3-2-9">3-2-9你是如何理解fine-tune？有什么技巧</a></label></p></li><li><p><label><input type="checkbox"><a href="#3-2-10">3-2-10怎么观察CNN每个神经元学到了什么</a></label></p></li></ul><p>假设第k个filter是一个11 x 11的矩阵（一个神经元），可以用以下系数来表示第k个filter被激活的程度 <span class="math display">\[a^k = \sum_{i=1}^{11} \sum_{i=1}^{11} a_{ij}^k\]</span> 并通过梯度上升找到使 <span class="math inline">\(a^k\)</span>最大的x，该x表示的图像表示该filter对应的检测纹路。 <span class="math display">\[x^* = \mathop{arg \ \rm max}\limits_{x}  \ a^k\]</span></p><ul class="task-list"><li><p><label><input type="checkbox"><a href="">resnetskip-connection</a></label></p><p>假如，我们在这个block的旁边加了一条“捷径”（如图5橙色箭头），也就是常说的“skipconnection”。假设左边的上一层输入为x，虚线框的输出为f(x)，上下两条路线输出的激活值相加为h(x)，即h(x)= F(x) + x，得出的h(x)再输入到下一层。</p><figure><img src="https://pic2.zhimg.com/v2-79e9feb7ee38f64c6cc15c501327b7bd_b.jpg" alt="img"><figcaption aria-hidden="true">img</figcaption></figure><p>图6</p><p>当进行后向传播时，右边来自深层网络传回来的梯度为1，经过一个加法门，橙色方向的梯度为dh(x)/dx=1，蓝色方向的梯度也为1。这样，经过梯度传播后，现在传到前一层的梯度就变成了[1,0.0001,0.01]，多了一个“1”！<strong>正是由于多了这条捷径，来自深层的梯度能直接畅通无阻地通过，去到上一层，使得浅层的网络层参数等到有效的训练！</strong></p></li></ul><h3 id="rnn">RNN</h3><ul class="task-list"><li><p><label><input type="checkbox"><a href="#3-3-1">3-3-1简述RNN模型原理，说说RNN适合解决什么类型问题？为什么</a></label></p><p>RNN能够很好地处理文本数据变长并且有序的输入序列。它模拟了人阅读一篇文章的顺序，从前到后阅读文章中的每一个单词，将前面阅读到的有用信息编码到状态变量中去，从而拥有一定的记忆能力，可以更好地理解之后的文本。</p><p>不定长；下一时刻的状态于当前输入以及当前状态有关</p></li><li><p><label><input type="checkbox"><a href="#3-3-2">3-3-2RNN和DNN有何异同？</a></label></p><p>相同点：一个长度为T的序列用xun</p><p>不同点：RNN的循环性，序列的每个时刻都执行相同的任务，每个时刻的输出依赖于当前时刻的输入和上一时刻的隐藏状态</p></li><li><p><label><input type="checkbox"><a href="#3-3-3">3-3-3RNN为什么有记忆功能？</a></label></p><p>RNN是包含<u>循环</u>的网络，将前面输入的有用信息编码到状态变量中去，从而拥有了一定的记忆功能。</p></li><li><p><label><input type="checkbox"><a href="#3-3-4">3-3-4长短期记忆网络LSTM是如何实现长短期记忆功能的？</a></label></p><p>LSTM可以对有价值的信息进行长期记忆。</p><p>与RNN不同的是，LSTM记忆单元c的转移不一定完全取决于激活函数计算得到的状态，还由输入门和遗忘门共同控制。</p><p>在一个训练好的LSTM模型中，当输入序列中没有重要信息时，遗忘门的值接近于1，输入门的值接近于0，表示过去的记忆被完整保存，而输入信息被放弃，从而实现长期记忆功能。</p><p>当输入序列中存在重要信息时，LSTM应把他存入记忆中，此时输入门接近于1；</p><p>当输入序列中存在重要信息且该信息意味着之前的记忆不再重要时，输入门的值接近1，遗忘门的值接近0。</p></li><li><p><label><input type="checkbox"><a href="#3-3-5">3-3-5长短期记忆网络LSTM各模块都使用什么激活函数，可以使用其他激活函数么？</a></label></p><p>百面p245。输入门、输出门、遗忘门使用sigmoid函数作为激活函数；在生成候选记忆时，使用双曲正切函数Tanh作为激活函数。</p><p>首先这两个激活函数都是饱和的，如果使用非饱和函数如ReLU，将难以实现门控的效果。</p><p>sigmoid作为门控信号的原因：sigmoid函数的输出在0~1之间，符合门控的物理定义。并且是饱和的，当输入较大或者较小时，其输出会非常接近1或0，从而保证该门的开或关。</p><p>Tanh用于生成候选记忆的原因：输出在-1~1之间，这与大多数场景下特征分布式以0为中心相吻合；并且Tanh函数在输入为0附近相比sigmoid有更大的梯度，通常使模型收敛更快。</p></li><li><p><label><input type="checkbox"><a href="#3-3-6">3-3-6GRU和LSTM有何异同</a></label></p><p>GRU：（1）将遗忘门和输入门合成了一个单一的更新门，只有两个门：更新门、复位门。（2）GRU不再区分cell的状态<span class="math inline">\(\overrightarrow{\mathrm{C}}\)</span>和cell的输出<span class="math inline">\(\overrightarrow{\mathbf{h}}\)</span>。</p><p><img src="https://upload-images.jianshu.io/upload_images/42741-dd3d241fa44a71c0.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1000/format/webp">reset gate <span class="math inline">\(r_t\)</span>：计算候选隐层 <span class="math inline">\(\tilde{h}_{t}\)</span> 时用来控制需要保留多少之前的记忆 <span class="math inline">\(h_{t-1}\)</span>，比如如果 <span class="math inline">\(r_t\)</span> 为0，那么<span class="math inline">\(\tilde{h}_{t}\)</span>只包含当前词的信息。</p><p>update gate <span class="math inline">\(z_t\)</span>：控制需要从前一时刻的隐藏层<span class="math inline">\(h_{t-1}\)</span>中遗忘多少信息，需要加入多少当前时刻的隐藏层信息 <span class="math inline">\(\tilde{h}_{t}\)</span>，最后得到 <span class="math inline">\(h_{t}\)</span></p><blockquote><p>一般来说那些具有短距离依赖的单元reset gate比较活跃（如果 <img src="https://www.zhihu.com/equation?tex=r_t" alt="[公式]"> 为1，而<img src="https://www.zhihu.com/equation?tex=z_t" alt="[公式]"> 为0那么相当于变成了一个标准的RNN，能处理短距离依赖），具有长距离依赖的单元updategate比较活跃。</p></blockquote><p><strong>相同点</strong></p><p>（1）都会有门操作，决定是否保留上时刻的状态，和是否接收此时刻的外部输入，LSTM是用遗忘门（forget gate <span class="math inline">\(f_t\)</span>）和输入门（input gate <span class="math inline">\(i_t\)</span>）来做到的，GRU则是只用了一个更新门（update gate <span class="math inline">\(z_t\)</span> ）</p><p>（2）遗忘门或者更新门选择不重写（overwritten）内部的memory，那么网络就会一直记住之前的重要特征，那么会对当前或者未来继续产生影响。缓解梯度消失。</p><p><strong>不同点</strong></p><p>（1）首先就是 LSTM 有一个输出门来控制 memory content的曝光程度（exposure），而 GRU 则是直接输出。</p><p>（2）另一点是要更新的 new memory content 的来源也不同。<span class="math inline">\(\tilde{h}_{t}\)</span>会通过重置门（reset gate）控制从<span class="math inline">\(h_{t-1}\)</span> 中得到信息的力度，而<span class="math inline">\(\tilde{c}_{t}\)</span>则没有，而是直接输入<span class="math inline">\(h_{t-1}\)</span>。</p><p>（3）相同个数参数的情况下，GRU 会比 LSTM 稍好一些</p></li><li><p><label><input type="checkbox"><a href="#3-3-7">3-3-7什么是Seq2Seq模型？该模型能解决什么类型问题？</a></label></p><p>seq2seq模型是将一个序列信号，通过编码和解码生成一个新的序列信号，输入和输出序列的长度实现并不知道。seq2seq的模型的核心思想由编码输入和解码输出两个环节构成。在经典的实现中，编码器和解码器各由一个循环神经网络构成，两个循环神经网络是共同训练的。</p><p>解决问题：机器翻译、语音识别、自动对话</p></li><li><p><label><input type="checkbox"><a href="#3-3-8">3-3-8注意力机制是什么？Seq2Seq模型引入注意力机制主要解决什么问题？</a></label></p><p>编码-解码架构的主要缺点：编码器<code>RNN</code>输出的上下文<code>C</code>的维度太小，难以恰当的概括一个长的输入序列的完整信息。</p><ul class="task-list"><li><label><input type="checkbox">注意力机制主要解决问题：</label></li></ul><p>（1）随着输入序列增长，模型性能发生显著下降。</p><p>​因为编码时输入序列的全部信息压缩到了一个定长向量表示中。随着序列增长，句子越前面的词的信息丢失就越严重。</p><p>（2）seq2seq输出序列中，常常会损失部分输入序列的信息。</p><p>​这是因为在解码时，当前词及对应的源语言词的上下文信息和位置信息在编解码过程中丢失了。</p><p>原编码-解码架构</p><figure><img src="http://huaxiaozhuan.com/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/imgs/dl_rnn/decoder_encoder.jpg" alt="img"><figcaption aria-hidden="true">img</figcaption></figure><p>加入attention</p><figure><img src="http://huaxiaozhuan.com/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/imgs/dl_rnn/attention.jpg" alt="img"><figcaption aria-hidden="true">img</figcaption></figure><p>attention本身可以理解为一种<strong>对齐关系</strong>，<strong>给出了模型输入、输出之间的对齐关系，解释了模型到底学到了什么知识。</strong></p><ul class="task-list"><li><label><input type="checkbox">注意力打分函数的计算方式</label></li></ul><figure><img src="C:\Users\Kevin\AppData\Roaming\Typora\typora-user-images\1566132296775.png" alt="1566132296775"><figcaption aria-hidden="true">1566132296775</figcaption></figure><p><img src="https://cdn.mathpix.com/snip/images/kLGt8ZRlTjA0PT7LYTuqIC2-ePp9Eh30KX5HANtYs4o.original.fullsize.png"></p><ul class="task-list"><li><label><input type="checkbox">attention的分类</label></li></ul><p><strong>global attention vs local attention</strong></p><p>上述的 <code>attention</code> 机制中为了计算上下文向量 <span class="math inline">\(\overrightarrow{\mathbf{c}}_{i}\)</span>，需要考虑 <code>encoder</code>的所有隐向量（<code>global attention</code>）。当输入序列较长时（如一段话或一篇文章），计算效率较低。</p><p><code>local attention</code> <u>在计算上下文向量 <span class="math inline">\(\overrightarrow{\mathbf{c}}_{i}\)</span>时只需要考虑 <code>encoder</code>的部分隐向量：首选预测<code>encoder</code> 端对齐的位置 <span class="math inline">\(p_{i}\)</span>，然后基于位置 <span class="math inline">\(p_{i}\)</span> 选择一个窗口来计算上下文向量 <span class="math inline">\(\overrightarrow{\mathbf{c}}_{i}\)</span>。</u></p><p><strong>self attention</strong></p><p>传统的 <code>attention</code> 是基于<code>encoder</code> 端和<code>decoder</code> 端的隐向量来计算<code>attention</code>的，得到的是输入序列的每个 <code>input</code>和输出序列的每个 <code>output</code> 之间的依赖关系。</p><p><code>self attention</code> 计算三种 <code>attention</code>：</p><ul><li>在<code>encoder</code> 端计算自身的<code>attention</code>，捕捉<code>input</code> 之间的依赖关系。</li><li>在 <code>decoder</code> 端计算自身的<code>attention</code>，捕捉<code>output</code> 之间的依赖关系。</li><li>将 <code>encoder</code> 端得到的 <code>self attention</code> 加入到<code>decoder</code> 端得到的<code>attention</code>中，捕捉输入序列的每个 <code>input</code>和输出序列的每个 <code>output</code> 之间的依赖关系。</li></ul></li><li><p><label><input type="checkbox"><a href="">RNN的长期依赖（Long-TermDependencies）问题是什么？怎么解决</a></label></p><p>长期依赖问题是：随着输入序列的增长，模型的性能发生显著下降，RNN难以捕捉长距离输入之间的依赖。</p><p>从结构上来看，理论上RNN可以学习捕捉到长距离依赖，但是实践中使用BPTT算法学习的RNN并不能成功捕捉长距离的医疗关系，主要源于深度神经网络中的<strong>梯度消失</strong>。</p><p>解决方法：</p><p>（1）LSTM、GRU等模型加入门控机制，捕捉长期记忆，很大程度上弥补了梯度消失</p><p>（2）残差结构</p><p>（2）设计多个时间尺度的模型：在细粒度的时间尺度上处理近期信息、在粗粒度时间尺度上处理远期的信息。得到粗粒度时间尺度方法1跳跃链接：增加从远期的隐变量到当前隐变量的直接连接；2.是删除连接：主动删除时间跨度为 1 的连接，并用更长的连接替换。</p></li><li><p><label><input type="checkbox"><a href="">RNN为什么会产生梯度消失或者梯度爆炸</a></label></p><p>主要由于权重矩阵 <span class="math inline">\(W\)</span>在不同时间步被重复使用，导致形成 <span class="math inline">\(W\)</span>的幂乘</p><ol type="1"><li><p>长期依赖的问题是深度学习中的一个主要挑战，其产生的根本问题是：<u>经过许多阶段传播之后，梯度趋向于消失或者爆炸。</u></p><ul><li>长期依赖的问题中，<u>梯度消失占大部分情况，而梯度爆炸占少数情况</u>。但是梯度爆炸一旦发生，就优化过程影响巨大。</li><li><code>RNN</code>涉及到许多相同函数的多次复合作用，每个时间步一次。这种复合作用可以导致极端的非线性行为。因此在<code>RNN</code>中，长期依赖问题表现得尤为突出。</li></ul></li><li><p>考虑一个没有非线性、没有偏置非常简单的循环结构： <span class="math inline">\(\overrightarrow{\mathbf{h}}^{(t)}=\mathbf{W}\overrightarrow{\mathbf{h}}^{(t-1)}\)</span>。则有：</p><p><img src="https://cdn.mathpix.com/snip/images/XOmA-zH1pvODDFxVhIO99BFDrSBP_MyM2xjKywDNnyo.original.fullsize.png"></p><p>设<span class="math inline">\(\mathbf{N}\)</span> 可以正交分解时：<span class="math inline">\(\mathbf{W}=\mathbf{Q} \mathbf{\Lambda}\mathbf{Q}^{T}\)</span> 。其中 0 为正交矩阵，<span class="math inline">\(\mathbf{\Lambda}\)</span>为特征值组成的三角阵。则： <span class="math display">\[\begin{array}{c}{\overrightarrow{\mathbf{h}}^{(t)}=\mathbf{Q}\mathbf{\Lambda}^{t} \mathbf{Q}^{T} \overrightarrow{\mathbf{h}}^{(0)}}\\ {\nabla_{\overrightarrow{\mathbf{h}}^{(0)}} L=\frac{\partial\overrightarrow{\mathbf{h}}^{(t)}}{\partial\overrightarrow{\mathbf{h}}^{(0)}}\nabla_{\overrightarrow{\mathbf{h}}^{(t)}} L=\mathbf{Q}\mathbf{\Lambda}^{t} \mathbf{Q}^{T}\nabla_{\overrightarrow{\mathbf{h}}^{(t)}} L}\end{array}\]</span></p><ul><li><p>前向传播：</p><ul><li>对于特征值的幅度不到 1 的特征值对应的 <span class="math inline">\(\overrightarrow{\mathbf{h}}^{(0)}\)</span>的部分将随着 <span class="math inline">\(t\)</span> 衰减到 0 。</li><li>对于特征值的幅度大于 1 的特征值对应的<span class="math inline">\(\overrightarrow{\mathbf{h}}^{(0)}\)</span>的部分将随着<span class="math inline">\(t\)</span> 指数级增长。</li></ul></li><li><p>反向传播：</p><ul><li><p>对于特征值幅度不到1的梯度的部分将随着<span class="math inline">\(t\)</span> 衰减到 0 。</p></li><li><p>对于特征值幅度大于1的梯度的部分将随着 <span class="math inline">\(t\)</span> 指数级增长 。</p></li></ul></li></ul></li><li><p>若考虑非线性和偏置，即： <span class="math inline">\(\overrightarrow{\mathbf{h}}^{(t+1)}=\tanh\left(\overrightarrow{\mathbf{b}}+\mathbf{W}\overrightarrow{\mathbf{h}}^{(t)}+\mathbf{U}\overrightarrow{\mathbf{x}}^{(t+1)}\right)\)</span>，有： <span class="math display">\[\frac{\partial \overrightarrow{\mathbf{h}}^{(t+1)}}{\partial\overrightarrow{\mathbf{h}}^{(t)}}=\operatorname{diag}\left(1-\left(\overrightarrow{\mathbf{h}}^{(t+1)}\right)^{2}\right)\mathbf{w}\]</span></p><ul><li><p>前向传播：</p><p><u>由于每一级的 <span class="math inline">\(\overrightarrow{\mathbf{h}}\)</span> 的幅度被<span class="math inline">\(\tanh (\cdot)\)</span> 函数限制在<code>(-1,1)</code> 之间，因此前向传播并不会指数级增长。</u></p><p><u>这也是为什么 <code>RNN</code> 使用 <code>tanh</code>激活函数，而不使用 <code>relu</code>的原因。</u></p></li><li><p>反向传播：</p></li></ul><p>由于隐状态的幅度被<span class="math inline">\(\tanh (\cdot)\)</span>函数限制在 <code>(-1,1)</code> 之间，因此 <span class="math inline">\(\operatorname{diag}\left(1-\left(\overrightarrow{\mathbf{h}}^{(t+1)}\right)^{2}\right)\mathbf{W}\)</span> 对 <span class="math inline">\(\mathbf{W}\)</span>进行了一定程度上的缩小。<span class="math inline">\(\overrightarrow{\mathbf{h}}^{(t+1)}\)</span>越大，结果越小。</p><ul><li>如果 <span class="math inline">\(\mathbf{W}\)</span>的特征值经过这样的缩小之后，在每个时刻都远小于1（因为每个时刻缩小的比例会变化），则该梯度部分将衰减到0 。</li><li>如果 <span class="math inline">\(\mathbf{W}\)</span>的特征值经过这样的缩小之后，在每个时刻都远大于1，则该梯度部分将指数级增长。</li><li>如果 <span class="math inline">\(\mathbf{W}\)</span>的特征值经过这样的缩小之后，在不同的时刻有时候小于1有时候大于1（因为每个时刻缩小的比例会变化），则该梯度部分将比较平稳。</li></ul></li></ol></li><li><p><label><input type="checkbox"><a href="">RNN如何解决梯度爆炸问题</a></label></p><p>梯度截断：对梯度值进行缩放，使得梯度的模不超过 <span class="math inline">\(\eta\)</span>。假设 <span class="math inline">\(g\)</span> 是梯度向量， <span class="math inline">\(|g|&gt;\eta\)</span>，那么 <span class="math display">\[g=\frac{\eta g}{|g|}\]</span></p></li><li><p><label><input type="checkbox"><a href="">RNN如何解决梯度消失问题</a></label></p><p>（1）LSTM、GRU等模型加入门控机制，捕捉长期记忆，很大程度上弥补了梯度消失。</p><p>​ 背后的思路是让路径的梯度乘积接近1</p><p>（2）多时间尺度（增加跳跃连接、删除连接）、泄露单元（线性自连接单元）</p><p>（3）引入残差结构</p><p>（4）RNN+初始化权重矩阵为单位矩阵</p></li><li><p><label><input type="checkbox"><a href="">LSTM的结构，每个门的作用，计算公式</a></label></p><p><img src="https://upload-images.jianshu.io/upload_images/17477516-8ef0e22accfffd33.png?imageMogr2/auto-orient/"></p><p>经典的LSTM中第t步计算公式为 <span class="math display">\[\begin{array}{l}{i_{t}=\sigma\left(W_{i} x_{t}+U_{i}h_{t-1}+b_{i}\right)} \\ {f_{t}=\sigma\left(W_{f} x_{t}+U_{j}h_{t-1}+b_{f}\right)} \\ {o_{t}=\sigma\left(W_{\sigma} x_{t}+U_{o}h_{t-1}+b_{o}\right)} \\ {\tilde{c}_{t}=\operatorname{Tanh}\left(W_{c}x_{t}+U_{c} h_{t-1}\right)} \\ {c_{t}=f_{t} \odot c_{t-1}+i_{t} \odot\tilde{c}_{t}} \\ {h_{t}=o_{t} \odot \tanh\left(c_{t}\right)}\end{array}\]</span></p><ul class="task-list"><li><p><label><input type="checkbox"><a href="">LSTM变种有哪些</a></label></p><p>“<strong>peephole connection</strong>”：让 门控层也会接受细胞状态的输入。</p><p><img src="https://upload-images.jianshu.io/upload_images/42741-0f80ad5540ea27f9.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1000/format/webp"></p><p><strong>coupled forget and inputgates</strong>：将输入门和遗忘们耦合在一起，输入和遗忘是同步的。</p><p><img src="https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-var-tied.png"></p><p><strong>GRU</strong></p></li><li><p><label><input type="checkbox"><a href="">LSTM为什么可以解决梯度消失</a></label></p><p><strong>将连乘关系转换为相加的线性关系</strong></p><p>在LSTM中 <span class="math inline">\(c_{t}=f_{t} \odot c_{t-1}+i_{t}\odot \tilde{c}_{t}\)</span> ，其中 <span class="math inline">\(c_{t-1}\)</span>是此前的信息， <span class="math inline">\(\tilde{\mathbf{c}}_{t}\)</span>是当前即刻的新信息，<span class="math inline">\(c_t\)</span> 是最终的信息。可以看到 <span class="math inline">\(c_t\)</span>和 <span class="math inline">\(c_{t-1}\)</span>此时是线性关系，不再是RNN中的连乘关系，梯度以线性在中间节点流动，因此可以保证很长时间的记忆。</p><p>进一步地，如果门控信号考虑bias，同时忽略输入变量 <span class="math inline">\(h_{j-1}\)</span>的作用，隐含层关系表示为： <span class="math display">\[c_{j}=\sigma\left(W^{f} X_{j}+b^{f}\right) c_{j-1}+\sigma\left(W^{i}X_{j}+b^{i}\right) \sigma\left(W X_{j}+b\right)\]</span> 于是，需要连乘的项表示为： <span class="math display">\[\frac{\partial c_{j}}{\partial c_{j-1}}=\sigma\left(W^{f} X_{j}+b\right)\]</span>该值范围在0~1之间。但是在实际参数更新中，可以通过控制bias比较大，使得该值接近于1；在这种情况下，即使通过很多次连乘的操作，梯度也不会消失，仍然可以保持“长距”连乘项的存在。即总可以通过选择合适的参数，在不发生梯度爆炸的情况下，找到合理的梯度方向来更新参数，而且这个方向可以充分地考虑远距离的隐含层信息的传播影响。</p></li></ul></li></ul><h1 id="二数学相关">二、数学相关</h1><h2 id="概率论和统计学">6、 概率论和统计学</h2><p>伯努利分布（0-1分布）</p><p>单个二值随机变量的分布，x的取值为0或者1；随机变量为1的概率p，为0的概率是1-p<span class="math display">\[p({\rm x} = x) = p^x(1-p)^{1-x}\]</span></p><ul class="task-list"><li><p><label><input type="checkbox"><a href="#6-1-1">6-1-1说说你是怎样理解信息熵的？</a></label></p></li><li><p><label><input type="checkbox"><a href="#6-1-2">6-1-2能否从数据原理熵解析信息熵可以表示随机变量的不确定性？</a></label></p></li><li><p><label><input type="checkbox"><a href="#6-1-3">6-1-3怎样的模型是最大熵模型？它有什么优点</a></label></p></li><li><p><label><input type="checkbox"><a href="#6-1-4">6-1-4什么是Beta分布？它与二项分布有什么关系？</a></label></p><p>参考知乎回答：https://www.zhihu.com/question/30269898</p><p>beta分布可以看做一个概率的概率分布，当你不知道一个东西的具体概率是多少时，它可以给出了所有概率出现的可能性大小。<strong>它是对二项分布中成功概率p的概率分布的描述。它的形式如下：</strong></p><p>beta分布与二项分布是共轭先验的（<a href="https://link.zhihu.com/?target=https%3A//en.wikipedia.org/wiki/Conjugate_prior%23Example">Conjugate_prior</a>）。所谓共轭先验就是先验分布是beta分布，而后验分布同样是beta分布。以棒球运动员击球为例，结果很简单：<span class="math display">\[\operatorname{Beta}\left(\alpha_{0}+\operatorname{hits}, \beta_{0}+\text{ misses }\right)\]</span></p></li><li><p><label><input type="checkbox"><a href="#6-1-5">6-1-5什么是泊松分布？它与二项分布有什么关系？</a></label></p></li><li><p><label><input type="checkbox"><a href="#6-1-6">6-1-6什么是t分布？他与正态分布有什么关系？</a></label></p></li><li><p><label><input type="checkbox"><a href="#6-1-7">6-1-7什么是多项式分布？具体说明？</a></label></p></li><li><p><label><input type="checkbox"><a href="#6-1-8">6-1-8参数估计有哪些方法？</a></label></p><p><strong>极大似然估计MLE</strong></p><p><strong>最大后验概率估计MAP</strong></p><p><strong>期望极大化EM</strong></p><p>EM算法解决这个的思路是使用启发式的迭代方法，既然我们无法直接求出模型分布参数，那么我们可以先猜想隐含参数（EM算法的 E步），接着基于观察数据和猜测的隐含参数一起来极大化对数似然，求解我们的模型参数（EM算法的M步)。由于我们之前的隐含参数是猜测的，所以此时得到的模型参数一般还不是我们想要的结果。我们基于当前得到的模型参数，继续猜测隐含参数（EM算法的E步），然后继续极大化对数似然，求解我们的模型参数（EM算法的M步)。以此类推，不断的迭代下去，直到模型分布参数基本无变化，算法收敛，找到合适的模型参数。</p><p>一个最直观了解 EM 算法思路的是 K-Means 算法。在 K-Means聚类时，每个聚类簇的质心是隐含数据。我们会假设 K 个初始化质心，即 EM算法的 E步；然后计算得到每个样本最近的质心，并把样本聚类到最近的这个质心，即 EM算法的 M 步。重复这个 E 步和 M 步，直到质心不再变化为止，这样就完成了K-Means 聚类。</p><p>EM算法和极大似然估计的前提是一样的，都要假设数据总体的分布，如果不知道数据分布，是无法使用EM算法的)。</p><p>EM算法是通过不断求解下界的极大化逼近求解对数似然函数极大化的算法</p></li><li><p><label><input type="checkbox"><a href="#6-1-9">6-1-9点估计和区间估计都是什么？</a></label></p></li><li><p><label><input type="checkbox"><a href="#6-1-10">6-1-10讲解一下极大似然估计，以及适用场景？</a></label></p><p>在统计学中，常常使用极大似然估计法来估计参数。即找到一组参数，使得在这组参数下，我们数据的似然度（概率）最大。<strong>(极大似然估计：就是利用已知的样本结果信息，反推最具有可能（最大概率）导致这些样本结果出现的模型参数值，即‘模型已定，参数未知’</strong>)</p><p><strong>极大似然估计的前提一定是要假设数据总体的分布，如果不知道数据分布，是无法使用极大似然估计的</strong></p><p>求极大似然估计的步骤</p><p>（1）写出似然函数；</p><p>（2）对似然函数取对数，并整理；</p><p>（3）求导数，令导数为 0，得到似然方程；</p><p>（4）解似然方程，得到的参数。</p><p>应用场景</p><p>（1）<strong>回归问题中的极小化平方和</strong></p><p>（2）<strong>分类问题中极小化交叉熵</strong></p></li><li><p><label><input type="checkbox"><a href="#6-1-10">6-1-11讲解一下最大后验概率估计，以及适用场景？</a></label></p></li></ul><p><strong>极大似然估计中采样需满足一个重要的假设，就是所有的采样都是独立同分布的。</strong></p><p>那么我们就知道了极大似然估计的核心关键就是对于一些情况，样本太多，无法得出分布的参数值，可以采样小样本后，利用极大似然估计获取假设中分布的参数。</p><p>极大似然估计就是经验风险最小化的一个例子。当模型是条件概率分布、损失函数是对数损失函数时，经验风险最小化就等价于极大似然估计。</p><p>最大后验概率是计算给定数据条件下模型的条件概率，即后验概率。使用模型的先验分布是贝叶斯学习的特点。</p><ul class="task-list"><li><p><label><input type="checkbox"><a href="">频率学派和贝叶斯学派什么区别？</a></label></p><p><strong>频率学派</strong></p><p>频率学派是上帝视角，认为频率是固定的，事件在多次重复实验中趋于一个稳定的值p，那么这个值就是该事件的概率。</p><p>他们认为模型参数是个定值，希望通过类似解方程组的方式从数据中求得该未知数。这就是频率学派使用的参数估计方法-<strong>极大似然估计（MLE）</strong>，这种方法往往在<u>大数据量的情况</u>下可以很好的还原模型的真实情况。</p><p><strong>贝叶斯派</strong></p><p>他们认为世界是不确定的，因获取的信息不同而异。假设对世界先有一个预先的估计，然后通过获取的信息来不断调整之前的预估计。他们认为模型参数源自某种潜在分布，希望从数据中推知该分布。对于数据的观测方式不同或者假设不同，那么推知的该参数也会因此而存在差异。这就是贝叶斯派视角下用来估计参数的常用方法-<strong>最大后验概率估计（MAP）</strong></p><p>这种方法在先验假设比较靠谱的情况下效果显著，随着数据量的增加，先验假设对于模型参数的主导作用会逐渐削弱，相反真实的数据样例会大大占据有利地位。极端情况下，比如把先验假设去掉，或者假设先验满足均匀分布的话，那她和极大似然估计就如出一辙了。</p></li><li><p><label><input type="checkbox"><a href="">MLE和MAP的联系</a></label></p><h2 id="mle与map的联系"><strong>MLE与MAP的联系</strong></h2><!-- **-**在介绍经验风险与结构风险最小化的时候以具体的逻辑回归（LR）与概率矩阵分解（PMF）模型来介绍MLE和MAP，接下里从宏观的角度，不局限于具体的某个模型来推导MLE与MAP。**-**假设数据![[公式]](https://www.zhihu.com/equation?tex=x_1%2C+x_2%2C+...%2C+x_n+)是满足独立同分布（i.i.d.）的一组抽样![[公式]](https://www.zhihu.com/equation?tex=X+%3D+%28x_1%2C+x_2%2C+...%2C+x_n%29)，接下来就利用两种参数估计方法来求解。- MLE对参数![[公式]](https://www.zhihu.com/equation?tex=%5Ctheta)的估计方法可以如下：![img](https://pic3.zhimg.com/v2-72984f949719a9ac476dfdbaa90bb046_b.jpg)- MAP对![[公式]](https://www.zhihu.com/equation?tex=%5Ctheta)的估计方法可以如下推导：![img](https://pic2.zhimg.com/v2-356eca57f5aa3911dd6e00c0e053f21d_b.jpg)**-**所以MAP和MLE在优化时的不同就是在于增加了一个先验项![[公式]](https://www.zhihu.com/equation?tex=-+%5Clog+P%28%5Ctheta%29)。**-**通过以上的分析可以大致给出他们之间的联系： ![[公式]](https://www.zhihu.com/equation?tex=MAP%28%5Ctheta%29%5Capprox+MLE%28%5Ctheta%29%2BP%28%5Ctheta%29) 。 --></li></ul><p>[ ] <a href="">大数定理和中心极限定理</a></p><h2 id="最优化问题">7、 最优化问题</h2><p>[ ] <a href="#7-1-1">7-1-1 什么是梯度？</a></p><p>梯度的方向是函数值增加最快的方向，梯度的相反方向是函数值减小的最快的方向。</p><ul class="task-list"><li><p><label><input type="checkbox"><a href="">为什么梯度的负方向是局部下降最快方向？</a></label></p><p>对 <span class="math inline">\(f(x+v)\)</span>在 <span class="math inline">\(x\)</span> 处进行泰勒一阶展开 <span class="math display">\[f(x+v) \approx f(x)+\nabla f(x)^{T} v\]</span> 这里有 <span class="math display">\[f(x)-f(x+v) \approx-\nabla f(x)^{T} v\]</span> 其中<span class="math inline">\(\nabla f(x)^{T}\)</span>和<span class="math inline">\(v\)</span>均为向量，<span class="math inline">\(-\nabla f(x)^{T}v\)</span>就是两个向量进行内积，而向量进行内积的最大值就是两者共线的时候，也就是<span class="math inline">\(v\)</span> 的方向和 <span class="math inline">\(-\nabla f(x)^{T}\)</span>方向相同时，内积最大。该内积值代表了最大的下降量。</p></li><li><p><label><input type="checkbox"><a href="#7-1-1">7-1-2梯度下降找到的一定是下降最快的方法？</a></label></p><p>梯度下降法并不一定是全局下降最快的方向，它只是目标函数在当前的点的切平面（当然高维问题不能叫平面）上下降最快的方向。在practicalimplementation中，牛顿方向（考虑海森矩阵）才一般被认为是下降最快的方向，可以达到superlinear的收敛速度。</p></li></ul><p>[ ] <a href="#7-1-1">7-1-3 牛顿法和梯度法有什么区别？</a></p><p>梯度下降法：<span class="math inline">\(\overrightarrow{\mathbf{x}}_{k+1}=\overrightarrow{\mathbf{x}}_{k}-\epsilon\overrightarrow{\mathbf{g}}\)</span></p><p>牛顿法：<span class="math inline">\(\overrightarrow{\mathbf{x}}_{k+1}=\overrightarrow{\mathbf{x}}_{k}-\mathbf{H}^{-1}\overrightarrow{\mathbf{g}}\)</span></p><ol type="1"><li><p>梯度法对目标函数进行一阶泰勒展开，梯度就是目标函数的一阶信息；</p></li><li><p>牛顿法对目标函数进行二阶泰勒展开，Hessian矩阵就是目标函数的二阶信息。</p></li><li><p>牛顿法的收敛速度一般要远快于梯度法，但是在高维情况下Hessian矩阵求逆的计算复杂度很大，而且当目标函数非凸时，牛顿法有可能会收敛到鞍点。</p></li><li><p>因为梯度法旨在朝下坡移动，而牛顿法目标是寻找梯度为0的点。</p></li><li><p>位于一个极小值点附近时，牛顿法比梯度下降法能更快地到达极小值点。</p><p>如果在一个鞍点附近，牛顿法效果很差，因为牛顿法会主动跳入鞍点。而梯度下降法此时效果较好（除非负梯度的方向刚好指向了鞍点）。</p></li><li><p>梯度下降法中，每一次 <span class="math inline">\(\overrightarrow{\mathbf{x}}\)</span>增加的方向一定是梯度相反的方向 <span class="math inline">\(-\epsilon_{k}\nabla_{k}\)</span> 。增加的幅度由<span class="math inline">\(\epsilon_{k}\)</span>决定，若跨度过大容易引发震荡。</p><p>而牛顿法中，每一次 <span class="math inline">\(\overrightarrow{\mathbf{x}}\)</span>增加的方向是梯度增速最大的反方向 <span class="math inline">\(-\mathbf{H}_{k}^{-1}\nabla_{k}\)</span>（它通常情况下与梯度不共线）。增加的幅度已经包含在<span class="math inline">\(\mathbf{H}_{k}^{-1}\)</span>中（也可以乘以学习率作为幅度的系数）。</p></li></ol><p>[ ] <a href="#7-1-1">7-1-4 什么是拟牛顿法？</a></p><p>在牛顿法的迭代中，需要计算海森矩阵的逆矩阵 <span class="math inline">\(\mathbf{H}^{-1}\)</span>，这一计算比较复杂。可以考虑用一个<span class="math inline">\(n\)</span> 阶矩阵 <span class="math inline">\(\mathbf{G}_{k}=G\left(\overrightarrow{\mathbf{x}}^{&lt;k&gt;}\right)\)</span>来近似代替 。</p><p>如果选择 <span class="math inline">\(G_{k}\)</span> 作为<span class="math inline">\(\mathbf{H}_{k}^{-1}\)</span> 的近似时，<span class="math inline">\(\mathbf{G}_{k}\)</span> 同样要满足两个条件：</p><ul><li><p><span class="math inline">\(\mathbf{G}_{k}\)</span>必须是正定的。</p></li><li><p><span class="math inline">\(\mathbf{G}_{k}\)</span>满足下面的拟牛顿条件：<span class="math inline">\(\mathbf{G}_{k+1}\overrightarrow{\mathbf{y}}_{k}=\vec{\delta}_{k}\)</span></p><p>因为 <span class="math inline">\(\mathbf{G}_{0}\)</span>是给定的初始化条件，所以下标从 <span class="math inline">\(k+1\)</span>开始。</p></li></ul><p>按照拟牛顿条件选择<span class="math inline">\(\mathbf{G}_{k}\)</span>作为<span class="math inline">\(\mathbf{H}_{k}^{-1}\)</span>的近似或者选择<span class="math inline">\(\mathbf{B}_{k}\)</span>作为<span class="math inline">\(\mathbf{H}_{k}\)</span>的近似的算法称为拟牛顿法</p><p>按照拟牛顿条件，在每次迭代中可以选择更新矩阵 <span class="math display">\[  \mathbf{G}_{k+1}=\mathbf{G}_{k}+\Delta \mathbf{G}_{k}  \]</span></p><p>[ ] <a href="#7-1-1">7-1-5讲解什么是拉格朗日乘子法、对偶问题、kkt条件?</a></p><p><strong>凸优化问题</strong></p><ul class="task-list"><li><p><label><input type="checkbox">拉格朗日乘子法</label></p><p><strong>对偶问题</strong></p><p>将极小极大的原问题转为极大极小的对偶问题；通常对偶问题更好求解，可以通过求解对偶问题而得到原始问题的解，进而确定分离超平面和决策函数（SVM）。</p><p><strong>KKT条件</strong></p></li><li><p><label><input type="checkbox"><a href="#7-1-1">7-1-6是否所有的优化问题都可以转化为对偶问题？</a></label></p><p>对所有实数域上的优化问题都有其对偶问题</p></li><li><p><label><input type="checkbox"><a href="#7-1-1">7-1-7讲解SMO（SequentialMinimalOptimization）算法基本思想？</a></label></p><p>SMO（序列最小优化算法）是一种启发式算法，是支持向量机学习的一种快速算法，其特点是不断地将原二次规划问题分解为只有两个变量的二次规划子问题，并对子问题进行解析求解，直到所有变量满足KKT条件为止，这时这个最优化问题的解就得到了。这样通过启发式的方法得到原二次规划问题的最优解。因为子问题有解析解，所以每次计算子问题都很快，虽然计算子问题次数很多，但在总体上还是高效的。</p><p>整个SMO算法包括两部分：（1）求解两个变量二次规划的解析方法（2）选择变量的启发式方法</p></li><li><p><label><input type="checkbox"><a href="#7-1-1">7-1-8为什么深度学习不用二阶优化？</a></label></p><p>深度学习的目标函数复杂，非凸；目标函数非凸时，牛顿法有可能会收敛到鞍点</p></li></ul><p>[ ] <a href="#7-1-1">7-1-9 机器学习优化器总结</a></p><h4 id="梯度下降-gradient-descent-gd">1. 梯度下降 (Gradient Descent,GD)</h4><p>梯度下降是一种基于梯度信息来更新参数的优化方法。假设损失函数为 <span class="math inline">\(J(\theta)\)</span>，对于每次迭代，更新权重的方式为：<span class="math display">\[\theta_{t+1} = \theta_t - \eta \nabla J(\theta_t),\]</span> 其中，$ _t $ 是第 $ t $ 次迭代时的参数，$ $ 是学习率，<span class="math inline">\(\nabla J(\theta_t)\)</span>是损失函数对参数的梯度。</p><ul><li><strong>是否收敛到最优值</strong>：在凸问题中，只要学习率 $$选得合适，梯度下降可以收敛到全局最优解。但对于<strong>非凸问题</strong>，它可能会收敛到局部最优解。</li><li><strong>优点</strong>：简单且易于实现。</li><li><strong>缺点</strong>：对于批量梯度下降，计算梯度会涉及整个训练集，计算成本高。</li></ul><h4 id="随机梯度下降-stochastic-gradient-descent-sgd">2. 随机梯度下降(Stochastic Gradient Descent, SGD)</h4><p>SGD是梯度下降的一个变种，它在每次更新时仅使用一个样本的梯度，而不是整个训练集的梯度：<span class="math display">\[\theta_{t+1} = \theta_t - \eta \nabla J(\theta_t; x_i, y_i)\]</span> 其中 $ (x_i, y_i) $ 是随机选择的训练样本。</p><ul><li><strong>是否收敛到最优值</strong>：在凸问题中，SGD在学习率逐渐衰减的情况下可以收敛到全局最优值，但波动较大。在非凸问题中，SGD可能会陷入局部最优，但随机性有时会帮助跳出局部最优。</li><li><strong>优点</strong>：计算开销低，每次迭代只计算一个样本的梯度。</li><li><strong>缺点</strong>：更新频繁，带有随机性，会造成损失函数在收敛过程中严重震荡。收敛较慢，更新过程存在噪声。</li></ul><h4 id="小批量梯度下降法mini-batch-gradient-descent-mbgd">3.<strong>小批量梯度下降法（Mini-batch Gradient Descent,MBGD）</strong></h4><p>小批量梯度下降是批量梯度下降和随机梯度下降的折中，使用一部分数据计算梯度，然后更新参数。这种方式可以降低参数更新时的方差，使得收敛更加稳定。但是对于非凸问题，依旧无法保证得到全局最优解。</p><p><strong>在梯度下降公式中，可以从两个角度进行改进。一是自适应选择学习率；二是梯度（动量）。</strong></p><p>首先，在修正梯度方面，主要有momentum动量法和nesterov 加速法。</p><h4 id="动量梯度下降-momentum-gd-和-nagnesterov-accelerated-gradient">4.<strong>动量梯度下降 (Momentum GD) 和 NAG（Nesterov acceleratedgradient）</strong></h4><p>动量法：参数更新时在一定程度上保留之前更新的方向，同时又利用当前batch的梯度微调最终的更新方向，简言之就是通过积累之前的动量来(previous_sum_of_gradient)加速当前的梯度，可能更加稳定、更有利于跳出局部最优。</p><p>动量法的更新公式为： <span class="math display">\[v_{t+1} = \gamma v_t + \eta \nabla J(\theta_t), \\\theta_{t+1} = \theta_t - v_{t+1}\]</span> 其中， $ $ 是动量因子（通常取值接近于 1），$ v_t $是动量向量。</p><ul><li><strong>是否收敛到最优值</strong>：在凸问题中，动量法可以比标准梯度下降更快收敛。在非凸问题中，它同样可能收敛到局部最优，但动量项可能有助于避免一些局部最优点。</li><li><strong>优点</strong>：加快收敛速度，减少震荡。</li><li><strong>缺点</strong>：动量项的选取较为敏感。</li></ul><p>NAG 进一步引入了nesterov动量，先在计算梯度更新前做一个矫正，更新公式为： <span class="math display">\[v_{t+1} = \gamma v_t + \eta \nabla J(\theta_t - \gamma v_t), \\\theta_{t+1} = \theta_t - v_{t+1}.\]</span></p><p>传统的优化算法要么将学习率设置为常数要么根据训练次数调节学习率。往往忽视了学习率其他变化的可能性。然而，学习率对模型的性能有着显著的影响，因此需要采取一些策略来想办法更新学习率，从而提高训练速度。如果学习率太小，则梯度很大的参数会有一个很慢的收敛速度；如果学习率太大，则已经优化得差不多的参数可能会出现不稳定的情况。</p><p><strong>自适应学习率算法主要有：AdaGrad算法，RMSProp算法，Adam算法以及AdaDelta算法等。</strong></p><h4 id="adagrad-adaptive-gradient-algorithm">5. <strong>AdaGrad(Adaptive Gradient Algorithm)</strong></h4><p>AdaGrad根据历史梯度信息来调整学习率，能够自动缩放每个参数反比于其所有梯度历史总和的平方根。更新公式为：<span class="math display">\[\theta_{t+1, i} = \theta_{t,i}- \frac{\eta}{\sqrt{G_{t,ii} + \epsilon}}g_{t,i}.\]</span> 其中，<span class="math inline">\(g_{t,i}\)</span> 为 <span class="math inline">\(t\)</span>时刻，参数 <span class="math inline">\(\theta_{t,i}\)</span> 的梯度。<span class="math inline">\(G_t\)</span> 是对角矩阵，<span class="math inline">\((i,i)\)</span>元素为到第$ t $次迭代为止，参数<span class="math inline">\(\theta_{t,i}\)</span> 的累积梯度平方和。</p><ul><li><strong>是否收敛到最优值</strong>：AdaGrad在凸问题中可以收敛到最优解，但在非凸问题中，学习率可能会变得非常小，导致无法继续有效更新。</li><li><strong>优点</strong>：具有损失函数最大梯度的参数相应地有个快速下降的学习率，而具有小梯度的参数在学习率上有相对较小的下降。</li><li><strong>缺点</strong>：中后期，分母上梯度累加的平方和会越来越大，学习率会逐渐减小到接近0，使得训练提前结束，无法学习。</li></ul><h4 id="rmsprop-root-mean-square-propagation">6. <strong>RMSProp (RootMean Square Propagation)</strong></h4><p>RMSProp通过调整每个参数的学习率来解决梯度震荡问题。其核心思想是对每个参数的梯度平方值进行指数加权平均，并使用这个平均值来调整每个参数的更新步长：<span class="math display">\[E[g^2]_t = \beta E[g^2]_{t-1} + (1 - \beta) g_t^2,\theta_{t+1} = \theta_t - \frac{\eta}{\sqrt{E[g^2]_t + \epsilon}} g_t\]</span> 其中，$ g_t $ 是梯度，$ E[g^2]_t $ 是梯度平方的移动平均，<span class="math inline">\(\beta\)</span>是衰减因子，<span class="math inline">\(\epsilon\)</span> 是防止除零的小量。</p><ul><li><strong>是否收敛到最优值</strong>：RMSProp能够在一定程度上控制学习率的大小，使得在深度学习中的表现较好。在非凸问题中，它能够有较好的局部收敛表现。</li><li><strong>优点</strong>：能够动态调整学习率，对稀疏数据有较好的处理能力。</li><li><strong>缺点</strong>：可能会在学习率过小的情况下导致收敛变慢。</li></ul><h4 id="adadelta">7. <strong>Adadelta</strong></h4><p>Adadelta 是 <strong>AdaGrad</strong> 的改进版，旨在解决 AdaGrad中学习率逐渐衰减至过小的问题。</p><p>Adadelta的主要思想是通过使用<strong>指数加权移动平均</strong>（ExponentialMoving Average, EMA）来代替 AdaGrad中的累积平方梯度和累计学习率。通过这种方式，它能够更稳定地调整学习率，同时避免学习率在训练过程中过度减小。</p><p>Adadelta不仅对梯度平方进行加权平均，还对参数更新的量进行加权平均，因此它不依赖于预设的全局学习率。</p><p>(1). <strong>梯度平方的指数加权移动平均</strong>： <span class="math display">\[   E[g^2]_t = \rho E[g^2]_{t-1} + (1 - \rho) g_t^2   \]</span></p><p>其中，$ g_t $$ 是在第 $ t $ 次迭代中计算的梯度，<span class="math inline">\(\rho\)</span> 是衰减率（通常取值在 0.9 左右），$E[g^2]_t $ 是梯度平方的移动平均值。</p><p>(2). <strong>参数更新的移动平均</strong>： <span class="math display">\[   \Delta \theta_t = - \frac{\sqrt{E[\Delta \theta^2]_{t-1} +\epsilon}}{\sqrt{E[g^2]_t + \epsilon}} g_t   \]</span> 其中，$ E[^2]_{t-1} $ 是之前参数更新量的移动平均值，$ $是一个用于防止除零的小量（通常取 $ 10^{-6} $）。</p><p>(3). <strong>更新移动平均</strong>： <span class="math display">\[   E[\Delta \theta^2]_t = \rho E[\Delta \theta^2]_{t-1} + (1 - \rho)(\Delta \theta_t)^2  \]</span></p><p>(4). <strong>参数更新</strong>： <span class="math display">\[   \theta_{t+1} = \theta_t + \Delta \theta_t  \]</span></p><ul><li><p><strong>是否收敛到最优值</strong>：在凸优化问题中，Adadelta可以收敛到全局最优解。在非凸问题中，它的表现依然较好，能够避免陷入局部最优点。不过，类似于其他基于梯度的优化方法，Adadelta在非凸问题中并不能保证一定收敛到全局最优解。</p></li><li><p><strong>AdaGrad</strong>使用的是累积平方梯度求和来更新学习率，导致学习率在训练过程中逐渐趋近于零，尤其是在处理长时间训练或大量数据时。这会使得AdaGrad 训练过程后期的学习率非常小，进而导致参数几乎无法更新。</p></li><li><p><strong>Adadelta</strong> 通过引入指数加权移动平均（EMA）代替了AdaGrad 中的累积平方梯度求和，避免了学习率过早衰减的现象。同时，Adadelta不再需要预设学习率，因为它会自动调整学习率。</p></li><li><p><strong>依赖于衰减率的选择</strong>：虽然不需要手动设置学习率，但衰减率$ $的选择依然是影响模型收敛速度的一个关键因素。对于不同的数据集和任务，可能需要针对衰减率进行调优。</p></li></ul><h4 id="adam-adaptive-moment-estimation">8. <strong>Adam (AdaptiveMoment Estimation)</strong></h4><p>Adam 是 RMSProp和动量法的结合，通过同时计算梯度的一阶和二阶矩的指数加权平均来调整学习率：<span class="math display">\[m_t = \beta_1 m_{t-1} + (1 - \beta_1) g_t, \\v_t = \beta_2 v_{t-1} + (1 - \beta_2) g_t^2.\]</span> <span class="math display">\[\hat{m}_t = \frac{m_t}{1 - \beta_1^t}, \quad \hat{v}_t = \frac{v_t}{1 -\beta_2^t}.\]</span> <span class="math display">\[\theta_{t+1} = \theta_t - \frac{\eta}{\sqrt{\hat{v}_t} + \epsilon}\hat{m}_t\]</span> 其中，$ m_t $ 和 $ v_t $ 分别是梯度的一阶和二阶矩，$ _1 $ 和 $_2 $ 是超参数。</p><ul><li><strong>是否收敛到最优值</strong>：Adam在许多实际问题中表现优越，但在某些情况下，Adam可能会收敛到次优解。理论上，它能收敛到局部最优，但是否能达到全局最优取决于问题的性质。</li><li><strong>优点</strong>：能够动态调整学习率，对稀疏数据和噪声鲁棒性强。</li><li><strong>缺点</strong>：较为复杂，依赖超参数的设置。</li></ul><h4 id="adamw-adaptive-moment-estimation">8. <strong>AdamW (AdaptiveMoment Estimation)</strong></h4><p><strong>AdamW</strong> 是 <strong>Adam</strong>优化算法的改进版本，它的主要改进是在 Adam的基础上引入了<strong>权重衰减（WeightDecay）</strong>的正确实现。这种权重衰减是通过将 L2正则化直接应用于<strong>参数更新公式</strong>，而不是像 Adam那样对梯度进行修正。这种改进旨在提高模型的泛化能力，尤其是避免深度学习模型中过拟合的问题。</p><ul><li><p><strong>Adam 中的错误正则化实现</strong>：在原版的 Adam中，权重衰减实际上是通过将梯度中的 L2 惩罚项添加到更新公式中。这种做法在Adam 中并不完全等同于对参数的惩罚，因为 Adam依赖于动量和梯度的调整，它使得实际的正则化效果被稀释或扭曲，导致权重衰减效果不理想。</p></li><li><p><strong>AdamW 的提出</strong>：为了解决这个问题，AdamW提出了更正的权重衰减实现。AdamW将权重衰减项直接应用到参数本身的更新步骤，而不是施加在梯度上。这种做法能够更加有效地抑制模型的过拟合，提高泛化能力。</p></li></ul><p>AdamW 基本上继承了 Adam的大部分更新过程，但在参数更新时引入了独立的权重衰减项。</p><p>(1). <strong>梯度的移动平均</strong>（一阶矩估计）： <span class="math display">\[   m_t = \beta_1 m_{t-1} + (1 - \beta_1) g_t   \]</span> 其中，$g_t $是在第 $ t $ 次迭代中计算的梯度，$ m_t <span class="math inline">\(是梯度的移动平均，\)</span>_1 $是动量衰减因子（通常取 0.9）。</p><p>(2). <strong>梯度平方的移动平均</strong>（二阶矩估计）： <span class="math display">\[   v_t = \beta_2 v_{t-1} + (1 - \beta_2) g_t^2  \]</span> 其中，$ v_t $ 是梯度平方的移动平均，$ _2 $是衰减因子（通常取 0.999）。</p><p>(3). <strong>偏差修正</strong>：为了消除初期时矩估计的偏差，需要进行偏差校正： <span class="math display">\[   \hat{m}_t = \frac{m_t}{1 - \beta_1^t}, \quad \hat{v}_t = \frac{v_t}{1- \beta_2^t}   \]</span></p><p>(4). <strong>参数更新</strong>（AdamW 核心改进部分）： AdamW的更新步骤不仅包含 Adam的参数更新公式，还直接在参数更新时引入了权重衰减项： <span class="math display">\[   \theta_{t+1} = \theta_t - \eta \frac{\hat{m}_t}{\sqrt{\hat{v}_t} +\epsilon} - \eta \lambda \theta_t   \]</span> 其中，$ $ 是权重衰减系数（即 L2 正则化系数），$ $是学习率。</p><p>AdamW 的关键在于第二个项 $_t$，它直接将权重衰减施加在参数更新上，而不是施加在梯度上。这种方式与传统SGD 中的权重衰减更一致。</p><ul><li><strong>Adam</strong>：权重衰减通过 L2正则化实现，并作用在梯度上。这种实现可能会导致正则化效果受到 Adam的梯度调整机制的干扰，导致模型参数更新不充分，特别是在学习率较小时。</li><li><strong>AdamW</strong>：权重衰减直接作用于参数本身，即在每次参数更新时独立加入一个基于参数的衰减项。这样可以保证权重衰减的效果更加直接和有效，避免了Adam对梯度的干扰。此外，这种权重衰减更加显式地对模型参数产生作用，从而能够更好地抑制模型过拟合，提高泛化性能。</li><li><strong>需要调优的超参数增加</strong>：相比 Adam，AdamW多了一个权重衰减系数 $ $，这增加了模型调优的复杂性。</li></ul><h1 id="三-llm-and-vlm">三、 LLM and VLM</h1><h2 id="大模型常用微调方法lora和ptuning的原理">1.大模型常用微调方法LORA和Ptuning的原理</h2><ul><li>LORA: Low-Rank Adaptation.核心是在大型语言模型上对指定参数增加额外的低秩矩阵，也就是在原始pre-trainedLM旁边增加一个旁路，做一个降维再升维的操作。假设模型中有一个需要更新的权重矩阵<span class="math inline">\(W \in \mathbb{R}^{d \timesk}\)</span>，LORA的思想是修改为： <span class="math inline">\(W' = W+ \delta W\)</span>，其中 <span class="math inline">\(\Delta W = A\times B\)</span>, <span class="math inline">\(A \in \mathbb{R}^{d\times r}\)</span>, <span class="math inline">\(B \in \mathbb{R}^{r\times k}\)</span>, 且 <span class="math inline">\(r &lt;&lt; \min\{d,k\}\)</span>。在模型训练过程中，固定PLM的参数，只训练降维矩阵 <span class="math inline">\(A\)</span> 与升维矩阵 <span class="math inline">\(B\)</span>。</li><li><h2 id="ptuning-prompt-tuning.">Ptuning: Prompt Tuning.</h2></li></ul><h2 id="diffusion-models-and-stable-diffusion">2. Diffusion models andStable diffusion</h2><ul><li><a href="https://lilianweng.github.io/posts/2021-07-11-diffusion-models/#prog-distll">Whatare Diffusion Models?</a></li><li><h2 id="diffusion-models-for-video-generation"><a href="https://lilianweng.github.io/posts/2024-04-12-diffusion-video/">DiffusionModels for Video Generation</a></h2></li></ul><h2 id="llm的幻觉的问题">3. LLM的幻觉的问题</h2><p><a href="https://aman.ai/primers/ai/hallucination/">NLP •Hallucination Mitigation</a></p><hr><h2 id="llm-alignment">4. LLM Alignment</h2><p>Many thanks to this blog: <a href="https://aman.ai/primers/ai/llm-alignment/">LLM Alignment</a></p><h3 id="overview">Overview</h3><ul><li>2017 年，OpenAI 在其论文 <a href="https://arxiv.org/abs/1706.03741">Deep reinforcement learning fromhuman preferences</a> 中提出了一种开创性的机器学习方法，称为"从人类反馈出发的强化学习"(RLHF)，特别关注人类偏好。这一创新概念自此激发了该领域的进一步研究和发展。</li><li>RLHF 概念:使用一个预先训练好的语言模型，由人类评估员对其输出进行排序。然后，这种排序会让模型对某些类型的回答产生偏好，从而产生更可靠、更安全的输出。</li><li>RLHF可以有效利用人类反馈来提高语言模型的性能。它将强化学习算法的优势与对人类输入的细微理解相结合，促进了模型的持续学习和改进。结合人类反馈，RLHF不仅能提高模型的自然语言理解和生成能力，还能提高其在文本分类或翻译等特定任务中的效率。此外，RLHF在解决语言模型中的偏差方面也发挥着至关重要的作用。通过允许人工输入来指导和纠正模型的语言使用，它可以促进更加公平和包容的交流。不过，在这一过程中，必须注意人为因素可能导致的偏差。</li></ul><h3 id="reinforcement-learning-强化学习基础概念">Reinforcement Learning强化学习基础概念</h3><p><img src="/2024/10/08/statistic/rl.png"></p><p>如图所示，agent 采取一定的 action，对于当前的action，环境会反馈其状态 state 以及 给出reward。其中，reward是要优化的目标，state是环境当前的状态，policy用于根据state选择action.</p><h3 id="reinforcement-learning-from-human-feedback-rlhf">ReinforcementLearning from Human Feedback (RLHF)</h3><p>LLM的最初目标是准确地预测下一个token。但是，这种方式无法保证输出的结果是有用、无害且诚实的，有可能产生不符合人类道德或安全标准的内容。为解决这一问题，需要有一种方式来引导模型输出符合人类价值观的结果。</p><p><img src="/2024/10/08/statistic/rlhf.png"></p><p>图中给出了使用RLHF训练LM的三个步骤，具体来说，</p><ol type="1"><li><p>Collect Demonstration Data, and Train a Supervised Policy.首先，从 prompts 中选择一个prompt；然后人类标注者给出希望得到的输出；最后这些经过标注后的数据用于对LM进行supervised fine-tune.</p></li><li><p>Collect Comparison Data, and Train a Reward Model.首先，选取一个prompt，模型给出几个可能的输出结果；标注者根据有用性、准确性等准则对结果进行从好到差的排序；这些排序后的数据用来训练一个reward model. Reward model用来评估模型输出结果的质量。</p></li><li><p>Optimize a Policy Against the Reward Model Using ReinforcementLearning. 产生新的prompt, 基于当前的policy, model 得到新的输出 response;Reward model 评估 response，然后得到 reward；基于得到的 reward以及一些强化学习算法，比如PPO，对 policy 进行更新。调整 policy是为了增加未来产生 higher-reward outputs 的可能性。</p></li></ol><p>Chip Huyen provides a zoomed out view of how the overall processworks in her flowchart below:</p><p><img src="/2024/10/08/statistic/rlhf1.jpeg"></p><h4 id="reward-model">REWARD MODEL</h4><p>Reward model 的主要功能是评估给定的输入（如文本序列）并产生scalarreward。这种reward 量化了输出与人类偏好或期望行为的一致程度。</p><p><img src="/2024/10/08/statistic/rlhf2.png"> Reward 模型的结构包括 -LM 分类器：一个二元分类器微调的LLM，可对哪种反应更符合人类偏好进行评分。 - valuenetworks：一个回归模型，根据输入预测人类偏好评分。 -评论生成器：经过训练的LM，可生成评价性评论，解释哪种回答更好以及原因。该评论可用于指令调整。</p><h4 id="optimizing-the-policy">Optimizing the Policy</h4><p><strong>策略（policy）</strong>：在强化学习中，策略是一组规则或决策机制，指导智能体（agent）根据它所处的环境状态或观察结果来选择行动。也就是说，策略定义了智能体如何在不同的情境下采取什么样的行为。</p><p><strong>PPO（Proximal PolicyOptimization，邻近策略优化）</strong>：是一种常用的强化学习算法。在PPO中，策略是通过反复迭代来优化的。其目标是最大化奖励，即让智能体的行为逐步改善，获得更高的回报。但是，PPO会确保策略的更新不会发生剧烈变化。这是通过引入一种约束，使更新后的策略保持与之前的策略相似性，以避免不稳定性或训练失败的情况。</p><p><strong>DPO（Direct PreferenceOptimization，直接偏好优化）</strong>：是一种不同的策略优化方法。在DPO中，策略直接基于人类偏好进行优化。具体来说，它通过二元交叉熵损失函数（binarycross entropyloss），增加模型生成的优选输出的相对对数概率，而减少非优选输出的概率。这种方法直接根据人类的反馈进行优化，旨在使模型生成更符合人类期望的输出。与此同时，DPO也通过KL散度约束来保持平衡，防止策略发生过大的偏离。</p><h4 id="training-llama-2">Training Llama 2</h4><p><img src="/2024/10/08/statistic/llama.jpeg"></p><p>以下是Llama 2 的主要训练阶段的介绍：</p><ol type="1"><li><strong>预训练阶段</strong>（Pretraining）：<ul><li>在最初的预训练阶段，Llama 2使用大量数据通过<strong>自监督学习</strong>进行训练。这一阶段让模型学习语言模式和上下文的基本结构，使其能够理解语言的基本规则和含义。</li><li>自监督学习的方式通常是通过预测文本中隐藏的部分（如下一句话或遮盖的单词）来训练模型，帮助它积累广泛的语言知识。</li></ul></li><li><strong>有监督微调阶段</strong>（Supervised Fine-Tuning）：<ul><li>在此阶段，模型进一步通过<strong>指令数据</strong>进行有监督微调。具体来说，模型会根据特定的指令进行训练，学习如何对不同的提示做出合适的响应。</li><li>这个过程使模型能够在实际应用中根据明确的要求或任务生成准确、相关的回答。</li></ul></li><li><strong>奖励模型创建（RLHF步骤1）</strong>（Reward Models Creation -RLHF Step 1）：<ul><li>为了进一步优化模型输出的质量，Llama 2创建了两个<strong>奖励模型</strong>，一个针对<strong>帮助性（helpfulness）</strong>，另一个针对<strong>安全性（safety）</strong>。</li><li>这些奖励模型通过<strong>人类偏好数据</strong>训练，预测在两种不同的输出中哪一个更符合人类的判断。此阶段基于二元比较，模型通过评估每对输出的优劣来学习。</li></ul></li><li><strong>边际损失与排名</strong>（Margin Loss and Ranking）：<ul><li>Llama 2使用二元比较数据集来优化排名。在每次比较中，标注者只需要选择两种响应中的一个，并通过<strong>边际标签</strong>来表示偏好的强度。这种边际标签可以用于进一步计算<strong>排名损失</strong>，提高模型对不同偏好的敏感性。</li></ul></li><li><strong>拒绝采样与PPO对齐（RLHF步骤2）</strong>（Rejection Samplingand PPO - RLHF Step 2）：<ul><li>在最后一步，Llama 2使用<strong>拒绝采样</strong>和<strong>邻近策略优化（PPO）</strong>来进一步优化模型。</li><li>拒绝采样是指从模型生成的多个输出中，选择<strong>奖励最高</strong>的输出用于更新梯度，从而增强模型生成高质量输出的能力。</li><li>之后通过PPO算法对模型进行进一步对齐，使其生成的回答更加安全且有帮助，同时确保优化过程中策略更新的稳定性。</li></ul></li></ol><p>总的来说，Llama 2的训练流程结合了大规模的自监督学习、基于指令的有监督微调，以及基于人类偏好的强化学习，通过一系列精细的步骤来提升模型的语言理解、输出的帮助性和安全性。</p><h4 id="proximal-policy-optimization-ppo">Proximal Policy Optimization(PPO)</h4><p>建议先阅读以下两篇优秀博客： - <a href="https://www.cnblogs.com/xingzheai/p/15826847.html">详解策略梯度算法</a>- <a href="https://www.cnblogs.com/xingzheai/p/15931681.html">详解近端策略优化</a></p><p><strong>PPO-clip</strong>:在PPO（邻近策略优化）中，代理损失函数（surrogate loss）是通过当前策略和参考策略下执行同一动作的概率比率来定义的。这一比率用于引导策略向那些能够获得更高奖励的动作倾斜，同时确保策略更新的幅度不会过大，从而保持训练的稳定性。为防止策略的更新幅度过大，PPO引入了剪裁，限制比率在一定范围内。通过在一定阈值外“剪裁”比率的变化，模型可以避免发生过大的更新，从而保证训练过程的稳定性。</p><p>定义 <span class="math inline">\(\pi_{\theta}\)</span>为当前策略（参数为 <span class="math inline">\(\theta\)</span> 的一个网络），<span class="math inline">\(\pi_{ref}\)</span>是实际的、可参考的策略空间。<span class="math inline">\(A(s_t,a_t)\)</span>为在状态 <span class="math inline">\(s_t\)</span>下采取行为 <span class="math inline">\(a_t\)</span>时得到的奖励。近端策略优化裁剪函数为： <span class="math display">\[L(\theta) = E_{(s_t, a_t) \sim \pi_{ref}} \min{(\frac{p_{\theta}(a_t |s_t)}{p_{\pi_{ref}}(a_t|s_t)} A(s_t, a_t), clip(\frac{p_{\theta}(a_t |s_t)}{p_{\pi_{ref}}(a_t|s_t)}, 1-\epsilon, 1+\epsilon)A(s_t, a_t))},\]</span> <span class="math inline">\(\epsilon\)</span>是一个超参数，要需要我们调整的，一般设置为0.1或0.2。</p><p><strong>PPO-penalty</strong>: 在PPO中，除了使用剪裁目标函数（clippedobjective）外，另一种常见的方法是直接在目标函数中加入KL散度惩罚项。这意味着算法会根据新策略与参考策略的偏离程度对目标函数进行惩罚。具体损失函数为：<span class="math display">\[L(\theta) = E_{(s_t, a_t) \sim \pi_{ref}} \frac{p_{\theta}(a_t |s_t)}{p_{\pi_{ref}}(a_t|s_t)} A(s_t, a_t) - \betaKL(\pi_{ref}||\pi_{\theta}),\]</span></p><p>通过<strong>最大化目标函数</strong>得到最优策略。对于大规模语言模型（LLM）来说，这个目标函数反映了模型对齐的目标，比如生成<strong>有帮助</strong>、<strong>真实</strong>、<strong>无害</strong>的回答。</p><p><strong>参考策略 (ReferencePolicy)</strong>：参考策略是训练过程中用作<strong>基准</strong>或<strong>对照</strong>的一套策略。它通常是一个<strong>稳定的策略</strong>，模型可以从这个基准出发，或者在训练过程中参考该策略来指导学习。它确保最优策略的更新不会偏离初始策略太远，防止训练过程中产生剧烈变化或不稳定的行为。</p><h3 id="reinforcement-learning-with-ai-feedback-rlaif">ReinforcementLearning with AI Feedback (RLAIF)</h3><p>RLAIF使用AI生成的偏好（而不是人工标注的偏好）来训练大规模语言模型（LLMs）。这种方法通过利用强大的预训练模型（如GPT-4）生成反馈，为训练其他LLM提供高效、成本更低的替代方案。在RLAIF中，反馈生成的语言模型相当于充当了“虚拟人工标注者”的角色。它评估训练中的模型生成的多个输出，选择优选响应或提供改进建议。</p><h4 id="direct-preference-optimization-dpo">Direct PreferenceOptimization (DPO)</h4><p>本文前面讨论的 RLHF主要包括两个阶段：根据人类偏好标签训练奖励模型，然后使用强化学习（RL）对LM 进行微调，使其与这些偏好保持一致。然而，RLHF存在复杂性和不稳定性问题，它需要拟合一个奖励模型，然后训练一个策略来优化该奖励，这就容易产生稳定性问题。</p><p>DPO算法摆脱了传统RL方法中的两个阶段。通过定义新的损失函数来训练LLM，以避免不稳定性问题。DPO使用一种特殊格式的数据集，形式为：&lt;prompt, worse completion,bettercompletion&gt;（即“提示，较差的完成，较好的完成”）。在训练过程中，DPO的损失函数鼓励模型增加较好完成的概率，同时降低较差完成的概率。这个过程是通过加权实现的，权重基于隐含的奖励模型。这里的关键在于，LLM本身充当了奖励模型，因此不再需要一个显式的奖励模型。下图给出了DPO和RLHF的区别。</p><p><img src="/2024/10/08/statistic/DPO.jpg"></p><p><strong>Binary Cross-Entropy Loss</strong> DPO通过使用二元交叉熵（Binary Cross-Entropy,BCE）损失函数来优化语言模型以更好地与人类偏好对齐的训练方法。对于每个输入，模型会生成两个响应，并由人类标注者指明他们的偏好（哪个响应更好）。DPO通过比较模型生成的响应对（即优选响应和不优选响应）与人类偏好进行训练。</p><p>损失定义如下： <span class="math display">\[L_{DPO}(\theta) = -E_{(x, y_w, y_l) \sim D} [\log \sigma (\beta\log\frac{\pi_{\theta}(y_w| x)}{\pi_{ref}(y_w|x)} - \beta \beta\log\frac{\pi_{\theta}(y_l| x)}{\pi_{ref}(y_l|x)})],\]</span> 其中，<span class="math inline">\(\pi_{\theta}\)</span>为要训练的策略模型， <span class="math inline">\(\pi_{ref}\)</span>是参考的策略模型；<span class="math inline">\(y_w\)</span> 和 <span class="math inline">\(y_l\)</span> 分别表示优选response 和 不优选的response. <span class="math inline">\(\beta\)</span>控制待训练模型与参考策略模型的接近程度。<span class="math inline">\(\sigma\)</span> 为 logistic 函数。</p><ul><li>DPO标志着语言模型训练方法的转变，通过将强化学习与人类反馈（RLHF）过程整合为<strong>单个的端到端</strong>优化步骤，简化了模型的训练。</li></ul><p><strong>DPO 的训练过程</strong> -选择一个已经经过基础指令调优的语言模型作为参考模型，这个模型提供了良好的基础。-使用不同的采样/解码方法（例如不同的温度设置）对同一提示生成成对输出，并让人类选择他们喜欢的哪一个。这一过程将产生一个人类偏好/反馈的数据集。-在LLM上添加一个线性层，使得模型能够输出一个标量值。这一层将帮助模型在训练过程中产生更具体的数值输出。-使用DPO损失，该损失函数基于二元交叉熵损失。计算参考模型和正在调优模型的标量输出的对数比率，并乘以一个散度参数，以调整模型的输出。-在训练完成后，去掉最后的线性层，这样就得到了一个基于人类反馈微调的LLM。</p><p>通过以上步骤，DPO方法通过简化RLHF过程，去掉了复杂的强化学习步骤和专门的奖励模型，使得模型训练更为高效和直接。这样，最终得到的模型能够更好地反映人类的偏好，提供更优质的输出</p><h4 id="kahneman-tversky-optimization-kto">Kahneman-Tversky Optimization(KTO)</h4><p>人类在面对不确定事件时，由于‘厌恶损失’，往往会做出无法最大化期望值的决策。直接以人的偏好指导大模型的训练，其训练的数据中包含了大量的人类偏好，往往无法做出期望最大的决策。KTO是一种对齐手段，将重点从传统训练目标（如下一个标记预测或拟合配对偏好数据）转向直接优化被<strong>认为有价值或可取</strong>的输出。</p><p>KTO消除了对配对偏好排名或比较数据的需求，显著简化了数据要求。它只需要二元标签，指示某个LLM输出是可取的还是不可取的。这种二元偏好数据的需求使KTO在现实场景中更为实用，因为收集详细的偏好数据往往比较困难。</p><p><strong>前景理论 (prospect theory)</strong></p><p>KTO 的灵感来自 Daniel Kahneman 和 Amos Tversky提出的决策行为模型，特别是他们的前景理论 (prospect theory)。KTO将这些概念调整为损失函数，通过捕捉人类的偏差（如损失规避和风险敏感性），使LLM 与人类反馈保持一致。</p><p>在前景理论中，人类在不确定性下的决策行为偏离了预期效用最大化的原则，主要是因为一些心理偏差，如损失厌恶（lossaversion）和非线性概率加权（nonlinear probabilityweighting）。这些概念是KTO损失函数的基础。</p><p><strong>1. 价值函数 (ValueFunction)</strong>：前景理论中的价值函数用于描述人们如何看待收益和损失的差异。它具有以下特征：-<strong>对收益的凹性</strong>：当收益增加时，价值函数是凹的，这意味着人们在获得相同金额的收益时，所感受到的价值增加会逐渐减小。这反映了人们在面对收益时的风险厌恶（riskaversion）。</p><ul><li><p><strong>对损失的凸性</strong>：当面临损失时，价值函数是凸的，这意味着在损失相同金额时，所感受到的损失会逐渐增大，反映了人们在面对损失时的风险寻求（risk-seeking）行为。</p></li><li><p><strong>损失的影响大于收益</strong>：损失对人们的情感影响通常大于收益，这一点通过损失厌恶参数<span class="math inline">\(\lambda\)</span>来建模。该参数通常大于1，意味着人们在面对损失时的感受强于获得相同金额收益时的感受。</p></li></ul><p><strong>2. 数学表达式</strong>. 价值函数 <span class="math inline">\(v(x)\)</span> 可以用以下公式表示： <span class="math display">\[v(x) = \begin{cases}x^\alpha &amp; \text{if } x \geq 0 \\-\lambda (-x)^\beta &amp; \text{if } x &lt; 0\end{cases}\]</span> 其中： - $ (0,1)$ 和 <span class="math inline">\(\beta \in(0,1)\)</span> 控制对收益和损失的减敏感性（diminishingsensitivity）。这意味着随着收益或损失的增加，人们的感知效应会逐渐减弱。- $ $是损失厌恶因子，通常大于1，这表示人们对损失的反应比对收益更为强烈。</p><p><strong>3. 概率加权函数 (Probability Weighting Function)</strong>:人们在判断概率时，往往会倾向于高估小概率事件和低估大概率事件。尽管这一元素并非KTO的核心部分，但它强调了主观不确定性感知如何影响决策。这种加权使得人们在面对不确定性时的决策并不是完全理性的，而是受到了心理因素的影响。</p><p>Kahneman-Tversky Optimization (KTO)的损失函数是基于前景理论构建的，其设计目标是直接最大化语言模型生成输出的效用。以下是KTO 损失函数的关键要素及其解释：</p><p><strong>KTO‘s loss function</strong></p><ul><li><p>KTO 使用了一个 <strong>逻辑函数 <span class="math inline">\(\sigma\)</span></strong>，而不是经典前景理论中的分段价值函数。这种逻辑函数保持了对收益的<strong>凹性</strong>和对损失的<strong>凸性</strong>，反映了人类对风险的感知。</p></li><li><p><strong>风险厌恶参数 <span class="math inline">\(\beta\)</span></strong>被纳入模型中，用于控制风险厌恶程度。这一参数影响价值函数饱和的陡峭程度，进而影响模型如何感知收益和损失。</p></li><li><p>在 KTO 中，传统的损失厌恶参数 <span class="math inline">\(\lambda\)</span>被替换为两个独立的超参数：<strong><span class="math inline">\(\lambda_D\)</span></strong>（用于积极反馈的输出）和<strong><span class="math inline">\(\lambda_U\)</span></strong>（用于消极反馈的输出）。允许模型根据输出类型的不同（积极或消极），以更细致的控制方式来处理反馈，从而更好地反映人类的风险厌恶特性。</p></li><li><p>模型的参考点通过 <strong>KL 散度</strong>来定义，表示当前模型策略 <span class="math inline">\(\pi_\theta\)</span>与参考策略 <span class="math inline">\(\pi_{\text{ref}}\)</span>之间的差异。KL散度项控制当前模型输出与预训练参考模型的偏离程度，并作为优化中评估收益和损失的参考点<span class="math inline">\(z_0\)</span>。</p></li></ul><p>KTO（Kahneman-Tversky Optimization）损失函数的数学公式如下： <span class="math display">\[L_{KTO}(\pi_\theta, \pi_{\text{ref}}) = \mathbb{E}_{x,y \simD}[\lambda_y - v(x,y)], \\\quad \\v(x,y) =   \begin{cases}   \lambda_D \sigma(\beta(r_\theta(x,y) - z_0)), &amp; \text{if } y \sim\text{desirable} \\   \lambda_U \sigma(\beta(z_0 - r_\theta(x,y))), &amp; \text{if } y \sim\text{undesirable}   \end{cases}\]</span></p><p>其中：</p><ul><li><p><strong><span class="math inline">\(\mathbb{E}_{x,y \simD}\)</span></strong>：表示对数据集 <span class="math inline">\(D\)</span> 中的样本进行期望计算，其中 <span class="math inline">\(x\)</span> 是输入，<span class="math inline">\(y\)</span> 是模型生成的输出。</p></li><li><p><strong><span class="math inline">\(\lambda_y\)</span></strong>：代表与输出 <span class="math inline">\(y\)</span> 相关的损失厌恶参数，可以是<strong><span class="math inline">\(\lambda_D\)</span></strong>（用于积极输出）或<strong><span class="math inline">\(\lambda_U\)</span></strong>（用于消极输出），用于表示人类对损失的厌恶程度。</p></li><li><p><strong><span class="math inline">\(r_\theta(x,y)\)</span></strong>： $ r_(x,y) = . $该函数表示在当前策略 <span class="math inline">\(\pi_\theta\)</span>下生成输出 <span class="math inline">\(y\)</span> 的对数概率与参考策略<span class="math inline">\(\pi_{\text{ref}}\)</span>下生成同一输出的对数概率之比。它衡量了当前模型与参考模型在生成特定输出时的相对表现。</p></li><li><p><strong><span class="math inline">\(z_0\)</span></strong>： <span class="math inline">\(z_0 = KL(\pi_\theta(y'|x) \|\pi_{\text{ref}}(y'|x))\)</span>. 这里量化当前策略 <span class="math inline">\(\pi_\theta\)</span> 和参考策略 <span class="math inline">\(\pi_{\text{ref}}\)</span>之间的差异。它作为评估当前策略与参考策略偏离程度的参考点。</p></li><li><p><strong><span class="math inline">\(v(x,y)\)</span></strong>：价值函数，依赖于输出<span class="math inline">\(y\)</span> 的性质. <strong><span class="math inline">\(\sigma\)</span></strong>：逻辑函数，用于对价值函数进行调整，使其保持凹性（对于收益）和凸性（对于损失），模型就会在收益时更加规避风险，在损失时更加追求风险。<strong><span class="math inline">\(\beta\)</span></strong>：风险厌恶参数，控制风险厌恶的程度。增加<span class="math inline">\(\beta\)</span>会增加收益时的风险规避行为和损失时的风险追求行为。</p></li></ul><h3 id="ppo-dpo-以及-kto-的对比">PPO, DPO 以及 KTO 的对比</h3><table><colgroup><col style="width: 11%"><col style="width: 29%"><col style="width: 29%"><col style="width: 29%"></colgroup><thead><tr><th>Aspect</th><th>PPO</th><th>DPO</th><th>KTO</th></tr></thead><tbody><tr><td>目标</td><td>最大化预期奖励，同时防止策略更新过大（目标函数clip）。</td><td>根据人类偏好直接优化策略，使用二元分类目标（使用 KL散度约束）。</td><td>通过最大化 LLM生成的效用对齐模型，基于前景理论，不需要详细的偏好对。</td></tr><tr><td>输入</td><td>来自环境的状态和奖励。</td><td>来自环境的状态和人类偏好反馈。</td><td>带有二元标签（可取或不可取结果）的 LLM 输出。</td></tr><tr><td>输出</td><td>在环境中采取的行动。</td><td>在环境中采取的行动，与人类偏好对齐。</td><td>与简化人类效用函数对齐的 LLM 生成结果。</td></tr><tr><td>学习机制</td><td>使用clip替代目标的策略梯度来更新策略和价值网络。</td><td>在人类偏好数据上进行二元交叉熵优化，更新单个策略网络。</td><td>基于 LLM 输出与二元反馈的对齐进行优化，无需复杂的偏好模型。</td></tr><tr><td>网络结构</td><td>独立的策略网络和价值网络。</td><td>单个策略网络。</td><td>针对 KTO 方法学调整的 LLM 框架。</td></tr><tr><td>反馈机制</td><td>使用来自环境的奖励作为学习的反馈。</td><td>使用人类偏好数据作为直接反馈进行学习。</td><td>利用对 LLM 输出的二元反馈来指导对齐，无需复杂的偏好数据。</td></tr><tr><td>稳定性</td><td>目标函数中的剪辑机制保持策略更新的稳定性。</td><td>通过直接优化偏好，利用动态逐例重要性加权实现内在稳定性。</td><td>通过简化反馈机制和聚焦于效用最大化来实现稳定的对齐。</td></tr><tr><td>复杂性</td><td>由于双网络结构和奖励最大化与策略更新稳定性之间的平衡，较复杂。</td><td>更简单，因为它绕过显式的奖励建模，直接从人类偏好优化政策。</td><td>通过消除对详细偏好建模的需求，专注于二元效用优化，降低复杂性。</td></tr><tr><td>适用性</td><td>适用于各种 RL 环境，其中奖励信号可用。</td><td>在与人类偏好对齐至关重要的场景中特别有效。</td><td>在快速和简化对齐人类反馈的场景中尤为有用。</td></tr></tbody></table><h3 id="对齐可能引入的偏差以及解决策略">对齐可能引入的偏差以及解决策略</h3><p>在讨论 <strong>强化学习人类反馈（RLHF）</strong> 和<strong>强化学习人工反馈（RLAIF）</strong>时，一个重要的问题是：这些方法是否会给模型引入偏见？答案是肯定的，正如任何依赖人类输入的机器学习方法，RLHF也有引入偏见的潜力。</p><p><strong>可能引入的不同形式的偏见</strong></p><ol type="1"><li><p><strong>选择偏见</strong>：RLHF依赖于人类评估者的反馈，这些评估者可能会有自己的偏见和偏好，因此他们的反馈可能局限于他们能够关联的主题或情境。这可能导致模型没有接触到其在现实世界中将遇到的行为和结果的真实范围。</p></li><li><p><strong>确认偏见</strong>：人类评估者可能更倾向于提供确认他们已有信念或预期的反馈，而不是根据代理的表现提供客观反馈。这可能导致模型在某些行为或结果上受到强化，而这些行为或结果在长远来看可能并不理想或可取。</p></li><li><p><strong>评分者间的差异</strong>：不同的人类评估者可能对代理表现的质量有不同的看法或判断，导致agent收到的反馈不一致。这使得有效训练agent变得困难，并可能导致次优表现。</p></li><li><p><strong>反馈有限</strong>：人类评估者可能无法对agent表现的所有方面提供反馈，导致agent 学习的缺口，可能在某些情况下表现不佳。</p></li></ol><p><strong>缓解策略</strong></p><ol type="1"><li><p><strong>多样化评估者选择</strong>：选择具有不同背景和视角的评估者可以帮助减少反馈中的偏见，就像在工作场所中一样。这可以通过从不同的人口群体、地区或行业招募评估者来实现。</p></li><li><p><strong>共识评估</strong>：使用共识评估，即多个评估者对同一任务提供反馈，可以减少个体偏见的影响，提高反馈的可靠性。这几乎就像是对评估进行“归一化”。</p></li><li><p><strong>评估者的校准</strong>：通过提供培训和指导来校准评估者，帮助提高反馈的质量和一致性。</p></li><li><p><strong>反馈过程的评估</strong>：定期评估反馈过程，包括反馈质量和培训过程的有效性，可以帮助识别和解决可能存在的偏见。</p></li><li><p><strong>agent 表现的评估</strong>：定期评估 agent在各种任务和不同环境中的表现，可以确保其没有过拟合于特定示例，并且能够推广到新的情境。</p></li><li><p><strong>平衡反馈</strong>：将人类评估者的反馈与其他反馈来源（如自我对话或专家演示）进行平衡，有助于减少反馈中的偏见影响，提高训练数据的整体质量。</p></li></ol><h3 id="trl---transformer-reinforcement-learning">TRL - TransformerReinforcement Learning</h3><p><strong>TRL</strong>（Transformer ReinforcementLearning）库可用于通过<strong>监督微调（SFT）</strong>、<strong>奖励建模（RM）</strong>、<strong>近端策略优化（PPO）</strong>以及 <strong>直接偏好优化（DPO）</strong>等方法，对转换器语言模型和扩散模型进行微调和对齐。</p><h2 id="mixture-of-experts">5. Mixture of Experts</h2><p><a href="https://aman.ai/primers/ai/mixture-of-experts/">Mixture ofExperts</a></p><h2 id="encoder-vs.-decoder-vs.-encoder-decoder">6. Encoder vs. Decodervs. Encoder-Decoder？</h2><p><a href="https://discuss.huggingface.co/t/suggestions-and-guidance-finetuning-bert-models-for-next-word-prediction/14043">模型总结</a><img src="/2024/10/08/statistic/enc_dec.jpeg"></p><h3 id="编码器模型">编码器模型</h3><p>基于编码器的模型专注于理解输入文本。它们通过在包含重建损坏输入任务的预训练（如掩码部门tokens）中，捕获丰富的上下文信息。<strong>BERT</strong>（双向编码器表示的变换器）是编码器模型的一个重要例子。在BERT 中，部分输入标记会被特殊的 [MASK]标记替代，模型通过周围的上下文预测这些被掩蔽的标记。这使得 BERT能够学习双向表示，从而捕捉标记左侧和右侧的上下文。编码器模型在自然语言理解（NLU）任务中尤其有效，例如文本分类、情感分析和抽取式问答。这些任务受益于模型深刻理解和表示输入文本的能力。</p><p>BERT 可以利用双向语境进行预测 maskedtokens，相较于自回归模型，使用双向信息可以提高性能。然而，BERT在预训练时使用的 [MASK]等人工符号在微调时并不存在于真实数据中，这就造成了预训练数据与微调数据分布之间的异质性。此外，由于预测的tokens在输入中被mask，BERT无法像自回归那样使用乘积规则建立联合概率模型。换句话说，<strong>BERTassumes the predicted tokens are independent of each other given theunmasked tokens, which is oversimplified as high-order, long-rangedependency is prevalent in natural language.（在给定其他未被掩码的tokens时，BERT对每个被掩盖的单词独立地进行预测，所有需要预测的token之间被假设是相互独立的）</strong></p><p>BERT（及其所有变体，如 RoBERTa、DistilBERT、ALBERT 等）和 XLM就是编码器模型的例子。</p><p><img src="/2024/10/08/statistic/bert.jpg"></p><p><strong>优点：</strong></p><ol type="1"><li><strong>上下文依赖性：</strong>在BERT等Encoder模型中，模型可以同时获取左右两侧的上下文信息（双向上下文）。相比之下，像GPT这样的自回归模型只能访问当前位置之前的内容（单向上下文）。双向上下文对于理解文本中的复杂依赖关系和语义结构至关重要，因此Encoder模型在许多自然语言处理任务上表现优异。</li><li><strong>丰富的上下文理解：</strong>Encoder模型擅长捕捉输入数据中的复杂模式和关系，尤其适合需要深入理解和分析的任务，比如命名实体识别、文本分类、阅读理解等。这使得模型能够更好地理解句子的语义和结构，从而在相关任务中表现出色。</li></ol><p><strong>缺点：</strong></p><ol type="1"><li><strong>输入噪声：</strong>BERT在预训练阶段使用了人工符号（如[MASK]）来掩盖部分输入单词，但这些符号在实际应用和微调时并不存在。这种差异导致了预训练和微调之间数据分布的异质性，影响模型的迁移能力。</li><li><strong>独立性假设：</strong>BERT在预测被掩盖的单词时，假设每个掩盖的单词是独立于其他掩盖词的，只与未被掩盖的单词相关。例如，在句子“itshows that the housing crisis was turned into a bankingcrisis”中，如果掩盖了“banking”和“crisis”两个词，模型会分别预测它们，而不考虑二者之间的隐含关系。这种独立性假设限制了模型对掩盖词之间复杂依赖关系的捕捉，影响了在需要更精细理解的任务中的表现。</li></ol><h3 id="解码器模型">解码器模型</h3><p>基于解码器的模型（自回归模型）旨在执行文本生成任务。这些模型一次生成一个token，并使用之前生成的token作为上下文来预测下一个token。<strong>GPT</strong>（生成式预训练变换器）、GPT-2和 GPT-3是解码器模型的典型例子。这些模型在大规模语料库上进行预训练，以预测句子中的下一个词，从而能够生成连贯且语境相关的文本。解码器模型在自然语言生成（NLG）任务中表现出色，例如语言翻译、文本摘要和对话生成。它们生成流利且上下文适当的文本的能力使其成为从头生成内容任务的理想选择。</p><p>自回归语言模型利用上下文中的词来预测下一个词，通过估计文本语料库的概率分布来实现。具体来说，给定一个文本序列$ x = (x_1, , x_T) <span class="math inline">\(，自回归语言建模将似然分解为前向积：\)</span>$p(x) = <em>{t=1}^{T} p(x_t | x</em>{&lt;t}) $$</p><p>或者反向的形式： <span class="math display">\[p(x) = \prod_{t=1}^{T} p(x_t | x_{&gt;t})\]</span></p><p>通常使用<strong>多项式分布</strong>（MultinomialDistribution）来建模下一个词的生成过程。这是因为语言模型的目标是根据先前的词预测当前词的概率，而语言中词汇的生成通常是一个离散事件。因此，给定上下文$x_{&lt;t} $，下一个词 $ x_t $ 的条件概率可以表示为： <span class="math display">\[p(x_t \mid x_{&lt;t}) = \text{softmax}(z_t),\]</span> 其中，$ z_t $是通过神经网络计算得到的未归一化的得分，通常代表每个词在词汇表中的相对可能性。通过使用softmax函数，这些得分被转换为一个有效的概率分布，保证所有可能词的概率和为1。</p><p>在这种情况下，模型会学习每个条件分布的参数模型（例如神经网络）。由于自回归语言模型只训练编码单向上下文（要么是前向的，要么是后向的），因此它在建模深层双向上下文方面并不有效。下面的图示展示了前向和后向的方向性。</p><p><img src="/2024/10/08/statistic/auto1.jpg"></p><p><img src="/2024/10/08/statistic/auto2.jpg"></p><p><strong>优点：</strong></p><ol type="1"><li><strong>生成能力强</strong>：自回归语言模型非常适合生成式自然语言处理（NLP）任务。由于它们采用因果注意力机制来预测下一个标记，因此在内容生成方面自然适用。它们能够生成流畅且与上下文相关的文本，这使得它们在需要自然语言生成的任务中表现出色。</li><li><strong>训练数据生成简单</strong>：训练这些模型的数据生成相对简单，因为目标只是预测给定序列中的下一个标记。这利用了语言数据的固有结构，使得数据准备过程更加高效。</li></ol><p><strong>缺点：</strong></p><ol type="1"><li><strong>上下文限制</strong>：自回归语言模型只能使用前向上下文或后向上下文，这意味着它们不能同时利用双向上下文。这种限制可能会影响它们在需要深刻理解双向上下文的任务中的表现。例如，在处理复杂句子结构或含义依赖时，缺乏双向上下文可能导致理解不够准确。</li></ol><h4 id="编码器-解码器模型">编码器-解码器模型</h4><p>编码器-解码器模型（也称为seq2seq模型）结合了编码器和解码器架构的优势。这些模型使用编码器处理和理解输入序列，使用解码器生成输出序列。这种架构在输入和输出都是序列的任务中特别有效，这些序列可能具有不同的长度或格式，甚至是不同的语言。编码器将输入序列转换为固定长度的上下文向量或中间表示，捕捉输入的含义和上下文。解码器然后接收这个上下文向量，逐个标记地生成输出序列，通常采用类似于解码器模型中的自回归技术。</p><ul><li><strong>T5 (Text-To-Text TransferTransformer)</strong>（文本到文本的迁移变换器）是编码器-解码器模型的一个显著例子。T5将每个自然语言处理问题都视为一个文本到文本的问题，其中输入和输出都是文本序列。这种方法使T5 能够应用于广泛的任务，包括翻译、摘要和问答。<br></li><li><strong>BART (Bidirectional and Auto-RegressiveTransformers)</strong>（双向自回归变换器）是另一个强大的编码器-解码器模型。BART通过使用任意噪声函数破坏文本并学习重建原始文本进行预训练。这使得它在需要基于对输入的理解生成文本的任务（如摘要和对话生成）中非常有效。<br></li><li><strong>BigBird</strong>使用稀疏注意力机制来处理较长的序列。这使得它适合处理长文档的任务，如文档分类和长篇问答。</li></ul><p><strong>优点：</strong></p><p>编码器-解码器模型能够同时处理输入的理解和输出的生成，使它们在以下任务中特别有效：</p><ul><li><p>机器翻译：将一种语言翻译成另一种语言。</p></li><li><p>文本摘要：生成文本的简要概述。</p></li><li><p>对话生成：在对话系统中生成合适的回应。</p></li></ul><p><strong>缺点：</strong></p><ol type="1"><li><p><strong>计算资源需求高</strong>：编码器-解码器模型通常参数量庞大，尤其是在处理复杂任务时。它们需要大量的计算资源和内存，训练和推理速度可能较慢。</p></li><li><p><strong>长序列处理能力有限</strong>：虽然一些模型（如BigBird）专门针对长序列进行了优化，但传统的编码器-解码器模型在处理非常长的输入时仍然面临挑战，因为它们的输入长度受限。</p></li><li><p><strong>依赖于大量标注数据</strong>：这些模型通常需要大量的高质量标注数据进行训练，这在某些领域可能难以获得。此外，训练过程中数据的多样性和质量直接影响模型的性能。</p></li><li><p><strong>对输入输出长度不匹配的敏感性</strong>：编码器-解码器模型在处理输入和输出长度差异较大的任务时，可能会表现不佳。例如，当输入很长而输出很短时，模型可能难以有效地提取和生成信息。</p></li></ol><h3 id="总结">总结</h3><ul><li>编码器模型在需要理解和解释文本的任务中表现出色。由于其能够捕捉双向上下文，这使得它们适用于理解整个句子或文档上下文至关重要的任务。例如，命名实体识别、情感分析和文本分类等任务都依赖于模型对输入文本的深刻理解。</li><li>解码器模型则非常擅长生成文本，因此非常适合创意任务，如故事生成、聊天机器人回复和文本补全等。它们通过利用先前生成的内容作为上下文来预测下一个词，从而能够生成流畅且与上下文相关的文本。</li><li>编码器-解码器模型提供了一种灵活的架构，可以处理广泛的任务，从机器翻译、文本摘要到复杂的问题回答和文档生成。这种模型能够同时理解和生成文本，使其在需要深刻理解和流利文本生成的任务中非常有效。</li></ul><p>例如，在机器翻译中，编码器处理源语言的输入句子，生成一个上下文向量，而解码器则利用这个上下文向量生成目标语言的翻译。类似地，在文本摘要中，编码器阅读并理解原始文本，而解码器则生成一个简洁的摘要。这种架构的优势在于它能够结合理解和生成的能力，适用于多种自然语言处理任务。</p><h2 id="deepspeed-框架">6. Deepspeed 框架</h2><h2 id="decoder-only的模型推理阶段是串行进行计算速度很慢训练的时候实际上是并行训练的介绍训练的细节">6.decoderonly的模型，推理阶段是串行进行计算,速度很慢。训练的时候实际上是并行训练的，介绍训练的细节。</h2><h2 id="为什么视觉上使用bntransformer上使用ln-几种-normalization-介绍">8.为什么视觉上使用BN，transformer上使用LN ？几种 normalization 介绍。</h2><h2 id="overview-of-vision-language-models">9. Overview ofVision-Language Models</h2><p>https://aman.ai/primers/ai/interview/</p><hr><h1 id="reference">Reference</h1><p><a href="https://github.com/315386775/DeepLearing-Interview-Awesome-2024?tab=readme-ov-file">DeepLearing-Interview-Awesome-2024</a></p><p><a href="https://oi-wiki.org/basic/radix-sort/">几种排序算法</a></p><p><a href="https://github.com/zhengjingwei/machine-learning-interview?tab=readme-ov-file#1-1">machine-learning-interview</a></p><p><a href="https://aman.ai/primers/ai/">Distilled AI</a></p>]]></content>
      
      
      <categories>
          
          <category> Machine Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Machine Learning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Blip, Blip2 (Q-former), InstructBlip, CONCH, PULSE</title>
      <link href="/2024/09/12/multi_modalv2/"/>
      <url>/2024/09/12/multi_modalv2/</url>
      
        <content type="html"><![CDATA[<h2 id="a-very-nice-blog-blip系列文章小结">A very nice blog: <a href="http://www.myhz0606.com/article/blip_hub">BLIP系列文章小结</a></h2><h1>BLIP</h1><p><a href="https://arxiv.org/pdf/2201.12086">BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation</a></p><h2 id="model-architecture">Model Architecture</h2><p><img src="/2024/09/12/multi_modalv2/blip1.jpg" alt=""></p><p>整体上看，对于图像部分，有一个 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>N</mi></mrow><annotation encoding="application/x-tex">N</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.10903em;">N</span></span></span></span> 层的 ViT。对于文本部分，分别使用三个 text encoder 去计算三个不同的目标函数。在 blip 中，同种颜色代表同样的共享参数，</p><p>对于第一个 text encoder，具有 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>N</mi></mrow><annotation encoding="application/x-tex">N</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.10903em;">N</span></span></span></span> 层，主要是将文本特征和图像特征进行对比学习，计算 ITC loss 来进行分类任务。第二个 image-grounded text encoder。提取得到的图像特征通过 cross attention 进入模型，文本特征通过 self-attention 得到，然后进行融合得到多模态的特征，计算 ITM loss 来判断 image-text pairs 是否匹配。相较于第一个 text encoder，第二个 encoder 只需要学习额外的 cross attention 层。为了能够执行 生成式 任务，blip 添加了一个 decoder。由于 decoder 不能看到完整的句子，因此将 causal self-attention 替换掉前面 encoder 中的 bi self-attention，不过 cross-attention 和 feed forward 层依旧和前面的共享参数。这里 decoder 使用的 language modeling loss (LM loss)，根据前面的文本去预测后面的文本，而不是进行文本的完形填空 (i.e., MLM loss)。</p><p>不同 text encoder 使用不同的 token，分别是 [CLS]，[Encode]，[Decode]。</p><h2 id="capfilt">CapFilt</h2><p><img src="/2024/09/12/multi_modalv2/blip2.jpg" alt=""></p><p>对于从网络上获取的图文数据质量比较糟糕，图片对应的文本描述往往不够准确。针对这种情况，blip finetune 了一个 filter 来筛选图文对，一个 captioner 来生成合成的文本。<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">{</mo><msub><mi>I</mi><mi>w</mi></msub><mo separator="true">,</mo><msub><mi>T</mi><mi>w</mi></msub><mo stretchy="false">}</mo></mrow><annotation encoding="application/x-tex">\{I_w, T_w\}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">{</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.07847em;">I</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:-0.07847em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.02691em;">w</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.13889em;">T</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.02691em;">w</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">}</span></span></span></span> 是从 web 上获取的 noisy image-text pairs，<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">{</mo><msub><mi>I</mi><mi>h</mi></msub><mo separator="true">,</mo><msub><mi>T</mi><mi>h</mi></msub><mo stretchy="false">}</mo></mrow><annotation encoding="application/x-tex">\{I_h, T_h\}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">{</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.07847em;">I</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:-0.07847em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">h</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.13889em;">T</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">h</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">}</span></span></span></span> 是人工标注的 image-text pairs，通常认为是高质量的。对于预训练好的 blip 模型，首先基于 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">{</mo><msub><mi>I</mi><mi>h</mi></msub><mo separator="true">,</mo><msub><mi>T</mi><mi>h</mi></msub><mo stretchy="false">}</mo></mrow><annotation encoding="application/x-tex">\{I_h, T_h\}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">{</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.07847em;">I</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:-0.07847em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">h</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.13889em;">T</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">h</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">}</span></span></span></span>数据对两个预训练好的 text encoder 进行 finetune 得到 filter model。然后对 noisy data <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">{</mo><msub><mi>I</mi><mi>w</mi></msub><mo separator="true">,</mo><msub><mi>T</mi><mi>w</mi></msub><mo stretchy="false">}</mo></mrow><annotation encoding="application/x-tex">\{I_w, T_w\}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">{</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.07847em;">I</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:-0.07847em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.02691em;">w</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.13889em;">T</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.02691em;">w</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">}</span></span></span></span> 进行筛选。同时，基于 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">{</mo><msub><mi>I</mi><mi>h</mi></msub><mo separator="true">,</mo><msub><mi>T</mi><mi>h</mi></msub><mo stretchy="false">}</mo></mrow><annotation encoding="application/x-tex">\{I_h, T_h\}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">{</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.07847em;">I</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:-0.07847em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">h</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.13889em;">T</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">h</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">}</span></span></span></span>数据对预训练好的 decoder 进行 finetune，用来生成合成的 caption。由于生成的 caption 质量并不确定，因此将 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">{</mo><msub><mi>I</mi><mi>w</mi></msub><mo separator="true">,</mo><msub><mi>T</mi><mi>s</mi></msub><mo stretchy="false">}</mo></mrow><annotation encoding="application/x-tex">\{I_w, T_s\}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">{</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.07847em;">I</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:-0.07847em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.02691em;">w</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.13889em;">T</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">s</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">}</span></span></span></span> 再通过 filter 进行筛选。最终得到数据 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>D</mi><mo>=</mo><mo stretchy="false">{</mo><msub><mi>I</mi><mi>w</mi></msub><mo separator="true">,</mo><msub><mi>T</mi><mi>w</mi></msub><mo stretchy="false">}</mo><mo>+</mo><mo stretchy="false">{</mo><msub><mi>I</mi><mi>w</mi></msub><mo separator="true">,</mo><msub><mi>T</mi><mi>s</mi></msub><mo stretchy="false">}</mo><mo>+</mo><mo stretchy="false">{</mo><msub><mi>I</mi><mi>h</mi></msub><mo separator="true">,</mo><msub><mi>T</mi><mi>h</mi></msub><mo stretchy="false">}</mo></mrow><annotation encoding="application/x-tex">D = \{I_w, T_w\} + \{I_w, T_s\} + \{I_h, T_h\}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.02778em;">D</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">{</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.07847em;">I</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:-0.07847em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.02691em;">w</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.13889em;">T</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.02691em;">w</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">}</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">{</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.07847em;">I</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:-0.07847em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.02691em;">w</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.13889em;">T</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">s</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">}</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">{</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.07847em;">I</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:-0.07847em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">h</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.13889em;">T</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">h</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">}</span></span></span></span>。</p><h1>CONCH</h1><p><a href="https://www.nature.com/articles/s41591-024-02856-4">A visual-language foundation model for computational pathology</a></p><p>论文介绍了一种视觉-语言基础通用模型-CONCH，利用不同来源的组织病理学图像、生物医学文本和超过 117 万个图像标题对等数据，通过任务识别进行预训练。CONCH 以最先进的视觉语言基础预训练框架 CoCa 为基础，使用一个图像编码器、一个文本编码器和一个多模态融合解码器，并结合使用对齐目标函数和标题目标函数进行训练。其中，对齐目标损失的目的是在模型的表征空间中对齐图像和文本模态，而标题目标则是学习预测与图像相对应的标题。论文总共使用 14 种不同的基准数据集，研究了 CONCH 在一系列任务中的能力，包括图像分类、图像到文本和文本到图像检索、图像分割和图像标题生成。</p><h2 id="数据处理">数据处理</h2><p>为便于整理，论文将数据源分为两类：（1）EDU，包含从教育笔记中提取的数据；（2）PMC OA，从 PubMed Central Open Access Datase 下载的数据。</p><p>数据整理的挑战有两个：</p><ol><li>筛选组织病理学图像：下载的原始数据包含了组织病理学和非组织病理学的图像。</li><li>处理图像面板：大量数据以图像面板的形式呈现，面板中的图像由多个子图像组成，图像标题文本中有时同时或分别包含了多个子图像的描述。</li></ol><p>为了应对这些挑战，数据清理分为三个步骤：</p><ol><li>检测组织病理学图像：使用YOLOv5对象检测模型生成边界框 bounding boxes 以提取检测到的图像。这一步之前，作者首先通过生成合成数据来训练该对象检测模型。</li><li>分割图像标题：作者在整理 EDU 数据集时收集了一个包含图像标题说明和拆分后标题说明的数据集，对GPT模型进行微调，原始图像标题为输入，拆分后的图像标题为输出，最终使得模型具有实现自动拆分图像标题的能力。</li><li>子图像和标题进行对齐：首先在干净的EDU数据集上训练一个CLIP模型，将检测到的子图像与拆分后的标题说明进行对齐。使用训练后的模型，给定一组图像面板中的 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>m</mi></mrow><annotation encoding="application/x-tex">m</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathdefault">m</span></span></span></span> 幅检测到的图像和 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi></mrow><annotation encoding="application/x-tex">n</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathdefault">n</span></span></span></span> 个分割的文本，得到模型中的图像嵌入表征 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>u</mi><mn>0</mn><mo separator="true">,</mo><mi>u</mi><mn>1</mn><mo separator="true">,</mo><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mo separator="true">,</mo><mi>u</mi><mi>m</mi></mrow><annotation encoding="application/x-tex">{u0, u1, ..., um}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8388800000000001em;vertical-align:-0.19444em;"></span><span class="mord"><span class="mord mathdefault">u</span><span class="mord">0</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault">u</span><span class="mord">1</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord">.</span><span class="mord">.</span><span class="mord">.</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault">u</span><span class="mord mathdefault">m</span></span></span></span></span> 和文本嵌入表征 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>v</mi><mn>0</mn><mo separator="true">,</mo><mi>v</mi><mn>1</mn><mo separator="true">,</mo><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mo separator="true">,</mo><mi>v</mi><mi>n</mi></mrow><annotation encoding="application/x-tex">{v0, v1, ..., vn}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8388800000000001em;vertical-align:-0.19444em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.03588em;">v</span><span class="mord">0</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault" style="margin-right:0.03588em;">v</span><span class="mord">1</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord">.</span><span class="mord">.</span><span class="mord">.</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault" style="margin-right:0.03588em;">v</span><span class="mord mathdefault">n</span></span></span></span></span> 。然后两两计算余弦相似度，将相似度最高的作为一对图文数据。</li></ol><p>通过以上三步以及进一步数据清理，形成了一个包含<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>117</mn></mrow><annotation encoding="application/x-tex">117</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">1</span><span class="mord">1</span><span class="mord">7</span></span></span></span>万对人类组织病理学图像-说明的数据集。</p><h2 id="visual-language-pretraining">Visual-language pretraining</h2><p>在训练过程中，作者同时考虑了两种loss，一种是图文对比损失（image-to-text and text-to-image contrastive loss)，另一种是针对标题的loss。</p><p><img src="/2024/09/12/multi_modalv2/conch1.jpg" alt=""></p><p>模型框架：模型包括一个 image encoder <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mo stretchy="false">(</mo><mo>⋅</mo><mo separator="true">;</mo><mi>θ</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">f(\cdot; \theta)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.10764em;">f</span><span class="mopen">(</span><span class="mord">⋅</span><span class="mpunct">;</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault" style="margin-right:0.02778em;">θ</span><span class="mclose">)</span></span></span></span>，一个 text encoder <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>g</mi><mo stretchy="false">(</mo><mo>⋅</mo><mo separator="true">;</mo><mi>ϕ</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">g(\cdot; \phi)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.03588em;">g</span><span class="mopen">(</span><span class="mord">⋅</span><span class="mpunct">;</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault">ϕ</span><span class="mclose">)</span></span></span></span> 和一个图文融合的 decoder <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>h</mi><mo stretchy="false">(</mo><mo>⋅</mo><mo separator="true">;</mo><mi>ψ</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">h(\cdot; \psi)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault">h</span><span class="mopen">(</span><span class="mord">⋅</span><span class="mpunct">;</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault" style="margin-right:0.03588em;">ψ</span><span class="mclose">)</span></span></span></span>。</p><p>Image encoder 包含了一个 backbone (参数为 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>θ</mi><mtext>backbone</mtext></msub></mrow><annotation encoding="application/x-tex">\theta_{\text{backbone}}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.84444em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02778em;">θ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:-0.02778em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord text mtight"><span class="mord mtight">backbone</span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>) 和 两个 attention pooler 模块，参数分别为  <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>θ</mi><mtext>contrast</mtext></msub></mrow><annotation encoding="application/x-tex">\theta_{\text{contrast}}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.84444em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02778em;">θ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2805559999999999em;"><span style="top:-2.5500000000000003em;margin-left:-0.02778em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord text mtight"><span class="mord mtight">contrast</span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> 和  <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>θ</mi><mtext>caption</mtext></msub></mrow><annotation encoding="application/x-tex">\theta_{\text{caption}}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.980548em;vertical-align:-0.286108em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02778em;">θ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.317502em;"><span style="top:-2.5500000000000003em;margin-left:-0.02778em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord text mtight"><span class="mord mtight">caption</span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span></span></span></span>。 Backbone 使用的是标准的 ViT，具有12层的 transformer层，12 个 attention heads，embedding 的维度为 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>768</mn></mrow><annotation encoding="application/x-tex">768</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">7</span><span class="mord">6</span><span class="mord">8</span></span></span></span>，hidden dimension 是 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>3072</mn></mrow><annotation encoding="application/x-tex">3072</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">3</span><span class="mord">0</span><span class="mord">7</span><span class="mord">2</span></span></span></span>。Image 划分为 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>16</mn><mo>×</mo><mn>16</mn></mrow><annotation encoding="application/x-tex">16 \times 16</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.72777em;vertical-align:-0.08333em;"></span><span class="mord">1</span><span class="mord">6</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">1</span><span class="mord">6</span></span></span></span> 个 image tokens （<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>256</mn></mrow><annotation encoding="application/x-tex">256</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">2</span><span class="mord">5</span><span class="mord">6</span></span></span></span>个），并在每个token上面添加可学习的绝对位置编码。ViT 将RGB图像转换为 feature maps. 基于从 ViT最后一层输出的image token 的特征表示 （其实也是输入decoder cross-attention 中的 quey)，每一个 attention pooler 从不同数量的 image tokens 上去学习相应的信息。具体来说，第一个 attention pooler <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>f</mi><mrow><mtext>contrast</mtext><mo stretchy="false">(</mo><mo>⋅</mo><mo separator="true">;</mo><msub><mi>θ</mi><mtext>contrast</mtext></msub><mo stretchy="false">)</mo></mrow></msub></mrow><annotation encoding="application/x-tex">f_{\text{contrast}(\cdot; \theta_{\text{contrast}})}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.04964em;vertical-align:-0.3551999999999999em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.10764em;">f</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.34480000000000005em;"><span style="top:-2.5198em;margin-left:-0.10764em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord text mtight"><span class="mord mtight">contrast</span></span><span class="mopen mtight">(</span><span class="mord mtight">⋅</span><span class="mpunct mtight">;</span><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.02778em;">θ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.29634285714285713em;"><span style="top:-2.357em;margin-left:-0.02778em;margin-right:0.07142857142857144em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord text mtight"><span class="mord mtight">contrast</span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.143em;"><span></span></span></span></span></span></span><span class="mclose mtight">)</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.3551999999999999em;"><span></span></span></span></span></span></span></span></span></span> 使用一个 query 去学习一个 image token，用于捕捉image的全局特征。第二个 attention pooler <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>f</mi><mrow><mtext>caption</mtext><mo stretchy="false">(</mo><mo>⋅</mo><mo separator="true">;</mo><msub><mi>θ</mi><mtext>caption</mtext></msub><mo stretchy="false">)</mo></mrow></msub></mrow><annotation encoding="application/x-tex">f_{\text{caption}(\cdot; \theta_{\text{caption}})}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0719599999999998em;vertical-align:-0.3775199999999999em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.10764em;">f</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.34479999999999994em;"><span style="top:-2.5198em;margin-left:-0.10764em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord text mtight"><span class="mord mtight">caption</span></span><span class="mopen mtight">(</span><span class="mord mtight">⋅</span><span class="mpunct mtight">;</span><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.02778em;">θ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3340428571428571em;"><span style="top:-2.357em;margin-left:-0.02778em;margin-right:0.07142857142857144em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord text mtight"><span class="mord mtight">caption</span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2818857142857143em;"><span></span></span></span></span></span></span><span class="mclose mtight">)</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.3775199999999999em;"><span></span></span></span></span></span></span></span></span></span> 使用 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi><mo>=</mo><mn>256</mn></mrow><annotation encoding="application/x-tex">n = 256</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathdefault">n</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">2</span><span class="mord">5</span><span class="mord">6</span></span></span></span> 个 queries 去生成 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>256</mn></mrow><annotation encoding="application/x-tex">256</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">2</span><span class="mord">5</span><span class="mord">6</span></span></span></span> 个 image tokens，用于获取 image的细颗粒度的局部信息，进而生成 caption。</p><p>Text encoder 和 multimodal decoder 分别包括 12个 transformer layers，embedding dimension 为 768，hidden dimension 为 3072。Text encoder 通过嵌入表将离散的单词token映射为连续的嵌入向量，并添加了可学习的绝对位置embeddings。Text encoder 为每个tokenized的 caption 添加了一个&lt;CLS&gt; token，用于在Transformer注意力过程中提取文本说明的全局表征。</p><p>Multimodal decoder 在每个多头自注意力层之后插入了交叉注意力层，以整合来自图像token的信息。最后结合语言模型输出预测下一个token在支持的词汇表中的分布。</p><p>假设有 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>M</mi></mrow><annotation encoding="application/x-tex">M</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.10903em;">M</span></span></span></span> 个 image-caption 图文对 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><msub><mi>x</mi><mi>i</mi></msub><mo separator="true">,</mo><msub><mi>w</mi><mi>i</mi></msub><msubsup><mo stretchy="false">)</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>M</mi></msubsup></mrow><annotation encoding="application/x-tex">(x_i, w_i)_{i=1}^{M}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0999949999999998em;vertical-align:-0.258664em;"></span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:-0.02691em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose"><span class="mclose">)</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8413309999999999em;"><span style="top:-2.441336em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">i</span><span class="mrel mtight">=</span><span class="mord mtight">1</span></span></span></span><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.10903em;">M</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.258664em;"><span></span></span></span></span></span></span></span></span></span>，其中caption $ w_i = (<bos>, w_{i,1}, …, w_{i,T}, <eos>)$ 包含 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>T</mi></mrow><annotation encoding="application/x-tex">T</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.13889em;">T</span></span></span></span> 个 word tokens。对于一个图文对 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><msub><mi>x</mi><mi>i</mi></msub><mo separator="true">,</mo><msub><mi>w</mi><mi>i</mi></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(x_i, w_i)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:-0.02691em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span>，假设通过 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>f</mi><mrow><mtext>contrast</mtext><mo stretchy="false">(</mo><mo>⋅</mo><mo separator="true">;</mo><msub><mi>θ</mi><mtext>contrast</mtext></msub><mo stretchy="false">)</mo></mrow></msub></mrow><annotation encoding="application/x-tex">f_{\text{contrast}(\cdot; \theta_{\text{contrast}})}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.04964em;vertical-align:-0.3551999999999999em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.10764em;">f</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.34480000000000005em;"><span style="top:-2.5198em;margin-left:-0.10764em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord text mtight"><span class="mord mtight">contrast</span></span><span class="mopen mtight">(</span><span class="mord mtight">⋅</span><span class="mpunct mtight">;</span><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.02778em;">θ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.29634285714285713em;"><span style="top:-2.357em;margin-left:-0.02778em;margin-right:0.07142857142857144em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord text mtight"><span class="mord mtight">contrast</span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.143em;"><span></span></span></span></span></span></span><span class="mclose mtight">)</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.3551999999999999em;"><span></span></span></span></span></span></span></span></span></span> 得到的输出是 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>u</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">u_i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.58056em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault">u</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>，通过 text encoder <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>g</mi><mo stretchy="false">(</mo><mo>⋅</mo><mo separator="true">;</mo><mi>ϕ</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">g(\cdot;\phi)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.03588em;">g</span><span class="mopen">(</span><span class="mord">⋅</span><span class="mpunct">;</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault">ϕ</span><span class="mclose">)</span></span></span></span> 在 &lt;CLS&gt; token 处经过 l2 normalization 后得到的输出是 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>v</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">v_i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.58056em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.03588em;">v</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>，那么loss是：</eos></bos></p><p><img src="/2024/09/12/multi_modalv2/itc.jpg" alt=""></p><p>其中前两项为 image-to-text and text-to-image contrastive loss, respectively, to maximize the cosine-similarity scores between paired image and text embeddings relative to remaining negative pairings in the mini-batch. The last term seeks to maximize the log-likelihood of each observed token under the multimodal autoregressive language model ( jointly parameterized by the image encoder, text encoder and multimodal decoder), conditioned on previous tokens in the caption, as well as the corresponding image.</p><p>具体训练设置，主要包括以下几点：</p><ul><li>训练轮数：每个视觉-语言预训练实验都运行了 40 个epoch。</li><li>硬件配置：实验分布式运行在 8个NVIDIA A100 80-GB GPU 上，每个GPU上的本地批量大小为48。</li><li>梯度累积：为了达到更大的有效全局批量大小，使用了 梯度累积，实现了 1,536 的全局批量大小（48 × 8 GPU × 4次梯度累积）。</li><li>图像大小：输入图像大小为 448 × 448像素，其中：对较大的图像，首先将其较短边调整为448像素，并对其进行中心裁剪。对较小的图像，按需进行 零填充 以达到所需的尺寸。</li></ul><h2 id="evaluation">Evaluation</h2><h1>Quilt-LLaVA: Visual Instruction Tuning by Extracting Localized Narratives from Open-Source Histopathology Videos</h1><h2 id="abstract">Abstract</h2><p>组织病理学诊断需要对整张切片图像（WSI）进行全局分析，这就要求病理学家从不同的 WSI patch 中复合信息。然而，高分辨率的 WSI 对组织病理学多模式模型提出了挑战。训练组织病理学多模态模型需要用于微调的数据集，而目前的数据集包含单个图像patch的信息，没有每个 patch 间的空间概念，也没有更广泛的 WSI 视图。为了弥补这一不足，本文推出了 QUILT- INSTRUCT，这是一个包含 107,131 个组织病理学特定指令问/答对的大型数据集，这些指令以构成 WSI 的诊断相关图像patch为基础。数据集是利用 YouTube 上的组织病理学教育视频收集的，该视频通过自动提取叙述者的光标位置来提供叙述的局部定位。QUILT-INSTRUCT 支持上下文推理，从整个 WSI 中提取诊断和支持事实。利用 QUILT-INSTRUCT，我们训练出了 QUILT-LLAVA，它的推理能力超越了给定的单个图像patch，能够跨patch进行诊断推理。为了评估 QUILT-LLAVA，我们提出了一个全面的评估数据集，该数据集由 985 幅图像和 1283 个人工生成的问题解答组成。我们还使用公开的组织病理学数据集对 QUILT-LLAVA 进行了全面评估，结果显示 QUILT-LLAVA 在相对 GPT-4 分数上明显优于 SOTA <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>10</mn></mrow><annotation encoding="application/x-tex">10%</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">1</span><span class="mord">0</span></span></span></span> 以上，在开放集和封闭集 VQA 上分别优于 SOTA <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>4</mn></mrow><annotation encoding="application/x-tex">4%</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">4</span></span></span></span> 和 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>9</mn></mrow><annotation encoding="application/x-tex">9%</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">9</span></span></span></span>。</p><h2 id="quilt-instruct-数据集构建">QUILT- INSTRUCT 数据集构建</h2><p>从 4149 个 YouTube 教育视频中构建了 QUILT-INSTRUCT，总时长超过 1000 小时。这些视频是最近组织病理学数据集 QUILT 的一部分。</p><p>在教育视频中，专家在讲述高分辨率 WSI 时往往会停顿一下，然后再用光标指示重点突出区域。我们通过三个步骤将非结构化视频转换为可用的视觉教学数据：首先，我们在视频中定位叙述者的光标。然后，对光标的位置进行时空聚类，以便在图像中将组织病理学概念视觉化。最后，利用提取的标题，使用 LLM 生成指令调整数据集 - QUILT-INSTRUCT。这一过程涉及提示（prompting）技术，从为每个图像patch 生成不同 Q/A 对的独立提示，到结合 WSI 中各patch信息的基于推理的提示，从而生成推理诊断的 Q/A 对。</p><p><img src="/2024/09/12/multi_modalv2/quilt1.jpg" alt=""></p><p>论文介绍了两种不同类型的问答生成方法，用于处理基于病理学的 Whole Slide Images (WSI，整个切片图像) 的文本生成任务。</p><ul><li><p>Independent Prompts（独立提示）：</p><p>这一方法基于单个切片级别（patch-level）的文本输入进行问答生成。切片级别文本是与病理图像的某一小块相关的描述。这种提示生成的问答对是独立于整个图像或视频的上下文，仅依赖于该切片的内容，类似于文献[17]中的对话式和详细描述的生成方式。因为这些提示不依赖于其他信息源，因此被称为“独立提示”（Independent prompts）。</p></li><li><p>Reasoning-based Prompts（推理提示）：</p><p>这种方法利用整个视频中的上下文线索，特别是该视频围绕单个WSI的诊断展开，通过逐步揭示概念和线索。输入不仅包含切片级别的文本，还包括整个WSI的全局信息。因此，模型不仅仅基于当前的切片，还可以考虑到全局的诊断信息。通过这种方法，模型（如GPT-4）可以超越当前上下文进行推理，但仍然依靠从视频或图像中提取的事实信息，这样可以减少生成内容的虚构或不准确（即减少幻觉现象）。</p><p>简单来说，独立提示只基于局部信息生成问答，而推理提示则结合了全局的诊断线索，帮助模型更合理地推理并减少错误。</p></li></ul><h2 id="training-quilt-llava-evaluating-with-quilt-vqa">Training QUILT-LLAVA &amp; evaluating with QUILT-VQA</h2><p>论文使用 QUILT-INSTRUCT 来训练 QUILT-LLAVA。在 QUILT-INSTRUCT 之外单独设计 QUILT-VQA，以评估 QUILT-LLAVA。最后，从 QUILT-VQA 中生成指令遵循测试集，以评估 QUILT-LLAVA 的指令遵循能力。</p><h3 id="training-quilt-llava">Training QUILT-LLAVA</h3><p>LLAVA 由一个vision 模块、多层感知机（MLP）和大语言模型（LLM）组成。这个设计允许语言模型处理视觉信息。</p><ol><li>首先，MLP 最初作为一个投影器被训练，直到收敛。在这个阶段，LLM 和视觉模块都被冻结，不会更新权重。</li><li>随后，MLP 和 LLM 都会结合指令跟随数据进行微调，使模型与人类病理学家的诊断过程保持一致。</li></ol><p>LLAVA 使用预训练的 CLIP 图像编码器，但在这个特定领域中，使用了在公共病理学数据集（如 QUILT-NET [9] 和 PLIP [8]）上训练的预训练 CLIP 模型。作者还通过不同的图像编码器、训练策略和视觉提示进行消融实验，以测试其效果。</p><p><img src="/2024/09/12/multi_modalv2/quilt2.jpg" alt=""></p><p>具体来说，</p><ul><li><p>对齐视觉和语言模型:</p><p>作者首先在病理学领域内对视觉和语言模型进行对齐。为此，从 QUILT 数据集中提取了 723K 图像-文本对，并将描述文本转换为问答格式。问答对的生成方法是随机选择一个预定义的问题（见附录图18），将其添加到图像描述前，形成问答对。问题设计用于描述图像中可见的视觉信息。</p><p>在这一阶段，视觉和语言模型被冻结，仅训练 MLP 层，其任务是将来自图像编码器的嵌入映射到语言模型中，以便语言模型根据问题预测图像的描述。这一阶段的训练将病理学图像的嵌入与相应的文本嵌入对齐，确保了视觉信息能够被语言模型处理。</p></li><li><p>组织病理学数据集指令微调</p><p>论文使用 QUILT-INSTRUCT 对模型进行微调。在此阶段，冻结视觉编码器权重，继续训练 MLP 层和语言模块。</p></li></ul><h3 id="evaluation-data-generation-quilt-vqa">Evaluation Data Generation: QUILT-VQA</h3><p>在组织病理学领域，研究人员依靠 PathVQA [7] 和 PMC-VQA [31] 等评估数据集来评估其模型的性能。然而，这些数据集表现出明显的缺点，包括由于转述相同的问题而造成的严重重复。更糟糕的是，同一个问题经常会有相互矛盾的答案（见附录第 3.4 节）。相比之下，教育视频内容提供了一种宝贵的资源：解说员在解说过程中经常提出问题，然后自己给出答案，从而引入了互动元素。例如，解说员会说："你知道我们面对的是哪种器官吗？"然后接着详细说明：“是的，这是一个结肠”。视频中的这种问答形式提供了丰富的有机问答数据集，可以提取并重新用于评估。</p><p>PathVQA: 包含从教科书和数字图书馆中的 4998 个病理图像标题对中提取的 32799 个问题-答案对。问题分为开放式问题和封闭式问题，前者包括 “什么”、“哪里”、“何时”、“谁的”、“如何”、"多少 "等问题，后者包括 “是”/"否 "等问题。我们使用了评估集中的 6761 个样本。</p><p>PMC-VQA: 包含一个由 34823 对图像组成的 VQA 测试集，这些图像涵盖各种模式或病症。该数据集是从 PMC-OA 文章中的图像标题对中整理出来的，采用多选格式。论文从该数据集中检索到 PMC-VQA 子集，其中包括 2318 对组织病理学 VQA 对。</p><p>QUILT-VQA: 首先，视频的文字转录内容会被处理，识别出问号 ("?") 所在的位置。<br>如果问号出现在某个稳定文本块（视频中关联图像的描述文本）45秒的时间范围内，作者将扩展该文本块，确保其包含带有问号的完整句子。这种方法确保了问题和视频中的视觉内容相匹配。在数据预处理和问号映射完成后，作者使用GPT-4直接从文本中提取问答对。<br>具体地，GPT-4的输入是经过处理的稳定文本块以及其中带有问号的句子，表明这些句子包含提问内容。在GPT-4初步提取完问答对后，作者进行了手动验证，以确保每个问答对不仅在医学上具有相关性，还与该文本块的内容紧密对应。</p><p>在提取完成后，作者将问题分为两类：依赖图像的问答对（Image-dependent Q/A pairs）：共1055对，这类问题引用了讲述者对特定图像的描述。基于一般医学知识的问答对（General-knowledge Q/A pairs）：共228对，这类问题与更广泛的医学知识相关，而不仅仅依赖于某一特定图像。</p><p><img src="/2024/09/12/multi_modalv2/quilt3.jpg" alt=""></p><h3 id="evaluation-data-generation-instruction-follow-ing-test-set">Evaluation data generation: Instruction Follow- ing Test Set</h3><p>QUILT-VQA 的重点是评估 QUILT-LLAVA 的医学知识，除此之外，我们还旨在评估该模型在多模态对话中遵循指令的能力。为此，我们构建了一组 326 个问题，其中包括 256 个会话问题和 70 个详细描述问题，所有问题均来自 QUILT-VQA 中从未曾看过的视频中提取的图像-文本对话。为了生成这个评估集，我们采用了与生成 QUILT-INSTRUCT 时相同的基于会话和详细描述的提示。</p><h2 id="experiments">Experiments</h2><p>本节将介绍 QUILT-LLAVA 在组织病理学 VQA 基准测试中与现有 SOTA 多模态模型的性能对比情况。首先，我们使用 GPT-4 对生成结果与真实答案进行了比对。其次，执行开放式和封闭式 VQA 任务。最后，使用视觉提示和不同的训练模型进行消融实验。</p><ul><li><p>使用GPT-4 评估生成结果</p><p>评估的主要维度包括回答的帮助性、相关性、准确性和详细程度。</p><p>评估方法：使用 GPT-4 来对比不同模型的输出：候选模型（QUILT-LLAVA） 和 GPT-4。GPT-4 会根据这些维度（帮助性、相关性、准确性和详细程度）对两个模型的回答进行评分，评分范围为1到10分，分数越高表示整体表现越好。<br>除了分数外，GPT-4 还提供详细的解释，以帮助理解每个模型在生成回答时的表现，便于更好地分析模型的优势和不足。</p></li><li><p>可视化问题解答结果</p><p>这些VQA数据集包括开放式和封闭式问答题对。对于封闭式问题，准确率被用来衡量模型给出的正确答案的比例。与此相反，对于开放式问题，我们侧重于召回率，以评估模型的回答中包含真实tokens的频率。</p></li></ul><h2 id="conclusion-and-limitations">Conclusion and Limitations</h2><p>GPT-4 仍然容易生成不准确的信息，导致 QUILT-LLAVA 产生错误陈述或“幻觉”现象（即模型生成的信息与真实内容不符）。此外，尽管我们明确指示 GPT-4 不要这样做，但 GPT-4 有时还是会只从标题中获取信息，而不是从图像中提取信息。</p><h1>WSI ROI的提取</h1><p>划分为多少patch，如何解决position encoding，是一维的还是二维的？喂给模型的是什么？</p><h1>References:</h1><ul><li><a href="https://github.com/mli/paper-reading">沐神, 论文精读</a></li><li><a href="http://www.myhz0606.com/article/blip_hub">BLIP系列文章小结</a></li></ul>]]></content>
      
      
      <categories>
          
          <category> Foundation Model </category>
          
      </categories>
      
      
        <tags>
            
            <tag> multi-modal </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Llama 3.1</title>
      <link href="/2024/09/04/llama3.1/"/>
      <url>/2024/09/04/llama3.1/</url>
      
        <content type="html"><![CDATA[<h1 id="llama-3.1">Llama 3.1</h1><p><a href="https://arxiv.org/pdf/2407.21783">The Llama 3 Herd ofModels</a></p><h2 id="abstract">Abstract</h2><p>Modern artificial intelligence (AI) systems are powered by foundationmodels. This paper presents a new set of foundation models, called Llama3. It is a herd of language models that natively supportmultilinguality, coding, reasoning, and tool usage. <strong>Our largestmodel is a dense Transformer with 405B parameters and a context windowof up to 128K tokens.</strong> This paper presents an extensiveempirical evaluation of Llama 3. We find that Llama 3 deliverscomparable quality to leading language models such as GPT-4 on aplethora of tasks. We publicly release Llama 3, including pre-trainedand post-trained versions of the 405B parameter language model and ourLlama Guard 3 model for input and output safety. The paper also presentsthe results of experiments in which we integrate image, video, andspeech capabilities into Llama 3 via a compositional approach. Weobserve this approach performs competitively with the state-of-the-arton image, video, and speech recognition tasks. The resulting models arenot yet being broadly released as they are still under development.</p><h2 id="introduction">Introduction</h2><h2 id="pre-training">Pre-Training</h2><h2 id="section"></h2><!-- ![](./multi_modalv1/blip2.jpg) --><h1 id="references">References:</h1><ul><li><a href="https://github.com/mli/paper-reading">沐神,论文精读</a></li><li><a href="https://llama.meta.com/docs/overview">Meta</a></li></ul>]]></content>
      
      
      <categories>
          
          <category> LLM Model </category>
          
      </categories>
      
      
        <tags>
            
            <tag> gpt </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Flash Attention</title>
      <link href="/2024/09/01/flash_atttention/"/>
      <url>/2024/09/01/flash_atttention/</url>
      
        <content type="html"><![CDATA[<h1 id="clip">CLIP</h1><p><a href="https://arxiv.org/pdf/2103.00020">Learning TransferableVisual Models From Natural Language Supervision</a></p><p><img src="./multi_modalv1/clip1.jpg"></p><p>prompt template</p><h1 id="references">References:</h1><ul><li><a href="https://github.com/mli/paper-reading">沐神,论文精读</a></li></ul>]]></content>
      
      
      <categories>
          
          <category> Foundation Model </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Transformer </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Clip, ViLT, ALBEF, VLMO, Blip, CoCa, BeiTV</title>
      <link href="/2024/09/01/multi_modalv1/"/>
      <url>/2024/09/01/multi_modalv1/</url>
      
        <content type="html"><![CDATA[<h1 id="clip">CLIP</h1><p><a href="https://arxiv.org/pdf/2103.00020">Learning TransferableVisual Models From Natural Language Supervision</a></p><h2 id="approach">Approach</h2><p>核心思想：利用自然语言的监督信号来训练一个较好的视觉模型。</p><p>利用自然语言的监督信号去训练一个视觉模型的好处在于：</p><ol type="1"><li><p>以往的视觉模型训练需要对图片进行类别标注，消耗大量的人力资源。结合可以直接获取的文本信息，不需要对图片进行额外标注，数据规模更大，模型的输入输出不再是单一的标签，自由度更高。</p></li><li><p>相较于单一模态特征（比如单一视觉特征），使用多模态的特征，很容易进行zero shot 的迁移学习。</p></li></ol><p>目标函数的选择：如果使用预测目标函数，即根据图片去预测对应的文本，由于一张图片对应的文本描述具有多样性，从而会导致模型训练效率较低。对比之下，如果只考虑图片和文本是否匹配，这种对比目标函数可以将约束放松，提高模型的训练效率。</p><p><img src="/2024/09/01/multi_modalv1/clip1.jpg"></p><ol type="1"><li><p>contrastive pre-trainng: 模型的输入是 <span class="math inline">\(N\)</span> 个配对的图文，图像通过一个 imageencoder，文本通过一个 text encoder，对应得到 <span class="math inline">\(N\)</span> 个文本特征和 <span class="math inline">\(N\)</span>个图像特征。然后通过计算余弦相似度进行对比学习。矩阵对角线都属于正样本，剩余<span class="math inline">\((N^2 - N)\)</span> 个都是负样本。</p></li><li><p>creating dataset classifier from text:考虑到用于预训练的图文中的文本通常是一个句子，因此，在推理的时候会将label 转换为一个句子，即使用一个 prompt template，然后 fed into textencoder。</p></li><li><p>zero-shot prediction: 对于一张新的图片，通过 image encoder得到图片特征。所有感兴趣的标签通过 prompt engineering 后会变成句子， fedinto 预训练好的 textencoder，会得到相应的文本特征。将图像特征和若干个文本特征计算余弦相似度，然后通过softmax得到概率分布，最大概率对应的句子（标签）即为相应的物体。</p></li></ol><p>对于 image encoder， 可以选择 ResNets 或 vision transforemr。对于text encoder，可以选择 transformer。</p><p>伪代码：首先，文本使用 text encoder 进行特征提取，图像使用 imageencoder进行特征提取。然后投射层将不同模态的特征转换为相同纬度的向量，再进行<span class="math inline">\(l_2\)</span> norm的标准化处理得到用于对比的两个特征。通过计算余弦相似度得到 logits，和ground truth 计算交叉熵目标函数得到 loss。对于 clip而言，正样本都在对角线上，所以通过 labels = np.arange(n) 创建 groundtruth。这里分别针对 image 和 text 计算了两个对称的loss，再计算平均。</p><p><img src="/2024/09/01/multi_modalv1/clip2.jpg"></p><h2 id="prompt-engineering-and-ensembling">prompt engineering andensembling</h2><p>Prompt: 提示，文本的引导作用。</p><p>为什么需要 prompt engineering 和 prompt ensembling?</p><ol type="1"><li><p>词具有多义性。一个单词具有多个不同的含义。比如 'remote'该词可以作为 ‘遥控器’，也具有‘遥远的’含义。如果不结合上下文信息，textencoder 很难抽取正确的特征。</p></li><li><p>在预训练时，图像匹配的文本通常是一个句子，但是在推理时，文本输入通常是label 对应的一个单词，此时会出现 distribution gap 的问题。</p></li></ol><p>基于上述问题，作者提出一种 prompt template，即 "A photo of a{label}"。这种方式可以将标签转换为一个句子，避免了 distribution gap的问题，同时 label在句中的位置通常表示是个名词，一定程度上解决了多义性问题。</p><p>此外，也可以将一些先验信息加入 prompt template中，比如食物、动物的数据。</p><p>prompt ensembing: 使用多个 prompt template，然后将结果综合起来。</p><h2 id="模型评价">模型评价</h2><p>优势：适用于图像文本匹配。可以提前对数据库内的图像、文本进行特征抽取。对于新来的文本或者图像，只需要做一个简单的点乘，具有灵活性和高效性。</p><p>缺点：</p><ol type="1"><li>如果推理的数据集相较于预训练的数据集 out ofdistribution，Clip的泛化能力也会很差。</li><li>从给定的类别中进行判别选择，而不是生成新的输出。</li><li>...</li></ol><h1 id="how-to-train-really-large-models-on-many-gpus">How to TrainReally Large Models on Many GPUs?</h1><p><a href="https://lilianweng.github.io/posts/2021-09-25-train-large/">Blog</a></p><h1 id="vilt">ViLT</h1><p><a href="https://arxiv.org/pdf/2102.03334">ViLT: Vision-and-LanguageTransformer Without Convolution or Region Supervision</a></p><p>已有 vision-and-language pre-training (VLP) 工作的不足：</p><ol type="1"><li><p>抽取图像特征的效率较低，需要的时间远远高于模态融合部分。</p></li><li><p>使用已预训练好的模型抽取特征，可能泛化能力较弱，不是 end-to-end形式。</p></li></ol><p><img src="/2024/09/01/multi_modalv1/vilt3.jpg?300x300"></p><p>传统的 VLP 框架和 ViLT：</p><ol type="1"><li><p>使用特征检测的模型，对于给定的图片，通过 CNN backbone进行特征提取，然后基于这些特征使用 roi等抽取属于物体的特征，得到若干个离散物体的特征向量。文本通过 linearembedding得到文本特征。对于得到的图像序列和文本序列，再进行模态融合。使用目标检测抽取的特征的方式效率较低。</p></li><li><p>基于grid 特征检测的模型，对于给定的图片，通过 CNN backbone得到特征图，然后将特征图拉伸得到相应的序列。</p></li><li><p>ViLT，借鉴 ViT 的 patch embedding layer, ViLT 将图像划分为若干patches，然后通过一个 linear 投影层得到 patchembeddings。文本也是通过一个 linear 投影层得到 wordembeddings。最后两个序列都 fed into 一个 transformer。</p></li></ol><p>从时间上看，ViLT计算高效，参数量更少。基于目标检测的模型性能最好，基于grid特征检测的模型性能最差，ViLT处于两者中间。在使用较少参数量的前提下，效果也不相上下。</p><h2 id="taxonomy-of-vision-and-language-models">Taxonomy ofVision-and-Language Models</h2><p>We propose a taxonomy of vision-and-language models based on twopoints: (1) whether the two modalities have an even level ofexpressiveness in terms of dedicated parameters and/or computation; and(2) whether the two modalities interact in a deep network.</p><p><img src="/2024/09/01/multi_modalv1/vilt1.jpg"></p><p>图中，VE 表示如何抽取 visual embedding，TE 表示如何抽取 textembedding，MI 表示如何进行模态融合 (modality interaction)。</p><ol type="a"><li><p>轻量的文本 encoder，昂贵的视觉 encoder，轻量的 modalityinteraction。</p></li><li><p>visual encoder 和 text encoder的特征提取能力一样，计算量上基本等价，modality interaction部分使用简单的 dot-product。代表性模型为 CLIP。</p></li></ol><ol start="3" type="1"><li><p>text encoder 非常轻量，visual encoder使用目标检测模型，计算昂贵，modality interaction也很昂贵。代表性算法为：ViLBERT, UNITER。</p></li><li><p>轻量的 text encoder 和 visual encoder，复杂的 modalityinteraction。代表性算法为 ViLT。</p></li></ol><h2 id="modality-interaction-schema">Modality Interaction Schema</h2><p>模态融合主要包括两类：</p><ol type="1"><li><p>single-stream approaches:将抽取的文本特征序列和图像特征序列进行拼接作为输入。</p></li><li><p>dual-stream approaches:两个模型分别对两个序列进行处理，提取单一模态信息，然后再进行融合。</p></li></ol><p>本文使用 single-stream approach，避免引入更多的参数。</p><h2 id="vision-and-language-transformer">Vision-and-LanguageTransformer</h2><p><img src="/2024/09/01/multi_modalv1/vilt2.jpg"></p><p>对于多模态的 sing -streamapproaches，需要将两个模态的序列进行拼接，因此需要一个 modal-typeembedding 告诉模型该 token 属于哪个模态。此外，每个模态前面都需要加上[CLS] 特殊标记符。然后， patch embedding + position embedding +modal-type embedding 作为 transformer encoder 的输入。</p><p>本文主要使用了两类loss，分别是文本距离和语言完形填空部分，具体而言：</p><ol type="1"><li><p>image text matching loss:判断对于给定的配对图文，哪个是真的图文对，哪个是假的图文对。</p></li><li><p>word patch alignmentloss：计算文本特征输出和图像特征输出之间的距离。</p></li><li><p>masked language modeling loss: 单词重建的 loss。</p></li></ol><h1 id="albef-align-before-fuse">ALBEF, Align before Fuse</h1><p><a href="https://arxiv.org/pdf/2107.07651">Align before Fuse: Visionand Language Representation Learning with Momentum Distillation</a></p><p>https://blog.salesforceairesearch.com/align-before-fuse/https://nancyyanyu.github.io/posts/paper-albef/</p><p>模型设计方面： -在多模态学习中，视觉特征远远大于文本特征，因此需要使用较为强大的视觉模型(bigViT)。 -此外，模态之间的融合也至关重要，因此模态融合模型也需要足够大（bigmodality interaction）。</p><p>loss 方面： - Clip 使用的是 image text contrastive (ITC)loss，效果不错，可以采用。 - ViLT使用的是 Image TextMatching的loss（ITM），word patch alignment (WPA) loss，文本的单词和image 的 patch 之间进行对应关系。但是 WPA loss 计算非常慢，因此不考虑。- Bert中常用的计算 loss 方式是 Mask Language Modeling (MLM)，mask掉某个词然后再去预测这个词。比较常用。</p><p>总之，直观上来说，结合 ITC、MLM 和 ITM 的 loss 应该效果不错。</p><p>结合上面的考虑，ALBEF 使用复杂的 image encoder (12 blocks) 和multimodal encoder (6 blocks) ，相对轻量的 text encoder (6blocks)。并且，考虑使用 image-text contrastive loss (ITC) 对 imageembedding 和 text embedding 进行对齐，最后还使用了 ITM 和 MLM loss。</p><h2 id="introduction">Introduction</h2><p>已有的工作使用同一个 transformer-based multimodal encoder 去同时model visual tokens 和 word tokens。并且对于 visual tokens还是基于目标检测的模型 (region-based image features)。由于使用的 visualencoder 提取器是基于<strong>预训练</strong>的目标检测器，而不是end-to-end 训练得到的，这种方式得到的 visual tokens 和 word tokens并不匹配，从而使得模态融合 encoder 训练困难。(注意，ALBEF 和 ViLT都想丢弃使用目标检测器的 encoder，但是二者出发点不同， ViLT是从提升计算效率角度出发。)</p><p>因此，本文贡献： 1. 提出一种对比学习的 loss，对 image 和 text 在fusing 之前进行对齐。 2. 针对 noisy web data，提出 momentumdistillation，通过生成伪标签达到自训练。noisy web data:从网络上获取的图像-文本是具有噪音的。比如从搜索引擎获取的图文，文本中包含搜索引擎需要的关键词，但是文本并没有对图像进行很好的描述。</p><h2 id="model-architecture">Model Architecture</h2><p><img src="/2024/09/01/multi_modalv1/albef1.jpg"></p><p>image: 对于给定图像，将其划分为若干个 patches，然后 fed into 一个 12层的 vision transformer encoder。</p><p>原始 bert 有 12 层，考虑到 image encoder 要比文本模型大，用于融合的multimodal encoder 也是越复杂越好。这里将 bert model进行拆分，维持原始计算参数量。 text: bert model 的前六层用作 textencoder。</p><p>融合部分: bert 的后六层用作 multimodal encoder。</p><p>除了 ViT 和 BERT 模型外，还有一个 momentum model。该模型也包含了 ViT和 BERT 模型，只不过是将左侧模型的参数进行移动平均得到，用来产生更多的负样本对 以及 momentum distilation。</p><p>目标函数：ITC loss</p><h1 id="vlmo">VLMO</h1><p>使用一个共享的 self-attention 层，然后使用不同的 feed forward层去学习不同的模态特征。</p><h1 id="blip">BLIP</h1><p><a href="https://arxiv.org/pdf/2201.12086">BLIP: BootstrappingLanguage-Image Pre-training for Unified Vision-Language Understandingand Generation</a></p><h2 id="model-architecture-1">Model Architecture</h2><p><img src="/2024/09/01/multi_modalv1/blip1.jpg"></p><p>整体上看，对于图像部分，有一个 <span class="math inline">\(N\)</span>层的 ViT。对于文本部分，分别使用三个 text encoder去计算三个不同的目标函数。在 blip 中，同种颜色代表同样的共享参数，</p><p>对于第一个 text encoder，具有 <span class="math inline">\(N\)</span>层，主要是将文本特征和图像特征进行对比学习，计算 ITC loss来进行分类任务。第二个 image-grounded textencoder。提取得到的图像特征通过 cross attention 进入模型，文本特征通过self-attention 得到，然后进行融合得到多模态的特征，计算 ITM loss 来判断image-text pairs 是否匹配。相较于第一个 text encoder，第二个 encoder只需要学习额外的 cross attention 层。为了能够执行 生成式 任务，blip添加了一个 decoder。由于 decoder 不能看到完整的句子，因此将 causalself-attention 替换掉前面 encoder 中的 bi self-attention，不过cross-attention 和 feed forward 层依旧和前面的共享参数。这里 decoder使用的 language modeling loss (LMloss)，根据前面的文本去预测后面的文本，而不是进行文本的完形填空 (i.e.,MLM loss)。</p><p>不同 text encoder 使用不同的 token，分别是[CLS]，[Encode]，[Decode]。</p><h2 id="capfilt">CapFilt</h2><p><img src="/2024/09/01/multi_modalv1/blip2.jpg"></p><p>对于从网络上获取的图文数据质量比较糟糕，图片对应的文本描述往往不够准确。针对这种情况，blipfinetune 了一个 filter 来筛选图文对，一个 captioner来生成合成的文本。<span class="math inline">\(\{I_w, T_w\}\)</span> 是从web 上获取的 noisy image-text pairs，<span class="math inline">\(\{I_h,T_h\}\)</span> 是人工标注的 image-textpairs，通常认为是高质量的。对于预训练好的 blip 模型，首先基于 <span class="math inline">\(\{I_h, T_h\}\)</span>数据对两个预训练好的 textencoder 进行 finetune 得到 filter model。然后对 noisy data <span class="math inline">\(\{I_w, T_w\}\)</span> 进行筛选。同时，基于 <span class="math inline">\(\{I_h, T_h\}\)</span>数据对预训练好的 decoder 进行finetune，用来生成合成的 caption。由于生成的 caption质量并不确定，因此将 <span class="math inline">\(\{I_w, T_s\}\)</span>再通过 filter 进行筛选。最终得到数据 <span class="math inline">\(D =\{I_w, T_w\} + \{I_w, T_s\} + \{I_h, T_h\}\)</span>。</p><h1 id="coca">Coca</h1><h1 id="beitv">BeiTV</h1><h1 id="references">References:</h1><ul><li><a href="https://github.com/mli/paper-reading">沐神,论文精读</a></li></ul>]]></content>
      
      
      <categories>
          
          <category> Foundation Model </category>
          
      </categories>
      
      
        <tags>
            
            <tag> multi-modal </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Transformer, KV Cache, MHA, MQA, GQA, BERT, ViT, MAE, Swin Transformer</title>
      <link href="/2024/08/26/Transformer/"/>
      <url>/2024/08/26/Transformer/</url>
      
        <content type="html"><![CDATA[<h1 id="attention-is-all-you-need">Attention is all you need</h1><h2 id="comparison-with-rnn-and-cnn">Comparison with RNN and CNN</h2><p>RNN: 对于给定一个序列，从左向右进行计算。对于第<span class="math inline">\(t\)</span>个词，会对应一个隐藏状态向量 <span class="math inline">\(h_t\)</span>。该隐藏状态向量 <span class="math inline">\(h_t\)</span> 是由前一个词的隐藏状态向量 <span class="math inline">\(h_{t-1}\)</span> 和当前位置 <span class="math inline">\(t\)</span>的输入词决定的。因此，历史信息可以通过隐藏状态 <span class="math inline">\(h_{t-1}\)</span> 传送到当下。</p><ul><li><p>优点：可以处理时序信息。</p></li><li><p>缺点：（1）由于是序列计算，无法进行并行计算，计算性能较差。（2）如果时序很长，历史信息可以无法有效传输到后面。虽然可以设置较大的<span class="math inline">\(h_{t}\)</span> 缓解该问题，但是存储 <span class="math inline">\(h_t\)</span> 会提升内存的需求。</p></li></ul><p>CNN:</p><ul><li><p>优点：具有多个输出通道(多个卷积核)，每个输出通道可以识别不同的模式。</p></li><li><p>缺点：对于较长的序列，卷积核只能观察到距离较近的像素点，否则需要进行多层卷积操作。</p></li></ul><h2 id="model-architecture">Model Architecture</h2><p>当前的时序模型主要是encoder-decoder的架构。对于一个序列表示 <span class="math inline">\((x_1, ...,x_n)\)</span>，encoder将该序列映射为一个连续表征 <span class="math inline">\(\mathbf z = (\mathbf z_1,..., \mathbfz_n)\)</span>，其中 <span class="math inline">\(\mathbf z_i \inR^d\)</span>, <span class="math inline">\(d\)</span>为隐藏向量维度。对于encoder输出的 <span class="math inline">\(\mathbfz\)</span>，decoder <strong>依次</strong> 生成输出序列 <span class="math inline">\((y_1, ..., y_m)\)</span>。</p><p>注意，对于encoder而言，可以看到整个输入句子。但是，对于decoder而言，无法观察到序列后面的词，因此词是按照自回归模式一个一个生成的，<strong>过去时刻的输出也作为当前时刻的输入</strong>。</p><p><img src="/2024/08/26/Transformer/architecture.jpg?400x310"></p><h3 id="encoder-and-decoder-stacks">Encoder and Decoder Stacks</h3><p>Encoder: 包含 <span class="math inline">\(N=6\)</span>个layer，每个layer具有两个 sub-layers，其中第一个子层是一个 multi-headself-attention，第二个子层是一个position-wise fully connectedfeed-forward network(MLP)。对于每个子层，使用残差连接+layernorm，即Layernorm(<span class="math inline">\(x\)</span> + sublayer(<span class="math inline">\(x\)</span>))。每层的输出维度统一为<span class="math inline">\(d_{model}=512\)</span>。</p><p>Decoder: <span class="math inline">\(N=6\)</span>个layers。每个layer具有三个sub-layers，并且每个子层都使用残差连接+layernorm。对于第一个子层的self-attention，由于不能获取之后的输入，因此使用maskedMHA。</p><h3 id="attention">Attention</h3><p>Query: 需要查询的内容向量；</p><p>Key: 可以认为是用于被查询的关键信息向量；</p><p>Value: 通过将 query和key进行匹配对比，可以获得不同value的权重，然后基于该权重对value进行加权获得输出向量。</p><p>Scaled dot-product attention: <span class="math display">\[\begin{aligned}\text{Attention}(Q,K,V) = \text{softmax}(\frac{QK^T}{\sqrt{d_k}}V),\end{aligned}\]</span> 其中，query Q，key K 以及value V是等长的，都是 <span class="math inline">\(d_k\)</span>.</p><p>对于encoder，使用<strong>self-attention</strong>，query, key andvalue都是来自input embedding投影得到。</p><p>对于decoder, 使用<strong>masked self-attention</strong> 和<strong>cross-attention</strong>。对于cross-attention，key和value来自encoder的输出，query是来自decoder的下一刻的输入得到。通过计算query和key的相似度，对value进行加权得到输出。</p><h2 id="position-wise-feed-forward-networks"><strong>Position-wise</strong>feed-forward networks</h2><p>对于attention 外的sub-layers, 对于每一个position的输入使用<strong>同一个</strong>MLP进行映射： <span class="math display">\[\begin{aligned}\text{FFN}(x) = \text{max}(0, xW_1 + b_1)W_2 + b_2.\end{aligned}\]</span> 其中, <span class="math inline">\(x\)</span>是一个 <span class="math inline">\(512\)</span> 的向量，inner-layer hasdimensionality <span class="math inline">\(d_{ff}=2048\)</span>，输出也是一个 <span class="math inline">\(512\)</span> 向量。</p><h2 id="为什么需要除以sqrtd_k">为什么需要除以<span class="math inline">\(\sqrt{d_k}\)</span></h2><p><a href="https://mp.weixin.qq.com/s/h-24XRdJDDZDg65LTjXA0w">ref1</a></p><ol type="1"><li>当维度 <span class="math inline">\(d_k\)</span>比较大时，点积的大小会增大，元素的相对距离增大，进行softmax操作时，会推动softmax函数往仅有很小的梯度的方向靠拢，导致softmax函数容易导致梯度消失问题。</li><li>假设Q和K的均值为0，方差为1。它们的矩阵乘积将有均值为0，方差为<span class="math inline">\(d_k\)</span>。因此，<span class="math inline">\(d_k\)</span>的平方根被用于缩放（而非其他数值），因为，Q和K的矩阵乘积的均值本应该为0，方差本应该为1，这样会获得一个更平缓的softmax。</li><li>也可以使用其他缩放方式，只要能做到每层参数的梯度保持在训练敏感的范围内，不要太大，不要太小。那么这个网络就比较好训练。</li></ol><h2 id="mask-self-attention">Mask self-attention</h2><p>为了不看到 <span class="math inline">\(t\)</span>时刻之后的内容，对于点积矩阵的上半部分添加一个较小的数字，比如 <span class="math inline">\(-1e10\)</span>，这样经过softmax函数后对应位置会变成零。</p><h2 id="mha">MHA</h2><p><img src="/2024/08/26/Transformer/mha.jpg?400x300"></p><p>对原始的Q,K，V，先通过一个linear layer映射到低维向量，然后进行scaleddot-productattention操作，得到h个输出向量，再将h个输出向量进行拼接，最后再通过一个linearlayer回到<span class="math inline">\(d_{model}\)</span>维度。</p><p>直接进行dot-product时，没有什么需要学习的参数。而使用MHA时，linearlayer的投影参数 <span class="math inline">\(W^Q, W^K, W^V\)</span>是需要学习的，因此可以学习到不同的模式信息。</p><p>计算公式： <span class="math display">\[\begin{aligned}\text{MultiHead}(Q,K,V) &amp;= \text{concat}(\text{head}_1,\text{head}_2,...,\text{head}_h)W^O,\\\text{head}_i &amp;= \text{Attention}(QW_i^Q, KW^K_i, VW^V_i),\end{aligned}\]</span> 其中, <span class="math inline">\(W_i^Q \in\mathbb{R}^{d_{model} \times d_k}\)</span>, <span class="math inline">\(W_i^K \in \mathbb{R}^{d_{model} \timesd_k}\)</span>, <span class="math inline">\(W_i^V \in\mathbb{R}^{d_{model} \times d_v}\)</span>, <span class="math inline">\(W_i^O \in \mathbb{R}^{hd_v \timesd_{model}}\)</span>. In this paper, they set <span class="math inline">\(h=8\)</span>, <span class="math inline">\(d_k =d_v = d_{model}/h\)</span>.</p><h2 id="kv-cache-原理-mha-mqa-gqa">KV Cache 原理, MHA, MQA, GQA</h2><p><a href="https://mp.weixin.qq.com/s/mKdliGu4WhUx4PHatBpewA">ref1</a></p><p><a href="https://www.linsight.cn/3dc22f96.html">ref2</a></p><h2 id="positional-encoding">Positional Encoding</h2><p>对于rnn而言，当前时刻的输入包含了上一时刻的输出，依次传递序列信息。而attention是考虑所有词之间的关联性，权重与序列信息无关，并没有将序列/位置信息考虑进去。如果将句子的词打乱，语义可能有所不同，但是attention无法捕捉这种情况。在transformer中，通过将position进行encoding记录时序信息，然后和词的embedding相加作为输入。</p><p><img src="/2024/08/26/Transformer/positional.png"> <!-- $$\begin{aligned}PE(pos, 2i) &= sin(pos/10000^{2i/d_{model}}) \\PE(pos, 2i+1) &= cos(pos/10000^{2i/d_{model}}) \\\end{aligned}$$ --></p><p>pos is the index of the word in the sentence. (0-30) <span class="math inline">\(2i\)</span> and <span class="math inline">\(2i+1\)</span> is the index of the column, d_modelis the number of columns, it is a hyper-parameter(120). For eachword(token), we encode it to a vector with dimension d_model accordingto its position.</p><p>Here we use denominator <span class="math inline">\(10000^{2i/d_{model}}\)</span> to make sure thepositional encoding is different for different tokens. The sin and cosare periodic functions, if we don't use the denominator, then thepositional encoding could be same for different tokens.</p><ul><li>If there are two different sentences with the same size, will thepositional encodings be the same?? yes.</li></ul><h2 id="complexity">Complexity</h2><p><img src="/2024/08/26/Transformer/complexity.jpg"></p><h1 id="bert-bidirectional-encoder-representations-from-transformers">BERT:Bidirectional Encoder Representations from Transformers</h1><ul><li>GPT: 单向，使用过去的信息预测未来。</li><li>ELMo: 基于rnn的架构，双向rnn,在用到一些下游任务时，需要对架构进行调整。</li><li>BERT: 相较于gpt, 可以使用左右侧信息，进行双向预测。相较于ELMo,基于transformer架构，结构简单，只需要修改最上层。</li></ul><p>Bert结合了ELMo的双向性和gpt的transformer架构，将预测未来变成<strong>完形填空</strong>。</p><h2 id="framework">Framework</h2><p>Bert主要包括两部分，pre-training andfine-tuning。在pre-training阶段，模型在一个没有进行标注的数据上进行训练，是一个self-supervised per-trainingtask。在fine-tuning阶段，用同一个Bert模型，但是模型首先被初始化为预训练的权重，然后再在有标注的数据上进行微调。每一个下游任务都会创建一个不同的模型进行微调。</p><p><img src="/2024/08/26/Transformer/bert.jpg"></p><p>Model architecture: a multi-layer bidirectional transformerencoder.</p><p><strong>主要包括三个参数</strong> 1. number of layers/ transformerblocks, i.e., <span class="math inline">\(L\)</span>.</p><ol start="2" type="1"><li><p>hidden dimension, i.e., <span class="math inline">\(H\)</span>(<span class="math inline">\(d_{model}\)</span>).</p></li><li><p>the number of attention heads, i.e., <span class="math inline">\(A\)</span>.</p></li></ol><p>两个模型： 1. Bert <span class="math inline">\(_{base}\)</span>:<span class="math inline">\(L=12, H=768, A=12\)</span>, total parametersis <span class="math inline">\(110M\)</span>.</p><ol start="2" type="1"><li>Bert <span class="math inline">\(_{large}\)</span>: <span class="math inline">\(L=24, H=1024, A=16\)</span>, total parameters is<span class="math inline">\(340M\)</span>.</li></ol><p><strong>如何根据超参数设置计算所需要训练的参数量？</strong></p><p>对于transformer架构，输入是字典（句子）的大小，这里假设为<span class="math inline">\(30k\)</span>。通过嵌入层得到输出，输出维度为 <span class="math inline">\(H\)</span> (<span class="math inline">\(d_{model}\)</span>)。输出的embedding会喂给transformer blocks，transformer block中包括两部分，分别是self-attention和 mlp。 对于self-attention，dot-production 没有学习参数，但是对于MHA，会对 Q, K, V分别通过一个 linear layer 映射到低维向量，然后进行scaled dot-product attention 操作，得到 <span class="math inline">\(A\)</span> 个输出向量，再将 <span class="math inline">\(A\)</span> 个输出向量进行拼接，最后再通过一个linear layer 回到 <span class="math inline">\(H\)</span> (<span class="math inline">\(d_{model}\)</span>) 维度。在 MHA中，头的个数乘以低维投影的维度 = <span class="math inline">\(H\)</span>(<span class="math inline">\(d_{model}\)</span>)，因此低维投影部分的参数量为<span class="math inline">\(3 \times H \times H\)</span>。这里乘以 <span class="math inline">\(3\)</span> 的原因是 Q, K, V分别通过一个 linearlayer进行投影操作。同样，对于得到的低维投影向量进行拼接后还会进行一次投影，可学习参数量是<span class="math inline">\(H \times H\)</span>。因此，一个self-attention 层的可学习参数量为 $ 4 H H = 4 H^2$（观察上文中的MHA结构图，可以发现有 4 个linear模块）。接下来是 mlp, mlp具有两个全连接层，第一个全连接层的输入输出是 <span class="math inline">\(H \times 4H\)</span>，第二个全连接层的输入输出是<span class="math inline">\(4H \times H\)</span>，总共为 <span class="math inline">\(8H^2\)</span>。因此，一个transformerblock的可学习参数总共为 <span class="math inline">\(12H^2\)</span>。</p><p>假设模型有 <span class="math inline">\(L\)</span>个blocks，那么该模型的可学习参数总量为<span class="math inline">\(30k \times H + L \times H^2 \times12\)</span>。</p><p>对于Bert <span class="math inline">\(_{base}\)</span>，<span class="math inline">\(L=12, H=768, A=12\)</span>，根据公式计算得到：$30k+ 12 ^2 = 108,514,656 110M $。</p><p><strong>输入输出</strong></p><p>对于transformer而言，输入是一个序列对，编码器和解码器会分别输入一个序列。Bert只有一个编码器，输入是一个序列，可以是一段连续的文字，未必是真正语义上的句子，也可以包含两个句子。</p><p>论文使用 <strong>WordPiece</strong>embeddings。通常来讲，如果根据空格对句子进行划分，每个词为一个token，那么词字典会非常大，输入的嵌入层的token很多，增加可学习参数。WordPiece根据词出现的频率进行划分，如果一个词出现的频率不大，那么将该词切开，看它的一个子序列。如果它的某一个子序列出现的概率较大，那么只保留这个子序列。这种方式可以将一个较长的句子切分成出现频率较高的几个子序列，类似于词根，从而减少词典大小。</p><p>对于一个序列，序列的第一个词token永远是一个特殊的记号 [CLS]，表示classification。这个词的作用是用来表示整个序列层面的信息。虽然该token被放在序列的开头，但是由于Bert使用的是transformer的编码器，依旧可以注意到整个序列中的词，所以可以放在第一个位置。</p><p>由于两个句子被连接到一起作为一个序列进行输入，有两种方式来区分不同的句子。一种是在句子后面添加一个特殊的标记token[SEP]。其次，添加一个可学习的嵌入层来表示每个token所属于的句子id。具体来说，如下图所示，对于一个输入的序列，共有三个嵌入层。首先第一个嵌入层tokenembedding 是对每个词元输出一个向量；第二个是 segmentembedding层，该层的输入是 2，表示该词元属于哪个句子。第三个是<strong><em>可学习的</em></strong> positionembedding层，输入的是每个词元在该序列里的位置信息，从 0开始。最终得到的输出是词元本身的嵌入+所属句子的嵌入+位置嵌入。</p><p><img src="/2024/08/26/Transformer/bert1.jpg"></p><h2 id="pre-training-bert">Pre-training BERT</h2><p>We do not use traditional left-to-right or right-to-left languagemodels to pre-train BERT. Instead, we pre-train BERT using twounsupervised tasks, described in this section.</p><h3 id="task-1-masked-lm">Task #1: Masked LM</h3><p>如果一个词元是由WordPiece生成，那么该词元有<span class="math inline">\(15\%\)</span>的概率被随机替换为掩码[Mask]。对于特殊词元，比如第一个词元[CLS]和中间分割词元[SEP]，就不进行替换。但是这种操作会带来一个问题，在训练的时候会有序列中的词元被替换为特殊token[Mask]，但是在 fine-tune 的时候，输入的序列不会包含 [Mask] 这种token，从而导致两个输入数据的分布不一致。</p><p>为了减少这种情况，预训练时并不总是用实际的 [MASK]标记来替换被掩码的词。训练数据生成器会随机选择 <span class="math inline">\(15\%\)</span> 的词元位置进行预测。如果第 <span class="math inline">\(i\)</span> 个标记被选中，那么该词元 (1) 有 <span class="math inline">\(80\%\)</span> 的概率被替换为 [MASK]；（2）<span class="math inline">\(10\%\)</span>的概率替换为字典中的随机词元；（3）<span class="math inline">\(10\%\)</span> 的概率保持不变。模型会预测这 <span class="math inline">\(15\%\)</span> 的词元，而不是构建整个输入序列。用<span class="math inline">\(T_i\)</span> 表示预测原始标记的<span class="math inline">\(15\%\)</span>词元的交叉熵损失。</p><h3 id="task-2-next-sentence-prediction-nsp">Task #2: Next SentencePrediction (NSP)</h3><p>许多重要的下游任务，如问题解答（QA）和自然语言推断（NLI），都是基于对两个句子之间关系的理解。为了训练一个能理解句子关系的模型，本文预先训练了一个binarizednext sentence predictiontask。该任务可以从任何语料库中生成。具体来说，每个预训练的序列包含两个句子A 和 B，<span class="math inline">\(50\%\)</span> 的情况下, B 是 A后面的实际下一句（标记为 IsNext），<span class="math inline">\(50\%\)</span> 的情况下，B是语料库中的随机句子（标记为 NotNext）。如下图所示。</p><p><img src="/2024/08/26/Transformer/bert3.jpg?400x300"></p><p>其中，‘flight ##less’是一个词，但是该词出现的概率较低，所以WordPiece中被划分为两个词，'##'表示与前面词相连。 如图 1 所示，特殊标记[CLS]对应的输出向量 <span class="math inline">\(C\)</span> 用来进行下一个句子预测。</p><h2 id="fine-tuning-bert">Fine-tuning BERT</h2><p>输入：对于每项任务，只需将任务的输入输出转换为Bert要求格式的输入，然后end-to-end微调所有参数即可。预训练阶段的句子A 和句子 B 类似于：(1) 解析中的句子对；(2) 推断中的假设-前提对；(3)问题解答中的问题-答案对；(4) 当进行文本分类时，句子+<span class="math inline">\(\emptyset\)</span>。</p><p>输出：对于token-level tasks，比如问题解答。将每个词元的embedding fedinto 一个输出层；对于分类任务，[CLS] 表示被fed into一个输出层。然后end-to-end进行调参。</p><h2 id="transformer-位置编码的几种方式">Transformer位置编码的几种方式</h2><p><a href="https://www.kexue.fm/archives/8130">苏剑林：让研究人员绞尽脑汁的Transformer位置编码</a></p><h1 id="vit-iclr-2021">ViT (ICLR 2021)</h1><p>paper: AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGERECOGNITION AT SCALE</p><h2 id="introduction">Introduction</h2><p>Transformer在cv中应用的难点：Transformer中一个self-attention的计算复杂度为<span class="math inline">\(O(n^2)\)</span>。对于2d图像，如果简单地将图像拉成一维序列，每个像素点作为一个词元。对于一个<span class="math inline">\(256 \times256\)</span>的图像，那么self-attention的计算复杂度将会是 <span class="math inline">\((224 \times 224)^2 =50176^2\)</span>。对于较大的图像，序列长度很长，计算复杂度很高。</p><p>在中小型数据集上，ViT的效果较弱，弱于ResNets。主要原因是Transformer不具备归纳偏置(inductive bias)能力。卷积神经网络具有归纳偏置能力，该能力类似于一种先验知识/假设。比如对于卷积神经网络来说，具有两个inductive bias:</p><ol type="1"><li><p>locality。卷积神经网络假设相邻区域具有相邻特征，因此通过滑动窗口进行学习。</p></li><li><p>translation equivariance，平移等价性。<span class="math inline">\(f(g(x)) =g(f(x))\)</span>。在卷积神经网络中，无论是对图片中物体先进行平移再卷积，还是先卷积再平移，只要是对于同一个不变的输入，结果不变。卷积神经网络具备这两个归纳偏置能力，等同于拥有很多先验信息，可以在中小型数据集上表现不错。</p></li></ol><p>通过在大数据集上进行预训练，作者发现，即使transformer不具备归纳偏置能力，依旧具有很好的表现，且可以扩展到下游任务中。</p><p>以往的工作要么是将卷积神经网络和自注意力结合，要么直接用自注意力取代卷积神经网络。但是没有工作直接只用transformer到图像领域。因此，本文直接使用标准的transformer结构，只是对图片进行预处理，划分成块。</p><h2 id="model-architecture-1">Model Architecture</h2><p><img src="/2024/08/26/Transformer/vit.jpg"></p><p>如 Figure 1所示，首先将原始图片划分为 <span class="math inline">\(3\times 3 = 9\)</span> 个patch(token)，然后按照顺序组成一个序列，通过一个 linear layer得到patchembedding。为了表示每个 patch在原始图片中的位置信息，类似于transformer，添加了一个 positionembedding。然后通过一个标准的transformer encoder得到输出。对于分类任务，仿照 BERT，在序列的开头添加一个特殊标记[CLS]，<strong>位置为 0</strong>。因为使用的 transformer 架构，该 token可以注意到序列中其他 patch 的信息，所以可以根据该 token的输出进行判断得到有效信息。</p><p>具体来说，对于大小为 <span class="math inline">\(224 \times 224\times 3\)</span> 的图片，将其划分为 $16 $ 大小的 patch，那么可以得到<span class="math inline">\(224^2/16^2 = 196\)</span> 个 patch，每个patch 的大小为 <span class="math inline">\(16 \times 16 \times 3 =768\)</span>。<span class="math inline">\(196\)</span> 个维度为 <span class="math inline">\(768\)</span> 的 token 通过 linear projection 得到<span class="math inline">\(196\)</span> 个 patchembeddings。再加上特殊字符 [CLS] 对应的embedding，共有 <span class="math inline">\(197\)</span> 个 embeddings。通过将 patchembeddings 和 position embeddings 相加，得到的 transformer encoder输入大小为 <span class="math inline">\(197 \times 768\)</span>。</p><p>对于 transformer block，假设有 <span class="math inline">\(12\)</span> 个 head。通过将输入进行投影得到的<span class="math inline">\(Q\)</span>, <span class="math inline">\(K\)</span>, <span class="math inline">\(V\)</span>的大小为 <span class="math inline">\(197 \times 64\)</span> (<span class="math inline">\(768/12 = 64\)</span>)。最后将 <span class="math inline">\(12\)</span> 个头的输出向量进行拼接得到 <span class="math inline">\(197 \times 768\)</span>。过一层 layernorm 后将输出fed into MLP layer。在 MLP 这层，会将维度放大 <span class="math inline">\(4\)</span> 倍，然后缩小投射回去。即 <span class="math inline">\(197 \times 3012\)</span> --&gt; <span class="math inline">\(197 \times 768\)</span>。</p><p>具体计算公式如下： <img src="/2024/08/26/Transformer/vit1.jpg"></p><p><strong><em>How can I use ViT with a different resolution?</em></strong></p><p>当输入更高分辨率的图像时 (e.g., <span class="math inline">\(512\times 512\)</span>)，论文会保持 patch size不变，这将导致分割后得到的序列长度增加。尽管理论上 ViT可以处理任意长度的序列，但是，位置编码会不同，预训练时的位置编码就没有用。针对这种情况，作者对预训练好的位置编码进行2D 插值来扩充序列。</p><p><a href="https://github.com/huggingface/transformers/issues/12167">githubref</a></p><p><strong><em>为什么不直接对 transformer 的 <span class="math inline">\(n\)</span> 个输出embeddings 进行 global averagepooling，然后基于得到的特征进行分类预测？</em></strong></p><p>作者通过实验表示， global average pooling 方式和加 class token [CLS]两种方式都可以。</p><h2 id="position-embedding">Position Embedding</h2><p>在本文中，作者对比了四种对2D图像使用 position embedding进行编码的方式：</p><ol type="1"><li><p>不提供位置信息；</p></li><li><p>一维positional embedding：将输入的patch 按照栅格顺序（从左到右按行读取） 进行编码。</p></li><li><p>二维positional embedding：将输入视为二维的 patch网格。在这种情况下，需要学习两组嵌入，每组嵌入的大小为 D/2 (<span class="math inline">\(D = d_{model}\)</span>)，分别针对 X 轴和 Y轴。然后，根据输入路径上的坐标，我们将 X 嵌入和 Y 嵌入拼接起来，得到该patch 的最终位置嵌入 <span class="math inline">\(D\)</span>维。</p></li><li><p>Relative positional embeddings：详见 <a href="https://www.kexue.fm/archives/8130">苏剑林：让研究人员绞尽脑汁的Transformer位置编码</a>。</p></li></ol><h2 id="模型参数量以及显存间的计算">模型参数量以及显存间的计算</h2><p><a href="https://mp.weixin.qq.com/s/OfgEoh5UXSqNBTMuTSC12w">现在 LLM的大小为什都设计成 6/7B、13B 和 130B 几个档次？</a></p><p><a href="https://mp.weixin.qq.com/s/bhPo3FO_3AFQpgff-wmDoQ">如何根据模型参数量估计大模型微调和推理所需显存?</a></p><h1 id="mae">MAE</h1><p><a href="https://arxiv.org/pdf/2111.06377">Masked Autoencoders AreScalable Vision Learners</a></p><p>随机遮住大量的 patches， 然后在 pixel space 重构这些缺失的patches，得到原始完整的图片。使用一个非对称的编码器和解码器的架构，非对称的原因是指：编码器只看到未被遮挡的patches，这样会提高计算效率，降低内存需要。</p><p><img src="/2024/08/26/Transformer/mae.jpg?300x300"></p><p><strong>Masking:</strong> 将图像划分为一个个不重叠的patches，然后进行随机均匀采样少量patches，并屏蔽（即去除）剩余的部分。</p><p><strong>Encoder:</strong> Our encoder is a ViT but applied only onvisible, unmasked patches. Just as in a standard ViT, our encoder embedspatches by a linear projection with added positional embeddings, andthen processes the resulting set via a series of Transformer blocks.However, our encoder only operates on a small subset (e.g., 25%) of thefull set. Masked patches are removed; no mask tokens are used. Thisallows us to train very large encoders with only a frac- tion of computeand memory</p><p><strong>Decoder:</strong> The input to the MAE decoder is the fullset of tokens consisting of (i) encoded visible patches, and (ii) masktokens. See Figure 1. Each mask token is a shared, learned vector thatindicates the presence of a missing patch to be predicted(每一个被盖住的块都被表示为同一个可学习的向量). We add<strong>positional embeddings</strong> to all tokens in this full set;without this, mask tokens would have no information about their locationin the image. The decoder has another series of Transformer blocks. TheMAE decoder is only used during pre-training to perform the imagereconstruction task (only the encoder is used to produce imagerepresentations for recognition)</p><h1 id="swin-transformer">Swin Transformer</h1><p><a href="https://arxiv.org/pdf/2103.14030">Swin Transformer:Hierarchical Vision Transformer using Shifted Windows</a></p><h2 id="introduction-1">Introduction</h2><p>ViT始终都是在全图上计算自注意力，计算复杂度是图片大小（像素数量）的平方级。Swintransformer则是在小窗口内计算自注意力，只要窗口大小固定，自注意力的计算复杂度固定。SwinTransformer的复杂度始终与图像的像素数量（而非图像的边长）成线性关系。</p><p>具体来说，假设图片大小为 <span class="math inline">\(N \timesN\)</span>，共有 <span class="math inline">\(L = N^2\)</span>个像素。标准的 transformer 的计算复杂度为 <span class="math inline">\((N^2)^2 = L^2 = O(L^2)\)</span>。假设 swintransformer 的每个窗口固定大小为 <span class="math inline">\(M \timesM\)</span>，单个窗口的计算复杂度为 <span class="math inline">\((M^2)^2\)</span>。共有 <span class="math inline">\((N/M)^2\)</span> 个窗口，那么总计算复杂度为 <span class="math inline">\((N/M)^2 * (M^2)^2 = N^2 M^2 = L \times M^2 =O(L)\)</span>。因此，swin transformer 可以将复杂度降低到 linearcomputation complexity （相较于图片像素数量）。</p><p><img src="/2024/08/26/Transformer/swin1.jpg?300x300"></p><p><img src="/2024/08/26/Transformer/swin2.jpg?300x300"></p><p>捕捉多尺度的特征。</p><h1 id="references">References:</h1><ul><li><p><a href="https://github.com/mli/paper-reading">沐神,论文精读</a></p></li><li><p><a href="http://nlp.seas.harvard.edu/2018/04/03/attention.html">TheAnnotated Transformer</a></p></li><li><p><a href="https://arxiv.org/pdf/1706.03762">Attention is all youneed</a></p></li><li><p><a href="https://jalammar.github.io/illustrated-transformer/" class="uri">https://jalammar.github.io/illustrated-transformer/</a></p></li><li><p><a href="https://arxiv.org/pdf/2010.11929">ViT</a></p></li><li><p><a href="https://arxiv.org/pdf/2103.14030">Swin Transformer:Hierarchical Vision Transformer using Shifted Windows</a></p></li></ul>]]></content>
      
      
      <categories>
          
          <category> Foundation Model </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Transformer </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>行测</title>
      <link href="/2024/08/26/%E8%A1%8C%E6%B5%8B-%E8%A8%80%E8%AF%AD%E7%90%86%E8%A7%A3/"/>
      <url>/2024/08/26/%E8%A1%8C%E6%B5%8B-%E8%A8%80%E8%AF%AD%E7%90%86%E8%A7%A3/</url>
      
        <content type="html"><![CDATA[<h1 id="言语理解">言语理解</h1><p>题型：</p><ol type="1"><li>中心理解，包括概括中心和文段中心有关。</li><li>细节判断。</li><li>语句连贯，语句排序、语句填空、接语选择</li><li>逻辑填空</li></ol><h2 id="中心理解">中心理解</h2><ol type="1"><li>提问方式：</li></ol><ul><li>这段文字主要/旨在/重在/意在...</li><li>这段文字想要说明/论述/强调的是...</li><li>这段文字的主旨/主题/观点是... ...</li></ul><ol start="2" type="1"><li><p>解决思路：<strong><em>寻找中心句 (谁做了什么？)注意表述扩大范围、无中生有的不要选择。</em></strong></p></li><li><p>解决技巧：关键词；关键句。</p></li></ol><h3 id="关键词">关键词</h3><p>关联词帮助快速理顺文段脉络、分清句子主次。<strong><em>谁做了什么</em></strong></p><ol type="1"><li>转折关联词 -<strong><em>转折之后是重点，转折之前不能选。</em></strong></li></ol><p>虽然...,但是/然而/事实上/不过/其实/可是/却...;</p><ol start="2" type="1"><li>关键词之主题词</li></ol><ul><li><p>主题词是文段的切入点和叙述的核心话题，抓住主题词就能快速、准确找到文段中心，对应正确选项。</p></li><li><p>如何找主题词：所围绕的词；高频出现的词。</p></li><li><p>偏离主题的不能选择。</p></li></ul><ol start="3" type="1"><li>因果关联词 - <strong><em>结果是重点</em></strong></li></ol><p>标志词包括： - 因果：因为...,所以...。因此，因而，故而，于是等等。 -指代词：对此、这，这种、在这个意义上等。 -同意替换：换而言之、换句话说。</p><p>以上后面的关联词表示结果关系，结果是重点！</p><ol start="4" type="1"><li>递进关联词 - <strong><em>递进之后内容是重点</em></strong></li></ol><p>不但...,而且...。更，特别是，尤其，致命，核心，突出，根本的是等常见词。</p><ol start="5" type="1"><li>并列关联词 - <strong><em>识别并列，归纳概括总结</em></strong></li></ol><ul><li><p>有标志词：标点符号（；，、）；和、与、也、同时、另外、此外、加上等。一方面、另一方面等等。</p></li><li><p>无标志词：句式相同，排比句，</p></li></ul><h2 id="句子成分">句子成分</h2><ol type="1"><li><p>背景铺垫，交代事件事物发生的背景，通过背景引出话题。<strong><em>围绕背景的选项不用选择</em></strong></p></li><li><p>概念类，对文段中的概念进行解释说明。</p></li><li><p>举例类，通过列举具体实例来论证自己的观点。常见表述：（1）标志词：比如/譬如/例如/拿什么来说；（2）非标志词：人名、专业术语、数据等。<strong><em>举例子解释的是观点，围绕举例子的选项不选。</em></strong></p></li><li><p>原因解释。对某个结果或者某个现象进行具体的解释。<strong><em>所解释的结果/对象是重点。原因解释内的选项，一般不能选择。</em></strong></p></li><li><p>问题类，指出某些不足或矛盾，是需要解决的对象。</p></li><li><p>对策类。描述中有问题，如果有对策选项，那么选对策，否则选问题。</p></li></ol><h2 id="细节判断题">细节判断题</h2><ol type="1"><li>看清提问，选是还是选非；</li><li>勾画文段句子主体；</li><li>勾画选项主体；</li><li>文选对比。</li></ol><p>有可能出现： 1. 无中生有。某个话题未提及。 2.偷换概念。包括扩大、缩小、替换、混搭。 3.偷换/强加逻辑。将某一逻辑关系偷换为另一逻辑关系。 4.偷换时态。时态词。过去时、完成时、将来时。 5.偷换语气、程度。可能/必须，一定/或许。</p><p>细节判断，如果出现中心句，属于中心理解式细节判断。</p><h2 id="语句填空">语句填空</h2><p>要保证文段核心话题的一致性，内容的一致性。 1. 横线开头，总领全文。 2.横线在中间，承上启下。 3. 横线在结尾，总结全文。</p><h2 id="接语选择">接语选择</h2><p>围绕尾句话题展开的为正确选项。</p><p>最不可能讲述的是。。， 选择文中已经讲述过的。</p><ol type="1"><li>尾句有关联标志词的，比如转折、递进、结果。直接围绕尾句话题展开即可。</li><li>尾句无关联标志词的，如果为对策性表述，则围绕对策展开；如果是问题性表述，则围绕问题展开。</li></ol><h2 id="语句排序">语句排序</h2><ol type="1"><li>先观察选项布局。</li><li>确定、对比首句：背景引入、下定义、话题概念范围（概念范围大者比小者更适合做首句）</li><li>尾句对比。</li><li>代入验证。</li></ol><h2 id="逻辑填空">逻辑填空</h2><ol type="1"><li>尊重语义，尊重文段。</li><li>关注搭配主体。比如，成绩不断<strong>逼近</strong>人体能力的极限。</li></ol><h1 id="资料分析">资料分析</h1><h2 id="速算技巧">速算技巧</h2><h3 id="估算的原则">估算的原则</h3><p>保留三位有效数字的误差上限为 <span class="math inline">\(5‰\)</span>，保留两位有效数字的误差上限为 <span class="math inline">\(5\%\)</span>。</p><p>判断原则： 1. 首位均不同，保留两位即可；首位有相同的，保留三位。 2.看选项。选项之间误差在 <span class="math inline">\(10\%\)</span>以上，保留两位即可；选项之间误差在<span class="math inline">\(10\%\)</span>以内，保留三位。 3.看选项。如果选项首位都不同，只关注首位是什么即可。</p><h3 id="截位直除法">截位直除法</h3><p>一步除法：<strong>只估算分母</strong>即可。</p><p>多步连除：分子分母同时截位。比如 <span class="math inline">\((a/b)/(c/d)\)</span>.</p><h3 id="估算法">估算法</h3><p>乘法估算：乘法一大一小，按照比例增加或减少。</p><p>特殊分数：<span class="math inline">\(1/6=16.7\%, 1/7=14.3\%,1/8=12.5\%, 1/9=11.1\%\)</span>.</p><h1 id="判断推理">判断推理</h1><h2 id="图形推理">图形推理</h2><p>图像推理的考点：位置规律，样式规律，属性规律，数量规律，特殊规律，空间重构，立体图形。</p><h3 id="位置规律">位置规律</h3><p><strong>元素组成相同，优先考虑位置规律。</strong></p><p>位置主要有两种： 1.平移。方向（直线（上下、左右、斜对角线）、绕圈（顺逆时针））。常见步数（恒定、等差递增）。2.旋转（顺时针、逆时针，常见角度：45，90，180）、翻转（左右翻转、上下翻转）。</p><p>上下、左右翻转属于转动。</p><h3 id="样式规律">样式规律</h3><p><strong>元素组成相似时，比如相同线条重复出现。</strong></p><ol type="1"><li>加减同异</li></ol><ul><li>相加、相减；</li><li>去同存异；（重点，主要是线的变化，“消消乐”）</li><li>去异求同；</li></ul><p>先横着看，没有规律的话再竖着看，再从下往上看或者米字形/s型开。</p><ol start="2" type="1"><li>黑白运算</li></ol><ul><li>特征：图形轮廓和分割区域相同，内部的颜色不同</li><li>方法：相同位置运算</li></ul><p>同颜色方块数量相同，优先考虑平移；同颜色方块数量不同，优先考虑黑白运算。比如黑块+白块=白块，黑块+黑块=黑块等类似定义运算。</p><h3 id="属性规律">属性规律</h3><p><strong>元素组成不相同、不相似时，优先考虑属性规律。先看是否具有对称性，如果没有看曲直性、开闭性。</strong></p><ol type="1"><li>对称性。（对称轴方向（依次旋转）和数量），轴对称，中心对称。</li></ol><p>中心对称：把一个图形绕着某个点旋转180度，如果图形重合，那么该图形叫做中心对称图形，旋转点叫做对称中心。</p><p>有可能需要注意对称轴和图形有没有重合，是否自带对称轴。</p><p>如果有一个不是对称图形，那么该题不考察对称性。</p><ol start="2" type="1"><li>曲直性。</li></ol><p>全部由曲线构成、全部由直线构成、全部由直线+曲线构成，外直内曲、外曲内直。</p><ol start="3" type="1"><li>开闭性。</li></ol><p>图形全部封闭、全部开放、半开放半封闭</p><p>如果完整的图形留了小开口，可以考虑开闭性。</p><p>线面连接：几个面由线进行连接。</p><h3 id="数量规律">数量规律</h3><p>元素组成不相同、不相似时，优先考虑属性规律。如果数量规律明显，那么考虑。</p><p>考点包括：点、线、面、素、角。</p><ol type="1"><li>点：线与线的交点，切点也是交点。直线与直线、曲线与曲线、直线与曲线的交点。图形特征：（1）线条交叉明显；（2）一团线比较乱；（3）相切较多时。</li><li>线：</li></ol><ul><li>直线数特征图看直线数量：多边形、单一直线（不考虑射线 ）。</li><li>曲线数特征图看曲线：全曲线图、圆、弧（一条平滑的曲线、没有折点）。</li><li>直线和曲线数量的差。第一条直线和最后一条直线之间的垂直或者平行或者相对或者相向关系。</li><li>数笔画数目，是否一笔画（图形由一笔画成，线条不能重复来回画），可以观察线条之间是否连通。奇点数为0或者2时为一笔画。奇点：由一个点发射出奇数条线。<strong>端点属于奇点，奇点数一定是偶数个。笔画数=奇点数/2；非联通图，分开数。</strong>常见的笔画特征图及其变形：五角星、‘日’及其变形、‘田’及其变形、圆和相切/相交。</li></ul><ol start="3" type="1"><li>面：</li><li>素：</li><li>角：</li></ol><h2 id="类比推理">类比推理</h2><h2 id="定义判断">定义判断</h2><h2 id="逻辑推理">逻辑推理</h2><h1 id="数量关系">数量关系</h1><h2 id="代入排除法">代入排除法</h2><p>可以用于：固定题型如年龄问题、余数问题、多位数、不定方程以及不会的题目。</p><h2 id="数字特性">数字特性</h2><ol type="1"><li>奇偶特性。</li></ol><p>奇数+-奇数=偶数，偶数+-偶数=偶数，奇数+-偶数=奇数。<strong>和差同性（无论是差还是和，得到的数字性质一样）</strong></p><p>奇数 <span class="math inline">\(\times\)</span> 奇数=奇数，偶数<span class="math inline">\(\times\)</span> 偶数=偶数，奇数 <span class="math inline">\(\times\)</span> 偶数=偶数。</p><p>适用于 知差求和，知和求差，2倍类，平均分，不定方程。</p><p>例题：y =8x-15，那么y一定是奇数。直接看选项。可以进一步代入选项，x一定是正整数。</p><p>例题：x+y=50，那么x-y一定是偶数。</p><ol start="2" type="1"><li>整除特性。</li></ol><p>一个数能被 2（5）整除，末一位能被 2（5）整除。</p><p>一个数能被 4 （25）整除，末两位能被 4（25）整除。</p><p>一个数能被 8（125）整除，末三位能被 8（125）整除。</p><p>一个数能被 3（9）整除，其各位数字之和能被 3（9）整除。</p><ol start="3" type="1"><li>倍数特性。</li></ol><p>普通倍数，因子倍数，比例倍数。适用于倍数、百分数、分数、比例等题型。</p><p>因子倍数： <span class="math inline">\(x = a \times b \timesc\)</span>.</p><p>比例倍数：<span class="math inline">\(a:b = m:n\)</span>(m和n互质)，那么 <span class="math inline">\(a\)</span> 是 <span class="math inline">\(m\)</span> 的倍数， <span class="math inline">\(b\)</span> 是 <span class="math inline">\(n\)</span> 的倍数， <span class="math inline">\(a-b\)</span> 是 <span class="math inline">\(m-n\)</span> 的倍数， <span class="math inline">\(a+b\)</span> 是 <span class="math inline">\(m+n\)</span> 的倍数。<span class="math inline">\(a/(a+b) = m/(m+n)\)</span>.</p><h2 id="方程法">方程法</h2><p>原则：1. 优先设所求量； 2. 设小不设大； 3. 设中间变量。</p><p>对于不定方程组，具有无数组解。但是要根据具体问题考虑可行解。可以利用奇偶特性、因子倍数、尾数法等。</p><p>例题： <span class="math inline">\(3x+10y=41\)</span>，那么可行解包括$x = 7, y = 2$。</p><p><strong>先看尾数，其次看倍数，再看奇偶特性，最后考虑代入。</strong></p><h2 id="工程问题">工程问题</h2><p>工程总量 = 效率 x 时间</p><h2 id="行程问题">行程问题</h2><h2 id="排列组合">排列组合</h2><h2 id="概率问题">概率问题</h2><h2 id="经济利润">经济利润</h2><h1 id="references">References:</h1><ul><li><p>B站 老闻言语基础精讲班</p></li><li><p>李梦娇 常识判断</p></li><li><p>刘文超 资料分析/判断推理/数量关系</p></li></ul>]]></content>
      
      
      <categories>
          
          <category> 行测 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 行测 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Generative Models</title>
      <link href="/2023/11/29/Generative-Models/"/>
      <url>/2023/11/29/Generative-Models/</url>
      
        <content type="html"><![CDATA[<h1 id="generative-models">Generative Models</h1><h2 id="autoregressive-models-gan-flow-based-models-vae">AutoregressiveModels, GAN, Flow-based Models, VAE</h2><p>GAN: refer to <a href="https://lilianweng.github.io/lil-log/2017/08/20/from-GAN-to-WGAN.html">FromGAN to WGAN</a></p><p>VAE: refer to <a href="https://lilianweng.github.io/lil-log/2018/08/12/from-autoencoder-to-beta-vae.html">FromAutoencoder to Beta-VAE</a></p><p>Flow-based Models: refer to <a href="https://lilianweng.github.io/lil-log/2018/10/13/flow-based-deep-generative-models.html">Flow-basedDeep Generative Models</a></p><p>Autoregressive Models: refer to <a href="https://lrjconan.github.io/UBC-EECE571F-DL-Structures/">AutoregressiveModels</a></p><p>Reference: <a href="https://deepgenerativemodels.github.io/syllabus.html">CS236 - Fall2023 Deep Generative Models</a></p><p>Reference: <a href="https://lrjconan.github.io/UBC-EECE571F-DL-Structures/">EECE 571F(2023 Winter Term 1): Deep Learning with Structures</a></p><h2 id="energy-based-models-ebms">Energy-based Models (EBMs)</h2><h3 id="parameterizing-probability-distributions">Parameterizingprobability distributions</h3><p>In generating models, we want to learn a probability distribution<span class="math inline">\(p_{\theta}(x)\)</span>, which closelymatches the true data distribution <span class="math inline">\(p_{data}(x)\)</span>. The probability shouldsatisfy the following two conditions:</p><ul><li><p>non-negative: <span class="math inline">\(p_{\theta}(x) \geq0\)</span>.</p></li><li><p>sum to one: <span class="math inline">\(\int p_{\theta}(x)dx =1\)</span> or <span class="math inline">\(\sum_{x}p(x) =1\)</span>.</p></li></ul><p>It's not hard to choose a non-negative function, for example, givenany function <span class="math inline">\(f_{\theta}(x)\)</span>, we canchoose <span class="math inline">\(g_{\theta}(x) = f_{\theta}(x)^2,g_{\theta} = \exp(f_{\theta}(x)), g_{\theta}(x) =|f_{\theta}(x)|\)</span>, etc. However, <span class="math inline">\(g_{\theta}(x)\)</span> might not sum to one. Thesolution is to normalize <span class="math inline">\(g_{\theta}(x)\)</span> by dividing the sum of<span class="math inline">\(g_{\theta}(x)\)</span> over all possible<span class="math inline">\(x\)</span>. <span class="math display">\[p_{\theta}(x) = \frac{g_{\theta}(x)}{\sum_{x}g_{\theta}(x)} =\frac{g_{\theta}(x)}{\int g_{\theta}(x) \text{d}x} =\frac{g_{\theta}(x)}{Z(\theta)},\]</span> where <span class="math inline">\(Z(\theta)\)</span> is calledthe Partition function / Normalization constant.</p><p>Example:</p><ul><li><p>Gaussian: <span class="math inline">\(g_{(\mu, \sigma)}(x) =e^{-\frac{(x-\mu)^2}{2 \sigma^2}}\)</span>, volume is <span class="math inline">\(Z(\mu, \sigma) = \int e^{-\frac{(x-\mu)^2}{2\sigma^2}} \text{d}x = \sqrt{2 \pi \sigma^2}\)</span>.</p></li><li><p>Exponential: <span class="math inline">\(g_{\lambda}(x) =e^{-\lambda x}\)</span>, volume is <span class="math inline">\(Z(\lambda) = \int_0^{\infty} e^{-\lambda x}\text{d}x = 1/\lambda\)</span>.</p></li><li><p>Exponential family: <span class="math inline">\(g_{\theta}(x) =h(x) e^{\theta^T T(x)}\)</span>, volume is <span class="math inline">\(Z(\theta) = \int h(x) e^{\theta^T T(x)}\text{d}x\)</span>.</p></li><li><p>Beta, Poisson, Gamma, Dirichlet, etc.</p></li></ul><p>Generally, we can choose <span class="math inline">\(g_{\theta}(x)\)</span> so that <span class="math inline">\(Z(\theta)\)</span> is analytically. But how aboutusing the models that <span class="math inline">\(Z(\theta)\)</span> isnot easy to compute analytically?</p><h3 id="energy-based-models">Energy-based Models</h3><p>EBMs has the following form: <span class="math display">\[p_{\theta}(x) = \frac{1}{\int \exp(f_{\theta}(x))\text{d}x}e^{f_{\theta}(x)} = \frac{1}{Z(\theta)} e^{f_{\theta}(x)}.\]</span></p><p>Why do we choose <span class="math inline">\(f_{\theta}(x)\)</span>as the form of <span class="math inline">\(e^{f_{\theta}(x)}\)</span>?</p><ul><li>We want to capture large variations in probability. We usually touse log-probability.</li><li>Exponential families. Many distributions can be written in thisform.</li><li>Some physical meaning. <span class="math inline">\(-f_{\theta}(x)\)</span> is called the energy.</li></ul><p>Pros:</p><ul><li>We can use any function <span class="math inline">\(f_{\theta}(x)\)</span> to parameterize theprobability distribution.</li><li>Stable training.</li><li>Relatively high sample quality.</li></ul><p>Cons:</p><ul><li>Sampling from <span class="math inline">\(p_{\theta}(x)\)</span> ishard.</li><li>Evaluating and optimizing likelihood is <span class="math inline">\(p_{\theta}(x)\)</span> is hard.</li><li>Curse of dimensionality. Computing <span class="math inline">\(Z(\theta)\)</span> numerically scalesexponentially with the dimensionality of <span class="math inline">\(x\)</span>.</li></ul><h4 id="ebms-with-discrete-observable-variables-and-discrete-latent-variables-restricted-boltzmann-machinerbm">EBMswith Discrete Observable Variables and Discrete Latent Variables:Restricted Boltzmann machine(RBM)</h4><p>Suppose we have binary visible units <span class="math inline">\(x\)</span>, binary hidden units(latent variables)<span class="math inline">\(h\)</span>, the energy function is: <span class="math display">\[E(x, h) = - a^T x - b^T h - x^T W h\]</span> where <span class="math inline">\(a, b, W\)</span> areparameters.</p><p>The probability distribution is: <span class="math display">\[p(x, h) = \frac{1}{Z} e^{-E(x, h)} = \frac{1}{Z} e^{a^T x + b^T h + x^TW h}\]</span> where <span class="math inline">\(Z = \sum_{x, h} e^{-E(x,h)}\)</span>.</p><p><img src="/2023/11/29/Generative-Models/bipartite_model.jpg"> Whyrestricted? - Only one layer of hidden units. - No connections betweenhidden units.</p><p>Bipartite graph: conditional independence <span class="math display">\[\begin{aligned}p(x|h) &amp;= \prod_{i=1}^D p(x_i|h) \\p(h|x) &amp;= \prod_{j=1}^H p(h_j|x)\end{aligned}\]</span></p><p>Formally, we have <span class="math display">\[\begin{aligned}p(x|h = \tilde{h}) \propto \exp(- E_{\theta}(x, h = \tilde{h})) \propto\exp(-\tilde{a}^Tx) = \prod_{i} \exp(-\tilde{a}_i x_i)\end{aligned}\]</span></p><h4 id="inference-gibbs-sampling">Inference: Gibbs Sampling</h4><p>In inference, we want to compute the maximum a posterior(MAP) <span class="math inline">\(p(h|x)\)</span> and computing the marginals <span class="math inline">\(p(x)\)</span>.</p><ul><li>Due to the conditional independence, we can compute <span class="math inline">\(p(h|x)\)</span> in parallel.</li><li>But, the marginal <span class="math inline">\(p(x)\)</span> isintractable. We need to use Markov Chain Monte Carlo(MCMC) to samplefrom <span class="math inline">\(p(x)\)</span>.</li></ul><p>Gibbs sampling is a special case of MCMC. It can draw samples from<span class="math inline">\(p(x_1, x_2,...,x_n)\)</span> by iterativelysampling from the conditional distributions <span class="math inline">\(p(x_i|x_1, x_2,...,x_{i-1},x_{i+1},...,x_n)\)</span>.</p><p>In RBM, we do not iterative over individual variables. Instead, we doblock-Gibbs sampling, i.e., sampling a block of variables conditioned onthe other block.</p><blockquote><p>Given initial sample <span class="math inline">\((x^{(0)},h^{(0)})\)</span>,</p><p>for <span class="math inline">\(t = 1, 2, ..., T\)</span>:</p><p><span class="math display">\[\begin{aligned}h^{(t)} &amp;\sim p(h|x = x^{(t-1)}), \\x^{(t)} &amp;\sim p(x|h = h^{(t)}),\end{aligned}\]</span></p><p>Return <span class="math inline">\((x^{(T)}, h^{(T)})\)</span>.</p></blockquote><p>For both <span class="math inline">\(p(h|x)\)</span> and <span class="math inline">\(p(x|h)\)</span>, we can sampling in parallel.</p><p>Remark: <strong>the Gibbs sampler can generate random variables froma (marginal) distribution indirectly.</strong> After sampling manyiterations, <span class="math inline">\((x^{(T)}, h^{(T)})\)</span>follows the distribution <span class="math inline">\(p(x, h)\)</span>,<span class="math inline">\(x^{(T)}\)</span> follows the marginaldistribution <span class="math inline">\(p(x)\)</span>, and <span class="math inline">\(h^{(T)}\)</span> follows the marginal distribution<span class="math inline">\(p(h)\)</span>. For more detials:</p><p><a href="https://uh.edu/~cmurray/courses/econ_7395/Explaining%20the%20Gibbs%20Sampler.pdf">Explainingthe Gibbs sampler</a></p><p><a href="https://edisciplinas.usp.br/pluginfile.php/7733433/mod_resource/content/1/aula9slidesT.pdf#:~:text=%E2%80%9CThe%20Gibbs%20sampler%20is%20a,this%20scheme%20may%20seem%20mysterious.">MarkovChain Monte Carlo.Gibbs Sampler.</a></p><h4 id="learning-contrastive-divergence">Learning: ContrastiveDivergence</h4><p>In RBMs, we want to learn the parameters <span class="math inline">\(\theta\)</span> by maximizing the summedlog-likelihood of the training data <span class="math inline">\(\logp_{\theta}(x)\)</span>. The problem is that the partition function <span class="math inline">\(Z(\theta)\)</span> is intractable. Contrastivedivergence(CD) is a method to approximate the gradient of thelog-likelihood.</p><p>Since <span class="math display">\[\begin{aligned}    \frac{\partial p_{\theta}(x)}{\partial \theta} &amp;=\frac{1}{p_{\theta}(x)} \frac{\partial p_{\theta}(x)}{\partial \theta}\\    &amp;= \frac{1}{p_{\theta}(x)} \frac{\partial \int p_{\theta}(x,h)\text{d}h}{\partial \theta} \\    &amp;= \frac{1}{p_{\theta}(x)} \int \frac{\partial p_{\theta}(x,h)}{\partial \theta}\text{d}h \\    &amp;=  \frac{1}{p_{\theta}(x)} \int \frac{\frac{1}{Z}\exp(-E_{\theta}(x, h))}{\partial \theta} \text{d}h \\    &amp;= \frac{1}{p_{\theta}(x)} \int (-\frac{1}{Z^2}\exp(-E_{\theta}(x, h)) \frac{\partial Z}{\partial \theta} - \frac{1}{Z}\exp(-E_{\theta}(x, h)) \frac{\partial E_{\theta}(x, h)}{\partial\theta}) \text{d}h \\    &amp;= -\frac{1}{p_{\theta}(x)} \int \frac{1}{Z}\frac{\partial  Z}{\partial \theta} p_{\theta}(x, h) \text{d}h  -\frac{1}{p_{\theta}(x)}  \int  \frac{\partial E_{\theta}(x, h)}{\partial\theta} p_{\theta}(x, h) \text{d}h \\    &amp;= - \int \frac{1}{Z} \frac{\partial  Z}{\partial \theta}p_{\theta}(h | x) \text{d}h  - \int \frac{\partial E_{\theta}(x,h)}{\partial \theta} p_{\theta}(h | x) \text{d}h  \\    &amp;= - \frac{1}{Z} \frac{\partial  Z}{\partial \theta} - \int\frac{\partial E_{\theta}(x, h)}{\partial \theta} p_{\theta}(h | x)\text{d}h  \\    &amp;= - \frac{1}{Z} \frac{\partial  \int \int  \exp(-E_{\theta}(x,h)) \text{d}x \text{d}h}{\partial \theta} -\mathbb{E}_{p_{\theta}(h|x)}[\frac{\partial E_{\theta}(x, h)}{\partial\theta}] \\    &amp;=  -\int \int   (-\frac{\partial E_{\theta}(x, h)}{\partial\theta}) p_{\theta}(x,h) \text{d}x \text{d}h -\mathbb{E}_{p_{\theta}(h|x)}[\frac{\partial E_{\theta}(x, h)}{\partial\theta}] \\    &amp;= \mathbb{E}_{p_{\theta}(h|x)}[-\frac{\partial E_{\theta}(x,h)}{\partial \theta}] - \mathbb{E}_{p_{\theta}(x, h)}[-\frac{\partialE_{\theta}(x, h)}{\partial \theta}]  \\\end{aligned}\]</span> Here we don't know the distribution <span class="math inline">\(p_{\theta}(h|x)\)</span>. Maximizing the summedlog-likelihood of the training data <span class="math inline">\(\logp_{\theta}(x)\)</span> is equivalent to minimizing the KL divergencebetween the real data distribution <span class="math inline">\(p_{data}(x)\)</span> and the model distribution<span class="math inline">\(p_{\theta}(x)\)</span>: <span class="math display">\[\begin{aligned}    \min_{\theta} \text{KL}(p_{data}(x) || p_{\theta}(x)) =\min_{\theta} \int p_{data}(x) \log p_{data}(x) \text{d}x - \intp_{data}(x) \log p_{\theta}(x) \text{d}x.\end{aligned}\]</span> Since the entropy of <span class="math inline">\(p_{data}(x)\)</span> is: <span class="math display">\[\begin{aligned}    H(p_{data}(x)) &amp;= - \int p_{data}(x) \log p_{data}(x) \text{d}x\end{aligned}\]</span> The cross-entropy of <span class="math inline">\(p_{data}(x)\)</span> and <span class="math inline">\(p_{\theta}(x)\)</span> is: <span class="math display">\[\begin{aligned}    H(p_{data}(x), p_{\theta}(x)) &amp;= -\int p_{data}(x) \logp_{\theta}(x) \text{d}x \\\end{aligned}\]</span> And <span class="math inline">\(H(p_{data}(x), p_{\theta}(x))= H(p_{data}(x)) + \text{KL}(p_{data}(x) || p_{\theta}(x))\)</span>.</p><p>The entropy of <span class="math inline">\(p_{data}(x)\)</span> is aconstant, so minimizing the KL divergence is equivalent to minimizingthe cross-entropy of <span class="math inline">\(p_{data}(x)\)</span>and <span class="math inline">\(p_{\theta}(x)\)</span>, which isequivalent to maximizing： <span class="math display">\[\begin{aligned}    \max_{\theta} \int p_{data}(x) \log p_{\theta}(x) \text{d}x.\end{aligned}\]</span></p><p>We can use stochastic gradient ascent to maximize the above equation.The gradient is: <span class="math display">\[\begin{aligned}    \frac{\partial}{\partial \theta} \int p_{data}(x) \log p_{\theta}(x)\text{d}x &amp;= \int p_{data}(x) \frac{\partial}{\partial \theta} \logp_{\theta}(x) \text{d}x \\    &amp;= \mathbb{E}_{p_{\theta}(h|x)p_{data}(x)}[-\frac{\partialE_{\theta}(x, h)}{\partial \theta}] - \mathbb{E}_{p_{\theta}(x,h)}[-\frac{\partial E_{\theta}(x, h)}{\partial \theta}].\end{aligned}\]</span> We can use Monte Carlo to approximate the above equation.</p><ul><li>For the first expectation <span class="math inline">\(\mathbb{E}_{p_{\theta}(h|x)p_{data}(x)}[-\frac{\partialE_{\theta}(x, h)}{\partial \theta}]\)</span>, we can first sample <span class="math inline">\(x\)</span> from <span class="math inline">\(p_{data}(x)\)</span> (we don't know thedistribution of real data, but we have training data.), then sample<span class="math inline">\(h\)</span> from <span class="math inline">\(p_{\theta}(h|x)\)</span>.</li><li>For the second expectation <span class="math inline">\(\mathbb{E}_{p_{\theta}(x, h)}[-\frac{\partialE_{\theta}(x, h)}{\partial \theta}]\)</span>, we can use<strong>finite-step</strong> Gibbs sampler.</li></ul><p>In this way, we don't need to compute the partition function <span class="math inline">\(Z(\theta)\)</span>. This method is calledContrastive Divergence(CD).</p><h4 id="ebms-with-continuous-observable-variables-and-discrete-latent-variables-grbms">EBMswith Continuous Observable Variables and Discrete Latent Variables:GRBMs</h4><p>Here, we consider continuous observable variables <span class="math inline">\(v\)</span> and binary units (latent variables)<span class="math inline">\(h\)</span>. The energy function is: <span class="math display">\[E_{\theta}(v, h) = \frac{1}{2}(\frac{v-\mu}{\sigma})^T(\frac{v-\mu}{\sigma}) - (\frac{v}{\sigma^2}) Wh - b^T h.\]</span> The conditional independence still holds: <span class="math display">\[\begin{aligned}p(v|h) &amp;= \mathcal{N}(v | Wh + \mu, \text{diag}(\sigma^2)) \\p(h_j = 1|v) &amp;= [\text{Sigmoid}(W^T \frac{v}{\sigma^2} + b)]_j\end{aligned}\]</span></p><h3 id="modern-ebms">Modern EBMs</h3><h4 id="ebms-with-learnable-energy-functions">EBMs with Learnable EnergyFunctions</h4><p>For RBMs, we designed the energy function in advance, and it impliedconditional independence. But, in general, it's hard to design theenergy function in advance.Thus, we want to learn the energy function<span class="math inline">\(E_{\theta}(x)\)</span> from data.</p><p>One way is to use deep neural networks to parameterize the energyfunction <span class="math inline">\(E_{\theta}(x)\)</span>. Forexample, we can use U-Net architecture：</p><p><img src="/2023/11/29/Generative-Models/u-net.png"></p><p>The energy obtained by the energy function is a scalar and the outputof the U-Net is a tensor. Thus, we need to design some readout choicesto get the scalar energy. For example, <span class="math display">\[\begin{aligned}    E_{\theta}(x) &amp;= x^T f_{\theta}(x), \\    E_{\theta}(x) &amp;= (x - f_{\theta}(x))^2, \\    E_{\theta}(x) &amp;= f_{\theta}(x)^2 .\\\end{aligned}\]</span> Empirically, the first choice is better.</p><h4 id="inference-langevin-monte-carlo">Inference: Langevin MonteCarlo</h4><p>After learning the energy function <span class="math inline">\(E_{\theta}(x)\)</span>, how to sample from <span class="math display">\[p_{\theta}(x) =\frac{1}{Z}\exp(-E_{\theta}(x))\]</span></p><p>One way is to use Langevin Monte Carlo(LMC). The stochasticdifferential equation(SDE) of LMC is: <span class="math display">\[\begin{aligned}    \text{d}x = \nabla \log p_{\theta} (x) \text{d}t + \sqrt{2} \text{d}B_t\end{aligned}\]</span> where <span class="math inline">\(B_t\)</span> is a standardBrownian motion. The first term <span class="math inline">\(\nabla \logp_{\theta} (x) \text{d}t\)</span> is called the drift term, whichdominates the movement of the particle. The second term <span class="math inline">\(\sqrt{2} \text{d} B_t\)</span> is called thediffusion term, which includes the stochasticity of the process.</p><p>One can prove Langevin diffusion is irreducible, strong Feller, andaperiodic. Thus, <strong>the stationary distribution of the Langevindiffusion is <span class="math inline">\(p_{\theta}(x)\)</span>, and wecan use Langevin diffusion to sample from <span class="math inline">\(p_{\theta}(x)\)</span>.</strong></p><p>To turn the Langevin diffusion into a sampling algorithm, we need todiscretize the SDE. The simplest way is to use Euler-Maruyamadiscretization: <span class="math display">\[\begin{aligned}    \text{d}x &amp;= \nabla \log p_{\theta} (x) \text{d}t + \sqrt{2}\text{d} B_t \\    x_{t+\eta} &amp;= x_t + \nabla \log p_{\theta} (x_t) (t+\eta - t) +\sqrt{2} (B_{t+\eta} - B_t) \\    &amp;= x_t + \eta \nabla \log p_{\theta} (x_t) + \sqrt{2 \eta}\epsilon, \quad \epsilon \sim N(0, I)\end{aligned}\]</span> where <span class="math inline">\(\eta\)</span> is the stepsize.</p><ul><li>If we ignore the noise term, we are using gradient ascent tomaximize the density, this means we are trying to fing the 'mode' of<span class="math inline">\(x\)</span>. The 'mode' is the mean of thedistribution. In order to generate more samples, we add noise term.</li></ul><p>Sampling algorithm: - Given initial sample <span class="math inline">\(x^{(0)}\)</span> and step size <span class="math inline">\(\eta\)</span>. - for <span class="math inline">\(t= 1, 2, ..., T\)</span>: <span class="math inline">\(x^{(t)} = x^{(t-1)}+ \eta \nabla \log p_{\theta} (x^{(t-1)}) + \sqrt{2 \eta} \epsilon,\quad \epsilon \sim N(0, I)\)</span> - Return <span class="math inline">\(x^{(T)}\)</span>.</p><p>In EBMs, the score function <span class="math inline">\(\nabla \logp_{\theta} (x)\)</span> is the derivative of the energy function <span class="math inline">\(\log p_{\theta} (x)\)</span> with respect to <span class="math inline">\(x\)</span>, and <span class="math display">\[\nabla_x \log p_{\theta} (x) = \nabla_x (-E_{\theta}(x) - \log Z)=-\nabla E_{\theta}(x).\]</span> here, <span class="math inline">\(Z\)</span> doesn't depend on<span class="math inline">\(x\)</span>.</p><p>Noticed that there is another score function <span class="math inline">\(\nabla \log p_{\theta} (x)\)</span>, which is thederivative of the probability distribution <span class="math inline">\(\log p_{\theta} (x)\)</span> with respect to <span class="math inline">\(\theta\)</span>.</p><h4 id="learning-contrastive-divergence-1">Learning: ContrastiveDivergence</h4><p>Similar to RBMs, we can use contrastive divergence to update <span class="math inline">\(\theta\)</span>. The gradient is <span class="math display">\[\int p_{data} \frac{\partial \log p_{\theta}(x)}{\partial \theta}\text{d} x = \mathbb{E}_{p_{data}(x)}[- \frac{\partialE_{\theta}(x)}{\partial \theta}] -\mathbb{E}_{p_{\theta}(x)}[-\frac{\partial E_{\theta}(x)}{\partial\theta}].\]</span></p><p>For the second expectation, we can use Langevin Monte Carlo samplingto sample from <span class="math inline">\(p_{\theta}(x)\)</span>, andthen estimate the expectation.</p><h4 id="score-matching">Score Matching</h4><p>In contrastive divergence, <strong>at each trainingiteration</strong>, we use Langevin Monte Carlo to sample from <span class="math inline">\(p_{\theta}(x)\)</span>, and then estimate theexpectation. However, the Langevin Monte Carlo sampling is notefficient, expecially in high-dimensional space. Thus, we need to trainthe model without sampling.</p><p>Score matching is a method to train the model without sampling. Theidea is to minimize the difference between the score function <span class="math inline">\(\nabla \log p_{\theta} (x)\)</span> and the scorefunction of the data distribution <span class="math inline">\(\nabla\log p_{data} (x)\)</span>.</p><p>The (stein) score function is: <span class="math display">\[s_{\theta}(x) = \nabla \log p_{\theta} (x) = -\nabla E_{\theta}(x),\]</span> which is independent of the partition function <span class="math inline">\(Z(\theta)\)</span> and needs <strong>the pdf isdifferentiable</strong>.</p><p><strong>Fisher divergence</strong> between two distributions <span class="math inline">\(p(x)\)</span> and <span class="math inline">\(q(x)\)</span> is: <span class="math display">\[D_F(p(x), q(x)) = \frac{1}{2} \mathbb{E}_{x \sim p(x)}[||\nabla_x \logp(x) - \nabla_x \log q(x)||_2^2].\]</span> Score matching is to minimize the Fisher divergence between<span class="math inline">\(p_{\theta}(x)\)</span> and <span class="math inline">\(p_{data}(x)\)</span>: <span class="math display">\[\begin{aligned}    \min_{\theta} D_F(p_{\theta}(x), p_{data}(x)) &amp;= \min_{\theta}\frac{1}{2} \mathbb{E}_{x \sim p_{data}(x)}[||\nabla_x \log p_{data}(x)- s_{\theta}(x)||_2^2] \\    &amp;= \min_{\theta} \frac{1}{2} \mathbb{E}_{x \simp_{data}(x)}[||\nabla_x \log p_{data}(x) - (-\nabla_x E_{\theta}(x))||_2^2]\end{aligned}\]</span> Since we don't know the real data distribution, we need todeal with <span class="math inline">\(\nabla_x \logp_{data}(x)\)</span>. Assume that $ p_{data}(x)$ decays to 0sufficiently rapidly as <span class="math inline">\(x \rightarrow \pm\infty\)</span>, one can derive the following equation: <span class="math display">\[\begin{aligned}   &amp;\frac{1}{2} \mathbb{E}_{x \sim p_{data}(x)}[||\nabla_x \logp_{data}(x) - \nabla_x \log p_{\theta}(x)||_2^2] \\    =&amp;\mathbb{E}_{x \sim p_{data}(x)}[\frac{1}{2} ||\nabla_x \logp_{\theta}(x)||^2_2 + \text{tr}(\nabla^2_x \log p_{\theta}(x))] +\text{const},\end{aligned}\]</span> where <span class="math inline">\(tr(\nabla^2_x \logp_{\theta}(x))\)</span> is the trace of the Hessian matrix of <span class="math inline">\(\log p_{\theta}(x)\)</span>. Therefore, we can usemonte carlo to estimate the above loss: <span class="math display">\[\begin{aligned}   &amp;\mathbb{E}_{x \sim p_{data}(x)}[\frac{1}{2} ||\nabla_x \logp_{\theta}(x)||^2_2 + \text{tr}(\nabla^2_x \log p_{\theta}(x))], \\   =&amp; \frac{1}{n} \sum_{i=1}^n [\frac{1}{2} ||\nabla_x \logp_{\theta}(x_i)||^2_2 + \text{tr}(\nabla^2_x \log p_{\theta}(x_i))] \\   =&amp; \frac{1}{n} \sum_{i=1}^n [\frac{1}{2} ||\nablaE_{\theta}(x_i)||^2_2 + \text{tr}(\nabla^2_x \log p_{\theta}(x_i))] \\\end{aligned}\]</span> Then, we can use stochastic gradient descent to minimize theabove loss.</p><p>Note: computing the trace of the Hessian matrix <span class="math inline">\(\text{tr}(\nabla^2_x \log p_{\theta}(x))\)</span>is expensive.</p><p>Conclusions:</p><ul><li>we have used two distances for training EBMs:<ul><li>KL divergence, which is equal to maximum likelihood. (contrastivedivergence).</li><li>Fisher divergence, which is equal to score matching.</li></ul></li><li>Energy-based models are very felxible probabilistic models withintracable partition functions.</li><li>Sampling is hard and requires MCMC.</li><li>Computing the likelihood is hard.</li><li>Comparing the likelihood/probability of two different points istractable.</li><li>Contrastive divergence is a good approximation to maximumlikelihood. But, it needs sampling for each iteration.</li><li>Sampling free methods: score matching, noise contrastive estimation,adversarial optimization, etc.</li></ul><h2 id="score-based-models">Score-Based Models</h2><p>How to represent probability distribution function <span class="math inline">\(p(x)\)</span> in different models:</p><ul><li><p>GAN: min-max loss</p></li><li><p>Autoregressive models: <span class="math inline">\(p_{\theta}(x)= \prod_{i=1}^{d} p_{\theta}(x_i | x_{&lt;i})\)</span></p></li><li><p>Flow-based models: <span class="math inline">\(p_{\theta}(x) =p(z) |\det(J_{f_{\theta}}(x))|\)</span>, <span class="math inline">\(z =f_{\theta}(x)\)</span>.</p></li><li><p>VAE: use ELBO obj and latent variables</p></li><li><p>EBMs: <span class="math inline">\(p_{\theta}(x) =\frac{1}{Z}\exp(-E_{\theta}(x))\)</span></p></li></ul><p>Pros: except for GAN, these models are maximizing the likelihood.</p><p>Cons: They need special atchitectures or surrogate losses.</p><p>Remember that the score function is: <span class="math display">\[s_{\theta}(x) = \nabla \log p_{\theta} (x).\]</span> As shown in the following figure, score function is thegradient of the log probability function <span class="math inline">\(\log p_{\theta}(x)\)</span> and the direction ofthe score function is the vector to the mode of the distribution. Thismeans that the score function directly models the vector field ofgradients.</p><p><img src="/2023/11/29/Generative-Models/score.jpg"></p><p>Score matching is not limited to EBMs. We can use score matching totrain other models, such as autoregressive models, flow-based models,etc.</p><p><img src="/2023/11/29/Generative-Models/scorebased-models.jpg"></p><p>We want to train a score-based model <span class="math inline">\(s_{\theta}\)</span> to estimate the score <span class="math inline">\(\nabla_{x} \log p_{data}(x)\)</span>, we use theaverage Euclidean distance between the score function <span class="math inline">\(s_{\theta}(x)\)</span> and the score <span class="math inline">\(\nabla_{x} \log p_{data}(x)\)</span> over thewhole space as the loss function: <span class="math display">\[\begin{aligned}    \frac{1}{2} \mathbb{E}_{x \sim p_{data}(x)}[||s_{\theta}(x) -\nabla_{x} \log p_{data}(x)||_2^2],  (\text{Fisher divergence})\end{aligned}\]</span> which is equal to minimize: <span class="math display">\[\begin{aligned}    \mathbb{E}_{x \sim p_{data}(x)}[\frac{1}{2} ||s_{\theta}(x) ||^2_2 +\text{tr}(\nabla_x s_{\theta}(x))],  (\text{Score matching})\end{aligned}\]</span></p><p>We need to compute the value of the score function <span class="math inline">\(s_{\theta}(x)\)</span> and the trace of theJacobian matrix <span class="math inline">\(\text{tr}(\nabla_xs_{\theta}(x))\)</span>. Thus, the score model must be efficient toevaluate. Since the score models is not scalable, computing the trace ofthe Jacobian matrix <span class="math inline">\(\text{tr}(\nabla_xs_{\theta}(x))\)</span> in the backpropagation is order of <span class="math inline">\(O(d)\)</span>, where <span class="math inline">\(d\)</span> is the dimension of <span class="math inline">\(x\)</span>. We need to find an efficient way totrain the score model.</p><h3 id="denoising-score-matching">Denoising Score Matching</h3><p>Consider the perturbed distribution: <span class="math display">\[q_{\sigma}(\tilde{x}|x) = \mathcal{N}(\tilde{x}|x, \sigma^2 I), \qquadq_{\sigma}(\tilde{x}) = \int p(x)q_{\sigma}(\tilde{x}|x) \text{d}x.\]</span> Instead of estimating <span class="math inline">\(\nabla_x\log q_{\theta}(x)\)</span>, we can estimate <span class="math inline">\(\nabla_{\tilde{x}} \logq_{\sigma}(\tilde{x})\)</span>. It's easier to estimate and when thenoise level is small, <span class="math inline">\(q_{\sigma}(\tilde{x})\approx p(\tilde{x})\)</span>.</p><p>Therefore, we can use denoising score matching to match the score ofa noise-perturbed distribution: <span class="math display">\[\begin{aligned}    &amp;\frac{1}{2}E_{\tilde{x} \simq_{\sigma}}[\|\nabla_{\tilde{x}}\log q_{\sigma}(\tilde{x}) -s_{\theta}(\tilde{x})\|_2^2] \\    =&amp; \frac{1}{2} E_{x \sim p_{data}(x), \tilde{x} \simq_{\sigma}(\tilde{x}|x)}[\|s_{\theta}(\tilde{x})- \nabla_{\tilde{x}}\log q_{\sigma}(\tilde{x}|x)\|_2^2] + \text{const}.\end{aligned}\]</span> In this form, we don't need to compute the trace of theJacobian matrix <span class="math inline">\(\text{tr}(\nabla_xs_{\theta}(x))\)</span>. Since <span class="math inline">\(q_{\sigma}(\tilde{x}|x) = \mathcal{N}(\tilde{x}|x,\sigma^2 I)\)</span>, <span class="math inline">\(\nabla_{\tilde{x}}\log q_{\sigma}(\tilde{x}|x) = -\frac{\tilde{x} - x}{\sigma^2}\)</span>.It's more efficient to optimize for high dimensional data.</p><p>Con: notice that, we use score function to estimate thenoise-perturbed distribution, which means <strong>we cannot estimate thescore of the clean data</strong>.</p><h4 id="denoising">Denoising</h4><p>Denoising: after training a score model, we can use langevin MCsampling to get noise samples from <span class="math inline">\(q_{\sigma}(\tilde{x})\)</span>. According toTweedie's formula: <span class="math display">\[E_{x \sim p(x|\tilde{x})}[x] = \tilde{x} + \sigma^2 \nabla_x \logq_{\sigma}(\tilde{x}) \approx \tilde{x} + \sigma^2s_{\theta}(\tilde{x}).\]</span> [remember <span class="math inline">\(s_{\theta}(\tilde{x}) =\nabla_{\tilde{x}}\log q_{\sigma}(\tilde{x})\)</span>]</p><p>Langevin MCMC: from scores to samples:</p><ul><li>Given initial sample <span class="math inline">\(x^{(0)}\)</span>.</li><li>for <span class="math inline">\(t = 1, 2, ..., T\)</span>:</li><li><span class="math inline">\(\qquad z^{(t)} \sim \mathcal{N}(0,I)\)</span></li><li><span class="math inline">\(\qquad\tilde{x}^{(t)} =\tilde{x}^{(t-1)} + \frac{\epsilon}{2} \nabla s_{\theta}(\tilde{x}) +\sqrt{\epsilon} z^t\)</span></li><li>Return <span class="math inline">\(\tilde{x}^{(T)}\)</span>.</li></ul><p>If noise <span class="math inline">\(\epsilon \to 0\)</span> and<span class="math inline">\(T \to \infty\)</span>, <span class="math inline">\(\tilde{x}^{(T)} \sim p_{data}(x)\)</span>.</p><h4 id="multi-scale-noise-perturbation">Multi-scale NoisePerturbation</h4><p>When using the denoising score matching, we only use the observedsamples to estimate the scores. Thus, the estimated scores in lowdensity regions are not accurate. Moreover, langevin MCMC converges veryslowly.</p><p><img src="/2023/11/29/Generative-Models/low_density.png"></p><p>One way to improve the accuracy of the estimated scores in lowdensity regions is to increase the noise level <span class="math inline">\(\sigma\)</span>. As shown in the following figure,the estimated scores in low density regions are more accurate when thenoise level <span class="math inline">\(\sigma\)</span> is large.</p><p><img src="/2023/11/29/Generative-Models/add_noise.png"></p><ul><li>High noise provides useful directional information for Langevindynamics.</li><li>But perturbed density no longer approximates the true datadensity.</li></ul><p>Multi-scale noise perturbation: perturb data with different levels ofnoise simulteanously, and aggregate the information from all noiselevels.</p><p><img src="/2023/11/29/Generative-Models/multi_noise.png"></p><p>If the noise levle <span class="math inline">\(\sigma\)</span> islarge, the perturbed data quality is worse, but the estimated score ismore close to the perturbed scores. It's a trade-off between the dataquality and estimated score accuracy.</p><p>When the noise is small, the perturbed distribution is close to theoriginal data distribution, but the estimation errors in low densityregions are still high.</p><p>When we add larger and larger noise, the estimation score is close tothe perturbed data score, but the perturbed data score differs from theoriginal data score.</p><p>In order to achieve the best data quality and estimation accuracy atthe same time, we should consider all perturbations jointly instead offocusing on only one perturbation.</p><p><strong>Training procedure</strong>: Assume we have <span class="math inline">\(L\)</span> noise levels <span class="math inline">\(\sigma_1, \sigma_2, ..., \sigma_L\)</span> andcorresponding perturbed data distributions <span class="math inline">\(q_{\sigma_1}(\tilde{x}), q_{\sigma_2}(\tilde{x}),..., q_{\sigma_L}(\tilde{x})\)</span>. For each perturbed datadistribution, we can easily sample from them, and use score estimationto estimate the corresponding scores. However, this method requires alarge number of separate score models to be learned independently, whichis very costly, exspecially when the number of noise levels <span class="math inline">\(L\)</span> is large.</p><p>One way is to train a single conditional score network for all noiselevels. The score model will take <span class="math inline">\(\sigma\)</span> as an input. This model is namedthe <strong>Noise Conditional Score Network</strong>.</p><p><img src="/2023/11/29/Generative-Models/noise-score.png"></p><p>The loss function is a weighted combination of denoising scorematching loss with different noise levels: <span class="math display">\[\begin{aligned}    &amp;\frac{1}{L}\sum_{l=1}^L \lambda(\sigma_i) E_{\tilde{x} \simq_{\sigma_i}}[\|\nabla_{\tilde{x}}\log q_{\sigma_i}(\tilde{x}) -s_{\theta}(\tilde{x}, \sigma_i)\|_2^2]  \\    =&amp; \frac{1}{L}\sum_{l=1}^L \lambda(\sigma_i)    E_{x \sim p_{data}(x), z \sim N(0, I)}[\|s_{\theta}(x+\sigma_i z,\sigma_i) + \frac{z}{\sigma_i}\|_2^2] + \text{const}.\end{aligned}\]</span></p><p>[compute the loss in parallel?]</p><p>About the weighting function <span class="math inline">\(\lambda(\sigma_i)\)</span>, we can set <span class="math inline">\(\lambda(\sigma_i) = \sigma_i^2\)</span>.</p><p>About choosing the noise level <span class="math inline">\(\sigma_i\)</span>:</p><ul><li><p>The largest noise level <span class="math inline">\(\sigma_1\)</span> approximates the maximumpairwise distance between data points.</p></li><li><p>The smallest noise level <span class="math inline">\(\sigma_L\)</span> should be small enough so thatthe noise in final samples is negligible.</p></li><li><p>Adjacent noise scales should have sufficient overlap tofacilitate transitioning across noise scales in annealed Langevindynamics. One way is to use geometric sequence: <span class="math display">\[\frac{\sigma_1}{\sigma_2} = \frac{\sigma_2}{\sigma_3} = ... =\frac{\sigma_{L-1}}{\sigma_L},  \quad \sigma_1 &gt; \sigma_2 &gt; ...&gt; \sigma_L.\]</span></p></li></ul><p><strong>Sampling procedure</strong>: we use annealed Langevindynamics to sample from the noise conditional score network using scoresof different noise levels.</p><p>We first use Langevin dynamics to sample from the most perturbed datadistribution. Then, the resulting samples will be used as initialsamples for sampling from the next noise level. We continue in thisfashion and finally use Langevin dynamics to sample from the leastperturbed data distribution.</p><p><img src="/2023/11/29/Generative-Models/annealed.png"></p><p><strong><span class="math inline">\(s_{\theta}\)</span> is shared ateach iteration, since we only have one network.</strong></p><p>Conclusions:</p><ul><li>Gradients of distributions (scores) can be estimated easily</li><li>Flexible architecture choices — no need to benormalized/invertible</li><li>Stable training — no minimax optimization</li><li>Better or comparable sample quality to GANs</li><li>Exact likelihood computation</li></ul><h2 id="introduction-about-diffusion-model">Introduction about diffusionmodel</h2><p>Reference: <a href="https://lilianweng.github.io/posts/2021-07-11-diffusion-models/">Whatare Diffusion Models?</a></p><h3 id="ddpm-denoising-diffusion-probabilistic-models">DDPM: DenoisingDiffusion Probabilistic Models</h3><p>Diffusion model is a generative model:</p><p><img src="/2023/11/29/Generative-Models/image.png"></p><ul><li>diffusion process: add noise to a real image, finally we get a noiseimage.</li><li>reverse process: from noise image to generate real image.</li></ul><ol type="1"><li><p>training phase from a real image datasets ---&gt; throughdiffusion process ---&gt; noise images ---&gt; through reverse process---&gt; real images</p></li><li><p>inference phase</p></li></ol><p>sampling noise images from a gaussian distribution, then use thepre-trained reverse process to generate images.</p><h4 id="diffusion-process">Diffusion process</h4><p>add noise to a clean image <span class="math inline">\(X_0\)</span>and then we get noisy image <span class="math inline">\(X_1, X_2, ...,X_T\)</span>.</p><p>Now, we focus on the process from image <span class="math inline">\(X_{t-1}\)</span> to <span class="math inline">\(X_t\)</span>. <span class="math display">\[X_t = \sqrt {1 - \beta_t}X_{t-1} + \sqrt {\beta_t} Z_{t}, \quad Z_t \simN(0, I)\]</span> &gt;Remark: the noise scale <span class="math inline">\(\beta_t\)</span> will be increased gradually. $_t$ increases from <span class="math inline">\(10^{-4}\)</span> to <span class="math inline">\(2*10^{-2}\)</span> linearly. <span class="math inline">\(T = 2000\)</span>.</p><p>Let <span class="math inline">\(1 - \beta_t = \alpha_t\)</span>, thenwe have <span class="math display">\[X_t = \sqrt{\alpha_t}X_{t-1} + \sqrt{1 - \alpha_t} Z_t \\X_{t-1} = \sqrt{\alpha_{t-1}}X_{t-2} + \sqrt{1 - \alpha_{t-1}} Z_{t-1}\\\]</span> Combine these two equlities, we have <span class="math display">\[\begin{aligned}X_t &amp;= \sqrt{\alpha_t}X_{t-1} + \sqrt{1 - \alpha_t} Z_t \\&amp;= \sqrt{\alpha_t}(\sqrt{\alpha_{t-1}}X_{t-2} + \sqrt{1 -\alpha_{t-1}} Z_{t-1})+ \sqrt{1 - \alpha_t} Z_t \\&amp;= \sqrt{\alpha_t \alpha_{t-1}}X_{t-2} + \sqrt{\alpha_t(1 -\alpha_{t-1})} Z_{t-1}+ \sqrt{1 - \alpha_t} Z_t \\&amp; = \sqrt{\alpha_t \alpha_{t-1}}X_{t-2} + + \sqrt{1 - \alpha_t\alpha_{t-1}} Z,   \quad Z \sim N(0, I) \\&amp;= ... \\&amp;= \sqrt{\alpha_t \alpha_{t-1}...\alpha_1}X_{0} + + \sqrt{1 -\alpha_t \alpha_{t-1}... \alpha_{1}} Z \\&amp;= \sqrt{\bar{\alpha}_t}X_{0} + + \sqrt{1 - \bar{\alpha}_t}Z,   \qquad \bar{\alpha}_t = \prod_{i = 1}^{t}\alpha_i.\end{aligned}\]</span></p><h4 id="the-relation-between-ddpm-and-sde">The relation between DDPM andSDE</h4><p>DDPM: <span class="math inline">\(x_i = \sqrt{1 - \beta_i} x_{i-1} +\sqrt{\beta_i}z_{i-1}\)</span></p><p>SDE: <span class="math inline">\(\text{d}x = f(x, t)\text{d}t + g(x,t)\text{d}w\)</span>,</p><p>the solution is <span class="math display">\[x_t = x_0 + \int_0^t f(x, t)dt + \int_0^t g(x, t)dw\]</span> where <span class="math inline">\(w\)</span> is a Brownianmotion.</p><p>How to get the mean and convariance of <span class="math inline">\(x_t\)</span> ?</p><p>We can use FPK equation. See more on <a href="https://users.aalto.fi/~ssarkka/course_s2014/handout3.pdf">FPKequation</a>.</p><p>The problem is the mean and covariance of <span class="math inline">\(x_t\)</span> is dependent on the expectation of<span class="math inline">\(p(x(t))\)</span>, which we don't know.</p><h4 id="ddpm-variance-preserving-sde">DDPM / Variance preservingSDE</h4><p><span class="math display">\[x_i = \sqrt{1 - \beta_i} x_{i-1} +\sqrt{\beta_i}z_{i-1}\]</span></p><p>Let <span class="math inline">\(\beta(t=i/N) = N \beta_i, X(t = i/N)= x_i, Z(i/N) = z_i, \Delta t = 1/N\)</span>.</p><p>Then we have <span class="math display">\[\begin{aligned}    X(t + \Delta t) &amp;= \sqrt{1 - \beta(t+\Delta t)\Delta t}X(t) +\sqrt{\beta(t+\Delta t)\Delta t}Z(t) \\    &amp; \approx X(t)  - \frac{1}{2} \beta(t) \Delta t X(t) +\sqrt{\beta(t)\Delta t}Z(t) \\\end{aligned}\]</span></p><p><span class="math display">\[\text{d} x = - \frac{1}{2} \beta(t) x(t) dt + \sqrt{\beta(t)} \text{d}w,\]</span> where <span class="math inline">\(w\)</span> is a Brownianmotion.</p><p>The mean and covariance of <span class="math inline">\(x_t\)</span>is <span class="math display">\[\begin{aligned}    \mathbb{E}[x(t)] &amp;= \mathbb{E}[x(0)]e^{-\frac{1}{2}\int_0^t\beta(s)ds} \\    \text{cov}[x(t)] &amp;= I -  I \times e^{-\frac{1}{2}\int_0^t\beta(s)ds} \leq I.\end{aligned}\]</span></p><h4 id="score-based-model-variance-exploding-sde">SCORE-based model /Variance Exploding SDE</h4><p><span class="math display">\[x_i = x_{i-1} + \sqrt{\sigma_{i}^2 -\sigma^2_{i-1}}z_{i-1}\]</span></p><p>Let <span class="math inline">\(X(t = i/N) = x_i\)</span>, <span class="math inline">\(\sigma(t = i/N) = \sigma_i, Z(t = 1/N) = z_i,\Delta t = 1/N\)</span>.</p><p><span class="math display">\[\begin{aligned}    X(t + \Delta t) &amp;= X(t) + \sqrt{\sigma(t+\Delta t)^2 -\sigma(t)^2}Z(t) \\    &amp; \approx X(t) + \sqrt{\frac{\Delta \sigma(t)^2}{\Delta t}\Delta t}Z(t) \\\end{aligned}\]</span> where <span class="math inline">\(w\)</span> is a Brownianmotion. <span class="math display">\[d x = \sqrt{\frac{d \sigma(t)^2}{dt}} d w\]</span></p><p>The mean and covariance of <span class="math inline">\(x_t\)</span>is <span class="math display">\[\begin{aligned}    \mathbb{E}[x(t)] &amp;= \mathbb{E}[x(0)]  \\    \text{cov}[x(t)] &amp;= \sigma^2(t) I.\end{aligned}\]</span> Here, <span class="math inline">\(\sigma^2(t)\)</span> isnon-decreasing variance. Thus, the variance will be exploded.</p><h2 id="evaluating-generative-models">Evaluating Generative Models</h2><h3 id="model-families">Model families</h3><ul><li><p>Probability density/mass functions</p><ul><li>Autoregressive models: <span class="math inline">\(p_{\theta}(x) =\prod_{i=1}^{d} p_{\theta}(x_i | x_{&lt;i})\)</span>.</li><li>Normalizing flow models: <span class="math inline">\(p_{\theta}(x) =p(z) |\det(J_{f_{\theta}}(x))|\)</span>, <span class="math inline">\(z =f_{\theta}(x)\)</span>.</li><li>Latent variable models(VAEs): <span class="math inline">\(p_{\theta}(x) = \int p_{\theta}(x|z)p(z)\text{d}z\)</span>.</li><li>Energy-based models: <span class="math inline">\(p_{\theta}(x) =\frac{1}{Z}\exp(-E_{\theta}(x))\)</span>.</li></ul></li><li><p>Sample generation processes</p><ul><li>GANs: <span class="math inline">\(x = G_{\theta}(z)\)</span>, <span class="math inline">\(z \sim p(z)\)</span>.</li></ul></li><li><p>Score functions</p><ul><li>Score-based models: <span class="math inline">\(s_{\theta}(x) =\nabla_x \log p_{\theta} (x)\)</span>.</li></ul></li></ul><h3 id="distances-of-probability-distributions">Distances of probabilitydistributions</h3><ul><li><p>KL divergence(maximum likelihood): <span class="math inline">\(D_{KL}(p||q) = \int p(x) \log \frac{p(x)}{q(x)}\text{d}x\)</span></p><ul><li>Autoregressive models.</li><li>Normalizing flow models.</li><li>ElBO in VAEs.</li><li>Contrastive divergence in EBMs.</li></ul></li><li><p>f-divergences, Wasserstein distances</p><ul><li>GANs (f-GANs, WGANs)</li></ul></li><li><p>Fisher divergence(score matching): denoising score matching,sliced score matching</p><ul><li>Score-based models</li><li>Energy-based models</li></ul></li><li><p>Noise-contrastive estimation</p><ul><li>Energy-based models</li></ul></li></ul><h3 id="evaluation---density-estimation">Evaluation - Densityestimation</h3><p>We can use likelihood as a matric for density estimation:</p><ul><li>Split dataset into train, validation, test sets.</li><li>Learn model <span class="math inline">\(p_{\theta}(x)\)</span> usingthe train set.</li><li>Tune hyperparameters using the validation set.</li><li>Evaluate likelihood on the test set: <span class="math inline">\(\mathbb{p_{data}}[\logp_{\theta}(x)]\)</span>.</li></ul><p>However, the likelihood is intractable for many models. Not allmodels have tractable likelihoods. For example, GANs, VAEs, EBMs,etc.</p><p>For VAEs, we can compare the ELBO to log-likelihood. But, how aboutGANs and EBMs?</p><p>In general, unbiased estimation of probability density functions fromsamples is impossible. We can use approximation methods, such as kerneldensity estimation.</p><h4 id="kernel-density-estimation">Kernel density estimation:</h4><p>Given an intractable density model <span class="math inline">\(p_{\theta}(x)\)</span> and limited samples <span class="math inline">\(S = \{x_i\}_{i=1}^n\)</span>, we can use kerneldensity estimation to estimate the density function <span class="math inline">\(p_{\theta}(x)\)</span>. For a new data point <span class="math inline">\(x\)</span>, we can estimate the density of <span class="math inline">\(x\)</span> as: <span class="math display">\[\hat{p}_{\theta}(x) = \frac{1}{n} \sum_{i \in S} K(\frac{x -x_i}{\sigma}),\]</span> where <span class="math inline">\(K\)</span> is a kernelfunction, <span class="math inline">\(\sigma\)</span> is thebandwidth.</p><ul><li>Gaussian kernel: <span class="math inline">\(K(x) =\frac{1}{\sqrt{2\pi}}\exp(-\frac{1}{2}x^2)\)</span>.</li><li>A kernel is a function that satisfies the following properties:<ul><li><span class="math inline">\(K(x) \geq 0\)</span>.</li><li><span class="math inline">\(\int K(x) \text{d}x = 1\)</span>.</li><li><span class="math inline">\(K(x) = K(-x)\)</span>.</li></ul></li><li>Bandwidth <span class="math inline">\(\sigma\)</span> controls thesmoothness of the density estimate.<ul><li>Small <span class="math inline">\(\sigma\)</span>:undersmoothed</li><li>Large <span class="math inline">\(\sigma\)</span>: oversmoothed</li><li><span class="math inline">\(\sigma\)</span> is a hyperparameter. Wecan use cross-validation to choose <span class="math inline">\(\sigma\)</span>.</li></ul></li><li>KDE is very unreliable in higher dimensions.</li></ul><h3 id="importance-sampling-for-latent-variable-models">Importancesampling for latent variable models</h3><p>For likelihood <span class="math inline">\(p(x)\)</span>, we can uselikehood weighting to estimate the likelihood: <span class="math display">\[p(x)= \mathbb{E}_{p(z)}[p(x|z)].\]</span></p><p>Monte Carlo sampling is one way to estimate the expectation. However,if <span class="math inline">\(p(z)\)</span> is far from <span class="math inline">\(p(z|x)\)</span>, the variance of the likehoodweighting is very large. For example, the probability of <span class="math inline">\(p(z)\)</span> is very small, but the probabilityof <span class="math inline">\(p(z|x)\)</span> is very large at someregions. Thus, we need another distribution <span class="math inline">\(q(z)\)</span>, which is close to <span class="math inline">\(p(z|x)\)</span>, to estimate the expectation.</p><p>Importance sampling is another way to estimate the expectation. Theidea is to sample from a proposal distribution <span class="math inline">\(q(z)\)</span>, then <span class="math display">\[\begin{aligned}\mathbb{E}_{p(z)}[p(x|z)] = \int p(z) p(x|z) \text{d}z = \int q(z)\frac{p(z)}{q(z)} p(x|z) \text{d}z = \mathbb{E}_{q(z)}[\frac{p(z)}{q(z)}p(x|z)].\end{aligned}\]</span> Then, we can use Monte Carlo sampling to estimate theexpectation.</p><p>Pros:</p><ul><li>Still unbiased.</li><li>We can choose <span class="math inline">\(q(z)\)</span> to havelower variance. One can prove that <span class="math inline">\(q(z)\)</span> should be high where <span class="math inline">\(|p(z)p(x|z)|\)</span> is high.</li></ul><p>Cons:</p><ul><li>We need to choose a good proposal distribution <span class="math inline">\(q(z)\)</span>.</li><li>It's unreliable in high dimensions.</li></ul><p>When to use importance sampling?</p><ul><li><span class="math inline">\(p(z)\)</span> is difficult to samplefrom.</li><li>We can evaluate <span class="math inline">\(p(z)\)</span>.</li><li><span class="math inline">\(q(x)\)</span> is easy to evaluate andsample from.</li><li>We can choose <span class="math inline">\(q(z)\)</span> to be highwhere <span class="math inline">\(|p(z)p(x|z)|\)</span> is high.</li></ul><p>Annealed importance sampling is another method to estimate thelikelihood.</p><h3 id="sample-quality">Sample quality</h3><h4 id="inception-score-is">Inception score (IS)</h4><p>Inception score is a metric for evaluating the quality of generatedimages. The idea is to use a pretrained Inception network to classifythe generated images.</p><p>Assumption 1: we are evaluating sample quality for generative modelstrained on labeled datasets.</p><p>Assumption 2: We have a good probabilistic classifier <span class="math inline">\(c(y|x)\)</span> for predicting the label <span class="math inline">\(y\)</span> of any point <span class="math inline">\(x\)</span>.</p><p>(A classifier can be trained on a large dataset, such asImageNet.)</p><p>We want a good generative model to satisfy two properties:</p><ul><li><p>Sharpness: the generated images should be sharp.</p><p><img src="/2023/11/29/Generative-Models/sharpness.png"></p><p><span class="math display">\[S = \exp(E_{x \sim p}[\int c(y | x)\logc(y|x) \text{d}y])\]</span> High sharpness implies classifier isconfident in making predictions for generated images, and <span class="math inline">\(c(y|x)\)</span> has low entropy.</p></li><li><p>Diversity: the generated images should be diverse.</p><p><img src="/2023/11/29/Generative-Models/diversity.png"></p><p><span class="math display">\[D = \exp(E_{x \sim p}[\int c(y|x) \logc(y) \text{d} y]), \qquad c(y) = E_{x \sim p} [c(y|x)]\]</span></p><p>High diversity implies the generated images are diverse, and <span class="math inline">\(c(y)\)</span> has high entropy.</p></li></ul><p>Inception score combines these two properties: <span class="math display">\[IS = D \times S.\]</span></p><p>Higher IS implies better sample quality.</p><h4 id="frechet-inception-distance-fid">Frechet inception distance(FID)</h4><p>Inception score only considers the samples from <span class="math inline">\(p_{\theta}(x)\)</span>, but ignores the real datadistribution <span class="math inline">\(p_{data}(x)\)</span>.</p><p>FID is a metric for evaluating the quality of generated images. Theidea is to use a pretrained Inception network to extract features fromthe generated images and real images. Then, we can compute the Frechetdistance between the two feature distributions.</p><ul><li>Let <span class="math inline">\(\mathcal{G}\)</span> be thegenerated samples and <span class="math inline">\(\mathcal{T}\)</span>be the test dataset.</li><li>Compute feature representations <span class="math inline">\(F_{\mathcal{G}}\)</span> and <span class="math inline">\(F_{\mathcal{T}}\)</span>.</li><li>Fit a multivariate Gaussian to <span class="math inline">\(F_{\mathcal{G}}\)</span> and <span class="math inline">\(F_{\mathcal{T}}\)</span>. Let <span class="math inline">\(\mu_{\mathcal{G}}\)</span> and <span class="math inline">\(\mu_{\mathcal{T}}\)</span> be the mean vectors and<span class="math inline">\(\Sigma_{\mathcal{G}}\)</span> and <span class="math inline">\(\Sigma_{\mathcal{T}}\)</span> be the covariancematrices.</li><li>FID is defined as the Wasserstein-2 distance between the twoGaussians: <span class="math display">\[FID(\mathcal{G}, \mathcal{T}) = ||\mu_{\mathcal{G}} -\mu_{\mathcal{T}}||_2^2 + \text{tr}(\Sigma_{\mathcal{G}} +\Sigma_{\mathcal{T}} -2(\Sigma_{\mathcal{G}}\Sigma_{\mathcal{T}})^{1/2}).\]</span></li></ul><p>Lower FID implies better sample quality.</p><h4 id="kernel-inception-distance-kid">Kernel inception distance(KID)</h4><p>Maximum mean discrepancy (MMD) is a two-sample test statistic thatmeasures the distance between two distributions by computing differencesin their moments. Using the kernel trick, we can compute the MMD betweentwo distributions: <span class="math display">\[MMD(p, q) = E_{x, x' \sim p} [K(x, x')] + E_{y, y' \sim q}[K(y, y')] - 2E_{x \sim p, y \sim q} [K(x, y)].\]</span></p><p>Kernel inception distance (KID) is a metric for evaluating thequality of generated images. The idea is to use a pretrained Inceptionnetwork to extract features from the generated images and real images.Then, we can compute the MMD between the two feature distributions.</p><p>FID VS. KID:</p><ul><li>FID can only be positive, and it's biased, KID is unbiased.</li><li>The computation time of FID is <span class="math inline">\(O(n)\)</span>, but the computation time of KID is<span class="math inline">\(O(n^2)\)</span>.</li></ul><h3 id="evaluation---latent-representations">Evaluation - latentrepresentations</h3><p>What is a good latent representation? For downstream tasks, we canevaluate the quality of latent representations by evaluating theperformance of the downstream tasks, such as reconstruction,classification, etc.</p><p>For unsupervised learning, there is no one-size-fits-all metric forevaluating the quality of latent representations. We can use thefollowing metrics to evaluate the quality of latent representations:</p><h4 id="clustering">clustering</h4><p>Representations that can be grouped into clusters are potentiallyuseful. For example, the representations of a generated model for MNISTcan be grouped into different clusters, where each cluster correspondsto one or more digits.</p><p>For labelled datasets, there are many evaluation metrics. The lablesare only used for evaluation, not for clustering.</p><pre><code>from sklearn.metrics.cluster import completeness_score, homogeneity_score, v_measure_score</code></pre><h4 id="compression-or-reconstruction">compression orreconstruction</h4><p>Latent representations can be evaluated based on the maximumcompression they can achieve without significant loss in reconstructionquality.</p><p>Some metrics: Mean Squared Error (MSE), Peak signal-to-noise ratio(PSNR), Structural similarity (SSIM), etc.</p><h4 id="disentanglement">disentanglement</h4><p>We want representations that disentangle independent andinterpretable factors of variation in the observed data.</p><p>Some quantitative metrics:</p><ul><li>Beta-VAE metric: accuracy of a linear classifier that predicts afixed factor of variation.</li><li>Factor-VAE, Mutual Information Gap (MIG), SAP score, DCIdisentanglement, Modularity, etc.</li></ul><h2 id="reference">Reference</h2><ul><li><p><a href="https://users.aalto.fi/~asolin/sde-book/sde-book.pdf">SDEBOOK</a></p></li><li><p><a href="https://arxiv.org/abs/2006.11239">Denoising DiffusionProbabilistic Models. Jonathan et al.</a></p></li><li><p><a href="https://yang-song.net/blog/2021/score/">GenerativeModeling by Estimating Gradients of the Data Distribution</a></p></li><li><p><a href="https://lilianweng.github.io/posts/2021-07-11-diffusion-models/">Whatare Diffusion Models?</a></p></li><li><p><a href="https://deepgenerativemodels.github.io/syllabus.html">CS236 - Fall2023 Deep Generative Models</a></p></li><li><p><a href="https://lrjconan.github.io/UBC-EECE571F-DL-Structures/">EECE 571F(2023 Winter Term 1): Deep Learning with Structures</a></p></li><li><p><a href="https://arxiv.org/abs/2101.03288">How to Train YourEnergy-Based Models. Yang Song and Durk Kingma.</a></p></li><li><p><a href="https://glizen.com/radfordneal/ftp/ais-rev.pdf">Importancesampling</a></p></li></ul>]]></content>
      
      
      <categories>
          
          <category> Generative Model </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Diffusion </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>The Effect of Data Centering on PCA Models</title>
      <link href="/2023/11/07/The%20Effect%20of%20Data%20Centering%20on%20PCA%20Models/"/>
      <url>/2023/11/07/The%20Effect%20of%20Data%20Centering%20on%20PCA%20Models/</url>
      
        <content type="html"><![CDATA[<p>A common misconception in PCA is that PC 1 is the mean of the datawhen the data are not centered. It was shown in this paper that PC 1 isnot the mean of the data but it can point in the direction of the mean.The extent to which PC 1 points in the direction of the mean depends onhow far away the data set mean is from the origin. More details can befound in the paper [1].</p><h2 id="references">References</h2><p>[1] <a href="https://eigenvector.com/wp-content/uploads/2020/06/EffectofCenteringonPCA.pdf">TheEffect of Data Centering on PCA Models</a></p>]]></content>
      
      
      <categories>
          
          <category> Statistic </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Statistic </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Monte Carlo Gradient Estimation in Machine Learning</title>
      <link href="/2023/10/21/Monte-Carlo-Gradient-Estimation/"/>
      <url>/2023/10/21/Monte-Carlo-Gradient-Estimation/</url>
      
        <content type="html"><![CDATA[<h1 id="monte-carlo-methods-and-stochastic-optimisation">Monte CarloMethods and Stochastic Optimisation</h1><p>The mean-value analysis problem: <span class="math display">\[\mathcal{F}(\theta) := \int p(x; \theta) f(x; \phi) \text{d}x =\mathbb{E}_{p(x; \theta)} f(x; \phi),  \enspace (1)\]</span> where <span class="math inline">\(f(x; \phi)\)</span> is thecost with parameters <span class="math inline">\(\phi\)</span>, <span class="math inline">\(p(x;\theta)\)</span> is the measure, a probabilitydistribution that is continuous in its domain and differentiable with<span class="math inline">\(\theta\)</span>.</p><p>This objective is very common in variational inference, reinforcementlearning, etc.</p><p>To learn the distribution parameter <span class="math inline">\(\theta\)</span>, we need to consider the gradient:<span class="math display">\[\eta := \nabla_{\theta} \mathcal{F}(\theta) = \nabla_{\theta}\mathbb{E}_{p(x; \theta)} f(x; \phi). \enspace  (2)\]</span> <span class="math inline">\(\eta\)</span> is called thesensitivity analysis of <span class="math inline">\(\mathcal{F}\)</span>.</p><p>Problems:</p><ul><li>not able to evaluate the expectation in closed form;</li><li><span class="math inline">\(x\)</span>: high-dimensional, <span class="math inline">\(\theta\)</span>: high-dimensional;</li><li>the cost funtion may not be differential or a black-boxfunction.</li></ul><h2 id="monte-carlo-estimators">Monte Carlo Estimators</h2><p>We can numerically evaluate the integral by first drawing<strong>independent</strong> samples <span class="math inline">\(\hat{x}^{(1)}, ..., \hat{x}^{(N)}\)</span> fromthe distribution <span class="math inline">\(p(x; \theta)\)</span>, andthen computing the averaged of the function evaluated at these samples:<span class="math display">\[\bar{\mathcal{F}}_N = \frac{1}{N} \sum_{n=1}^{N} f(\hat{x}^{(n)}),\qquad \hat{x}^{(n)} \sim p(x; \theta), \quad n = 1,..., N.\]</span> <span class="math inline">\(\bar{\mathcal{F}}_N\)</span> is arandom variable and is called Monte Carlo estimator of eq. (1).</p><p>Remark 1. As long as we can write an integral in the form of eq. (1)(a product of a function and a distribution that we can easily samplefrom), we can apply the Monte Carlo method.</p><h3 id="four-properties">Four Properties</h3><ul><li><p>Consistency. As <span class="math inline">\(N \to\infty\)</span>, the estimator <span class="math inline">\(\bar{\mathcal{F}}_N \to \mathbb{E}_{p(x; \theta)}f(x; \phi)\)</span>. This can be easily satisfied according to thestrong law of large number.</p></li><li><p>Unbiasedness. <span class="math display">\[\mathbb{E}_{p(x; \theta)} [\bar{\mathcal{F}}_N] = \mathbb{E}_{p(x;\theta)} [\frac{1}{N} \sum_{n=1}^{N} f(\hat{x}^{(n)})] = \frac{1}{N}\sum_{n=1}^{N} \mathbb{E}_{p(x; \theta)} [f(\hat{x}^{(n)})] =\mathbb{E}_{p(x; \theta)}[f(x)].\]</span> (if we repeat the estimation process many times, the estimateis centered on the the actual value of the integral on average)</p></li><li><p>Minimum variance. If we consider two <strong>unbiased</strong>estimators using the same number of sampling <span class="math inline">\(N\)</span>, we will prefer the estimator that haslower variance. [<strong>if MC estimator has the minimumvariance?</strong>]</p></li><li><p>Computational efficiency. for example: a computational costlinear in the number of parameters, can be computed inparallel.</p></li></ul><h3 id="the-central-role-of-gradient-estimation-eq.-2">The central roleof Gradient Estimation eq. (2)</h3><p>some examples in different areas.</p><h4 id="variational-inference">Variational Inference</h4><p>Variational inference is a general method for approximating complexand unknown distributions by the closest distribution within a tractablefamily. Consider a generic probabilistic model <span class="math inline">\(p(x|z)p(z)\)</span> that defines a generativeprocess in which observed data <span class="math inline">\(x\)</span> isgenerated from a set of unobserved variables z using a data distribution<span class="math inline">\(p(x|z)\)</span> and a prior distribution<span class="math inline">\(p(z)\)</span>. The posterior distribution ofthis generative process <span class="math inline">\(p(z|x)\)</span> isunknown, and is approximated by a variational distribution <span class="math inline">\(q(z|x; \theta)\)</span> with variationalparameters <span class="math inline">\(\theta\)</span>. The objective is<span class="math display">\[\max_{\theta, \phi} \mathbb{E}_{q(z|x;\theta)} [\log p(x|z;\phi) - \log\frac{q(z|x;\theta)}{p(z)}].\]</span> Optimising the distribution <span class="math inline">\(q\)</span> requires the gradient of the objectivewith respect to the variational parameters <span class="math inline">\(\theta\)</span>: <span class="math display">\[\eta = \nabla_{\theta}\mathbb{E}_{q(z|x;\theta)} [\log p(x|z;\phi) -\log \frac{q(z|x;\theta)}{p(z)}].\]</span></p><h4 id="reinforcement-learning">Reinforcement Learning</h4><p>Model-free policy search is an area of reinforcement learning wherewe learn a policy|a distribution over actions|that on average maximisesthe accumulation of long-term rewards. Through interaction in anenvironment, we can generate trajectories <span class="math inline">\(\tau = (s_1, a_1, s_2, a_2, ... , s_T , a_T)\)</span> that consist of pairs of states st and actions at for timeperiod <span class="math inline">\(t = 1, ... , T\)</span>. A policy islearnt by following the policy gradient:</p><p><span class="math display">\[\eta = \nabla_{\theta} \mathbb{E}_{p(\tau;\theta)} [\sum_{t=0}^{T}\gamma^t r(s_t,a_t)]\]</span> The cost is the return over the trajectory, which is aweighted sum of rewards obtained at each time step <span class="math inline">\(r(s_t, a_t)\)</span>, with the discount facthor<span class="math inline">\(\gamma \in [0, 1]\)</span>. The measure isthe joint distribution over states and actions <span class="math inline">\(p(\tau;\theta) = \prod_{t=0}^{T-1} [p(s_{t+1}|s_t,a_t)p(a_t|s_t; \theta)]p(a_T |s_T ; \theta)\)</span>.</p><h2 id="intuitive-analysis-of-gradient-estimators">Intuitive Analysis ofGradient Estimators</h2><p>The gradients <span class="math inline">\(\nabla_{\theta}\mathbb{E}_{p(x;\theta)}[f(x)]\)</span> can be computed in two ways:</p><ul><li><p>Derivatives of Measure. The gradient can be computed bydifferentiation of the measure <span class="math inline">\(p(x;\theta)\)</span>. Gradient estimators in this class include the scorefunction estimator and the measure-valued gradient.</p></li><li><p>Derivatives of Paths. The gradient can be computed bydifferentiation of the cost <span class="math inline">\(f(x)\)</span>,which encodes the pathway from parameters <span class="math inline">\(\theta\)</span>, through the random variable <span class="math inline">\(x\)</span>, to the cost value, such as thepathwise gradient, harmonic gradient estimators and finite dfferences,and Malliavin-weighted estimators.</p></li></ul><p>We focus on three classes of gradient estimators: the score function,pathwise and measure-valued gradient estimators. <strong>All threeestimators satisfy two desirable properties: consistent and unbiased;but they differ in their variance behaviour and in their computationalcost.</strong></p><h3 id="intuitive-comparision">Intuitive comparision</h3><p>Consider the stochastic gradient problem (2) that uses Gaussianmeasures for three simple families of cost functions, quadratics,exponentials and cosines: <span class="math display">\[\eta = \nabla_{\theta} \int \mathcal{N}(x | \mu, \sigma^2) f(x; k)\text{d}x; \quad \theta \in \{\mu, \sigma \}; \quad f \in \{ (x-k)^2,\exp(-kx^2), \cos(kx)\}.\]</span></p><p><img src="/2023/10/21/Monte-Carlo-Gradient-Estimation/figure2.png"></p><p><img src="/2023/10/21/Monte-Carlo-Gradient-Estimation/figure3.png"></p><p>The computational cost:</p><ul><li><p>Both of the score-function and pathwise estimators can becomputed using a single sample in the Monte Carlo estimator (N = 1),even for multivariate distributions, making them computationallycheap.</p></li><li><p>The measure-valued derivative estimator will require 2Devaluations of the cost function for D dimensional parameters, and forthis the reason will typically not be preferred in high-dimensionalsettings.</p></li><li><p>If the cost function is not differentiable, then the pathwisegradient will not be applicable.</p></li></ul><p>Several criteria to be judged when choosing an unbiased gradientestimator:</p><ul><li>computational cost;</li><li>implications on the use of differentiable and non-differentiablecost functions;</li><li>the change in behaviour as the cost itself changes;</li><li>the availability of effective variance reduction techniques toachieve low variance.</li></ul><h1 id="score-function-gradient-estimators-likelihood-ratio-method-reinforce-estimator">ScoreFunction Gradient Estimators (likelihood ratio method, REINFORCEestimator)</h1><h2 id="score-function">Score function</h2><p>The score function is the derivative of the log-probability of thedistribution <span class="math inline">\(\nabla_{\theta} \log p(x;\theta)\)</span> with respect to its parameters <span class="math inline">\(\theta\)</span>: <span class="math display">\[\nabla_{\theta} \log p(x; \theta) = \frac{\nabla_{\theta} p(x;\theta)}{p(x; \theta)}.\]</span></p><p>properties: 1, Its expectation is zero: <span class="math display">\[\mathbb{E}_{p(x; \theta)} [\nabla_{\theta} \logp(x; \theta)] = \int p(x; \theta) \frac{\nabla_{\theta} p(x;\theta)}{p(x; \theta)} \text{d}x =  \nabla_{\theta} \int p(x; \theta)\text{d}x = \nabla_{\theta} 1 = 0.\]</span></p><p>2, Its variance is the Fisher information matrix.</p><p>Using the score function, we can derive a general-purpose estimatorfor the sensitivity analysis of eq. (2): <span class="math display">\[\begin{aligned}\eta &amp;= \nabla_{\theta} \mathbb{E}_{p(x; \theta)} [f(x)] \\&amp;= \nabla_{\theta} \int p(x; \theta) f(x) \text{d}x \\&amp;= \int f(x) \nabla_{\theta} p(x; \theta) \text{d}x \\&amp;= \int p(x; \theta) f(x) \nabla_{\theta} \log p(x; \theta)\text{d}x \\&amp;= \mathbb{E}_{p(x; \theta)} [f(x) \nabla_{\theta} \log p(x;\theta)].\end{aligned}\]</span> The form is what we need - a product of a distribution we cansample from and a function we can evaluate. Then, use the Monte Carloestimator, we have <span class="math display">\[\bar{\eta} = \frac{1}{N} \sum_{n=1}^{N} f(\hat{x}^{(n)}) \nabla_{\theta}\log p(\hat{x}^{(n)}; \theta), \quad \hat{x}^{(n)} \sim p(x; \theta).\]</span></p><p><strong>Notice that, in the third line, we have exchanged the orderof the integral and the derivative. We will discuss the validity of thisexchange later.</strong></p><h2 id="estimator-properties">Estimator Properties</h2><h3 id="unbiasedness">Unbiasedness</h3><p>When the interchange between differentiation and integration isvalid, we will obtain an <strong>unbiased</strong> estimator of thegradient. Intuitively, since differentiation is a process of limits, thevalidity of the interchange will relate to the conditions for which itis possible to exchange limits and integrals, in such cases most oftenrelying on the use of the <strong>dominated convergence theorem or theLeibniz integral rule</strong> (Flanders, 1973; Grimmett and Stirzaker,2001). The interchange will be valid if the following conditions aresatisfied:</p><ol type="a"><li><p>The measure <span class="math inline">\(p(x; \theta)\)</span> iscontinuously differentiable with respect to <span class="math inline">\(\theta\)</span>;</p></li><li><p>The product <span class="math inline">\(f(x) p(x;\theta)\)</span> is both integrable and differentiable for <span class="math inline">\(\theta\)</span>;</p></li><li><p>There exists an integrable function <span class="math inline">\(g(x)\)</span> such that <span class="math inline">\(\sup_{\theta} \|f(x) \nabla_{\theta} p(x; \theta)\|_1 \leq g(x)\)</span> for <span class="math inline">\(\forallx\)</span>.</p></li></ol><p>These assumptions usually hold in machine learning applications.</p><h3 id="abosolute-continuity">Abosolute Continuity</h3><ul><li><p>Example (Bounded support). Consider the score-function estimatorfor a cost <span class="math inline">\(f(x) = x\)</span> anddistribution <span class="math inline">\(p(x; \theta) = \frac{1}{\theta}1_{0 &lt; x &lt; \theta}\)</span>, which is differential in <span class="math inline">\(\theta\)</span> when <span class="math inline">\(x\in (0, \theta)\)</span>; the score function is <span class="math inline">\(\nabla_{\theta} \log p(x; \theta) =-\frac{1}{\theta}\)</span>. The true gradient is: <span class="math inline">\(\nabla_{\theta} \mathbb{E}_{p(x;\theta)} [x] =\nabla_{\theta} (\frac{1}{\theta} \int_{0}^{\theta} \frac{x^2}{2}) =\frac{1}{2}\)</span>. The score-funtion gradient is: <span class="math inline">\(\mathbb{E}_{p(x;\theta)} [x \frac{-1}{\theta}] =-\frac{\theta/2}{\theta} = -\frac{1}{2}\)</span>.</p><p>Why the score-function estimator fails to provide the correctgradient? The reason is that the measure is not absolutely continuouswith respect to <span class="math inline">\(\theta\)</span> at theboundary of the support.</p></li></ul><p>Let's explain the absolute continuity in detail. <span class="math display">\[\begin{aligned}  \nabla_{\theta} \mathbb{E}_{p(x;\theta)}[f(x)] &amp;= \int\nabla_{\theta} p(x;\theta) f(x) \text{d}x \\&amp;= \int \lim_{h \to 0} \frac{p(x; \theta + h) - p(x;\theta)}{h} f(x)\text{d}x \\&amp;= \lim_{h \to 0} \frac{1}{h} \int p(x; \theta) \frac{p(x; \theta +h) - p(x;\theta)}{p(x;\theta)} f(x) \text{d}x\\&amp;= \lim_{h \to 0} \frac{1}{h} \int p(x; \theta) (\frac{p(x; \theta +h)}{p(x;\theta)}-1) f(x)  \text{d}x\\&amp;= \lim_{h \to 0} \frac{1}{h}(\mathbb{E}_{p(x;\theta)}[\omega(\theta, h) f(x)] -\mathbb{E}_{p(x;\theta)}[f(x)])   \end{aligned}\]</span> where the ratio <span class="math inline">\(\omega(\theta, h):= \frac{p(x; \theta+h)}{p(x;\theta)}\)</span>. The estimator makes animplicit assumption of absolute continuity, where <strong>we require<span class="math inline">\(p(x; \theta+h) &gt; 0\)</span> for allpoints where <span class="math inline">\(p(x; \theta) &gt;0\)</span>.</strong> Not all distributions of interest satisfy thisproperty, and failures of absolute continuity can result in a biasedgradient.</p><h3 id="estimator-variance">Estimator Variance</h3><p>Define the estimator mean as <span class="math inline">\(\mu(\theta):= \mathbb{E}_{p(x;\theta)}[\bar{\eta}_N]\)</span>, for <span class="math inline">\(N=1\)</span>, The variance of the score functionestimator is: <span class="math display">\[\begin{aligned}\text{Var}_{p(x;\theta)}(\bar{\eta}_1) &amp;=\mathbb{E}_{p(x;\theta)}[(f(x) \nabla_{\theta} \log p(x; \theta))^2] -\mu(\theta)^2,\end{aligned}\]</span> or <span class="math display">\[\begin{aligned}\text{Var}_{p(x;\theta)}(\bar{\eta}_1) &amp;= \lim_{h \to 0} \frac{1}{h}\mathbb{E}_{p(x;\theta)}[(\omega(\theta, h) - 1)^2f(x)^2] -\mu(\theta)^2, \\&amp; \geq \sup_{h} \frac{(\mu(\theta + h) -\mu(\theta))^2}{\mathbb{E}_{p(x;\theta)}[\omega(\theta, h) - 1]^2}.\end{aligned}\]</span> Three sources of variance:</p><ul><li><p>importance ratio <span class="math inline">\(\omega(\theta,h)\)</span> (the need for absolute continuity);</p></li><li><p>The dimensionality of the parameters <span class="math inline">\(x\)</span>;</p></li><li><p>The variance of the cost function <span class="math inline">\(f(x)\)</span>.</p></li></ul><p><img src="/2023/10/21/Monte-Carlo-Gradient-Estimation/figure4.jpg"></p><h3 id="computational-cost">Computational Cost</h3><p>The computational cost of the score function estimator is low, it isthe order of <span class="math inline">\(O(N(D+L))\)</span> for <span class="math inline">\(N\)</span> samples, <span class="math inline">\(D\)</span> dimensional distributional parameters<span class="math inline">\(\theta\)</span> and <span class="math inline">\(L\)</span> is the cost of evaluating the costfunction.</p><h3 id="conclusion">Conclusion</h3><ul><li><p>The score funtion only need the final value of the cost in itscomputation and it makes no assumptions about the internal structure ofthe cost function. Any type of cost function can be used.</p></li><li><p>The measure must be differentiable with respect to the parameters<span class="math inline">\(\theta\)</span>, and we can easily samplefrom the measure. It is applicable to both discrete and continuousdistributions.</p></li><li><p>The estimator can be implemented using only a single sample,making it computationally efficient.</p></li><li><p>There are too many sources of variance, we can use some variancereduction techniques to reduce the variance.</p></li></ul><h1 id="pathwise-gradient-estimators">Pathwise Gradient Estimators</h1><h2 id="sampling-paths-sampling-process">Sampling Paths (samplingprocess)</h2><p>For continuous distribution, the alternative way to generate samples<span class="math inline">\(\hat{x}\)</span> from the distribution <span class="math inline">\(p(x; \theta)\)</span> is to sample from a simplerbase distribution <span class="math inline">\(p(\epsilon)\)</span> whichis independent of the parameters <span class="math inline">\(\theta\)</span>, and then transform the samplesthrough a deterministic <strong>path</strong> <span class="math inline">\(g(\epsilon;  \theta)\)</span>: <span class="math display">\[\hat{x} \sim p(x; \theta) \quad \equiv \quad \hat{x} = g(\epsilon;\theta), \quad \epsilon \sim p(\epsilon).\]</span></p><p>This transformation is described by the rule for the change ofvariables for probability: <span class="math display">\[p(x; \theta) = p(\epsilon) |\nabla_{\epsilon} g(\epsilon;\theta)|^{-1}.\]</span></p><h3 id="one-liners-reparameterization-trick">One-liners[Reparameterization Trick]</h3><p>One whidely-known example is sampling from a multivariate Gaussiandistribution <span class="math inline">\(p(\bf x; \bf \theta) =\mathcal{N}(\mathbf x|\mathbf \mu, \mathbf \Sigma)\)</span>:</p><ol type="1"><li>First sample from a standard Gaussian distribution <span class="math inline">\(p(\mathbf \epsilon) = \mathcal{N}(\mathbf\epsilon|\mathbf 0, \mathbf I)\)</span>;</li><li>Then transform the samples through the local-scale transformation<span class="math inline">\(g(\epsilon; \theta) = \mu + \mathbf L\epsilon\)</span>, where <span class="math inline">\(\mathbf{LL}^T =\mathbf \Sigma\)</span>.</li></ol><p><span class="math display">\[\hat{x} = \mu + \mathbf L \epsilon, \quad \epsilon \sim\mathcal{N}(\mathbf 0, \mathbf I), \quad \mathbf L \mathbf L^T = \mathbf\Sigma.\]</span></p><p><img src="/2023/10/21/Monte-Carlo-Gradient-Estimation/figure7.png"></p><p>Many such transformations exist for common distributions, includingDirichlet, Gamma, and many others. These types of transformations arecalled <strong>one-liners</strong> because they can be implemented in asingle line of code.</p><p>The expectation of eq. (1) is then: <span class="math display">\[\begin{aligned}\mathbb{E}_{p(x; \theta)} [f(x)] = \mathbb{E}_{p(\epsilon)}[f(g(\epsilon; \theta))]\end{aligned}\]</span> It is often used in Monte Carlo methods, and is called<strong>reparameterisation</strong> trick.</p><h2 id="gradient-estimators">Gradient Estimators</h2><p>Assume that we have a distribution <span class="math inline">\(p(x;\theta)\)</span> with known <strong>differentiable</strong> samplingpath <span class="math inline">\(g(\epsilon; \theta)\)</span> and basedistribution <span class="math inline">\(p(\epsilon)\)</span>. Thegradient estimator for the sensitivity analysis of eq. (2) is: <span class="math display">\[\begin{aligned}\eta &amp;= \nabla_{\theta} \mathbb{E}_{p(x; \theta)} [f(x)] \\&amp;= \nabla_{\theta} \int p(\epsilon) f(g(\epsilon; \theta))\text{d}\epsilon \\&amp;= \mathbb{E}_{p(\epsilon)}[\nabla_{\theta} f(g(\epsilon;\theta))]\\\bar{\eta} &amp;= \frac{1}{N} \sum_{n=1}^{N} \nabla_{\theta}f(g(\hat{\epsilon}^{(n)}; \theta)), \quad \hat{\epsilon}^{(n)} \simp(\epsilon).   \qquad (3)\end{aligned}\]</span></p><h3 id="decoupling-sampling-and-gradient-computation">DecouplingSampling and Gradient Computation</h3><p>The pathwise estimator (3) is limited to those distributions forwhich we simultaneously have a differential path and use this same pathto generate samples. But, <strong>sampling from a distribution may notprovide a differentiable path</strong>. Thus, we can expand theapplicability of the pathwise gradient by <strong>decoupling</strong>these two processes.</p><p>The pathwise estimator can be rewritten in a more general form:</p><p><span class="math display">\[\begin{aligned}  \eta &amp;= \nabla_{\theta} \mathbb{E}_{p(\mathbf x;\theta)}[f(\mathbfx)] \\  &amp;= \mathbb{E}_{p(\epsilon)}[\nabla_{\theta} f(\mathbf x)|_{\mathbfx = g(\epsilon;\theta)}] \\  &amp;= \int p(\epsilon) \nabla_{\mathbf x} f(\mathbf x)|_{\mathbf x =g(\epsilon;\theta)} \nabla_{\theta} g(\epsilon; \theta) \text{d}\epsilon \\  &amp;= \int p(\mathbf x;\theta) \nabla_{\mathbf x} f(\mathbfx)\nabla_{\theta} \mathbf x \text{d} \mathbf x \\  &amp;= \mathbb{E}_{p(\mathbf x;\theta)}[\nabla_{\mathbf x} f(\mathbfx) \nabla_{\theta} \mathbf x].\end{aligned}\]</span></p><p>One way to compute <span class="math inline">\(\nabla_{\theta}\mathbf x\)</span> is to use <span class="math inline">\(\nabla_{\theta}g(\epsilon; \theta)\)</span>, but this form is not alwaysconvenient.Another way to compute <span class="math inline">\(\nabla_{\theta} \mathbf x\)</span> is to use theinverse of the path <span class="math inline">\(g^{-1}(x;\theta)\)</span>. <span class="math inline">\(g^{-1}(x; \theta)\)</span>can be thought as the 'standardisation path' of the random variable--that is the transformation that removes the dependence of the samplingon the distribution parameters, standardising it to a zero mean unitvariance-like form.</p><p>Consider the equation <span class="math inline">\(\epsilon =g^{-1}(x; \theta)\)</span>, evaluating the total derivative on bothsides: <span class="math display">\[\begin{aligned}  \nabla_{\theta} \epsilon &amp;= \nabla_{\theta} g^{-1}(x;\theta) \\  0 &amp;= \nabla_{x} g^{-1}(x;\theta) \nabla_{\theta} x +\nabla_{\theta} g^{-1}(x;\theta) \\  \text{thus,} \nabla_{\theta} x &amp;= - (\nabla_{x}g^{-1}(x;\theta))^{-1} \nabla_{\theta} g^{-1}(x; \theta).\end{aligned}\]</span></p><p>In this form, we can apply pathwise gradient estimator to a far widerset of distributions and paths, such as for the Beta, Gamma, andDirichlet distributions.</p><ul><li>Example (Univariate distributions). For univariate distribution<span class="math inline">\(p(x;\theta)\)</span>, we can use thesampling path given by the inverse CDF: <span class="math inline">\(x =g(\epsilon;\theta) = F^{-1}(\epsilon; \theta)\)</span>, where <span class="math inline">\(\epsilon \sim \mathcal{U}[0, 1]\)</span>.Computing the derivative <span class="math inline">\(\nabla_{\theta} x =\nabla_{\theta} F^{-1} (\epsilon; \theta)\)</span> is often complicatedand expensive. We can obtain an alternative expression by consideringthe inverse path <span class="math inline">\(g^{-1}(x;\theta) = F(x;\theta)\)</span>, we have: <span class="math display">\[\nabla_{\theta} x = -\frac{\nabla_{\theta} F(x;\theta)}{\nabla_x F(x;\theta)} = - \frac{\nabla_{\theta} F(x;\theta)}{p(x; \theta)}.\]</span></li></ul><h3 id="bias-and-variance">Bias and variance</h3><p>When deriving the pathwise estimator, we exploited an interchangebetween differentiation and integration. If this interchange is valid,then the estimator is <strong>unbiased</strong>.</p><p>The variance of the pathwise estimator can be shown to be bounded bythe squared Lipschitz constant of the cost function <span class="math inline">\(f(x)\)</span>. (1) The variance bounds that existare independent of the dimensionality of the parameters <span class="math inline">\(\theta\)</span>, which means we can getlow-variance gradient estimates, even in high-dimensional space. (2) Asthe cost funtion becomes highly-variable, i.e., the Lipschitz constantincreases, the variance of the pathwise estimator can be higher thanthat of the score function methods.</p><p>The pathwise estimator will not always have lower variance whencompared to other methods since the variance is bounded by the Lipschitzconstant of the cost function.</p><h3 id="computational-cost-1">Computational cost</h3><p>The pathwise gradient estimator is restricted to differentiable costfunctions, which is a limitation when compared to the score functionestimator. Rapid convergence can be obtained even when using only asingle sample to compute the gradient, as is often done in practice.There is a trade-off between the number of samples used and theLipschitz constant of the cost function, and may require more samples tobe used for functions with higher Lipschitz constants. Thisconsideration is why we will find that regularisation that promotessmoothness of the functions we learn is important for successfulapplications.</p><p>The computational cost of the pathwise estimator is the same as thescore function estimator and is of the order <span class="math inline">\(O(N(D+L))\)</span> for <span class="math inline">\(N\)</span> samples, <span class="math inline">\(D\)</span> dimensional distributional parameters<span class="math inline">\(\theta\)</span> and <span class="math inline">\(L\)</span> is the cost of evaluating the costfunction and its gradient.</p><h3 id="conclusion-1">Conclusion</h3><ul><li>The pathwise estimator is only applicable to differentiable costfunctions.</li><li>When using the pathwise estimator, we do not need to know themeasure <span class="math inline">\(p(x; \theta)\)</span>, but we needto know the deterministic and differentiable sampling path <span class="math inline">\(g(\epsilon; \theta)\)</span> and the basedistribution <span class="math inline">\(p(\epsilon)\)</span>.</li><li>The estimator can be implemented using only a single sample, makingit computationally efficient.</li><li>We might need to control the smoothness of the cost function toensure that the variance of the estimator is low, and may need to employvariance reduction techniques.</li></ul><h3 id="gumbel-softmax-estimator">Gumbel softmax estimator</h3><h2 id="measure-valued-gradient-estimators">Measure-Valued GradientEstimators</h2><h3 id="weak-derivatives-measure-valued-derivatives">Weak Derivatives(measure-valued derivatives)</h3><p>Consider the derivative of a density function <span class="math inline">\(p(x; \theta)\)</span> with respect to a singleparameter <span class="math inline">\(\theta_i\)</span>, with <span class="math inline">\(i\)</span> the index on the set of distributionalparameters. The derivative <span class="math inline">\(\nabla_{\theta_i}p(x;\theta)\)</span> is not a density, since it may have negative valuesand does not integrate to one. Using the properties of signed measures,we can always decompose this derivative into a difference of twodensities, each multiplied by a constant: <span class="math display">\[\nabla_{\theta_i} p(x;\theta) = c_{\theta_i}^+ p^+(x;\theta) -c_{\theta_i}^- p^-(x;\theta),\]</span> where <span class="math inline">\(p^+, p^-\)</span> aredensities, referred to as the positive and negative parts of <span class="math inline">\(p\)</span>. By integrating both sides of theequation, we can obtain the constants <span class="math inline">\(c_{\theta_i}^+, c_{\theta_i}^-\)</span>: <span class="math display">\[\begin{aligned}  &amp;\int \nabla_{\theta_i} p(x;\theta) \text{d}x = \nabla_{\theta_i}\int p(x;\theta) \text{d}x  = 0; \\  &amp;\int c_{\theta_i}^+ p^+(x;\theta) - c_{\theta_i}^- p^-(x;\theta)\text{d}x = c_{\theta_i}^+ - c_{\theta_i}^- .\end{aligned}\]</span> Thus, we have: <span class="math display">\[c_{\theta_i}^+ = c_{\theta_i}^- := c_{\theta_i}\]</span> The decomposition of the derivative becomes: <span class="math display">\[\nabla_{\theta_i} p(x;\theta) = c_{\theta_i} (p^+(x;\theta) -p^-(x;\theta)).\]</span> The triple <span class="math inline">\((c_{\theta_i}, p^+,p^-)\)</span> is called the i-th <strong>weak derivative</strong> of<span class="math inline">\(p\)</span> with respect to <span class="math inline">\(\theta_i\)</span>.</p><p>For multivariate parameters <span class="math inline">\(\theta\)</span>, each dimension has onetriple.</p><p><img src="/2023/10/21/Monte-Carlo-Gradient-Estimation/figure5.png"></p><ul><li>The derivative is weak because we do not require the density to bedifferentiable.</li><li>The weak derivative is not unique, but always exists and can beobtained using the Hahn-Jordan decomposition of a signed measure intotwo measures that have complementary support. see <a href="https://www.math.uwaterloo.ca/~beforres/PMath451/Course_Notes/Chapter4.pdf">signedmeasure</a>.</li></ul><h3 id="deriving-the-estimator">Deriving the estimator</h3><p>For D-dimensional parameters <span class="math inline">\(\theta\)</span>, we can write the gradient of theexpectation as: <span class="math display">\[\begin{aligned}\eta_i &amp;= \nabla_{\theta_i} \mathbb{E}_{p(x;\theta)}[f(x)] =\nabla_{\theta_i} \int p(x;\theta) f(x) \text{d}x \\&amp;= \int \nabla_{\theta_i} p(x;\theta) f(x) \text{d}x \\&amp;= \int c_{\theta_i} (p_i^+(x;\theta) - p_i^-(x;\theta)) f(x)\text{d}x \\&amp;= c_{\theta_i} (\mathbb{E}_{p_i^+(x;\theta)}[f(x)] -\mathbb{E}_{p_i^-(x;\theta)}[f(x)]). \\\bar{\eta}_{i, N} &amp;=  \frac{c_{\theta_i}}{N} (\sum_{n=1}^{N}f(\hat{x}^{+(n)}) - \sum_{n=1}^{N} f(\hat{x}^{-(n)})), \quad\hat{x}^{+(n)} \sim p_i^+(x;\theta), \quad \hat{x}^{-(n)} \simp_i^-(x;\theta).\end{aligned}\]</span> The positive and negative components may be differentdepending on which parameter of the measure the derivative is taken withrespect to. The constant <span class="math inline">\(c_{\theta_i}\)</span> will also change dependingthe parameter being differentiated.</p><ul><li>Example (Bernoulli measure-valued gradient). Consider the Bernoullidistribution <span class="math inline">\(p(x;\theta) = \theta^x(1-\theta)^{1-x}\)</span>, with <span class="math inline">\(x \in \{0,1\}\)</span> and <span class="math inline">\(\theta \in [0, 1]\)</span>.By taking the derivative with respect to <span class="math inline">\(\theta\)</span>, we have: <span class="math display">\[\begin{aligned}  \nabla_{\theta} \int p(x;\theta) f(x) \text{d}x&amp;=  \nabla_{\theta} (\theta f(1) + (1-\theta) f(0))\\  &amp;= f(1) - f(0).\end{aligned}\]</span> By weak derivative, we have: <span class="math display">\[\begin{aligned}\nabla_{\theta} \int p(x;\theta) f(x) \text{d}x &amp;= \int\nabla_{\theta} p(x;\theta) f(x) \text{d}x \\&amp;= \int \delta_1 f(x) - \delta_0 f(x) \text{d}x \\&amp;= f(1) - f(0).\end{aligned}\]</span> which is the same as the original gradient.</li></ul><h4 id="vector-case">Vector case</h4><p>If the measure is a factorised distribution <span class="math inline">\(p(x;\theta) = \prod_d p(x_d | \theta_d)\)</span>,then the positive component and negative component of the weakderivative will itself factorise across the dimensions. For the positivecomponent, this decomposition will be <span class="math inline">\(p^+_i(x;\theta) =p(x_{-i})p^+_i(x_i;\theta_i)\)</span>, which is the product of themarginal distribution <span class="math inline">\(p(x_{-i})\)</span> andthe positive component of the weak derivative with respect to <span class="math inline">\(\theta_i\)</span>. The negative component will be<span class="math inline">\(p^-_i(x;\theta) =p(x_{-i})p^-_i(x_i;\theta_i)\)</span>, which is the product of themarginal distribution <span class="math inline">\(p(x_{-i})\)</span> andthe negative component of the weak derivative with respect to <span class="math inline">\(\theta_i\)</span>.</p><h3 id="estimator-properties-1">Estimator Properties</h3><h4 id="domination">Domination</h4><p>Remember in the score function estimator, we need the measure to beabsolutely continuous with respect to <span class="math inline">\(\theta\)</span>. We explored one example where wewere unable to ensure domination, because no bounding constant appliesat the boundaries of the domain. For weak derivatives, we can alwaysensure the correctness of the interchange between differentiation andintegration： the fundamental property of weak derivatives states thatif the triple <span class="math inline">\((c, p^+, p^-)\)</span> is theweak derivative of <span class="math inline">\(p(x;\theta)\)</span>,then for every <strong>bounded continuous</strong> function <span class="math inline">\(f(x)\)</span>, we have: <span class="math display">\[\nabla_{\theta} \int f(x)  p(x;\theta) \text{d}x = c_{\theta} [\int f(x)p^+(x;\theta) \text{d}x - \int f(x) p^-(x;\theta) \text{d}x].\]</span></p><ul><li>Example (Bounded support). Consider the measure-valued estimator fora cost function <span class="math inline">\(f(x) = x\)</span> anddistribution <span class="math inline">\(p(x; \theta) = \frac{1}{\theta}1_{\{0 &lt; x &lt; \theta\}}\)</span>, which is differential in <span class="math inline">\(\theta\)</span> when <span class="math inline">\(x\in (0, \theta)\)</span>; The measure-valued derivative is <span class="math display">\[\begin{aligned}    \nabla_{\theta} \int f(x) \mathcal{U}_{[0, \theta]}(x) \text{d}x&amp;= \nabla_{\theta} (\frac{1}{\theta} \int_{0}^{\theta} f(x)\text{d}x) = \frac{1}{\theta} f(\theta) - \frac{1}{\theta^2}\int_{0}^{\theta} f(x) \text{d} x \\    &amp;= \frac{1}{\theta} (\int f(x) \delta_{\theta}(x) \text{d}x -\int f(x) \mathcal{U}_{[0, \theta]}(x) \text{d}x) \\\end{aligned}\]</span></li></ul><p>The measure-valued derivative is given by the triple <span class="math inline">\((\frac{1}{\theta}, \delta_{\theta},\mathcal{U}_{[0, \theta]})\)</span>. For specific cost function <span class="math inline">\(f(x) = x\)</span>, we have:</p><p>The true gradient is: <span class="math inline">\(\nabla_{\theta}\mathbb{E}_{p(x;\theta)} [x] = \nabla_{\theta} (\frac{1}{\theta}\int_{0}^{\theta} \frac{x^2}{2}) = \frac{1}{2}\)</span>. Themeasure-valued gradient is: <span class="math inline">\(\frac{1}{\theta}(\mathbb{E}_{\delta_{\theta}} [x] - \mathbb{E}_{\mathcal{U}_{[0,\theta]}[x]}) = \frac{1}{\theta} (\theta - \frac{\theta}{2}) =\frac{1}{2}.\)</span></p><h4 id="bias-and-variance-1">Bias and variance</h4><p>For bounded and continuous cost functions <span class="math inline">\(f\)</span>, by using the fundamental property ofweak derivatives, the measure-valued gradient estimator is<strong>unbiased</strong>.</p><p>The variance of the measure-valued gradient estimator is: <span class="math display">\[\begin{aligned}\text{Var}_{p(x;\theta)}[\bar{\eta}_1] =\text{Var}_{p^+(x;\theta)}[f(x)] + \text{Var}_{p^-(x;\theta)}[f(x)] - 2\text{Cov}_{p^+(x';\theta)p^-(x;\theta)} [f(x'), f(x)].\end{aligned}\]</span></p><ul><li>The variance depends on the choice of decomposition of the weakderivative into positive and negative components.</li><li>If the random variables can be 'coupled' in some way, where theyshare the same underlying source of randomness, this will reduce thevariance of the gradient estimator by increasing the covariance term.The most common way is to sample the variables <span class="math inline">\(x'\)</span> and <span class="math inline">\(x\)</span> using common random numbers. Anotherway is to use variance reduction techniques.</li><li>Figure 5 shows that the measure valued estimator is not sensitive tothe dimensionality of the parameters <span class="math inline">\(\theta\)</span>. It is however sensitive to themagnitude of the function.</li></ul><p><img src="/2023/10/21/Monte-Carlo-Gradient-Estimation/figure6.png"></p><h4 id="computational-cost-2">Computational cost</h4><p>Measure-valued gradients are much more computationally expensive thanthe score-function or pathwise gradients. This is because the gradientwe computed is the gradient for a single parameter: for every parameterwe require two evaluations of the cost function to compute its gradient.It is this structure of adapting the underlying sampling distributionsfor each parameter that leads to the low variance of the estimator butat the same time makes its application to high-dimensional parameterspaces prohibitive.</p><p>The computational cost of the measure-valued gradient estimator isthe order of <span class="math inline">\(O(2NDL)\)</span> for <span class="math inline">\(N\)</span> samples, <span class="math inline">\(D\)</span> dimensional distributional parameters<span class="math inline">\(\theta\)</span> and <span class="math inline">\(L\)</span> is the cost of evaluating the costfunction.</p><h3 id="conclusion-2">Conclusion</h3><ul><li>The measure-valued estimator can be used with any type of costfunction, differentiable or not. As long as we can evaluate the costfunction repeatedly for different inputs.</li><li>It is applicable to both discrete and continuous distributions.</li><li>Computationally expensice in high-dimensional parameter spaces.</li><li>We need methods to sample from the positive and negativemeasures.</li><li>Using the weak derivative <strong>requires manual derivation of thedecomposition at first</strong>, although for many common distributionsthe weak-derivative decompositions are known.</li></ul><h2 id="variance-reduction-techniques">Variance ReductionTechniques</h2><p>The gradient variance is one of the principal sources of performanceissues. This paper introduces four common methods to reduce the varianceof gradient estimators: large-samples, coupling, conditioning, andcontrol variates.</p><h3 id="large-samples">Large-Samples</h3><p>The simplest way to reduce the variance of the gradient estimator isto use more samples. The variance of an estimators will shrinks as <span class="math inline">\(O(\frac{1}{N})\)</span>, where <span class="math inline">\(N\)</span> is the number of samples. However, thecomputational cost will increase linearly with the number of samples.The computational cost can be reduces by parallelising the computationof the gradient across multiple processors. Sometimes, increasing thenumber of Monte Carlo samples will not be an option, such as when thecost function involves a real-world experiment or interaction with auser.</p><h3 id="coupling-and-common-random-numbers">Coupling and Common randomnumbers</h3><p>When consider the difference between two expectations of a function<span class="math inline">\(f(x)\)</span> under different butclosely-related distributions <span class="math inline">\(p_1(x)\)</span> and <span class="math inline">\(p_2(x)\)</span>: <span class="math display">\[\begin{aligned}\eta = \mathbb{E}_{p_1(x)}[f(x)] - \mathbb{E}_{p_2(x)}[f(x)]\end{aligned}\]</span></p><p>The direct method to compute the difference is to estimate eachexpectation separately using Monte Carlo sampling, and then compute thedifference between the two estimates： <span class="math display">\[\begin{aligned}\bar{\eta}_{ind} = \frac{1}{N} \sum_{n=1}^{N} f(\hat{x}_1^{(n)}) -\frac{1}{N} \sum_{n=1}^{N} f(\hat{x}_2^{(n)}),\end{aligned}\]</span> where <span class="math inline">\(\hat{x}_1^{(n)} \simp_1(x)\)</span> and <span class="math inline">\(\hat{x}_2^{(n)} \simp_2(x)\)</span>.</p><p>We can achieve a simple form of variance reduction by coupling <span class="math inline">\(\hat{x}_1^{(n)}\)</span> and <span class="math inline">\(\hat{x}_2^{(n)}\)</span>, so that each pair <span class="math inline">\((\hat{x}_1^{(n)}, \hat{x}_2^{(n)})\)</span> issampled from some joint distribution <span class="math inline">\(p_{12}(x_1, x_2)\)</span> with marginals <span class="math inline">\(p_1(x)\)</span> and <span class="math inline">\(p_2(x)\)</span>. The variance of the coupledestimator is: <span class="math display">\[\begin{aligned}\text{Var}_{p_12(x_1, x_2)}[\bar{\eta}_{cpl}] &amp;=\text{Var}_{p_12(x_1, x_2)}[f(x_1) - f(x_2)] \\&amp;= \text{Var}_{p_1(x_1)}[f(x_1)] + \text{Var}_{p_2(x_2)}[f(x_2)] - 2\text{Cov}_{p_12(x_1, x_2)}[f(x_1), f(x_2)] \\&amp;= \text{Var}_{p_1(x_1)p_2(x_2)}[\bar{\eta}_{ind}] - 2\text{Cov}_{p_1(x_1)}[f(x_1), f(x_2)].\end{aligned}\]</span> Thus, to reduce the variance we need to choose a coupling<span class="math inline">\(p_{12}(x_1, x_2)\)</span> such that <span class="math inline">\(f(x_1)\)</span> and <span class="math inline">\(f(x_2)\)</span> are positively correlated. Themost common way to achieve this is to use <strong>common randomnumbers</strong> when <span class="math inline">\(p_1(x_1)\)</span> and<span class="math inline">\(p_2(x_2)\)</span> are close or in a relatedfamily of distributions. This means that the random numbers used togenerate <span class="math inline">\(x_1\)</span> and <span class="math inline">\(x_2\)</span> are the same. For example, in theunivariate case, we can first sample <span class="math inline">\(u \sim\mathcal{U}[0,1]\)</span> and then apply the inverse CDF transformationto obtain <span class="math inline">\(x_1 = F_{p_1}^{-1}(u)\)</span> and<span class="math inline">\(x_2 = F_{p_2}^{-1}(u)\)</span>.</p><ul><li>Coupling may not always reduce the variance of the measure valuedestimator, depending on the cost function.</li></ul><h3 id="conditioning">Conditioning</h3><p>Rao-Blackwellisation is a variance reduction technique thatprobabilistically conditions our estimator on a subset of dimensions andintegrates out the remaining dimensions.</p><p>Assume that the dimensions <span class="math inline">\(\{1,...,D\}\)</span> of <span class="math inline">\(x\)</span> are partitionedinto a set of dimensions <span class="math inline">\(\mathcal{S}\)</span> and its complement <span class="math inline">\(\mathcal{S}^c = {1,...,D}\ \mathcal{S}\)</span>.The expectation <span class="math inline">\(g(x_{\mathcal{S}^c}) =\mathbb{E}_{p(x_{\mathcal{S}})}[f(x)|x_{\mathcal{S}^c}]\)</span>. We canestimate <span class="math inline">\(\mathbb{E}_{p(x)}[f(x)]\)</span> byperforming Monte Carlo integration over the dimensions <span class="math inline">\(\mathcal{S}^c\)</span>: <span class="math display">\[\begin{aligned}\bar{g}_{N} &amp;= \frac{1}{N} \sum_{n=1}^{N}g(\hat{x}_{\mathcal{S}^c}^{(n)}) \\&amp;= \frac{1}{N} \sum_{n=1}^{N}\mathbb{E}_{p(x_{\mathcal{S}})}[f(x)|\hat{x}_{\mathcal{S}^c}^{(n)}],  \quad\hat{x}_{\mathcal{S}^c}^{(n)} \sim p(x_{\mathcal{S}^c}).\end{aligned}\]</span></p><p>By law of total expectation <span class="math inline">\(\text{Var}(Y)= \mathbb{E}[\text{Var}(Y | X)] + \text{Var}(\mathbb{E}[Y |X])\)</span>, we have: <span class="math display">\[\begin{aligned}\text{Var}_{p(x)}[f(x)] &amp;=\mathbb{E}_{p(x_{\mathcal{S}^c})}[\text{Var}_{p(x_{\mathcal{S}})}[f(x)|x_{\mathcal{S}^c}]]+\text{Var}_{p(x_{S^c})}[\mathbb{E}_{p(x_{\mathcal{S}})}[f(x)|x_{\mathcal{S}^c}]]\\&amp;=\mathbb{E}_{p(x_{\mathcal{S}^c})}[\text{Var}_{p(x_{\mathcal{S}})}[f(x)|x_{\mathcal{S}^c}]]+ \text{Var}_{p(x_{\mathcal{S}^c})}[g(x_{\mathcal{S}^c})] \\&amp; \geq \text{Var}_{p(x_{\mathcal{S}^c})}[g(x_{\mathcal{S}^c})]\end{aligned}\]</span></p><p>Thus, for unconditional one <span class="math inline">\(\bar{f}_N =\frac{1}{N}\sum_{n=1}^{N} f(\hat{x}^{(n)})\)</span>, we have: <span class="math display">\[\begin{aligned}\text{Var}_{p(x)}[\bar{f}_N] = \frac{1}{N} \text{Var}_{p(x)}[f(x)] \geq\frac{1}{N}  \text{Var}_{p(x_{\mathcal{S}^c})}[g(x_{\mathcal{S}^c})] =\text{Var}_{p(x_{\mathcal{S}^c})}[\bar{g}_{N}].\end{aligned}\]</span></p><ul><li>Conditional estimator has lower variance than the unconditionalestimator.</li><li>This technique is useful in practice only if we can compute theconditional expectation <span class="math inline">\(g(x_{\mathcal{S}^c})\)</span> efficiently.</li></ul><h2 id="control-variates">Control Variates</h2><p>Since all the gradient estimators have the same form <span class="math inline">\(\mathbb{E}_{p(x;\theta)}[f(x)]\)</span>, we willfocus on this general form. The strategy is to replace the function<span class="math inline">\(f(x)\)</span> in the expectation with asubstitute function <span class="math inline">\(\tilde{f}(x)\)</span>whose expectation is the same as <span class="math inline">\(\mathbb{E}_{p(x;\theta)}[f(x)]\)</span>, but whosevariance is lower.</p><p>If we have a function <span class="math inline">\(h(x)\)</span> witha known expectation <span class="math inline">\(\mathbb{E}_{p(x;\theta)}[h(x)]\)</span>, then wecan construct a new function <span class="math display">\[\tilde{f}(x) =f(x) - \beta(h(x)-\mathbb{E}_{p(x;\theta)}[h(x)]).\]</span> Here <span class="math inline">\(h(x)\)</span> is a control variate. <span class="math inline">\(\beta\)</span> is a coefficient that affects thestrength of the control variate. Then we can get a control variateestimator: <span class="math display">\[\begin{aligned}\bar{\eta}_{N} &amp;= \frac{1}{N} \sum_{n=1}^{N}\tilde{f}(\hat{x}^{(n)}) \\&amp;= \bar{f} - \beta (\bar{h} - \mathbb{E}_{p(x;\theta)}[h(x)]).\end{aligned}\]</span></p><h3 id="bias-consistency-and-variance">Bias, consistency andvariance</h3><ol type="1"><li><p>Unbiasedness. The control variate estimator is unbiased： <span class="math display">\[\begin{aligned}\mathbb{E}_{p(x;\theta)}[\bar{\eta}_{N}] &amp;=\mathbb{E}_{p(x;\theta)}[\bar{f} - \beta (\bar{h} -\mathbb{E}_{p(x;\theta)}[h(x)]) ]\\&amp;= \mathbb{E}_{p(x;\theta)}[\bar{f}] \\&amp;= \mathbb{E}_{p(x;\theta)}[f(x)].\end{aligned}\]</span></p></li><li><p>Consistency. The control variate estimator is consistent: <span class="math display">\[\lim_{N \to \infty} \bar{\eta}_{N} =\mathbb{E}_{p(x;\theta)}[\tilde{f}(x)] = \mathbb{E}_{p(x;\theta)}[f(x)].\]</span></p></li><li><p>Variance. The variance of the control variate estimator is (N=1):<span class="math display">\[\begin{aligned}\text{Var}_{p(x;\theta)}[\tilde{f}] &amp;= \text{Var}_{p(x;\theta)}[f -\beta (h - \mathbb{E}_{p(x;\theta)}[h(x)]) ]\\&amp;= \text{Var}_{p(x;\theta)}[f] + \beta^2 \text{Var}_{p(x;\theta)}[h]- 2 \beta \text{Cov}_{p(x;\theta)}[f, h].\end{aligned}\]</span> By minimising the right-hand side of the equation with respectto <span class="math inline">\(\beta\)</span>, we can obtain the optimalvalue of <span class="math inline">\(\beta\)</span>: <span class="math display">\[\beta^* = \frac{\text{Cov}_{p(x;\theta)}[f,h]}{\text{Var}_{p(x;\theta)}[h]} =\sqrt{\frac{\text{Var}_{p(x;\theta)}[f]}{\text{Var}_{p(x;\theta)}[h]}}\text{Corr}(f, h).\]</span> Using the optimal value of <span class="math inline">\(\beta\)</span>, the potential variance reductionis: <span class="math display">\[\begin{aligned}\frac{\text{Var}_{p(x;\theta)}[\tilde{f}]}{\text{Var}_{p(x;\theta)}[f]}= \frac{\text{Var}_{p(x;\theta)}[f - \beta (h -\mathbb{E}_{p(x;\theta)}[h(x)])]}{\text{Var}_{p(x;\theta)}[f]} = 1 -\text{Corr}(f, h)^2 \leq 1.\end{aligned}\]</span></p></li></ol><ul><li>The stronger the correlation between <span class="math inline">\(f\)</span> and <span class="math inline">\(h\)</span>, the greater the potential variancereduction.</li><li>In practice, the optimal <span class="math inline">\(\beta^*\)</span> will not be known and it can beestimated using the same <span class="math inline">\(N\)</span> samples.But the samples used to estimate <span class="math inline">\(\bar{h}\)</span> will introduce a bias because<span class="math inline">\(\bar{\beta}_N\)</span> and <span class="math inline">\(\bar{h}\)</span> will no longer be independent. Inpractice, thi bias is often negligible and can be controlled since itdecreases quickly as the number of samples <span class="math inline">\(N\)</span> increases.</li></ul><h3 id="multiple-and-non-linear-controls">Multiple and Non-linearControls</h3><h3 id="designing-control-variates">Designing Control Variates</h3><ol type="1"><li><p>Baselines. One simple way to reduce the variance of ascore-function gradient estimator is to use the score function itself asa control variate, since its expectation under the measure is zero. Themodified estimator is: <span class="math display">\[\begin{aligned}\bar{\eta}_{N} &amp;= \frac{1}{N} \sum_{n=1}^{N} (f(\hat{x}^{(n)}) -\beta)\nabla_{\theta} \log p(\hat{x}^n;\theta), \hat{x}^{(n)} \simp(x;\theta) \\\end{aligned}\]</span> In reinforcement learning, <span class="math inline">\(\beta\)</span> is called a baseline and it can beestimated with a running average of the cost. While this approach iseasier to implement than optimising <span class="math inline">\(\beta\)</span> to minimise variance, it is notoptimal and does not guarantee lower variance compared to the vanillascore-function estimator.</p></li><li><p>Bounds. We can use bounds on the cost function <span class="math inline">\(f\)</span> as ways of specifying the form of thecontrol variate <span class="math inline">\(h\)</span>. This isintuitive because it maintains a correlation between <span class="math inline">\(f\)</span> and <span class="math inline">\(h\)</span>, and if chosen well, may be easilyintegrable against the measure and available in closed form. Thisapproach requires more knowledge of the cost function, since we willneed to characterise the cost analytically in some way to bound it. Ingeneral, unless the bounds used are tight, they will not be effective ascontrol variates, since the gap between the bound and the true functionis not controllable and will not necessarily give the information neededfor variance reduction.</p></li><li><p>Delta method. The delta method is a way of constructing a controlvariate by using the Taylor expansion of the cost function. Thisrequires a cost function that is differentiable so that we can computethe second-order Taylor expansion, but can be an effective and verygeneral approach for variance reduction that allows easy implementation.It can be used for variance reduction in both the score-functionestimator (Paisley et al., 2012) and the pathwise estimator(Miller etal., 2017).</p></li></ol><ul><li><p>Example. Define <span class="math inline">\(\gamma(x)\)</span> isthe gradient of the cost function <span class="math inline">\(f(x)\)</span>, <span class="math inline">\(H(x)\)</span> is the Hessian of the cost function<span class="math inline">\(f(x)\)</span>. The second-order Taylorexpansion of a cost function expand around point <span class="math inline">\(\mu\)</span> and its derivate are <span class="math display">\[\begin{aligned}  h(x) &amp;= f(\mu) + (x - \mu)^T \gamma(\mu) + \frac{1}{2} (x - \mu)^TH(\mu) (x - \mu), \\  \nabla_{x} h(x) &amp;= \gamma(\mu)^T + (x - \mu)^T H(\mu).\end{aligned}\]</span></p><p>We can use this expansion directly as a control variate for thescore-function estimator:</p></li></ul><p><span class="math display">\[\begin{aligned}\bar{\eta}_{SF} &amp;= \nabla_{\theta} \mathbb{E}_{p(x;\theta)}[f(x)] \\&amp;= \nabla_{\theta} \mathbb{E}_{p(x;\theta)}[f(x) - \beta^T h(x)] +\beta^T \nabla_{\theta} \mathbb{E}_{p(x;\theta)}[h(x)] \\&amp;= \mathbb{E}_{p(x;\theta)}[(f(x) - \beta^T h(x))\nabla_{\theta}\log p(x;\theta)] + \beta^T \nabla_{\theta}\mathbb{E}_{p(x;\theta)}[h(x)].\end{aligned}\]</span> In the Gaussian mean-field variational inference that Paisleyet al. (2012) consider, the second term is known in closed-form andhence does not require Monte Carlo approximation. <span class="math inline">\(\beta\)</span> is a multivariate controlcoefficient and is estimated separately.</p><p>For the pathwise estimator, using the sampling path <span class="math inline">\(x = g(\epsilon; \theta)\)</span>, we have: <span class="math display">\[  \begin{aligned}    \bar{\eta}_{PW} &amp;= \nabla_{\theta}\mathbb{E}_{p(x;\theta)}[f(x)] \\    &amp;= \nabla_{\theta} \mathbb{E}_{p(x;\theta)}[f(x) - \beta^T h(x)]+ \beta^T \nabla_{\theta} \mathbb{E}_{p(x;\theta)}[h(x)] \\    &amp;= \nabla_{\theta} \mathbb{E}_{p(x;\theta)}[f(g(\epsilon;\theta)) - \beta^T h(g(\epsilon; \theta))] + \beta^T \nabla_{\theta}\mathbb{E}_{p(x;\theta)}[h(x)] \\    &amp;= \mathbb{E}_{p(\epsilon)}[\nabla_{x}f(x) \nabla_{\theta}g(\epsilon; \theta) - \beta \nabla_x h(x)\nabla_{\theta} g(\epsilon;\theta)] + \beta^T \nabla_{\theta} \mathbb{E}_{p(x;\theta)}[h(x)].  \end{aligned}  \]</span> Assume that the final term is known in closed-form and doesnot require stochastic approximation.</p><h2 id="guidance-in-choosing-gradient-estimators">Guidance in ChoosingGradient Estimators</h2><p>The authors provide some guidance in choosing gradientestimators.</p><ul><li><p>If our estimation problem involves continuous functions andmeasures that are continuous in the domain, then using the pathwiseestimator is a good default. It is relatively easy to implement and itsdefault implementation, without additional variance reduction, willtypically have variance that is low enough so as not to interfere withthe optimisation.</p></li><li><p>If the cost function is not differentiable or is a black-boxfunction then the score-function or the measure-valued gradients areavailable. If the number of parameters is low, then the measure-valuedgradient will typically have lower variance and would be preferred. Butif we have a high-dimensional parameter set, then the score-functionestimator should be used.</p></li><li><p>If we have no control over the number of times we can evaluate ablack-box cost function, effectively only allowing a single evaluationof it, then the score function is the only estimator of the three wereviewed that is applicable.</p></li><li><p>The score-function estimator should, by default, always beimplemented with at least some basic variance reduction. The simplestoption is to use a baseline control variate estimated with a runningaverage of the cost value. • When using the score-function estimator,some attention should be paid to the dynamic range of the cost functionand its variance, and ways found to keep its value bounded within areasonable range, e.g. by transforming the cost so that it is zero mean,or using a baseline.</p></li><li><p>For all estimators, track the variance of the gradients ifpossible and address high variance by using a larger number of samplesfrom the measure, decreasing the learning rate, or clipping the gradientvalues. It may also be useful to restrict the range of some parametersto avoid extreme values, e.g. by clipping them to a desiredinterval.</p></li><li><p>The measure-valued gradient should be used with some couplingmethod for variance reduction. Coupling strategies that exploitrelationships between the positive and negative components of thedensity decomposition, and which have shared sampling paths, are knownfor the commonly-used distributions.</p></li><li><p>If we have several unbiased gradient estimators, a convexcombination of them might have lower variance than any of the individualestimators.</p></li><li><p>If the measure is discrete on its domain then the score-functionor measure-valued gradient are available. The choice will again dependon the dimensionality of the parameter space.</p></li><li><p>In all cases, we strongly recommend having a broad set of teststo verify the unbiasedness of the gradient estimator whenimplemented.</p></li></ul><h1 id="reference">Reference</h1><p>(https://pages.stat.wisc.edu/~shao/stat609/stat609-07.pdf)</p><p>https://bochang.me/blog/posts/measure-val-grad/</p><p><a href="https://www.math.uwaterloo.ca/~beforres/PMath451/Course_Notes/Chapter4.pdf">signedmeasure</a></p>]]></content>
      
      
      <categories>
          
          <category> Machine Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Monte Carlo Gradient Estimator </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Introduction to Deep Learning</title>
      <link href="/2023/09/11/deep-learning-with-structures/"/>
      <url>/2023/09/11/deep-learning-with-structures/</url>
      
        <content type="html"><![CDATA[<h2 id="brief-intro-to-deep-learning">Brief Intro to Deep Learning</h2><p>Deep learning: - Data: large datasets, e.g., ImageNet, etc.; - Model:deep neural networks, e.g., ResNet-152, etc.; - Learning algorithm:backpropagation, i.e., stochastic gradient descent (SGD).</p><p>In recurrent neural networks (RNNs): <strong>same</strong> neuralnetwork gets reused many times. <span class="math display">\[h^t = F(x^t, h^{t-1}, W).\]</span></p><p>Back propagation: - Loss is always a scalar value; - For the backwardpropogation, the gradient is in the form: <span class="math inline">\(J^T v\)</span>, where <span class="math inline">\(v\)</span> is a vector, and <span class="math inline">\(J\)</span> is a Jacobian matrix, <span class="math inline">\(T\)</span> means transpose; - For the forwardpropogation, the gradient is in the form: <span class="math inline">\(Jv\)</span>. - In BF process, the shape of a Jacobian matrix is <span class="math inline">\(m \times n\)</span>, where <span class="math inline">\(m\)</span> is the dimension of output, and <span class="math inline">\(n\)</span> is the dimension of input.</p><p>Consider Vector-by-Matrix Gradients:</p><p>Case 1: <span class="math inline">\(\bm{z} =\mathbf{W}\bm{x}\)</span>, where <span class="math inline">\(\bm x \in\mathbb{R}^{n \times 1}\)</span>, <span class="math inline">\(\mathbf{W}\in \mathbb{R}^{m \times n}\)</span>, <span class="math inline">\(\bm z\in \mathbb{R}^{m \times 1}\)</span>. <span class="math display">\[\begin{aligned}\frac{\partial \bm{z}}{\partial \bm{x}} &amp;= \mathbf{W}, \\\end{aligned}\]</span></p><p>Case 2： <span class="math inline">\(\bm{z} = \bm{x}\mathbf{W}\)</span>, where <span class="math inline">\(\bm x \in\mathbb{R}^{1 \times m}\)</span>, <span class="math inline">\(\mathbf{W}\in \mathbb{R}^{m \times n}\)</span>, <span class="math inline">\(\bm z\in \mathbb{R}^{1 \times n}\)</span>. <span class="math display">\[\begin{aligned}\frac{\partial \bm{z}}{\partial \bm{x}} &amp;= \mathbf{W}^T, \\\end{aligned}\]</span></p><p>Case 3: <span class="math inline">\(\bm z = \bm x\)</span>, then<span class="math inline">\(\frac{\partial \bm z}{\partial \bm x} =\mathbf{I}\)</span>.</p><p>Case 4: <span class="math inline">\(\mathbf{X} \in \mathbb{R}^{m\times n}\)</span>, then <span class="math inline">\(\frac{\partialscalar}{\partial \mathbf{X}}  \in \mathbb{R}^{m \times n}\)</span>, justas the same as the shape of <span class="math inline">\(\mathbf{X}\)</span>.</p><p>Case 5: <span class="math inline">\(\mathbf{Z} =\mathbf{X}\mathbf{W}\)</span>, where <span class="math inline">\(\mathbf{X} \in \mathbb{R}^{m \times n}\)</span>,<span class="math inline">\(\mathbf{W} \in \mathbb{R}^{n \timesw}\)</span>. Assume <span class="math inline">\(\frac{\partialLoss}{\partial \mathbf{Z}} = \mathbf{\delta} \in \mathbb{R}^{m \timesw}\)</span>. <span class="math display">\[\begin{aligned}\frac{\partial Loss}{\partial \mathbf{X}} &amp;= \mathbf{\delta}\mathbf{W}^T, \\\frac{\partial Loss}{\partial \mathbf{W}} &amp;= \mathbf{X}^T\mathbf{\delta}.\end{aligned}\]</span></p><p><img src="/2023/09/11/deep-learning-with-structures/bp.jpg"></p><p>Here we consider the simple example: <span class="math display">\[\begin{aligned}\bm h_1 = \bm x \mathbf{W}_1,  \qquad (1 \times 4) = (1 \times 3) \times(3 \times 4)\\\hat{\bm y} = \bm h_1 \mathbf{W}_2, \qquad (1 \times 2) = (1 \times 4)\times (4 \times 2)\\L = \|\hat{\bm y}\|_2^2. \qquad (1 \times 1) = (1 \times 2) \times (2\times 1)\end{aligned}\]</span> Then, for the backward propagation process, we have: <span class="math display">\[\begin{aligned}\frac{\partial L}{\partial \hat{\bm y}} &amp;= 2 \hat{\bm y}, \qquad (1\times 2) = (1 \times 2) \\\frac{\partial L}{\partial \mathbf{W}_2} &amp;=  (\frac{\partial\hat{\bm y}}{\partial \mathbf{W}_2})^T \frac{\partial L}{\partial\hat{\bm y}}= \bm h_1^T  \frac{\partial L}{\partial \hat{\bm y}} = 2 \bmh_1^T \hat{\bm y}, \qquad (4 \times 2) = (4 \times 1) \times (1 \times2)\\\frac{\partial L}{\partial \bm h_1} &amp;=  \frac{\partial L}{\partial\hat{\bm y}}\frac{\partial \hat{\bm y}}{\partial \bm h_1}=\frac{\partialL}{\partial \hat{\bm y}} \mathbf{W}_2^T = 2 \hat{\bm y} \mathbf{W}_2^T, \qquad (1 \times 4) = (1 \times 2) \times (2 \times 4)\\\frac{\partial L}{\partial \mathbf{W}_1} &amp;=  (\frac{\partial \bmh_1}{\partial \mathbf{W}_1})^T [\frac{\partial L}{\partial \hat{\bmy}}\frac{\partial \hat{\bm y}}{\partial \bm h_1}] = \bm x^T\frac{\partial L}{\partial \bm h_1} = 2 \bm x^T \hat{\bm y}\mathbf{W}_2^T, \qquad (3 \times 4) = (3 \times 1) \times (1 \times 4)\\\frac{\partial L}{\partial \bm x} &amp;=  \frac{\partial L}{\partial\hat{\bm y}}\frac{\partial \hat{\bm y}}{\partial \bm h_1} \frac{\partial\bm h_1}{\partial \bm x} = 2\hat{\bm y} \mathbf{W}_2^T \mathbf{W}_1^T,\qquad (1 \times 3) = (1 \times 2) \times (2 \times 4) \times (4 \times3)\end{aligned}\]</span></p><p>We can get similar results if <span class="math inline">\(\bm x \in\mathbb{R}^{3 \times 1}\)</span>.</p><h2 id="invariant-and-equivariant">Invariant and Equivariant:</h2><p><strong>Invariant</strong>: A mathematical object (or a class ofmathematical objects) remains unchanged after operations ortransformations of a certain type are applied to the objects <span class="math inline">\(F(g(x)) = F(x)\)</span>, e.g., max pooling;</p><ul><li>Symmetry Group: all transformations under which the object isinvariant</li></ul><p><strong>Equivariant</strong>: Applying a transformation and thencomputing the function produces the same result as computing thefunction and then applying the transformation <span class="math inline">\(F(g(x)) = g(F(x))\)</span>.</p><ul><li><p>Convolution is translation equivariant, i.e., Conv(Shift(X)) =Shift(Conv(X))!</p></li><li><p>Global pooling gives you shift-invariance!</p></li></ul><h3 id="permutation-invariance">Permutation Invariance</h3><p>Birkhoff Polytope: <span class="math display">\[B_n = \{P \in \mathbb{R}^{n \times n} | \forall i, j, P_{ij} \geq 0,\sum_{i=1}^n P_{ij} = 1, \sum_{j=1}^n P_{ij} = 1\}\]</span> This type of matrix is Doubly Stochastic Matrix.</p><p>Birkhoff–von Neumann Theorem: 1. Birkhoff Polytope is the convex hullof permutation matrices 2. Permutation matrices = Vertices of BirkhoffPolytope (S_n):</p><p><img src="/2023/09/11/deep-learning-with-structures/Birkhoff.png"></p><p>Assume <span class="math inline">\(\mathbf{X} \in \mathbb{R}^{n\times 3}\)</span>, permutation matrix <span class="math inline">\(\mathbf{P} \in \mathbb{R}^{n \times n}\)</span>.<span class="math inline">\(\bm Y \in \mathbb{R}^{1 \times K}\)</span>is the output probability of classes.</p><p>Permutation Invariance : <span class="math inline">\(\bm Y =f(\mathbf{PX})\)</span>, <span class="math inline">\(\forall \mathbf{P}\in S_n\)</span>.</p><p>Assume <span class="math inline">\(\mathbf{H} \in \mathbb{R}^{n\times d}\)</span> is the representations, and <span class="math inline">\(\mathbf{H} = f(\mathbf{X})\)</span>, then,</p><p>Permutation Equivariance: <span class="math inline">\(\mathbf{PH} =\mathbf{P}f(\mathbf{X}) = f(\mathbf{PX})\)</span>, where <span class="math inline">\(\mathbf{P} \in S_n\)</span> is a permutationmatrix.</p><ul><li>equivariant first, then move to invariant.</li></ul><p>Valid set funtions: the function is invariant to the order of theinput.</p><p>Theorem: A function <span class="math inline">\(f\)</span> operatingon a set <span class="math inline">\(X\)</span> having elements from acountable <strong>universe</strong>. If <span class="math inline">\(f\)</span> is a valid set function, then thereexists a function <span class="math inline">\(g\)</span> such that <span class="math inline">\(f(X) = \rho(\sum_{x \in X} \phi(x))\)</span>.</p><p>Proof: For the mapping: <span class="math inline">\(f(X) \toR\)</span>, the domain of <span class="math inline">\(f\)</span> is allsubsets in <span class="math inline">\(X\)</span>. For example: if <span class="math inline">\(X = \{a, b, c\}\)</span>, then the domain is a<strong>power set</strong> <span class="math inline">\(\{\phi, a, b, c,\{a, b\}, \{a, c\}, \{b, c\}, \{a, b, c\}\}\)</span>.</p><p>Sufficiency: summation is permutation invariant!</p><p>Necessity: find an unique representation of any set and then mapit:</p><p>why choose 4: base 3,4 ... are ok, but base 2 is not. Since base 2cannot guarantee the uniqueness of the representation.</p><h2 id="deep-learning-for-sequences">Deep learning for Sequences</h2><p>Applications:</p><ul><li><p>Language Model: <span class="math inline">\(P(x^{t+1}| x^{t},\cdots, x^1)\)</span>, where <span class="math inline">\(x^{t+1}\)</span> is the word we want topredict.</p></li><li><p>Machine Translation.</p></li></ul><p>Key challenges:</p><ul><li>Variable length input and output;</li><li>Order change may be crucial for cognition;</li><li>complex statistical dependencies (e.g. long-range ones).</li></ul><h3 id="transformer">Transformer:</h3><p><img src="/2023/09/11/deep-learning-with-structures/transformer.png"> Theoutput representation of the final encoder is the input of eachdecoder.</p><p><img src="/2023/09/11/deep-learning-with-structures/encoder_decoder.png">The decoder includes self-attention and encoder-decoder attention. Forthe self-attention, it uses <strong>masked multi-headattention</strong>, why????</p><p><strong>How to encode the input sequence?</strong></p><p><strong>Input embedding:</strong> Construct the one-hot vector foreach word. N words, each word will be mapping to a D dimension vector.Then, we can get a NxD matrix. D is the hyper-parameter.</p><ul><li><p>Will the input embedding manners affect the performance of themodel?</p><p>In general, we use the same vocabulary dictionary for the inputembedding.</p></li></ul><p><strong>positional encoding:</strong> <img src="/2023/09/11/deep-learning-with-structures/positional.png"> <span class="math display">\[\begin{aligned}PE(pos, 2i) &amp;= sin(pos/10000^{2i/d_{model}}) \\PE(pos, 2i+1) &amp;= cos(pos/10000^{2i/d_{model}}) \\\end{aligned}\]</span></p><p>pos is the index of the word in the sentence. (0-30) <span class="math inline">\(2i\)</span> and <span class="math inline">\(2i+1\)</span> is the index of the column, d_modelis the number of columns, it is a hyper-parameter(120). For eachword(token), we encode it to a vector with dimension d_model accordingto its position.</p><p>Here we use denominator <span class="math inline">\(10000^{2i/d_model}\)</span> to make sure thepositional encoding is different for different tokens. The sin and cosare periodic functions, if we don't use the denominator, then thepositional encoding could be same for different tokens.</p><ul><li>If there are two different sentences with the same size, will thepositional encodings be the same?? yes.</li></ul><p><strong>Self attention:</strong> <span class="math inline">\(X \inR^{tokes \times dim}\)</span>, <span class="math inline">\(W_Q \inR^{dim \times dim}\)</span>, <span class="math inline">\(Q = XW_Q \inR^{tokens \times dim}\)</span>.</p><p><span class="math inline">\(softmax(\frac{QK^T}{\sqrt{dim}}) \inR^{tokens \times tokens}\)</span> is the <strong>attentionmatrix</strong>, the dimension must be the same as the number oftokens.</p><p>we apply softmax in individual row, then the output of softmax is<span class="math inline">\(tokens \times tokens\)</span>.</p><p>The final output dimension is <span class="math inline">\(tokens\times dim\)</span>.</p><ul><li><p>Why does it need to divide by <span class="math inline">\(\sqrt{dim}\)</span>?</p><p>To keep the variance of each entry <span class="math inline">\(QK^T[i,k]\)</span> to be 1. <strong>[approach to1]</strong>. if we don't preserve the variance, then the gradient willbe larger and larger, and the model will be unstable.</p></li></ul><p><strong>Multi-head attention</strong> it can capture differentdependency of the input sequence. One choice is to input the same inputembedddings for each attention head, and then aggregate the output ofeach attention head. Another choice is to split the input embeddingsinto different parts, and then input different parts to differentattention heads. (The problem is not a convex problem, thus the weightsof each attention head may be different.)</p><p><strong>Layer normalization</strong> <img src="/2023/09/11/deep-learning-with-structures/layernorm.png"> it isapplied to each row of the output of multi-head attention. It is similarto batch normalization, but it is applied to each row, not each column.we want to learn <span class="math inline">\(\gamma\)</span> and <span class="math inline">\(\beta\)</span>, because we want to learn thedistribution of each row.</p><p><strong>Masked multi-head attention in decoder</strong> The decodermust be autoregressive. We need to input the previous words to predictthe next word and prevent attending from future. For a attention matrix,we can mask the upper triangle, then the values masked are zeros. But,if we mask the upper triangle, the sum of each row is not equal to 1,thus we need to adjust the attention matrix. Normally, the attentionmatrix is: <span class="math display">\[\begin{aligned}A = softmax(\frac{QK^T}{\sqrt{dim}})\end{aligned}\]</span> The dimension of the attention matrix is <span class="math inline">\(outputtokens \times outputtokens\)</span>. If wemask <span class="math inline">\(A_{ij}\)</span>, then the input ofsoftmax function will be: <span class="math display">\[\begin{aligned}A_{ij} = \frac{\exp \frac{ \sum_k Q_{ik}K_{jk} -\infty}{\sqrt{dim}}}{\sum \exp()}\end{aligned}\]</span></p><p>Shifted right: we already generate one token, and we want to predictthe next token, then we always focus on the right part of the outputsequence.</p><p><strong>Cross attention</strong> it is used in the decoder. The inputof the decoder is the output of the encoder. The encoder-decoderattention is similar to the self attention, but the query is the outputof the decoder, and the key and value are the output of the encoder.Here the output of encoder is the embaddings, and the docoder cangenerate the key and value from the embeddings. Cross attention cancapture the relationship between the input sentence and the outputsentence.</p><h2 id="graph-neural-networks-message-passing-models">Graph NeuralNetworks Message Passing Models</h2><p>Graph: multi-edges, nodes have types, edges have types.</p><ul><li>connectivity: adjacency list <span class="math inline">\(G = (V,E)\)</span> and adjacency matrix <span class="math inline">\(A\)</span>.<span class="math inline">\(|V| = n, |E| = m\)</span>. <span class="math inline">\(A \in \mathbb{R}^{n \times n}\)</span>.</li><li>features: node features <span class="math inline">\(X\)</span>, edgefeatures, graph features.</li></ul><p>if you want to permute a graph, then you need to<strong>left</strong> multiply the permutation matrix to the adjacencymatrix (change rows) and also right multiply the transpose permutationmatrix to the adjacency matrix(change columns): <span class="math display">\[\begin{aligned}A' = PAP^T\end{aligned}\]</span></p><p>For a graph <span class="math inline">\(A_1\)</span>, if there existsa permutation matrix <span class="math inline">\(P\)</span>, such that<span class="math inline">\(A_2 = PA_1P^T\)</span>, then <span class="math inline">\(A_1\)</span> and <span class="math inline">\(A_2\)</span> are graph isomorphic.</p><p>For a graph <span class="math inline">\(A\)</span>, if there exists apermutation matrix <span class="math inline">\(P\)</span>, such that<span class="math inline">\(A = PAP^T\)</span>, then <span class="math inline">\(A\)</span> is graph automorphism.</p><p>Given graph data <span class="math inline">\((A, X)\)</span> and<span class="math inline">\(f(A, X) \in \mathbb{R}^{n \timesd}\)</span>:</p><ul><li><p>invariance: <span class="math inline">\(f(PAP^T, PX)=f(A,X)\)</span>, <span class="math inline">\(\forall P \inS_n\)</span>.</p></li><li><p>equivariance: <span class="math inline">\(f(PAP^T, PX) = Pf(A,X)\)</span>, <span class="math inline">\(\forall P \inS_n\)</span>.</p></li></ul><p>Key challenges:</p><ul><li>unordered neighbors;</li><li>variable size of neighbors;</li><li>varying graph partitions.</li></ul><h3 id="message-passing-in-gnns">Message Passing in GNNs</h3><p>Feedforward networks: layers do not share message passing module.[don't share weights]. Usually, we call it 'layers'.</p><p>Recurrent networks: layers share message passing module.[reuseweights]. Usually, we call it 'steps' instead of 'layers'.</p><p>Even if we increase the number of nodes and edges, the model canstill work.</p><p><strong>ONLY USE ONE NETWORK FOR ALL NEIGHBORS</strong>, same acrossedges. if we use different networks for each neighbor, changing thenumber of nodes will affect the model. The model is not invariant to thenumber of nodes.</p><p>For undirected graph, the message passing is symmetric, i.e., m_ij =m_ji. It doesn't related to the order of nodes. For directed graph, m_ijmay not equal to m_ji.</p><p>We can also use transformers or gcn or LSTM to implement the messagepassing. Be careful that LSTM is not permutation invariant.</p><p>Tips: parallel message passing: compute messages for all nodes/edgesand compute updates for all nodes in parallel. Use dense operators onGPUs.</p><p>privacy and robust to attack in gnn.</p><p>we can also use transformers in encoding graph structure as anattention mask.</p><h2 id="graph-convolutional-networks">Graph Convolutional Networks</h2><p>laplace operator is the eigenfuction, why?</p><p>For <strong>undirected</strong> graph, Graph Laplacian: <span class="math inline">\(L = D - A\)</span>, where <span class="math inline">\(D\)</span> is the degree matrix, <span class="math inline">\(A\)</span> is the adjacency matrix. Laplacianmatrix can compute the difference between the node and its neighbors. Itis symmetric, diagonally dominant, positive semi-definite(eigenvaluesare nonnegative), and the number of zero eigenvalues is the number ofconnected components.</p><ul><li>Translation group;</li><li>Roto-translation group: SO(n): <span class="math inline">\(Q \in\mathbb{R}^{n \times n}, Q^TQ = Q Q^T = I, det(Q) = 1.\)</span></li></ul><p><span class="math inline">\(g = (X, R_\theta), g' = (X',R_{\theta'})\)</span></p><p><span class="math inline">\(g \cdot g' = g \cdot g' (x_0) =g(R_{\theta'}x_0 + X') = R_{\theta}R_{\theta'}x_0 +R_{\theta}X' + X = (R_{\theta'}X' + X, R_{\theta +\theta'})\)</span></p><ul><li>Scale-translation group.</li><li>Affine group.</li></ul><p>cross-correlations: <span class="math inline">\(f \star g (x) = \intf(x'-x)g(x')dx'\)</span>, in mathmetics, the order ofconvolution is inverse to the order in DL.</p><h2 id="autoregressive-models">Autoregressive Models</h2><p>Autoregressive model： <span class="math display">\[\begin{aligned}P(x_1, \cdots, x_n) = \prod_{i=1}^n P(x_i|x_1, \cdots, x_{i-1}) =\prod_{i=1}^n P(x_i|x_{&lt;i})\end{aligned}\]</span></p><p>For images, each <span class="math inline">\(x_i\)</span> is a pixelvalue, e.g., {0,...,255}. n = height * width. Each term <span class="math inline">\(P(x_i|x_{&lt;i})\)</span> can be modeled by asingle CNN/RNN/....</p><p>Why do we consider the same model for each term? Otherwise, thenumber of model is O(n).</p><p>PixelCNNs: conditioned on the pixels above and to the left of thepixel being predicted. At each step, we will mask the pixels below andright of the pixel being predicted. Then we use convolution on theimage, but it will yield high computation cost.</p><p>PixelRNNs: vectorize the image as a sequence of pixels, and then useRNN to model the sequence.</p><p>Masked Filter: we mask the filter to make sure the convolution isautoregressive. But it will yield blind spots.</p><p>blind spots: the model cannot see the pixels below and right of thepixel being predicted.</p><p>resovle blind spots: use a stack of masked convolutions.</p><p>The cons of softmax: if the number of dimension is very large, thenthe softmax will be very small. Thus we use the discretized mixturelogistic distribution: <span class="math display">\[\begin{aligned}P(x) = \sum_{k=1}^K \pi_k \sigma(\frac{x - \mu_k}{s_k})\end{aligned}\]</span> where <span class="math inline">\(\sigma\)</span> is thesigmoid function, <span class="math inline">\(\pi_k\)</span> is theweight of the <span class="math inline">\(k\)</span>-th component, <span class="math inline">\(\mu_k\)</span> is the mean of the <span class="math inline">\(k\)</span>-th component, <span class="math inline">\(s_k\)</span> is the scale of the <span class="math inline">\(k\)</span>-th component.</p><p>Due to the sequential nature of autoregressive sampling, it isslow.</p><ul><li><p>will autoregressive model focus more on the nearby locations?yes, because we use masked convolutions.</p></li><li><p>For directed graphs, we can generate the lower triangle of theadjacency matrix, and then generate the upper triangle.</p></li></ul><h2 id="generative-adversarial-networks-gans">Generative AdversarialNetworks (GANs)</h2><p>Generative models: generate data from noise.</p><p>Min-max loss: <span class="math display">\[\begin{aligned}\min_{\theta} \max_{\phi}\mathbb{E}_{x \sim p_{data}(x)}[\logD_{\phi}(x)] + \mathbb{E}_{z \sim p_z(z)}[\log(1 -D_{\phi}(G_{\theta}(z)))]\end{aligned}\]</span> where <span class="math inline">\(D\)</span> is thediscriminator, <span class="math inline">\(G\)</span> is the generator,<span class="math inline">\(x\)</span> is the real data, <span class="math inline">\(z\)</span> is the noise, <span class="math inline">\(p_{data}\)</span> is the distribution of the realdata, <span class="math inline">\(p_z\)</span> is the distribution ofthe noise.</p><p>The fake images are generated from noise (normal distribution), wecan not get the distribution of fake images. Why? Because the mappingfrom noise to fake images is not invertible. Thus, we cannot get thelikelihood of the fake images.</p><p>GAN is also called likihood-free model. We cannot get the likelihoodof the fake images.</p><p>For GANs, we can get images through one forward pass, but forautoregressive model, we need to generate images pixel in a sequentialmanner.</p><p>The output of the discriminator is a scalar, the probability of theinput image being real. The output of the generator is an image.</p><p>Fix generator, the optimal discriminator is: <span class="math display">\[\begin{aligned}D_{\phi}^*(x) = \frac{p_{data}(x)}{p_{data}(x) + p_{G_{\theta}}(x)}\end{aligned}\]</span> where <span class="math inline">\(p_{G_{\theta}}(x)\)</span>is the distribution of the fake images. When <span class="math inline">\(p_{G_{\theta}}(x) = p_{data}(x)\)</span>, then<span class="math inline">\(D_{\phi}^*(x) = 0.5\)</span>. However, wedon't know the distribution of the fake images and the distribution ofthe real images.</p><p>inner-loop is the discriminator, outer-loop is the generator. why?focus more on the generator, because the discriminator is easy to train??</p><h2 id="reference">Reference</h2><p><a href="https://lrjconan.github.io/UBC-EECE571F-DL-Structures/">EECE571F (2023 Winter Term 1): Deep Learning with Structures</a></p><p>http://cs231n.stanford.edu/slides/2018/cs231n_2018_ds02.pdf</p><p><a href="https://jalammar.github.io/illustrated-transformer/" class="uri">https://jalammar.github.io/illustrated-transformer/</a></p><p><a href="https://uvagedl.github.io/">UvA - An Introduction to GroupEquivariant Deep Learning</a></p>]]></content>
      
      
      <categories>
          
          <category> Graph </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Deep Learning with Structures </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Basic knowledge</title>
      <link href="/2023/09/06/Basic%20knowledge/"/>
      <url>/2023/09/06/Basic%20knowledge/</url>
      
        <content type="html"><![CDATA[<h2 id="graph-convolutional-autoencoders-with-co-learning-of-graph-structure-and-node-attributes">Graphconvolutional autoencoders with co-learning of graph structure and nodeattributes</h2><p>In this paper, they design the special graph encoder and decoder forthe tasks undertaken by the graph autoencoders. The task of the encoderis to embed the nodes into a new space, and then the latentrepresentation of each node is close to its neighbors[the encoder is alow-pass filter]. The decoder restores the original space from theembedded space by making the latent representation of each node awayfrom its neighbors[the decoder is a high-pass filter].</p><p>In this paper, they encode both the graph structure and the nodeattributes in the latent space with an improved GCN, which is a<strong>completely low-pass graph filter</strong>. Then, to reconstructthe node attributes X , they design a new <strong>high-pass graphdecoder</strong>. At the same time, we use the inner product layer toreconstruct the graph structure information. Last, the graph encoder andtwo sub-decoders are jointly optimized in a unified framework in such away that each can be beneficial to the other and finally lead to abetter graph embedding.</p><h3 id="normalized-adjacency-matrix-and-laplacian-matrices">Normalizedadjacency matrix and Laplacian matrices</h3><p>1, The normalized adjacency matrix is defined as: <span class="math display">\[\hat{A} = D^{-1/2}A D^{-1/2},\]</span> where<span class="math inline">\(A\)</span> is the adjacency matrix of graph<span class="math inline">\(G\)</span>. <span class="math inline">\(D =diag(d)\)</span>, <span class="math inline">\(d(i)\)</span> is thedegree of node <span class="math inline">\(i\)</span>.</p><p>2, The normalized Laplacian matrix is defined as: <span class="math display">\[L_s = I - \hat{A} = I - D^{-1/2}AD^{-1/2}.\]</span> Note that <span class="math inline">\(L_s = I -\hat{A} = D^{-1/2}(D - A) D^{-1/2} = D^{-1/2}L D^{-1/2}\)</span>, where<span class="math inline">\(L = D - A\)</span> is the unnormalizedLaplacian matrix of graph <span class="math inline">\(G\)</span>.</p><p>For the largest eigenvalue <span class="math inline">\(\lambda^s\)</span> of <span class="math inline">\(A\)</span> and the maximum degree <span class="math inline">\(\Delta\)</span> of a node in a graph, we have $d_{avg} ^s $. Normalizing the adjacency matrix can make its largesteigenvalue 1.</p><p>3, Let <span class="math inline">\(\alpha_1 \geq \alpha_2 \geq ...\geq \alpha_n\)</span> be the eigenvalues of <span class="math inline">\(\hat{A}\)</span>, <span class="math inline">\(\lambda^s_1 \leq \lambda^s_2 \leq ... \leq\lambda^s_n\)</span> be the eigenvalues of <span class="math inline">\(L_s\)</span>, then <span class="math display">\[ 1= \alpha_1 \geq ... \geq \alpha_n \geq -1, \quad 0=\lambda^s_1 \leq ...\leq \lambda^s_n \leq 2.\]</span></p><h3 id="graph-convolutional-networks">Graph convolutional networks</h3><p>GCN generalizes the convolutional neural networks on non-Euclideandomains. It uses the first-order approximation of Chebyshev polynomials:<span class="math display">\[g_{\theta} \star x \approx \theta (I_N + D^{-1/2}AD^{-1/2})X.\]</span> The spectral radius of <span class="math inline">\((I_N +D^{-1/2}AD^{-1/2})\)</span> is 2, and repeated application of thisoperator will cause numerical instabilities. To solve this problem, GCNuses a renormalization trick by adding a self-loop to each node, whichis equivalent to adding the identity matrix <span class="math inline">\(I_N\)</span> to the adjacency matrix <span class="math inline">\(A\)</span>: <span class="math inline">\(\tilde{A}= A + I\)</span>, the associated degree matrix <span class="math inline">\(\tilde{D} = D + I\)</span>. The new symmetricallynormalized matrix is <span class="math inline">\(\tilde{A}_{GCN} =\tilde{D}^{-1/2} \tilde{A} \tilde{D}^{-1/2}\)</span>. The one-layer GCNis <span class="math display">\[Z^{(m+1)} = \sigma(\tilde{A}_{GCN}Z^{(m)}W^{(m)}),\]</span> where <span class="math inline">\(Z^{(m)}\)</span> is thelatent representation matrix learned by the <span class="math inline">\(m\)</span>-th layer, <span class="math inline">\(Z^{(0)} = X\)</span>.</p><h3 id="graph-signal-processing">Graph signal processing</h3><p>In graph signal processing , the eigenvalues and eigenvectors of thegraph Laplacian correspond to the frequencies and Fourier basis.</p><p>The graph laplacian is defined as <span class="math inline">\(L =D-A\)</span>. By eigen-decomposition, <span class="math inline">\(L = U\Lambda U^{-1}\)</span>, where <span class="math inline">\(\Lambda =diag(\lambda_1, ..., \lambda_n)\)</span>, <span class="math inline">\(U= (u_1, u_2, ..., u_n)\)</span>. The eigenvalues <span class="math inline">\(\lambda_i, i \in [n]\)</span> can be considered tobe frequencies, and the associated eigenvectors <span class="math inline">\(u_i, i \in [n]\)</span> can be considered to be aFourier basis.</p><p>A graph signal <span class="math inline">\(f\)</span> can bedecomposed into a linear combination of basis signals <span class="math inline">\(u_i\)</span>: <span class="math display">\[f = Uc = \sum_{i=1}^n c_i u_i,\]</span> where <span class="math inline">\(c = (c_1, ...,c_n)^T\)</span>, <span class="math inline">\(c_i\)</span> is thecoefficient of <span class="math inline">\(u_i\)</span>, the magnitudeof <span class="math inline">\(c_i\)</span> represents the importance of<span class="math inline">\(u_i\)</span> in <span class="math inline">\(f\)</span>.</p><p>The smoothness of the basis signal <span class="math inline">\(u_i\)</span> is measured by the correspondingeigenvalue <span class="math inline">\(\lambda_i\)</span>. The smallerthe eigenvalue <span class="math inline">\(\lambda_i\)</span>, thesmoother the basis signal <span class="math inline">\(u_i\)</span>.<span class="math display">\[\sum_{e_{j,k} \in E} a_{j,k}[u_i(j) - u_i(k)]^2 = u_i^T L u_i =\lambda_i u_i^T u_i = \lambda_i.\]</span></p><p>The basic idea of graph filtering is to design a proper graph filterto produce the required signals for the downstream tasks. A graph filteris a function that takes a graph signal as input and <strong>outputs anew signal</strong>. A linear graph filter can be represented as amatrix <span class="math inline">\(G \in \mathbb{R}^{N \timesN}\)</span>, which is defined as <span class="math display">\[G = U p(\Lambda) U^{-1},\]</span> where <span class="math inline">\(p(\Lambda) =diag(p(\lambda_1), ..., p(\lambda_n))\)</span>. <span class="math inline">\(p(\cdot)\)</span> is the frequency responsefunction.</p><p>The output signal can be written as <span class="math display">\[y = Gf = U p(\Lambda) U^{-1} Uc = U p(\Lambda) c = \sum_{i=1}^np(\lambda_i) c_i u_i.\]</span></p><p>Definition 1 (completely low-pass graph filter). A completelylow-pass graph filter is a graph filter whose frequency responsefunction <span class="math inline">\(p(\cdot): \mathbb{R} \to\mathbb{R}^{+}\)</span> is a decreasing function with <span class="math inline">\(\lambda\)</span>.</p><ul><li>According to definition 1, the completely low-pass graph filterobtains a smooth graph output signal <span class="math inline">\(y\)</span> that consists of mostly low-frequencybasis signals, and as a result, the latent representation of each nodeis close to its neighbors.</li></ul><p>Definition 2 (completely high-pass graph filter). A completelyhigh-pass graph filter is a graph filter whose frequency responsefunction <span class="math inline">\(p(\cdot): \mathbb{R} \to\mathbb{R}^{+}\)</span> is an increasing function with <span class="math inline">\(\lambda\)</span>.</p><p>According to definition 2, the completely high-pass graph filterobtains an unsmooth graph output signal <span class="math inline">\(y\)</span> that consists of mostly high-frequencybasis signals, which makes the latent representation of each node faraway from its neighbors.</p><p>For GCN, the graph filter of GCN is <span class="math display">\[\tilde{A}_{GCN} = \tilde{D}^{-1/2} \tilde{A} \tilde{D}^{-1/2} = I - L_s= U (I - \Lambda^s) U^{-1}.\]</span> The frequency response function of GCN is <span class="math inline">\(p(\lambda^s_i) = 1 - \lambda_i^s\)</span>. Sincethe range of <span class="math inline">\(\lambda_i^s\)</span> is <span class="math inline">\([0, 2]\)</span>, the frequency response functionof GCN is a decreasing function with <span class="math inline">\(\lambda_i^s\)</span>. GCN is completely low-passgraph filter when <span class="math inline">\(\lambda_i^s \in [0,1]\)</span>, but not in <span class="math inline">\([1, 2]\)</span>.When <span class="math inline">\(\lambda_i^s \in [1, 2]\)</span>, <span class="math inline">\(p(\lambda^s_i)\)</span> will take a negative valuethat will introduce noise and undermine the performance. Thus, GCN isnot a completely low-pass graph filter.</p><h3 id="reference">Reference</h3><p>Jie Wang, Jiye Liang, Kaixuan Yao, Jianqing Liang, Dianhui Wang,Graphconvolutional autoencoders with co-learning of graph structure and nodeattributes,Pattern Recognition,Volume 121,2022,108215,ISSN 0031-3203, <a href="https://doi.org/10.1016/j.patcog.2021.108215" class="uri">https://doi.org/10.1016/j.patcog.2021.108215</a>.</p><p><a href="https://people.orie.cornell.edu/dpw/orie6334/Fall2016/lecture7.pdf" class="uri">https://people.orie.cornell.edu/dpw/orie6334/Fall2016/lecture7.pdf</a></p><h2 id="the-difference-between-adam-and-adamw">The difference betweenAdam and AdamW</h2><p><a href="https://towardsdatascience.com/why-adamw-matters-736223f31b5d" class="uri">https://towardsdatascience.com/why-adamw-matters-736223f31b5d</a></p><h2 id="why-regularization-can-reduce-overfitting">Why regularizationcan reduce overfitting?</h2><p><a href="http://neuralnetworksanddeeplearning.com/chap3.html#regularization" class="uri">http://neuralnetworksanddeeplearning.com/chap3.html#regularization</a></p><h2 id="cosine-decay-schedule-with-warm-up-period">Cosine decay schedulewith warm up period</h2><p>Loshchilov and Hutter, SGDR: Stochastic Gradient Descent with WarmRestarts. ICLR 2017. <a href="https://arxiv.org/abs/1608.03983" class="uri">https://arxiv.org/abs/1608.03983</a></p><p><a href="https://scorrea92.medium.com/cosine-learning-rate-decay-e8b50aa455b" class="uri">https://scorrea92.medium.com/cosine-learning-rate-decay-e8b50aa455b</a></p>]]></content>
      
      
      <categories>
          
          <category> Graph </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Graph </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Kolmogorov-Smirnov statistic</title>
      <link href="/2023/08/27/KS_test/"/>
      <url>/2023/08/27/KS_test/</url>
      
        <content type="html"><![CDATA[<h1 id="kolmogorov-smirnov-statistic">Kolmogorov-Smirnov statistic</h1><p>Consider any distribution <span class="math inline">\(D\)</span> on<span class="math inline">\(\mathbb{R}\)</span>, and CDF <span class="math inline">\(F(t) = \mathbb{P}_{x \sim D}(x \leqt)\)</span>.</p><p>Let <span class="math inline">\(X = (x_j)_{i \in [n]}\)</span> be<span class="math inline">\(n\)</span> samples drawn from <span class="math inline">\(D\)</span>.</p><p>Def: The empirical CDF (eCDF) of <span class="math inline">\(X\)</span> is defined as <span class="math inline">\(\hat{F}_n(t) = \frac{1}{n} \sum_{j=1}^n\mathbb{1}_{x_j \leq t}\)</span>.</p><p>Def: The Kolmogorov-Smirnov statistic is defined as <span class="math inline">\(D_n = \sup_{t \in \mathbb{R}} \{|\hat{F}_n(t) -F(t)|\}\)</span>.</p><p>Theorem [DKW, 1956]: <span class="math inline">\(\mathbb{P}(D_n &gt;\epsilon) \leq c e^{-2n\epsilon^2}\)</span>, where <span class="math inline">\(c\)</span> is a constant.</p><p>Theorem [Massart, 1990]: <span class="math inline">\(\mathbb{P}(D_n&gt; \epsilon) \leq 2 e^{-2n\epsilon^2}\)</span>.</p><p>Theorem [Harvey, 2020]: <span class="math inline">\(\mathbb{P}(D_n&gt; \epsilon) \leq \frac{4}{\epsilon}e^{-\frac{1}{2}n\epsilon^2}\)</span>.</p>]]></content>
      
      
      <categories>
          
          <category> Statistic </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Statistic </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Itô Calculus and Stochastic Differential Equations</title>
      <link href="/2023/08/24/Heuristic%20Solutions%20of%20SDEs/"/>
      <url>/2023/08/24/Heuristic%20Solutions%20of%20SDEs/</url>
      
        <content type="html"><![CDATA[<h1 id="the-stochastic-integral-of-itô">The Stochastic Integral ofItô</h1><p>A stochastic differential equation can be transformed into a vectordifferential equation of the form <span class="math display">\[\begin{aligned}\frac{\text{d}x}{\text{d} t}  &amp;= f(x, t) + \mathbb{L}(X, t) w(t), \\\end{aligned}\]</span> where <span class="math inline">\(w(t)\)</span> is a whiteGaussian noise with zero mean. Since <span class="math inline">\(w(t)\)</span> is discontinuous, we can not use theordinary differential equation to solve the above equation. Fortunately,we can reduce the problem to definition of a new king of integral, thestochastic integral of Itô.</p><p>We can integrate the SDE from initial time <span class="math inline">\(t_0\)</span> to final time <span class="math inline">\(t\)</span>: <span class="math display">\[\begin{aligned}x(t) - x(t_0) &amp;= \int_{t_0}^{t} f(x, t) \text{d} t + \int_{t_0}^{t}\mathbb{L}(x, t) w(t) \text{d} t. \\\end{aligned}\]</span> The first integral with respect to time on the right-hand sidecan be solved by Riemann integral or Lebesgue integral. The secondintegral is the problem we need to solve. We will first discuss thereason why we can not use the Riemann integral, Lebesgue integral andStieltjes integral to solve the second integral.</p><p>First, it cannot be solved by Riemann integral. The Riemann integralis defined as <span class="math display">\[\int_{t_0}^{t} \mathbb{L}(X, t) w(t) \text{d} t = \lim_{n \to \infty}\sum_{k=1}^{n} \mathbb{L}(x(t_k^{*}), t_k^{*}) w(t_k^{*}) (t_{k+1} -t_k),\]</span> where <span class="math inline">\(t_0 &lt; t_1 &lt; ...&lt;t_n= t\)</span>, and <span class="math inline">\(t_k^{*} \in [t_k,t_{k+1}]\)</span>. In Riemann integral, the upper and lower bounds ofthe integral are defined as the selections of <span class="math inline">\(t_k^*\)</span> such that the integral is maximizedand minimized. If the upper bound and lower bound converge to the samevalue, the Riemann integral exists. However, the white Gaussian noise isdiscontinuous and not bounded, it can take arbitrarily small and largevalues at every finite interval, so the upper and lower bounds of theintegral are not convergent. Therefore, the Riemann integral does notexist.</p><p>For Stieltjes integral, we need to interpret the increment <span class="math inline">\(w(t) \text{d}t\)</span> as an increment of anotherprocess <span class="math inline">\(\beta(t)\)</span>, thus the intergalbecomes <span class="math display">\[\int_{t_0}^{t} \mathbb{L}(x(t), t) w(t) \text{d} t = \int_{t_0}^{t}\mathbb{L}(x(t), t) \text{d} \beta(t).\]</span> Here <span class="math inline">\(\beta(t)\)</span> is aBrownian motion. Brownian motion is a continuous process. However, theBrownian motion is not differentiable, so the Stieltjes integral doesnot converge.</p><p>Both Stieltjes and Lebesgue integrals are defined as limits of theform <span class="math display">\[\int_{t_0}^{t} \mathbb{L}(x(t), t) \text{d} \beta = \lim_{n \to \infty}\sum_{k=1}^{n} \mathbb{L}(x(t_k^{*}), t_k^{*}) (\beta(t_{k+1}) -\beta(t_k)),\]</span> where <span class="math inline">\(t_0 &lt; t_1 &lt; ...&lt;t_n= t\)</span>, and <span class="math inline">\(t_k^{*} \in [t_k,t_{k+1}]\)</span>. Both of these definitions would require the limit tobe independent of the position on the interval <span class="math inline">\(t_k^{*} \in [t_k, t_{k+1}]\)</span>. However, inthis case, the limit is not independent of the position on the interval<span class="math inline">\(t_k^{*} \in [t_k, t_{k+1}]\)</span>, so theStieltjes and Lebesgue integrals do not exist.</p><h2 id="definition-of-the-stochastic-integral-of-itô">Definition of theStochastic Integral of Itô</h2><p>For Itô integral, it fixed the choice of <span class="math inline">\(t_k^{*}\)</span> to be <span class="math inline">\(t_k\)</span>, thus the limit becomes unique. TheItô integral is defined as <span class="math display">\[\int_{t_0}^{t} \mathbb{L}(x(t), t) \text{d} \beta = \lim_{n \to \infty}\sum_{k=1}^{n} \mathbb{L}(x(t_k), t_k) (\beta(t_{k+1}) - \beta(t_k)).\]</span></p><p>The SDE can be defined to be the Itô integral of the form <span class="math display">\[\begin{aligned}x(t) - x(t_0) &amp;= \int_{t_0}^{t} f(x(t), t) \text{d} t +\int_{t_0}^{t} \mathbb{L}(x, t) \text{d} \beta(t).\end{aligned}\]</span></p><p>The differential form is <span class="math display">\[\begin{aligned}\text{d} x &amp;= f(x, t) \text{d} t + \mathbb{L}(x, t) \text{d}\beta(t).\end{aligned}\]</span> or <span class="math display">\[\begin{aligned}\frac{\text{d} x}{\text{d} t } &amp;= f(x, t) + \mathbb{L}(x, t) \frac{\text{d} \beta(t)}{\text{d} t}.\end{aligned}\]</span></p><ul><li>Why don't we consider more general SDEs of the form <span class="math display">\[\begin{aligned}\frac{\text{d} x}{\text{d} t } &amp;= f(x(t), w(t), t),\end{aligned}\]</span> where the white noise <span class="math inline">\(w(t)\)</span> enters the system through anonlinear transformation. We can not rewrite this equation as astochastic integral with respect to a Brownian motion, and thus wecannot define the mathematical meaning of this equation.</li></ul><h2 id="itô-formula">Itô Formula</h2><p>Consider the stochastic integral <span class="math display">\[\int_{t_0}^{t} \beta(t) \text{d} \beta(t),\]</span> where <span class="math inline">\(\beta(t)\)</span> is astandard Brownian motion with zero mean and diffusion constant <span class="math inline">\(q = 1\)</span>. Based on the definition of the Itôintegral, we have <span class="math display">\[\begin{aligned}\int_{t_0}^{t} \beta(t) \text{d} \beta(t) &amp;= \lim_{n \to \infty}\sum_{k=1}^{n} \beta(t_k) (\beta(t_{k+1}) - \beta(t_k)) \\&amp;= \lim_{n \to \infty} \sum_{k=1}^{n} [-\frac{1}{2}(\beta(t_{k+1}) -\beta(t_k))^2 +　\frac{1}{2}(\beta^2(t_{k+1}) - \beta^2(t_k))]\\&amp;= -\frac{1}{2}t + \frac{1}{2}\beta^2(t).\end{aligned}\]</span> where <span class="math inline">\(0 = t_0 &lt; t_1 &lt; ...&lt; t_n = t\)</span> and <span class="math inline">\(\lim_{n \to\infty} \sum_{k=1}^{n} (\beta(t_{k+1}) - \beta(t_k))^2\)</span>. That isbecause <span class="math inline">\(\beta(t_{k+1}) - \beta(t_k) \simN(0, t_{k+1} - t_k) \sim N(0, \frac{t}{n})\)</span>.</p><p>So the Itô differential of <span class="math inline">\(\beta^2(t)/2\)</span> is <span class="math display">\[\begin{aligned}\text{d} \frac{\beta^2(t)}{2} &amp;= \beta(t) \text{d}\beta(t) +\frac{1}{2} \text{d}t.\end{aligned}\]</span> It is not the same as the ordinary differential of <span class="math inline">\(\beta^2(t)/2\)</span>: <span class="math display">\[\begin{aligned}\frac{\text{d} \beta^2(t)}{2} &amp;= \beta(t) \text{d}\beta(t).\end{aligned}\]</span> That is because the Itô integral fixes the choice of <span class="math inline">\(t_k^{*}\)</span> to be <span class="math inline">\(t_k\)</span>.</p><p>Theorem Itô formula: Let <span class="math inline">\(x(t)\)</span> bean Itô process(note: <span class="math inline">\(x(t)\)</span> is avector process) which is the solution of an SDE of the form <span class="math display">\[\begin{aligned}\text{d} x &amp;= f(x, t) \text{d} t + \mathbb{L}(x, t) \text{d}\beta(t),\end{aligned}\]</span> where <span class="math inline">\(\beta(t)\)</span> is aBrownian motion. Consider an arbitrary <strong>scalar</strong> function<span class="math inline">\(\phi(x(t), t)\)</span> of the process, theItô SDE of <span class="math inline">\(\phi\)</span> is <span class="math display">\[\begin{aligned}\text{d} \phi &amp;= \frac{\partial \phi}{\partial t} \text{d} t +\sum_{i}\frac{\partial \phi}{\partial x_i} \text{d} x_i + \frac{1}{2}\sum_{i,j}\frac{\partial^2 \phi}{\partial x_i \partial x_j} \text{d} x_i\text{d} x_j \\&amp;= \frac{\partial \phi}{\partial t} \text{d} t + (\nabla \phi)^T\cdot \text{d} x + \frac{1}{2} tr\{ \nabla \nabla^T \phi\} \text{d} x\text{d} x^T\end{aligned}\]</span> provided that the required partial derivatives exist, wherethe mixed partial derivatives are combined according to the rules <span class="math display">\[\begin{aligned}\text{d} \beta \text{d} t &amp;= 0, \\\text{d} t \text{d} \beta &amp;= 0, \\\text{d} \beta \text{d} \beta^T &amp;= Q \text{d} t.\end{aligned}\]</span> (Q is the diffusion matrix(covariance matrix) of the Brownianmotion). It can be derived from the Taylor expansion of <span class="math inline">\(\phi(x(t), t)\)</span>. Usually, in deterministiccase, we could ignore the second-order, we have <span class="math display">\[\begin{aligned}\text{d} \phi &amp;= \frac{\partial \phi}{\partial t} \text{d} t +\frac{\partial \phi}{\partial x} \text{d} x.\end{aligned}\]</span> In stochastic case, because <span class="math inline">\(\text{d} \beta \text{d} \beta^T = Q \text{d}t\)</span>, which is order one, the <span class="math inline">\(\text{d}x \text{d} x^T\)</span> is potentially of order one, so we need toconsider the second-order term.</p><ul><li>Here the Itô formula is derived for a scalar function <span class="math inline">\(\phi(x(t), t)\)</span>. However, for vectorfunction, it works for each of the components of a vector-valuedfunction separately and thus also includes the vector case.</li></ul><p>Example: We can apply the Itô formula to the function <span class="math inline">\(\phi(x(t), t) = x^2(t)/2\)</span>, with <span class="math inline">\(x(t) = \beta(t)\)</span>, where <span class="math inline">\(\beta(t)\)</span> is a standard Brownian motion(q=1). The Itô SDE of <span class="math inline">\(\phi\)</span> is <span class="math display">\[\begin{aligned}\text{d} \phi &amp;= \beta \text{d} \beta + \frac{1}{2} \text{d} \beta\text{d} \beta \\&amp;= \beta \text{d} \beta +\frac{1}{2} \text{d} t.\end{aligned}\]</span></p><h1 id="reference">Reference</h1><p>Simo Särkkä and Arno Solin (2019). Applied Stochastic DifferentialEquations. Cambridge University Press.</p>]]></content>
      
      
      <categories>
          
          <category> Generative Model </category>
          
      </categories>
      
      
        <tags>
            
            <tag> SDE </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Gaussian processes</title>
      <link href="/2023/08/06/gaussianprocess/"/>
      <url>/2023/08/06/gaussianprocess/</url>
      
        <content type="html"><![CDATA[<h1 id="jointly-gaussian-random-variables">Jointly Gaussian randomvariables</h1><p>Definition: Random variables (RV) <span class="math inline">\(X_1,..., X_n\)</span> are jointly Gaussian if any linear combination of themis Gaussian.</p><p>RV <span class="math inline">\(X = [X_1, ..., X_n]^{T}\)</span> isGaussian <span class="math inline">\(\leftrightarrows\)</span> Given anyscalars <span class="math inline">\(a_1,... a_n\)</span>, the RV <span class="math inline">\(Y = a_1 X_1 + a_2 X_2 + ... + a_n X_n\)</span> isGaussian distributed.</p><h2 id="pdf-of-jointly-gaussian-rvs-in-n-dimensions">Pdf of jointlyGaussian RVs in n dimensions</h2><p>Let <span class="math inline">\(X \in \mathbb{R}^n\)</span>, <span class="math inline">\(\mu = \mathbb{E}[X]\)</span>,</p><p>covariance matrix <span class="math display">\[C:= \mathbb{E}[(X -\mu)(X - \mu)^T] =\begin{pmatrix}    \sigma_{11}^2 &amp; \sigma_{12}^2 &amp; \cdots &amp; \sigma_{1n}^2\\    \sigma_{21}^2 &amp; \sigma_{22}^2 &amp; \cdots &amp; \sigma_{2n}^2\\    \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\    \sigma_{n1}^2 &amp; \sigma_{n2}^2 &amp; \cdots &amp; \sigma_{nn}^2\end{pmatrix}\]</span> Then, the pdf of RV <span class="math inline">\(X\)</span> canbe defined as <span class="math display">\[p(x) = \frac{1}{(2\pi)^{n/2} \text{det}^{1/2}(C)} \exp(-\frac{1}{2}(x -\mu)^T C^{-1} (x - \mu)).\]</span></p><ul><li><span class="math inline">\(C\)</span> is invertible</li><li>We can verify all linear combinations is Gaussian.</li><li>To fully specify the probability distribution of a Gaussian vector<span class="math inline">\(X\)</span>, the mean vector <span class="math inline">\(\mu\)</span> and covariance matrix <span class="math inline">\(C\)</span> suffice.</li></ul><h1 id="gaussian-processes">Gaussian processes</h1><p>Gaussian processes (GP) generalize Gaussian vectors to<strong>infinite</strong> dimensions.</p><p>Definition. <span class="math inline">\(X(t)\)</span> is a GP if anylinear combination of values <span class="math inline">\(X(t)\)</span>is Gaussian. That is, for arbitrary <span class="math inline">\(n &gt;0\)</span>, times <span class="math inline">\(t_1, ..., t_n\)</span> andconstants <span class="math inline">\(a_1, ..., a_n\)</span>, <span class="math inline">\(Y = a_1 X(t_1) + a_2 X(t_2) + ... + a_nX(t_n)\)</span> is Gaussian distributed.</p><ul><li><p>Time index <span class="math inline">\(t\)</span> can becontinuous or discrete.</p></li><li><p>Any linear functional of <span class="math inline">\(X(t)\)</span> is Gaussian distributed. Forexample, the integral <span class="math inline">\(Y = \int_{t_1}^{t_2}X(t) \text{d}t\)</span> is Gaussian distributed.</p></li></ul><h2 id="jointly-pdf-in-a-gaussian-process">Jointly pdf in a Gaussianprocess</h2><p>Consider times <span class="math inline">\(t_1,..., t_n\)</span>, themean value <span class="math inline">\(\mu(t_i)\)</span> is <span class="math display">\[\mu(t_i) = \mathbb{E}[X(t_i)].\]</span></p><p>The covariance between values at time <span class="math inline">\(t_i\)</span> and <span class="math inline">\(t_j\)</span> is <span class="math inline">\(C(t_i,t_j) := \mathbb{E}[(X(t_i) - \mu(t_i))(X(t_j) -\mu(t_j))^T]\)</span>.</p><p>The covariance matrix for values <span class="math inline">\(X(t_1),..., X(t_n)\)</span> is <span class="math display">\[C(t_1,..., t_n) =\begin{pmatrix}    C_{t_1, t_1} &amp; C_{t_1, t_2} &amp; \cdots &amp; C_{t_1, t_n}\\    C_{t_2, t_1} &amp; C_{t_2, t_2} &amp; \cdots &amp; C_{t_2, t_n} \\    \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\    C_{t_n, t_1} &amp; C_{t_n, t_2} &amp; \cdots &amp; C_{t_n, t_n}\end{pmatrix}\]</span>.</p><p>The jointly pdf of <span class="math inline">\(X(t_1),...,X(t_n)\)</span> is <span class="math inline">\(N([\mu(t_1), ...,\mu(t_n)]^T, C(t_1,..., t_n))\)</span>.</p><h2 id="mean-value-and-autocorrelation-functions">Mean value andautocorrelation functions</h2><p>To specify a Gaussian process, we only need to specify:</p><ul><li><p>Mean value function: <span class="math inline">\(\mu(t) =\mathbb{E}[x(t)]\)</span>.</p></li><li><p>Autocorrelation function (symmetric): <span class="math inline">\(R(t_1, t_2) = \mathbb{E}[X(t_1)X(t_2)]\)</span>.</p></li></ul><p>The autocovariance <span class="math inline">\(C(t_1, t_2) = R(t_1,t_2) - \mu(t_1) \mu (t_2)\)</span>.</p><p>More general, we consider GP with <span class="math inline">\(\mu(t)= 0\)</span>. [define new process <span class="math inline">\(Y(t) =X(t) - \mu(t)\)</span>]. In this case, <span class="math inline">\(C(t_1, t_2) = R(t_1, t_2)\)</span>.</p><p>All probs. in a GP can be expressed in terms of <span class="math inline">\(\mu(t)\)</span> and <span class="math inline">\(R(t, t)\)</span>. <span class="math display">\[p(x_t) = \frac{1}{\sqrt{2\pi (R(t,t) - \mu^2(t))}} \exp(- \frac{(x_t -\mu(t))^2}{2(R(t,t) - \mu^2(t))}).\]</span></p><h2 id="conditional-probabilities-in-a-gp">Conditional probabilities ina GP</h2><p>Consider a zero-mean GP <span class="math inline">\(X(t)\)</span>,two times <span class="math inline">\(t_1\)</span> and <span class="math inline">\(t_2\)</span>. The covariance matrix is <span class="math display">\[C = \begin{pmatrix}  R(t_1, t_1) &amp; R(t_1, t_2) \\  R(t_1, t_2) &amp; R(t_2, t_2)\end{pmatrix}\]</span></p><p>The jointly pdf of <span class="math inline">\(X(t_1)\)</span> and<span class="math inline">\(X(t_2)\)</span> is <span class="math display">\[p(x_{t_1}, x_{t_2}) = \frac{1}{2\pi \text{det}^{1/2} C}\exp(-\frac{1}{2}[x_{t_1}, x_{t_2}]^T C^{-1} [x_{t_1}, x_{t_2}])\]</span></p><p>The conditional pdf of <span class="math inline">\(X(t_1)\)</span>given <span class="math inline">\(X(t_2)\)</span> is <span class="math display">\[p_{X(t_1)| X(t_2)}(x_{t_1} | x_{t_2}) = \frac{p(x_{t_1},x_{t_2})}{p(x_{t_2})}. \qquad (1)\]</span></p><h1 id="brownian-motion-process-a.k.a-wiener-process">Brownian motionprocess (a.k.a Wiener process)</h1><p>Definition. A Brownian motion process (a.k.a Wiener process)satisfies</p><ol type="1"><li><p><span class="math inline">\(X(t)\)</span> is normally distributedwith zero mean and variance <span class="math inline">\(\sigma^2t\)</span>, <span class="math display">\[X(t) \sim N(0, \sigma^2t).\]</span></p></li><li><p>Independent increments. For all times <span class="math inline">\(0 &lt; t_1 &lt; t_2 &lt; \cdots &lt; t_n\)</span>,the random variables <span class="math inline">\(X(t_1), X(t_2) -X(t_1), ..., X(t_n) - X(t_{n-1})\)</span> are independent.</p></li><li><p>Stationary increments. Probability distribution of increment<span class="math inline">\(X(t+s) - X(s)\)</span> is the same asprobability distribution of <span class="math inline">\(X(t)\)</span>.<span class="math display">\[[X(t+s) - X(s)]  \sim N(0, \sigma^2t).\]</span></p></li></ol><ul><li>Brownian motion is a Markov process.</li><li>Brownian motion is a Gaussian process.</li></ul><h2 id="mean-and-autocorrelation-of-brownian-motion">Mean andautocorrelation of Brownian motion</h2><p>1, Mean funtion <span class="math inline">\(\mu(t) = \mathbb{E}[X(t)]= 0\)</span>.</p><p>2, Autocorrelation of Brownian motion <span class="math inline">\(R(t_1, t_2) = \sigma^2 \min\{t_1,t_2\}\)</span>.</p><p>Proof. Assume <span class="math inline">\(t_1 &lt; t_2\)</span>, thenautocorrelation <span class="math inline">\(R(t_1, t_2) =\mathbb{E}[X(t_1)X(t_2)] = \sigma^2 t_1\)</span>.</p><p>If <span class="math inline">\(t_1 &lt; t_2\)</span>, according toconditional expectations, we have <span class="math display">\[\begin{aligned}  R(t_1, t_2) = \mathbb{E}[X(t_1)X(t_2)] &amp;=\mathbb{E}_{X(t_1)}[\mathbb{E}_{X(t_2)}[X(t_1)X(t_2) | X(t_1)]] \\  &amp;= \mathbb{E}_{X(t_1)}[X(t_1)\mathbb{E}_{X(t_2)}[X(t_2) | X(t_1)]]\end{aligned}\]</span> According to equation (1), the condition distribution of <span class="math inline">\(X(t_2)\)</span> given <span class="math inline">\(X(t_1)\)</span> is <span class="math display">\[[X(t_2) | X(t_1)] \sim N(X(t_1), \sigma^2 (t_2 - t_1)),\]</span> thus, <span class="math inline">\(\mathbb{E}_{X(t_2)}[X(t_2) |X(t_1)] = X(t_1)\)</span>.</p><p><span class="math display">\[\begin{aligned}  R(t_1, t_2) = \mathbb{E}[X(t_1)X(t_2)] &amp;=\mathbb{E}_{X(t_1)}[\mathbb{E}_{X(t_2)}[X(t_1)X(t_2) | X(t_1)]] \\  &amp;= \mathbb{E}_{X(t_1)}[X(t_1) X(t_1)] \\  &amp;= \mathbb{E}_{X(t_1)}[X^2(t_1)] = \sigma^2 t_1.\end{aligned}\]</span></p><p>Similarly, if <span class="math inline">\(t_2 &lt; t_1\)</span>,<span class="math inline">\(R(t_1, t_2) = \sigma^2 t_2\)</span>.</p><h2 id="brownian-motion-with-drift-bmd">Brownian motion with drift(BMD)</h2><p>For Brownian motion, it is an unbiased random walk. Walker stepsright or left with the same probability <span class="math inline">\(1/2\)</span> for each direction (onedimension).</p><p>For BMD, it is a biased random walk. Walker steps right or left withdifferent probs.</p><p>For example, consider time interval <span class="math inline">\(h\)</span>, step size <span class="math inline">\(\sigma \sqrt{h}\)</span>, <span class="math display">\[p(X(t+h) = x + \sigma \sqrt{h} | X(t) = x) = \frac{1}{2} (1 +\frac{\mu}{\sigma} \sqrt{h}).\]</span> <span class="math display">\[p(X(t+h) = x - \sigma \sqrt{h} | X(t) = x) = \frac{1}{2} (1 -\frac{\mu}{\sigma} \sqrt{h}).\]</span></p><ul><li><p><span class="math inline">\(\mu &gt; 0\)</span>, biased to theright. <span class="math inline">\(\mu &lt; 0\)</span>, biased to theleft.</p></li><li><p><span class="math inline">\(h\)</span> needs to be small enoughto make <span class="math inline">\(|\frac{\mu}{\sigma} \sqrt{h} | \leq1\)</span>.</p></li></ul><p>In this BMD case, <span class="math inline">\(x(t) \sim N(\mu t,\sigma^2 t)\)</span>.</p><ul><li>Independent and stationary increments.</li></ul><p>(We omit the proof. More details can be found at <a href="https://www.hajim.rochester.edu/ece/sites/gmateos/ECE440/Slides/block_5_stationary_processes_part_a.pdf">Gaussianprocess</a> ).</p><h2 id="geometric-brownian-motion-gbm">Geometric Brownian motion(GBM)</h2><p>Definition. Suppose that <span class="math inline">\(Z(t)\)</span> isa standard Brownian motion <span class="math inline">\(Z(t) \sim N(0,t)\)</span>. Parameters <span class="math inline">\(\mu \in\mathbb{R}\)</span> and <span class="math inline">\(\sigma \in (0,\infty)\)</span>. Let <span class="math display">\[X(t) = \exp[(\mu - \frac{\sigma^2}{2})t + \sigma Z(t)], \qquad t \geq 0.\qquad (2)\]</span> The stochastic process <span class="math inline">\(\{X(t): t\geq 0\}\)</span> is geometric Brownian motion with drift parameter<span class="math inline">\(\mu\)</span> and volatility parameter <span class="math inline">\(\sigma\)</span>.</p><ul><li>The process is always positive, one of the reasons that geometricBrownian motion is used to model financial and other processes that<strong>cannot be negative</strong>.</li><li>For the stochastic process</li></ul><p><span class="math display">\[(\mu - \frac{\sigma^2}{2})t + \sigma Z(t) \sim N((\mu -\frac{\sigma^2}{2})t , \sigma^2 t),  \]</span></p><p>it is a BMD with drift parameter <span class="math inline">\(\mu -\sigma^2/2\)</span> and scale parameter <span class="math inline">\(\sigma\)</span>. Thus, the geometric Brownianmotion is just the exponential of this BMD process.</p><ul><li><p>Here <span class="math inline">\(X(0) = 1\)</span>, the processstarts at 1. For GBM starting at <span class="math inline">\(X(0) =x_0\)</span>, the process is <span class="math display">\[X(t) = x_0 \exp[(\mu - \frac{\sigma^2}{2})t + \sigma Z(t)], \qquad t\geq 0.\]</span></p></li><li><p>GBM is not a Gaussian process.</p></li></ul><p>From the definition of GBM (2), we can have the followingdifferential equation: <span class="math display">\[\begin{aligned}  \frac{\text{d}X}{\text{d} t} &amp;= \exp[(\mu - \frac{\sigma^2}{2})t +\sigma Z(t)][(\mu - \frac{\sigma^2}{2}) + \sigma\frac{\text{d}Z}{\text{d} t}] \\  &amp;= X [(\mu - \frac{\sigma^2}{2}) + \sigma\frac{\text{d}Z}{\text{d} t}] \\  &amp;= X \tilde{\mu} + \sigma X \frac{\text{d}Z}{\text{d} t},  \qquad(\tilde{\mu} := \mu - \frac{\sigma^2}{2})\end{aligned}\]</span> thus, Geometric Brownian motion <span class="math inline">\(X(t)\)</span> satisfies the stochasticdifferential equation <span class="math display">\[\begin{aligned}\frac{\text{d}X}{\text{d} t}  &amp;= X \tilde{\mu} + \sigma X\frac{\text{d}Z}{\text{d} t},  \\  \text{d}X &amp; = X \tilde{\mu} {\text{d} t} + \sigma X \text{d}Z.\end{aligned}\]</span></p><p>The second equation is the Black–Scholes model. In the Black–Scholesmodel, <span class="math inline">\(X(t)\)</span> is the stock price.</p><ul><li><a href="https://stats.libretexts.org/Bookshelves/Probability_Theory/Probability_Mathematical_Statistics_and_Stochastic_Processes_(Siegrist)/18%3A_Brownian_Motion/18.04%3A_Geometric_Brownian_Motion">GeometricBrownian motion</a></li></ul><h1 id="white-gaussian-process">White Gaussian process</h1><p>Definition. A white Gaussian noise (WGN) process <span class="math inline">\(W(t)\)</span> is a GP with</p><ol type="1"><li><p>zero mean: <span class="math inline">\(\mu(t) = \mathbb{E}[W(t)]= 0\)</span> for all <span class="math inline">\(t\)</span>.</p></li><li><p>Delta function antocorrelation: <span class="math inline">\(R(t_1, t_2) = \sigma^2 \delta(t_1 -t_2)\)</span>.</p></li></ol><p>Here the Dirac delta is often thought as a function that is 0everywhere and infinite at 0. <span class="math display">\[\delta(t) =\begin{cases}\infty, &amp; t=0 \\0, &amp; t\ne 0\end{cases}.\]</span> The Dirac delta is actually a distribution, a generalizationof functions, and it is defined through the integral of its product withan arbitrary function <span class="math inline">\(f(t)\)</span>. <span class="math display">\[\int_{a}^{b} f(t)\delta(t) \text{d} t=\begin{cases}f(0), &amp; a &lt; 0 &lt; b \\0, &amp; \text{otherwise}\end{cases}.\]</span></p><!-- since the autocorrelation function of W(t) is not really a function (it involves theDirac delta), WGN cannot model any real physical phenomena. Nonetheless, it is a convenientabstraction to generate processes that can model real physical phenomena. --><p>Properties of white Gaussian noise:</p><ol type="1"><li><p>For <span class="math inline">\(t_1 \ne t_2\)</span>, <span class="math inline">\(W(t_1)\)</span> and <span class="math inline">\(W(t_2)\)</span> are uncorrelated. <span class="math display">\[\mathbb{E}[W(t_1)W(t_2)] = R(t_1, t_2) = 0, \qquad t_1 \ne t_2.\]</span> This means <span class="math inline">\(W(t)\)</span> atdifferent times are independent.</p></li><li><p>WGN has infinite variance (large power). <span class="math display">\[\mathbb{E}[W^2(t)] = R(t, t) = \sigma^2 \delta(0) = \infty.\]</span></p></li></ol><ul><li>WGN is discontinuous almost everywhere.</li><li>WGN is unbounded and it takes arbitrary large positive and negativevalues at any finite interval.</li></ul><h2 id="white-gaussian-noise-and-brownian-motion">White Gaussian noiseand Brownian motion</h2><p>Remember that the Brownian motion is a solution to the differentialequation: <span class="math display">\[\frac{\text{d} X(t)}{\text{d}t} = W(t).\]</span> <strong>Why <span class="math inline">\(\frac{\text{d}X(t)}{\text{d}t}\)</span> is called white noise ?</strong></p><p>Proof. Assume <span class="math inline">\(X(t)\)</span> is theintegral of a WGN process <span class="math inline">\(W(t)\)</span>,i.e., <span class="math inline">\(X(t) = \int_{0}^{t} W(u) \text{d}u\)</span>.</p><p>Since integration is linear functional and <span class="math inline">\(W(t)\)</span> is a GP, <span class="math inline">\(X(t)\)</span> is also a GP.</p><p>A Gaussian process can be uniquely specified by its Mean valuefunction and Autocorrelation function.</p><ol type="1"><li>The mean function: <span class="math display">\[\mu(t) = \mathbb{E}[\int_{0}^{t} W(u) \text{d} u] = \int_{0}^{t}\mathbb{E} [W(u)] \text{d} u = 0.\]</span><br></li><li>The autocorrelation <span class="math inline">\(R_{X}(t_1,t_2)\)</span> with <span class="math inline">\(t_1 &lt; t_2\)</span>:<span class="math display">\[\begin{aligned}  R_{X}(t_1, t_2) &amp;= \mathbb{E}[(\int_{0}^{t_1} W(u_1) \text{d}u_1)(\int_{0}^{t_2} W(u_2) \text{d} u_2)] \\  &amp;= \mathbb{E}[\int_{0}^{t_1} \int_{0}^{t_2} W(u_1)  W(u_2)\text{d} u_1 \text{d} u_2] \\  &amp;= \int_{0}^{t_1} \int_{0}^{t_2} \mathbb{E}[W(u_1)  W(u_2)]\text{d} u_1 \text{d} u_2 \\  &amp;= \int_{0}^{t_1} \int_{0}^{t_2} \sigma^2 \delta(u_1 - u_2)\text{d} u_1 \text{d} u_2 \\  &amp;= \int_{0}^{t_1} \int_{0}^{t_1} \sigma^2 \delta(u_1 - u_2)\text{d} u_1 \text{d} u_2 + \int_{0}^{t_1} \int_{t_1}^{t_2} \sigma^2\delta(u_1 - u_2) \text{d} u_1 \text{d} u_2 \\  &amp;= \int_{0}^{t_1} \int_{0}^{t_1} \sigma^2 \delta(u_1 - u_2)\text{d} u_1 \text{d} u_2 + 0\\  &amp;= \int_{0}^{t_1} \sigma^2 \text{d} u_1 \\  &amp;= \sigma^2 t_1.\end{aligned}\]</span> If <span class="math inline">\(t_2 &lt; t_1\)</span>, we canobtain <span class="math inline">\(R_{X}(t_1, t_2) = \sigma^2t_2\)</span>. Thus, <span class="math inline">\(R_{X}(t_1, t_2) =\sigma^2 \min \{t_1, t_2\}\)</span>.</li></ol><p>The mean function and autocorrelation function are the same asBrownian motion!</p><p>Because a Gaussian process can be uniquely determined by its meanvalue function and autocorrelation function. We can conclude</p><ul><li>The integral of WGN is a Brownian motion process.</li><li>The derivative of Brownian motion is WGN.</li></ul><h1 id="reference">Reference</h1><p><a href="https://www.hajim.rochester.edu/ece/sites/gmateos/ECE440/Slides/block_5_stationary_processes_part_a.pdf">Gaussianprocess</a></p><p><a href="https://stats.libretexts.org/Bookshelves/Probability_Theory/Probability_Mathematical_Statistics_and_Stochastic_Processes_(Siegrist)/18%3A_Brownian_Motion/18.04%3A_Geometric_Brownian_Motion">GeometricBrownian motion</a></p><p><a href="http://www.columbia.edu/~ks20/FE-Notes/4700-07-Notes-GBM.pdf">GeometricBrownian motion</a></p><p><a href="https://www.seas.upenn.edu/~ese3030/homework/week_11/week_11_white_gaussian_noise.pdf">WhiteGaussian noise</a></p>]]></content>
      
      
      <categories>
          
          <category> Stochastic Process </category>
          
      </categories>
      
      
        <tags>
            
            <tag> SDE </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Ordinary Differential Equation</title>
      <link href="/2023/07/19/ODE/"/>
      <url>/2023/07/19/ODE/</url>
      
        <content type="html"><![CDATA[<h1 id="ordinary-differential-equation-ode">Ordinary DifferentialEquation (ODE)</h1><h2 id="the-defination-of-ode">The defination of ODE</h2><p>An ODE is an equation in which the unknown quantity is a function,and it also involves derivatives of the unknown function.</p><p>For example, the forced spring-mass system: <span class="math display">\[\frac{d^2 x(t)}{d t^2} + \gamma \frac{d x(t)}{dt} + v^2 x(t) =w(t).  \qquad (1)\]</span> In this equation:</p><ul><li><p><span class="math inline">\(v\)</span> and <span class="math inline">\(\gamma\)</span> are constants that determine theresonant angular velocity and damping of the spring.</p></li><li><p><span class="math inline">\(w(t)\)</span> is a given functionthat may or may not depend on time.</p></li><li><p>position variable <span class="math inline">\(x\)</span> iscalled dependent variable.</p></li><li><p>time <span class="math inline">\(t\)</span> is called independentvariable.</p></li><li><p>This equation is <strong>second order</strong>. It contains thesecond derivative and doesn't have higher-order terms.</p></li><li><p>This equation is <strong>linear</strong>. <span class="math inline">\(x(t)\)</span> is linearly. There is no terms like<span class="math inline">\(x^2(t), \log x(t)\)</span>...</p></li><li><p>This equation is <strong>inhomogeneous</strong>. Because itcontains forcing term <span class="math inline">\(w(t)\)</span>.</p></li></ul><h2 id="the-solution-to-ode">The solution to ODE</h2><p>It can be divided to two categories: - particular solution: afunction that satisfies the differential equation and does not containany arbitrary constants.</p><ul><li>general solution: a function that satisfies the differentialequation and contains free constants.</li></ul><p>To exactly solve the differential equation, it is necessary tocombine the general solution with some initial conditions (i.e., <span class="math inline">\(x(t_0)\)</span>, <span class="math inline">\(\frac{d x(t)}{dt} |_{t_0}\)</span>) or some other(boundary) conditions of the differential equation.</p><h2 id="different-formulation-of-ode">Different formulation of ODE</h2><p>It is common to omit the time <span class="math inline">\(t\)</span>,so equation (1) can also be writen is this form: <span class="math display">\[\frac{d^2 x}{d t^2} + \gamma \frac{d x}{dt} + v^2 x = w.  \]</span></p><p>Sometimes, time derivatives are also denoted with dots over thevariable, for example:</p><p><span class="math display">\[\ddot{x} + \gamma \dot x + v^2 x = w.  \qquad (2)\]</span></p><h2 id="state-space-form-of-the-differential-equation-first-order-vector-differential-equation">State-spaceform of the differential equation (first-order vector differentialequation)</h2><ul><li><p><strong>order</strong>: the order of a differential equation isthe order of the highest derivative that appears in theequation.</p></li><li><p>Order <span class="math inline">\(N\)</span> ODE can convert toOrder 1 vector ODE i.e., if we define a state variable <span class="math inline">\(\vec{x} = (x_1 = x, x_2 = \dot{x})\)</span>, thenwe can convert equation（2） to <span class="math display">\[\begin{pmatrix}\frac{d x_1 (t)}{dt} \\\frac{d x_2 (t)}{dt}\end{pmatrix}=\begin{pmatrix}0 &amp; 1 \\-v^2 &amp; -\gamma\end{pmatrix}\begin{pmatrix}x_1 (t) \\x_2 (t)\end{pmatrix} +\begin{pmatrix}0 \\1\end{pmatrix}w(t) \qquad (3)\]</span></p></li></ul><p>Define <span class="math inline">\(\frac{d \vec{x}(t)}{dt} =\begin{pmatrix}\frac{d x_1 (t)}{dt} \\\frac{d x_2 (t)}{dt}\end{pmatrix},\)</span> <span class="math inline">\(f(\vec{x}(t)) =\begin{pmatrix}0 &amp; 1 \\-v^2 &amp; -\gamma\end{pmatrix}\begin{pmatrix}x_1 (t) \\x_2 (t)\end{pmatrix}\)</span> and <span class="math inline">\(\mathbf{L} =\begin{pmatrix}0 \\1\end{pmatrix}.\)</span></p><p>Equation (3) can be seen to be a special case of this form: <span class="math display">\[\frac{d \vec{x}(t)}{dt} = f(\vec{x}(t), t) + \mathbf{L}(\vec{x}(t),t)\vec{w}(t)\]</span> where the vector-valued function <span class="math inline">\(\vec{x}(t) \in R^D\)</span> is the state of thesystem. <span class="math inline">\(f(\cdot, \cdot)\)</span> and <span class="math inline">\(\mathbf{L}(\cdot, \cdot)\)</span> are arbitraryfunctions. <span class="math inline">\(\vec{w}(t)\)</span> is some(vector-valued) forcing function, driving function, or input to thesystem.</p><ul><li><p>The first-order vector differential equation representation of an<span class="math inline">\(n\)</span>th-order differential equation isoften called the state-space form of the differential equation.</p></li><li><p>The theory and solution methods for first-order vectordifferential equations are easier to analyze.</p></li><li><p>And, <span class="math inline">\(n\)</span> th order differentialequations can (almost) always be converted into equivalent <span class="math inline">\(n\)</span>-dimensional vector-valued first-orderdifferential equations.</p></li></ul><h2 id="linear-odes">Linear ODEs</h2><p>Equation (3) is also a special case of the <strong>lineardifferential equations</strong>: <span class="math display">\[\frac{d \vec{x}(t)}{dt} = \mathbf{F}(t) \vec{x}(t) + \mathbf{L}\vec{w}(t).\]</span> where <span class="math inline">\(\mathbf{F}(t)\)</span> is amatrix-valued function of time. <span class="math inline">\(\mathbf{L}\)</span> is a matrix. <span class="math inline">\(\vec{w}(t)\)</span> is a vector-valued function oftime.</p><ul><li><strong>homogeneous</strong>: the equation is homogeneous if theforcing function <span class="math inline">\(\vec{w}(t)\)</span> is zerofor all <span class="math inline">\(t\)</span>.</li><li><strong>time-invariant</strong>: the equation is time-invariant if<span class="math inline">\(\mathbf{F}(t)\)</span> is constant for all<span class="math inline">\(t\)</span>.</li></ul><p>In the following sections, we first start with the simple scalarlinear time-invariant homogeneous differential equation with fixedinitial condition at <span class="math inline">\(t = 0\)</span>. Then,we will consider the multidimensional generalization of this equation.Besides, we also consider the linear time-invariant inhomogeneousdifferential equations. Finally, we will consider more generaldifferential equations.</p><h4 id="solutions-of-linear-time-invariant-differential-equations">Solutionsof Linear Time-Invariant Differential Equations</h4><p>Consider the <strong>scalar</strong> linear<strong>homogeneous</strong> differential equation with fixed initialcondition at <span class="math inline">\(t = 0\)</span>: <span class="math display">\[\frac{d x(t)}{dt} = F x(t),  x(0) = \text{given}, \qquad (4)\]</span> where <span class="math inline">\(F\)</span> is aconstant.</p><p>This equation can be solved by separation of variables: <span class="math display">\[\frac{d x(t)}{x(t)} = F dt.\]</span> Integrating the left-hand side from <span class="math inline">\(x(0)\)</span> to <span class="math inline">\(x(t)\)</span>, and right-hand side from <span class="math inline">\(0\)</span> to <span class="math inline">\(t\)</span>, we get <span class="math display">\[\log \frac{x(t)}{x(0)} = F t.\]</span> Combining the initial condition, we get <span class="math display">\[x(t) = x(0) e^{F t}.\]</span></p><p>Another way to solve this equation is to by intergrating both sidesof equation (4) from <span class="math inline">\(0\)</span> to <span class="math inline">\(t\)</span>. We get <span class="math display">\[x(t) = x(0) + \int_0^t F x(\tau) d \tau.\]</span> The we can substitute the solution back into the right-handside of the equation, and we get <span class="math display">\[\begin{aligned}x(t) &amp;= x(0) + \int_0^t F x(\tau) d \tau \\&amp;= x(0) + \int_0^t F [x(0) + \int_0^{\tau} F x(\tau') d\tau'] d \tau \\&amp;= x(0) + F x(0) t +  \int_0^t \int_0^{\tau}F^2 x(\tau') d\tau' d \tau \\&amp;= x(0) + F x(0) t +  F^2 x(0) \frac{t^2}{2} + \int_0^t\int_0^{\tau} \int_0^{\tau'}F^3 x(\tau'')  d \tau''d \tau' d \tau \\&amp;= ...\\&amp;= x(0) + F x(0) t +  F^2 x(0) \frac{t^2}{2} + F^3 x(0)\frac{t^3}{6} + \cdots\\&amp;=  (1 + Ft + F^2 t^2 /2 + F^3 t^3 /3! + \cdots) x(0)\\&amp;=  e^{F t} x(0).  \qquad (\text{Taylor expansion})\end{aligned}\]</span></p><table style="width:26%;"><colgroup><col style="width: 26%"></colgroup><thead><tr><th style="text-align: left;">For the <strong>multidimensional</strong>linear <strong>homogeneous</strong> differential equation with fixedinitial condition at <span class="math inline">\(t = 0\)</span>: <span class="math display">\[\frac{d \vec{x}(t)}{dt} = \mathbf{F} \vec{x}(t),  \vec{x}(0) =\text{given}, \qquad (5)\]</span> where <span class="math inline">\(\mathbf{F}\)</span> is aconstant matrix.</th></tr></thead><tbody><tr><td style="text-align: left;">Next, let's move to the linear<strong>time-invariant</strong> <strong>inhomogeneous</strong>differential equations: <span class="math display">\[\frac{d \vec{x}(t)}{dt} = \mathbf{F} \vec{x}(t) + \mathbf{L}\vec{w}(t),  \vec{x}(t_0) = \text{given}, \qquad (7)\]</span> where <span class="math inline">\(\mathbf{F}\)</span> is aconstant matrix, <span class="math inline">\(\mathbf{L}\)</span> is aconstant matrix, and <span class="math inline">\(\vec{w}(t)\)</span> isa vector-valued function of time.</td></tr><tr><td style="text-align: left;">First, we can move the <span class="math inline">\(\mathbf{F} \vec{x}(t)\)</span> to the left-handside of the equation and multify both sides with a integrating factor<span class="math inline">\(\exp(- \mathbf{F}t)\)</span>, and get <span class="math display">\[\begin{aligned}\exp(- \mathbf{F}t) \frac{d \vec{x}(t)}{dt} - \exp(-\mathbf{F}t)\mathbf{F} \vec{x}(t) = \exp(- \mathbf{F}t) \mathbf{L}\vec{w}(t),\end{aligned}\]</span> Since <span class="math inline">\(\frac{d}{dt} \exp(-\mathbf{F}t) \vec{x}(t) = \exp(- \mathbf{F}t) \frac{d \vec{x}(t)}{dt} -\exp(- \mathbf{F}t)\mathbf{F} \vec{x}(t)\)</span>, we can rewrite theabove equation as <span class="math display">\[\frac{d}{dt} \exp(- \mathbf{F}t) \vec{x}(t) = \exp(- \mathbf{F}t)\mathbf{L} \vec{w}(t).\]</span> Integrating both sides from <span class="math inline">\(t_0\)</span> to <span class="math inline">\(t\)</span>, we get <span class="math display">\[\exp(- \mathbf{F}t) \vec{x}(t) - \exp(- \mathbf{F}t_0) \vec{x}(t_0) =\int_{t_0}^t \exp(- \mathbf{F}\tau) \mathbf{L} \vec{w}(\tau) d \tau.\]</span> Then, we can get the solution of equation (7): <span class="math display">\[\vec{x}(t) = \exp(\mathbf{F}(t - t_0)) \vec{x}(t_0) + \int_{t_0}^t\exp(\mathbf{F}(t - \tau)) \mathbf{L} \vec{w}(\tau) d \tau. \qquad (8)\]</span></td></tr><tr><td style="text-align: left;">### Solutions of General Linear ODEs Theprevious section only consider the linear time-invariant differentialequations(<span class="math inline">\(F\)</span> is a constant). In thissection, we will consider the general linear differential equations withtime-varying coefficients.</td></tr><tr><td style="text-align: left;">For the linear<strong>homogeneous</strong> <strong>time-varying</strong> differentialequation with fixed initial condition at <span class="math inline">\(t =t_0\)</span>: <span class="math display">\[\frac{d \vec{x}(t)}{dt} = \mathbf{F}(t) \vec{x}(t), \quad \vec{x}(t_0) =\text{given}, \qquad (9)\]</span> where <span class="math inline">\(\mathbf{F}(t)\)</span> is amatrix-valued function of time. We can not use the exponential matrix tosolve this equation, because the <span class="math inline">\(\mathbf{F}(t)\)</span> is a time-varying matrix.But the solution of this equation has a general form: <span class="math display">\[\vec{x}(t) = \mathbf{\Psi}(t, t_0) \vec{x}(t_0), \qquad (10)\]</span> where <span class="math inline">\(\mathbf{\Psi}(t,t_0)\)</span> is a matrix-valued function of time, and it is called the<strong>transition matrix</strong>. The transition matrix <span class="math inline">\(\mathbf{\Psi}(t, t_0)\)</span> satisfies thefollowing differential properties: <span class="math display">\[\begin{aligned}\frac{\partial \mathbf{\Psi}(\tau, t)}{\partial \tau} &amp;=\mathbf{F}(\tau) \mathbf{\Psi}(\tau, t), \\\frac{\partial \mathbf{\Psi}(\tau, t)}{\partial t} &amp;= -\mathbf{\Psi}(\tau, t) \mathbf{F}(t), \\\mathbf{\Psi}(\tau, t) &amp;= \mathbf{\Psi}(\tau, s) \mathbf{\Psi}(s, t)\qquad (\text{11}) \\\mathbf{\Psi}(t, \tau) &amp;= \mathbf{\Psi}^{-1}(\tau, t)  \\\mathbf{\Psi}(t, t) &amp;= \mathbf{I}.\end{aligned}\]</span> In most cases, the transition matrix <span class="math inline">\(\mathbf{\Psi}(t, t_0)\)</span> does not have aclosed-form solution.</td></tr></tbody></table><p>For the linear <strong>inhomogeneous</strong><strong>time-varying</strong> differential equation with fixed initialcondition at <span class="math inline">\(t = t_0\)</span>: <span class="math display">\[\frac{d \vec{x}(t)}{dt} = \mathbf{F}(t) \vec{x}(t) + \mathbf{L}(t)\vec{w}(t), \quad \vec{x}(t_0) = \text{given}, \qquad (12)\]</span> where <span class="math inline">\(\mathbf{F}(t)\)</span> is amatrix-valued function of time, <span class="math inline">\(\mathbf{L}(t)\)</span> is a matrix-valued functionof time, and <span class="math inline">\(\vec{w}(t)\)</span> is avector-valued function of time. The solution of this equation has ageneral form: <span class="math display">\[\vec{x}(t) = \mathbf{\Psi}(t, t_0) \vec{x}(t_0) + \int_{t_0}^t\mathbf{\Psi}(t, \tau) \mathbf{L}(\tau) \vec{w}(\tau) d \tau. \qquad(13)\]</span></p><p>When <span class="math inline">\(\mathbf{F}(t)\)</span> and <span class="math inline">\(\mathbf{L}(t)\)</span> is constant, the solutionof equation (7) is a special case of (13) and we can verfiy that the<span class="math inline">\(\Psi(t, t_0) = \exp(\mathbf{F}(t -t_0))\)</span> satisfies properties (10).</p><hr><p>Fourier tansforms and Laplace transforms are two useful methods tosolve inhomogeneous linear time-invariant ODE (Note that<strong>time-invariant</strong>).</p><h2 id="fourier-transforms">Fourier Transforms</h2><p>The Fourier transform of a function <span class="math inline">\(x(t)\)</span> is defined as: <span class="math display">\[X(i w) = \mathcal{F}[x(t)] = \int_{-\infty}^{\infty} x(t) \exp(-iwt)\text{d}t, \qquad (14)\]</span> where <span class="math inline">\(i\)</span> is the imaginaryunit.</p><p>The inverse Fourier transform of (14) is: <span class="math display">\[x(t) = \mathcal{F}^{-1}[X(i w)] = \frac{1}{2\pi} \int_{-\infty}^{\infty}X(iw) \exp(i w t) \text{d}t.\]</span></p><p>Some useful properties:</p><ul><li><p>differention: <span class="math inline">\(\mathcal{F}[\frac{\text{d}^n x(t)}{\text{d} t^n}]= (iw)^{n} \mathcal{F}[x(t)]\)</span>.</p></li><li><p>convolution: <span class="math inline">\(\mathcal{F}[x(t) \asty(t)] = \mathcal{F}[x(t)] \ast \mathcal{F}[y(t)]\)</span>, where theconvolution <span class="math inline">\(\ast\)</span> is defined as<span class="math display">\[x(t) \ast y(t) = \int_{-\infty}^{\infty} x(t - \tau)y(\tau) \text{d}\tau\]</span></p></li></ul><p><strong>If we want to use Fourier transform to solve ODEs, theinitial condition must be 0.</strong></p><p>Now, let's use Fourier transform to solve the linear<strong>time-invariant</strong> <strong>inhomogeneous</strong>differential equations: <span class="math display">\[\frac{d \vec{x}(t)}{dt} = \mathbf{F} \vec{x}(t) + \mathbf{L}\vec{w}(t),  \quad \vec{x}(t_0) = 0,\]</span> where <span class="math inline">\(\mathbf{F}\)</span> is aconstant matrix, <span class="math inline">\(\mathbf{L}\)</span> is aconstant matrix, and <span class="math inline">\(\vec{w}(t)\)</span> isa vector-valued function of time.</p><p>Take Fourier tranforms componentwise give <span class="math display">\[(iw)\vec{X}(iw) = \mathbf{F} \vec{X}(iw) + \mathbf{L} \vec{W}(iw),\]</span> thus, we can get <span class="math display">\[\vec{X}(iw) = [(iw)\mathbf{I} - \mathbf{F}]^{-1} \ast \mathbf{L}\vec{W}(iw),\]</span> The solution is the inverse Fourier transform <span class="math display">\[x(t) = \mathcal{F}^{-1}[[(iw)\mathbf{I} - \mathbf{F}]^{-1} \ast\mathbf{L} \vec{W}(iw)] = \mathcal{F}^{-1}[((iw)\mathbf{I} -\mathbf{F})^{-1}]\ast \mathbf{L} \vec{w}(t), \qquad (15)\]</span></p><p>Since <span class="math inline">\(\vec{x}(t_0) = 0\)</span>, comparethe solution (15) with solution (8), we can obtain <span class="math display">\[\mathcal{F}^{-1}[((iw)\mathbf{I} - \mathbf{F})^{-1}] = \exp(\mathbf{F}t)u(t),\]</span> where <span class="math inline">\(u(t)\)</span> is theHeaviside step function, which is 0 for <span class="math inline">\(t&lt;0\)</span> and 1 for <span class="math inline">\(t \geq 0\)</span>.</p><h2 id="laplace-transforms">Laplace Transforms</h2><p>The Laplace transform of a function <span class="math inline">\(f(t)\)</span> is defined on space <span class="math inline">\(\{t | t\geq 0\}\)</span>: <span class="math display">\[F(s) = \mathcal{L}[f(t)] = \int_{0}^{\infty} f(t)\exp(-st) \text{d}t,\qquad (16)\]</span> where <span class="math inline">\(s = \sigma +iw\)</span>.</p><p>The inverse transform is <span class="math inline">\(f(t) =\mathcal{L}^{-1}[F(s)]\)</span>.</p><p>Remember that the Fourier transform needs the initial condition <span class="math inline">\(x(0) = 0\)</span>. But Laplace transform can takethe initial conditions into account. If <span class="math inline">\(x(0)= \text{given}\)</span>, then <span class="math display">\[\mathcal{L}[\frac{\text{d} x(t)}{\text{d} t}] = s \mathcal{L}[x(t)] -x(0) = s X(s) - x(0).\]</span></p><p><span class="math display">\[\mathcal{L}[\frac{\text{d}^n x(t)}{\text{d} t^n}] = s^n X(s) - s^{n-1}x(0) - \cdots - \frac{\text{d} x^{n-1}}{\text{d} t^{n-1}}(0).\]</span></p><p>Now, let's use Laplace transform to solve the linear<strong>time-invariant</strong> <strong>inhomogeneous</strong>differential equations: <span class="math display">\[\frac{d \vec{x}(t)}{dt} = \mathbf{F} \vec{x}(t) + \mathbf{L}\vec{w}(t),  \quad \vec{x}(t_0) = \text{given} \ne 0,\]</span> where <span class="math inline">\(\mathbf{F}\)</span> is aconstant matrix, <span class="math inline">\(\mathbf{L}\)</span> is aconstant matrix, and <span class="math inline">\(\vec{w}(t)\)</span> isa vector-valued function of time.</p><p>Take Laplace tranforms componentwise give <span class="math display">\[s X(s) - x(0) = \mathbf{F} X(s) + \mathbf{L} W(s).\]</span> Then, <span class="math display">\[X(s) = [s\mathbf{I}-\mathbf{F}]^{-1} x(0) +[s\mathbf{I}-\mathbf{F}]^{-1} \ast  \mathbf{L} W(s).  \qquad (17)\]</span></p><p>Compare the solution (17) with solution (8), we can obtain <span class="math display">\[\mathcal{L}^{-1}[(s\mathbf{I}-\mathbf{F})^{-1}] = \exp(\mathbf{F}t)\]</span> for <span class="math inline">\(t \geq 0\)</span>.</p><h1 id="reference">Reference</h1><p><a href="https://users.aalto.fi/~asolin/sde-book/sde-book.pdf">SimoSärkkä and Arno Solin (2019). Applied Stochastic Differential Equations.Cambridge University Press.</a></p>]]></content>
      
      
      <categories>
          
          <category> Generative Model </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Diffusion </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Convergence of Markov Chain</title>
      <link href="/2023/07/15/Convergence%20of%20MC/"/>
      <url>/2023/07/15/Convergence%20of%20MC/</url>
      
        <content type="html"><![CDATA[<h1 id="total-variation-distance">Total Variation Distance</h1><p>Define Total Variation Distance: <span class="math display">\[d_{TV}(\mu(x), v(x)) = \frac{1}{2} \sum_{x \in \Omega} |\mu(x) - v(x)|\]</span> This is equivalent to the following: <span class="math display">\[d_{TV}(\mu(x), v(x)) = \sum_{x \in \Omega^{-}} (v(x) - \mu(x))= \sum_{x \in \Omega^{+}} (\mu(x) - v(x))\]</span> where <span class="math inline">\(\Omega^{+} = \{x \in \Omega:\mu(x) \geq v(x)\}\)</span>, <span class="math inline">\(\Omega^{-} =\{x \in \Omega: \mu(x) &lt; v(x)\}\)</span>, and <span class="math display">\[d_{TV}(\mu(x), v(x)) = \max_{S \subset \Omega} |\mu(S) - v(S)|\]</span> where <span class="math inline">\(\mu(S) = \sum_{x \in S}\mu(x)\)</span>, <span class="math inline">\(v(S) = \sum_{x \in S}v(x)\)</span>.</p><p>Proof: Define <span class="math inline">\(\Omega^{+} = \{x \in\Omega: \mu(x) \geq v(x)\}\)</span>, <span class="math inline">\(\Omega^{-} = \{x \in \Omega: \mu(x) &lt;v(x)\}\)</span>, then <span class="math display">\[\begin{aligned}d_{TV}(\mu(x), v(x)) &amp;= \frac{1}{2} \sum_{x \in \Omega} |\mu(x) -v(x)| \\&amp;= \frac{1}{2} \sum_{x \in \Omega^{+}} (\mu(x) - v(x)) + \frac{1}{2}\sum_{x \in \Omega^{-}} (v(x) - \mu(x))\end{aligned}\]</span> Since <span class="math inline">\(\sum_{x \in \Omega} \mu(x) =1 = \sum_{x \in \Omega^{+}} \mu(x) + \sum_{x \in \Omega^{-}}\mu(x)\)</span>, we have <span class="math display">\[\begin{aligned}d_{TV}(\mu(x), v(x)) &amp;= \frac{1}{2} \sum_{x \in \Omega^{+}} (\mu(x)- v(x)) + \frac{1}{2} \sum_{x \in \Omega^{-}} (v(x) - \mu(x)) \\&amp;= \frac{1}{2} \sum_{x \in \Omega^{-}} (v(x) - \mu(x)) + \frac{1}{2}\sum_{x \in \Omega^{-}} (v(x) - \mu(x)) \\&amp;= \sum_{x \in \Omega^{-}} (v(x) - \mu(x)) \\&amp;= \sum_{x \in \Omega^{+}} (\mu(x) - v(x))\end{aligned}\]</span> If <span class="math inline">\(S = \Omega^{+}\)</span> or<span class="math inline">\(\Omega^{-}\)</span>, then <span class="math display">\[\begin{aligned}d_{TV}(\mu(x), v(x)) &amp;= \frac{1}{2} \sum_{x \in \Omega} |\mu(x) -v(x)| \\&amp;=  \max_{S \in \Omega} \sum_{x \in S} |\mu(x) - v(x)| \\\end{aligned}\]</span></p><p>If <span class="math inline">\(S\)</span> contains any elements <span class="math inline">\(x \in \Omega^{-}\)</span>, then <span class="math inline">\(d_{TV}(\mu(x), v(x)) = |\sum_{x \in S} (\mu(x) -v(x))| \leq \sum_{x \in \Omega^{+}} (\mu(x) - v(x))\)</span> when <span class="math inline">\(S = \Omega^{+}\)</span>.</p><h1 id="convergence-of-markov-chain">Convergence of Markov Chain</h1><p>Assume we start from state <span class="math inline">\(x\)</span>,run Markov chain for <span class="math inline">\(t\)</span> steps, thenwe get the distribution <span class="math inline">\(P_{x}^{t}\)</span>.If we want to prove <span class="math inline">\(P_{x}^{t}\)</span>converges to stationary distribution <span class="math inline">\(\pi\)</span>, we need to prove <span class="math inline">\(d_{TV}(P_{x}^{t}, \pi) \rightarrow 0\)</span> as<span class="math inline">\(t \rightarrow \infty\)</span>. Thus, we needto bound <span class="math inline">\(d_{TV}(P_{x}^{t},\pi)\)</span>.</p><p>Define <span class="math inline">\(d_{x}(t) := d_{TV}(P_{x}^{t},\pi)\)</span>.</p><p>Consider all possible initial states <span class="math inline">\(x\in \Omega\)</span>, we define <span class="math inline">\(d(t) :=\max_{x \in \Omega} d_{TV} (P_{x}^{t}, \pi)\)</span>.</p><p>Assume there are two Markov chains <span class="math inline">\(x_{t}\sim P_{x}^{t}\)</span>, <span class="math inline">\(y_{t} \simP_{y}^{t}\)</span>. <span class="math inline">\(x_{t}\)</span> and <span class="math inline">\(y_{t}\)</span> share the same transitionprobability matrix <span class="math inline">\(P\)</span> but start fromdifferent states. Then we define <span class="math display">\[\bar{d}(t):= \max_{x, y \in \Omega} d_{TV}(P_{x}^{t}, P_{y}^{t})\]</span>.</p><p>Next, we will prove this Lemma: &gt; Lemma 1. &gt; <span class="math display">\[d(t) \leq \bar{d}(t) \leq 2 d(t).\]</span></p><p>Proof: Let's prove the second inequality <span class="math inline">\(\bar{d}(t) \leq 2 d(t)\)</span>. <span class="math display">\[\begin{aligned}\bar{d}(t) &amp;= \max_{x, y \in \Omega} d_{TV}(P_{x}^{t}, P_{y}^{t}) \\&amp;= \max_{x, y \in \Omega} \frac{1}{2} \sum_{z \in \Omega}|P_{x}^{t}(z) - P_{y}^{t}(z)| \\&amp; = \max_{x, y \in \Omega} \frac{1}{2} \sum_{z \in \Omega}|P_{x}^{t}(z) - \pi(z) + \pi(z) - P_{y}^{t}(z)| \\&amp;\leq \max_{x, y \in \Omega} [\frac{1}{2} \sum_{z \in \Omega}|P_{x}^{t}(z) - \pi(z)| + \frac{1}{2} \sum_{z \in \Omega} |\pi(z) -P_{y}^{t}(z)| ]\\&amp;= \max_{x \in \Omega} d_{TV}(P_{x}^{t}, \pi) + \max_{y \in \Omega}d_{TV}(P_{y}^{t}, \pi) \\&amp;= d(t) + d(t) \\&amp;= 2 d(t)\end{aligned}\]</span></p><p>For the first inequality <span class="math inline">\(\bar{d}(t) \leq2 d(t)\)</span>, we need to prove <span class="math inline">\(d(t) \leq\bar{d}(t)\)</span>.</p><p>Define <span class="math display">\[S_{x,y}^{*} = \arg\max_{S \subset \Omega} （P_{x}^{t}(S) -P_{y}^{t}(S)）\]</span></p><p><span class="math display">\[S_{x}^{**} = \arg\max_{S \subset \Omega, y \in \Omega} （P_{x}^{t}(S) -P_{y}^{t}(S)）\]</span></p><p><span class="math display">\[S_{x}^{*} = \arg\max_{S \subset \Omega} \sum_{y \in \Omega}\pi(y)（P_{x}^{t}(S) - P_{y}^{t}(S)）\]</span></p><p><span class="math display">\[\begin{aligned}d(t) &amp;= \max_{x \in \Omega} d_{TV}(P_{x}^{t}, \pi) \\&amp;= \max_{x \in \Omega} \max_{S \subset \Omega}|P_{x}^{t}(S) -\pi(S)| \\&amp;= \max_{x \in \Omega} \max_{S \subset \Omega^{+}}(P_{x}^{t}(S) -\pi(S))  \\&amp;= \max_{x \in \Omega} \max_{S \subset \Omega^{+}}(P_{x}^{t}(S) -\sum_{y \in \Omega} \pi(y) P_{y}^{t}(S))  \qquad  (\pi(y) = \sum_{x \in\Omega} \pi(x) P_{x,y}) \\&amp;= \max_{x \in \Omega} \max_{S \subset \Omega^{+}} \sum_{y \in\Omega} \pi(y) (P_{x}^{t}(S) - P_{y}^{t}(S)).\end{aligned}\]</span> Since for all <span class="math inline">\(S\)</span>: <span class="math inline">\(（P_{x}^{t}(S_{x,y}^{*}) -P_{y}^{t}(S_{x,y}^{*})）\geq （P_{x}^{t}(S) -P_{y}^{t}(S)）\)</span></p><p>we have, <span class="math display">\[\begin{aligned}d(t) &amp;= \max_{x \in \Omega} \max_{S \subset \Omega^{+}} \sum_{y \in\Omega} \pi(y) (P_{x}^{t}(S) - P_{y}^{t}(S)) \\&amp; \leq \max_{x \in \Omega} \sum_{y \in \Omega} \pi(y)(P_{x}^{t}(S_{x,y}^{*}) - P_{y}^{t}(S_{x,y}^{*})) \\&amp;= \max_{x \in \Omega} \sum_{y \in \Omega} \pi(y)\max_{S}  (P_{x}^{t}(S) - P_{y}^{t}(S)) \\&amp; \leq  \max_{x \in \Omega} \sum_{y \in \Omega}\pi(y)   (P_{x}^{t}(S_{x}^{**}) - P_{y}^{t}(S_{x}^{**})) \\&amp; = \max_{x \in \Omega} \sum_{y \in \Omega} \pi(y) \max_{y,S}(P_{x}^{t}(S) - P_{y}^{t}(S)) \\&amp;= \max_{x, y \in \Omega}\max_{ S \subset \Omega}(P_{x}^{t}(S) -P_{y}^{t}(S))   \qquad (\sum_{y \in \Omega} \pi(y) = 1) \\&amp;= \max_{x, y \in \Omega} d_{TV}(P_{x}^{t}, P_{y}^{t}) \\&amp;= \bar{d}(t).\end{aligned}\]</span> that is, <span class="math inline">\(d(t) \leq \bar{d}(t) \leq2d(t)\)</span>.</p><p><strong>What we have proved is that, <span class="math inline">\(d(t)\)</span> is bounded by <span class="math inline">\(\bar{d}(t)\)</span>, and <span class="math inline">\(\bar{d}(t)\)</span> is controlled by <span class="math inline">\(\max_{x,y \in \Omega} d_{TV}(P_{x}^{t},P_{y}^{t})\)</span>. Let's find a way to bound <span class="math inline">\(d_{TV}(P_{x}^{t},P_{y}^{t})\)</span>!</strong></p><blockquote><p>Define 1. (Coupling) Assume <span class="math inline">\(x , y \in\Omega\)</span>, <span class="math inline">\(x \sim \mu, y \simv\)</span>, <span class="math inline">\(\mu, v\)</span> are twodistributions. A joint distribution <span class="math inline">\(w(x,y)\)</span> on <span class="math inline">\(\Omega \times \Omega\)</span>is called <strong>couping</strong> if <span class="math inline">\(\forall x \in \Omega\)</span>, <span class="math inline">\(\sum_{y}w(x, y) = \mu(x)\)</span>, <span class="math inline">\(\forall y \in \Omega\)</span>, <span class="math inline">\(\sum_{x} w(x, y) = v(y)\)</span>.</p></blockquote><blockquote><p>Lemma 2. Consider <span class="math inline">\(\mu, v\)</span> definedon <span class="math inline">\(\Omega\)</span>,<br>(a) For any coupling <span class="math inline">\(w(x, y)\)</span> of<span class="math inline">\(\mu, v\)</span>, <span class="math inline">\(d_{TV}(\mu, v) \leq P(x \ne y)\)</span>.<br>(b) There always exists a coupling <span class="math inline">\(w(x,y)\)</span> of <span class="math inline">\(\mu, v\)</span> such that<span class="math inline">\(d_{TV}(\mu, v) = P(x \ne y)\)</span>.</p></blockquote><p>Proof: (a) <span class="math inline">\(\forall z\)</span>, <span class="math inline">\(w(z, z) \leq \sum_{y \in \Omega} w(z, y) =\mu(z)\)</span>. Similarly, <span class="math inline">\(w(z, z) \leqv(z)\)</span>. Thus, <span class="math inline">\(w(z, z) \leq\min(\mu(z), v(z))\)</span>.</p><p><span class="math display">\[\begin{aligned}P(x \ne y) &amp;= 1 - P(x = y) \\&amp;= 1 - \sum_{z \in \Omega} w(z, z) \\&amp;\geq 1 - \sum_{z \in \Omega} \min(\mu(z), v(z)) \\&amp;= \sum_{z \in \Omega} \mu(z) - \sum_{z \in \Omega} \min(\mu(z),v(z)) \\&amp;= \sum_{z \in \Omega^{+}} (\mu(z) - v(z)) \\&amp;= d_{TV}(\mu, v)\end{aligned}\]</span></p><ol start="2" type="a"><li>We can construct a coupling <span class="math inline">\(w(x,y)\)</span>: <span class="math display">\[w(x, y) = \left\{\begin{aligned}&amp; \min \{\mu(x), v(y)\}, \quad x = y \\&amp;\frac{(\mu(x) - w(x, x))(v(y) - w(y,y))}{1 - \sum_{z \in\Omega}w(z,z)}, \quad x \ne y\end{aligned}\right.\]</span> For this joint distribution, we have <span class="math display">\[\begin{aligned}P(x \ne y) &amp;= 1 - P(x = y) \\&amp;= 1 - \sum_{z \in \Omega} w(z, z) \\&amp;= 1 - \sum_{z \in \Omega} \min(\mu(z), v(z)) \\&amp;= \sum_{z \in \Omega} \mu(z) - \sum_{z \in \Omega} \min(\mu(z),v(z)) \\&amp;= \sum_{z \in \Omega^{+}} (\mu(z) - v(z)) \\&amp;= d_{TV}(\mu, v)\end{aligned}\]</span> Thus, this joint distribution <span class="math inline">\(w(x,y)\)</span> satisfies <span class="math inline">\(d_{TV}(\mu, v) = P(x\ne y)\)</span>. Now, we need to prove that this joint distribution<span class="math inline">\(w(x, y)\)</span> is a coupling of <span class="math inline">\(\mu, v\)</span>.</li></ol><p><span class="math inline">\(\forall x \in \Omega\)</span>, <span class="math display">\[\begin{aligned}\sum_{y \in \Omega} w(x, y) &amp;= \sum_{y=x} \min \{\mu(x), v(y)\} +\sum_{y \in \Omega, y \ne x} \frac{(\mu(x) - w(x, x))(v(y) - w(y,y))}{1- \sum_{z \in \Omega}w(z,z)} \\&amp;= w(x, x) + \frac{(\mu(x) - w(x, x))}{1 - \sum_{z \in\Omega}w(z,z)} \sum_{y \ne x} (v(y) - w(y,y)) \\&amp;= w(x,x) + \frac{(\mu(x) - w(x, x))}{1 - \sum_{z \in \Omega}w(z,z)}(1 - v(x) - (\sum_{z} w(z,z) - w(x,x)))  \qquad (\sum_{y \ne x} v(y) = 1- v(x))\\&amp;= w(x,x) + \frac{(\mu(x) - w(x, x))}{1 - \sum_{z \in \Omega}w(z,z)}(1 - \sum_{z} w(z,z) + w(x,x) - v(x) ) \qquad (\sum_{y \ne x} w(y,y) =\sum_{z} w(z,z) - w(x, x))\end{aligned}\]</span> If $ x ^{+} = {x | (x) v(x)}$, then <span class="math inline">\(w(x,x) = v(x)\)</span>, <span class="math inline">\(\sum_{y \in \Omega} w(x, y)  = v(x) + (\mu(x) -v(x)) = \mu(x)\)</span>.</p><p>If $ x ^{-} = {x | (x) &lt; v(x)}$, then <span class="math inline">\(w(x,x) = \mu(x)\)</span>, <span class="math inline">\(\sum_{y \in \Omega} w(x, y)  = \mu(x) + 0 =\mu(x)\)</span>. Thus, <span class="math inline">\(w(x, y)\)</span> is acoupling of <span class="math inline">\(\mu, v\)</span>.</p><p>Last but not least, let's begin to prove the nonincreasing propertyof <span class="math inline">\(d(t)\)</span>!! <strong>Almost close tothe end!! ^o/</strong></p><blockquote><p>Lemma 3. Consider two Markov chains <span class="math inline">\(x^{t}\sim P_{x}^{t}\)</span>, <span class="math inline">\(y^{t} \simP_{y}^{t}\)</span>, <span class="math inline">\(x^{t}\)</span> and <span class="math inline">\(y^{t}\)</span> share the same transitionprobability matrix <span class="math inline">\(P\)</span> but start fromdifferent states. If <span class="math inline">\(x^t = y^t\)</span>,then <span class="math inline">\(x^{t+1} = y^{t+1}\)</span>, elif <span class="math inline">\(x^t \ne y^t\)</span>, then <span class="math inline">\(x^{t+1} \ne y^{t+1}\)</span>, <span class="math inline">\(x^{t+1}\)</span> and <span class="math inline">\(y^{t+1}\)</span> are independent.</p></blockquote><blockquote><p>Define 2. (Coupling of Markov chains) Consider two Markov chains<span class="math inline">\(x^{t} \sim P_{x}^{t}\)</span>, <span class="math inline">\(y^{t} \sim P_{y}^{t}\)</span>, <span class="math inline">\(x^{t}\)</span> and <span class="math inline">\(y^{t}\)</span> share the same transitionprobability matrix <span class="math inline">\(P\)</span> but start fromdifferent states. If <span class="math display">\[P(y^{t+1} | x^{t}, y^{t}) = P(y^{t+1} | y^{t})\]</span> and <span class="math display">\[P(x^{t+1} | x^{t}, y^{t}) = P(x^{t+1} | x^{t})\]</span> then we say <span class="math inline">\(x^{t}\)</span> and<span class="math inline">\(y^{t}\)</span> are coupled.</p></blockquote><p>Define <span class="math inline">\(w^{t} := w^{t}(x^t, y^t)\)</span>is a coupling of <span class="math inline">\(P_{x}^{t},P_{y}^{t}\)</span>, <span class="math inline">\(x^t \sim P_x^t, y \simP_{y}^{t}\)</span>, and <span class="math inline">\(w^{t}\)</span>satisfies Lemma 2(b).</p><p><span class="math display">\[P_{w^{t}}(x^{t+1} \ne y^{t+1}) = P_{w^{t}}(x^{t+1} \ne y^{t+1} | x^t \ney^t) p(x^t \ne y^t) + P_{w^{t}}(x^{t+1} \ne y^{t+1} | x^t = y^t) p(x^t =y^t)\]</span> If <span class="math inline">\(x^t = y^t\)</span>, accordingto Lemma 3, <span class="math inline">\(P_{w^{t}}(x^{t+1} \ne y^{t+1}) =P_{w^{t}}(x^{t+1} = y^{t+1} | x^t = y^t) p(x^t = y^t)  \leqP_{w^{t}}(x^{t} \ne y^t)\)</span>;</p><p>If <span class="math inline">\(x^t \ne y^t\)</span>, <span class="math inline">\(P_{w^{t}}(x^{t+1} \ne y^{t+1}) = P_{w^{t}}(x^{t+1}\ne y^{t+1} | x^t \ne y^t) p(x^t \ne y^t) \leq P_{w^{t}}(x^{t} \ney^t)\)</span>.</p><p><span class="math inline">\(d_x(t+1) = d_{TV}(P_{x}^{t+1},P_y^{t+1}） \leq P_{w^{t}}(x^{t+1} \ne y^{t+1}) \leq P_{w^{t}}(x^{t} \ney^t) = d_{TV}(P_x^t, P_y^{t}) = d_x(t)\)</span></p><p><span class="math inline">\(d_{x}(t) := d_{TV}(P_{x}^{t},\pi)\)</span>.</p><p><span class="math inline">\(d(t) = \max_{x \in \Omega}d_{x}(t)\)</span>.</p><p><span class="math inline">\(d(t) \leq \bar{d}(t) \leq2d(t)\)</span>.</p><p><span class="math inline">\(\bar{d}(t) := \max_{x, y \in \Omega}d_{TV}(P_{x}^{t}, P_{y}^{t})\)</span></p><p>???????????? Q: if we consider <span class="math inline">\(d_{TV}(P_x^t, \pi)\)</span>, then the coupling<span class="math inline">\(w^{t} := w^{t}(x^t, y)\)</span> should be acoupling of <span class="math inline">\(P_{x}^{t}, \pi\)</span>, <span class="math inline">\(x^t \sim P_x^t, y \sim \pi\)</span>.</p><p><span class="math inline">\(\pi\)</span> is different from <span class="math inline">\(P_y^t\)</span> at first.</p><p><span class="math display">\[d_x(t) = d_{TV}(P_{x}^{t}, \pi) \leq d(t) \leq \bar{d}(t) = \max_{x, y\in \Omega} d_{TV}(P_{x}^{t}, P_{y}^{t}) = P_{w^t}(x^t \ne y^t)\]</span></p><p>Now, we have already proved that <span class="math inline">\(d(t)\)</span> is nonincreasing. Next, we willprove that <span class="math inline">\(d(t)\)</span> converges to 0.</p><h1 id="useful-lectures">Useful Lectures:</h1><ul><li><a href="https://people.eecs.berkeley.edu/~sinclair/cs294/n7.pdf">L1</a></li><li><a href="https://courses.cs.duke.edu/spring13/compsci590.2/slides/lec5.pdf">MarkovChains and Coupling</a></li><li><a href="https://faculty.cc.gatech.edu/~vigoda/MCMC_Course/MC-basics.pdf">MarkovChains, Coupling, Stationary Distribution</a></li><li><a href="https://mpaldridge.github.io/math2750/S10-stationary-distributions.html">Stationarydistributions</a></li><li><a href="https://mpaldridge.github.io/math2750/S11-long-term-chains.html">Long-termbehaviour of Markov chains</a></li></ul>]]></content>
      
      
      <categories>
          
          <category> Stochastic Process </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Markov Chains </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Metropolis-Hastings</title>
      <link href="/2023/07/15/Metropolis-Hastings/"/>
      <url>/2023/07/15/Metropolis-Hastings/</url>
      
        <content type="html"><![CDATA[<h1 id="introduction">Introduction</h1><p>The first MCMC algorithm is the Metropolis algorithm, published byMetropolis et al. (1953). It was generalized by Hastings (1970) andPeskun (1973, 1981) towards statistical applications. After a long time,it was rediscovered by Geman and Geman (1984), Tanner and Wong (1987),and Gelfand and Smith (1990).</p><p>Assume the probability density <span class="math inline">\(\pi\)</span> is the target distribution and <span class="math inline">\(q(\cdot | \cdot)\)</span> is the proposaltransition distribution. From an initial state <span class="math inline">\(X_0\)</span>, the Metropolis-Hastings algorithmaims to generate a Markov chain <span class="math inline">\(\{X_1, X_2,...\}\)</span>, such that <span class="math inline">\(X_t\)</span>converges to distribution <span class="math inline">\(\pi\)</span>.</p><h1 id="metropolis-hastings-algorithm">Metropolis-Hastingsalgorithm</h1><p>Input: initial value <span class="math inline">\(X_0\)</span>,transition <span class="math inline">\(q(\cdot | \cdot)\)</span>, numberof iterations <span class="math inline">\(T\)</span>.</p><p>Output: Markov chain <span class="math inline">\(\{X_1, X_2, ...,X_T\}\)</span>.</p><p>For <span class="math inline">\(t = 1, 2, ..., T\)</span>:</p><ol type="1"><li>Generate <span class="math inline">\(u\)</span> from uniformdistribution <span class="math inline">\(U(0, I)\)</span>.</li><li>Generate <span class="math inline">\(X\)</span> from <span class="math inline">\(q(X | X_{t-1})\)</span>.</li><li>Compute the acceptance probability <span class="math inline">\(A(X,X_{t-1}) = \min\{1, \frac{\pi(X)q(X_{t-1} | X)}{\pi(X_{t-1})q(X |X_{t-1})}\}\)</span>.</li><li>if <span class="math inline">\(u \leq A(X, X_{t-1})\)</span>, then<span class="math inline">\(X_t = X\)</span>; else <span class="math inline">\(X_t = X_{t-1}\)</span>.</li></ol><p>Now, we prove that <span class="math inline">\(\pi\)</span> is one ofthe stationary distribution of the generated Markov chain.</p><p>Proof: Recall the detailed balance equation, for any <span class="math inline">\(i, j \in \Omega\)</span>, we have <span class="math display">\[\pi_i P_{i,j} = \pi_j P_{j,i},\]</span> then <span class="math inline">\(\pi\)</span> is a stationarydistribution of the Markov chain.</p><p>For the Metropolis-Hastings algorithm, we have the transitionprobability <span class="math display">\[K(X_t | X_{t-1}) = q(X_t| X_{t-1}) A(X_t, X_{t-1}) + \delta(X_t =X_{t-1}) (1 - \sum_{X \in \Omega} q(X|X_{t-1}) A(X, X_{t-1})).\]</span></p><p>Now we need to prove <span class="math display">\[\pi(X_{t-1})K(X_t | X_{t-1}) = \pi(X_t)K(X_{t-1} | X_t).\]</span></p><p>If <span class="math inline">\(X_t = X_{t-1}\)</span>, then theequation holds. If <span class="math inline">\(X_t \neqX_{t-1}\)</span>, then <span class="math display">\[\pi(X_{t-1})K(X_t | X_{t-1}) = \pi(X_{t-1})q(X_t| X_{t-1}) A(X_t,X_{t-1}) = \min\{\pi(X_{t-1})q(X_t| X_{t-1}), \pi(X_t)q(X_{t-1} |X_t)\}.\]</span> and <span class="math display">\[\pi(X_{t})K(X_{t-1} | X_{t}) = \pi(X_{t})q(X_{t-1}| X_{t}) A(X_{t-1},X_{t}) = \min\{\pi(X_{t})q(X_{t-1}| X_{t}), \pi(X_{t-1})q(X_{t} |X_{t-1})\}.\]</span> So, <span class="math inline">\(\pi(X_{t-1})K(X_t | X_{t-1}) =\pi(X_{t})K(X_{t-1} | X_{t})\)</span>. <span class="math inline">\(\pi\)</span> is a stationary distribution of theMarkov chain.</p><hr><p>Remark 1. The detailed balance condition is a sufficient but notnecessary condition for <span class="math inline">\(\pi\)</span> to be astationary distribution of the Markov chain. If we want to prove <span class="math inline">\(\pi\)</span> is the unique stationary distributionof the Markov chain, we need to prove the Markov chain is<strong>irreducible and positive recurrent</strong>. 【？？？】</p><p>Remark 2. The first initial state <span class="math inline">\(X_0\)</span> is randomly generated and usuallyremoved from the sample as burn-in or warm-up.</p><p>Q: <strong>Since it is recurrent, it must return to the initialvalues. Will this initial be rejected with a highprobability?</strong></p><p>Remark 3. In practice, the performances of the algorithm areobviously highly dependent on the choice of the transition <span class="math inline">\(q(\cdot | \cdot)\)</span>, since some choices seethe chain unable to converge in a manageable time.</p><p>Remark 4. We need to able to evaluate a function <span class="math inline">\(p(x) \propto \pi(x)\)</span>. Since we only needto compute the ratio <span class="math inline">\(\pi(y)/\pi(x)\)</span>,the proportionality constant is irrelevant. Similarly, we only careabout <span class="math inline">\(q(\cdot | \cdot)\)</span> up to aconstant</p><h1 id="reference">Reference</h1><ul><li>C.P. Robert. (2016). The Metropolis-Hastings algorithm.</li></ul>]]></content>
      
      
      <categories>
          
          <category> Stochastic Process </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Markov Chains </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Discrete Markov Chains</title>
      <link href="/2023/07/12/Markov-Chains/"/>
      <url>/2023/07/12/Markov-Chains/</url>
      
        <content type="html"><![CDATA[<h1 id="markov-chains-definitions-and-representations">Markov Chains:Definitions and Representations</h1><p>A stochastic process <span class="math inline">\(X = \{ x(t): t\inT\}\)</span> is a collection of random variables.</p><p>There are two elements:</p><ul><li>Time <span class="math inline">\(t\)</span>:<ul><li>discrete time (<span class="math inline">\(T\)</span> is a countablyinfinite set; under this case, we call 'Markov chain')</li><li>continuous time (under this case, we call 'Markov process')</li></ul></li><li>Space <span class="math inline">\(\Omega\)</span>:<ul><li>discrete space (<span class="math inline">\(X_{t}\)</span> comesfrom a countably infinite set)</li><li>continuous space.</li></ul></li></ul><p>Markov chain is a <strong>discrete-time</strong> process for whichthe future behaviour, given the past and the present, only depends onthe present and not on the past.</p><p>Markov process is the <strong>continuous-time</strong> version of aMarkov chain.</p><blockquote><p>Definition 1.[Markov chain] A discrete time stochastic process $ X_0,X_1, X_2, <span class="math inline">\(. . . is a Markov chainif\)</span>$ P(X_{t} = a_t | X_{t-1} = a_{t-1}, X_{t-2} = a_{t-2}, ...,X_0 = a_0) = P(X_{t} = a_t | X_{t-1} = a_{t-1}) = P_{a_{t-1}, a_{t}}$$</p></blockquote><p>Remark 1: This is time-homogeneous markov chain, for <span class="math inline">\(\forall t\)</span>, for <span class="math inline">\(\forall a_{t-1}, a_{t} \in \Omega\)</span>, thetransition probability <span class="math inline">\(P_{a_{t-1}, a_{t}}\)</span> is the same.</p><p>Remark 2: In DDPM, it is not a time-homogeneous chain, as thetransition probability at t is obtained by a network(t).</p><p>The state <span class="math inline">\(X_{t}\)</span> depends on theprevious state <span class="math inline">\(X_{t-1}\)</span> but isindependent of the particular history <span class="math inline">\(X_{t-2}, X_{t-3},...\)</span>. This is called the<strong>Markov property</strong> or <strong>memorylessproperty</strong>.</p><p>The Markov property does not imply that <span class="math inline">\(X_{t}\)</span> is independent of the randomvariables <span class="math inline">\(X_{0}\)</span>, <span class="math inline">\(X_{1}\)</span>,..., <span class="math inline">\(X_{t-2}\)</span>; it just implies that <strong>anydependency of <span class="math inline">\(X_{t}\)</span> on the past iscaptured in the value of <span class="math inline">\(X_{t-1}\)</span></strong>.</p><p>The Markov chain is <strong>uniquely</strong> defined by the one-steptransition probability matrix P: <span class="math display">\[P =\begin{pmatrix}P_{0,0} &amp; P_{0, 1} &amp; \cdots &amp; P_{0, j} &amp; \cdots\\\vdots &amp; \vdots &amp; \ddots &amp; \vdots&amp; \vdots\\P_{i,0} &amp; P_{i, 1} &amp; \cdots &amp; P_{i, j} &amp; \cdots\\\vdots &amp; \vdots &amp; \ddots &amp; \vdots&amp; \vdots\\\end{pmatrix}\]</span> where <span class="math inline">\(P_{i,j}\)</span> is theprobability of transition from state <span class="math inline">\(i\)</span> to state <span class="math inline">\(j\)</span>. <span class="math inline">\(P_{i,j} =P(X_{t} = j| X_{t-1} = i), i,j \in \Omega\)</span>. For <span class="math inline">\(i\)</span>, <span class="math inline">\(\sum_{j\geq 0} P_{i,j} = 1\)</span>.</p><h1 id="classification-of-states">Classification of States</h1><p>For simplicity, we assume that the state space <span class="math inline">\(\Omega\)</span> is finite. ## Communicatingclass</p><blockquote><p>Definition 2. [Communicating class] A state <span class="math inline">\(j\)</span> is reachable from state <span class="math inline">\(i\)</span> if there exists a positive integer<span class="math inline">\(n\)</span> such that <span class="math inline">\(P_{i,j}^{(n)} &gt; 0\)</span>. We write <span class="math inline">\(i \rightarrow j\)</span>. If <span class="math inline">\(j\)</span> is reachable from <span class="math inline">\(i\)</span>, and <span class="math inline">\(i\)</span> is reachable from <span class="math inline">\(j\)</span>, then the states <span class="math inline">\(i\)</span> and <span class="math inline">\(j\)</span> are said to<strong>communicate</strong>, denoted by <span class="math inline">\(i\leftrightarrow j\)</span>. A communicating class <span class="math inline">\(C\)</span> is a <strong>maximal</strong> set ofstates that communicate with each other. <strong>No state in <span class="math inline">\(C\)</span> communicates with any state not in<span class="math inline">\(C\)</span>.</strong></p></blockquote><h2 id="irreducible">Irreducible</h2><blockquote><p>Definition 3: A Markov chain is <strong>irreducible</strong> if allstates belong to <strong>one</strong> communicating class.</p></blockquote><p>This means that <strong>any state can be reached from any otherstate</strong>. For <span class="math inline">\(\forall i, j \in\Omega\)</span>, <span class="math inline">\(P_{i,j} &gt;0\)</span>.</p><blockquote><p>Lemma 1. A finite Markov chain is irreducible if and only if itsgraph representation is a strongly connected graph.</p></blockquote><h3 id="transient-vs-recurrent-states">Transient vs Recurrentstates</h3><p>Let <span class="math inline">\(r_{i,j}^{t}\)</span> denote theprobability that the chain, starting at state <span class="math inline">\(i\)</span>, <strong>the first time</strong>transition to state <span class="math inline">\(j\)</span> occurs attime <span class="math inline">\(t\)</span>. That is, <span class="math display">\[r_{i,j}^{t} = P(X_{t} = j, X_{s} \neq j, \forall 1 \leq s \leq t-1 |X_{0} = i)\]</span></p><blockquote><p>Definition 4. A state is <strong>recurrent</strong> if <span class="math inline">\(\sum_{t \geq 1} r_{i,i}^{t} = 1\)</span> and it is<strong>transient</strong> if <span class="math inline">\(\sum_{t \geq1} r_{i,i}^{t} &lt; 1\)</span>. A Markov chain is recurrent if everystate in the chain is recurrent.</p></blockquote><ul><li><p>If state i is recurrent then, once the chain visits that state,it will (with probability 1) eventually return to that state. Hence thechain will visit state <span class="math inline">\(i\)</span> over andover again, <strong>infinitely</strong> often.</p></li><li><p>A transient state has the property that a Markov chain startingat this state returns to this state only <strong>finitelyoften</strong>, with probability 1.</p></li><li><p>If one state in a communicating class is transient (respectively,recurrent) then all states in that class are transient (respectively,recurrent).</p></li></ul><blockquote><p>Definition 5. An irreducible Markov chain is called recurrent if atleast one (equivalently, every) state in this chain is recurrent. Anirreducible Markov chain is called transient if at least one(equivalently, every) state in this chain is transient.</p></blockquote><p>Let <span class="math inline">\(\mu_{i} = \sum_{t \geq 1} t \cdotr_{i,i}^{t}\)</span> denote the expected time to return to state <span class="math inline">\(i\)</span> when starting at state <span class="math inline">\(i\)</span>.</p><blockquote><p>Definition 6. A state <span class="math inline">\(i\)</span> is<strong>positive recurrent</strong> if <span class="math inline">\(\mu_{i} &lt; \infty\)</span> and <strong>nullrecurrent</strong> if <span class="math inline">\(\mu_{i} =\infty\)</span>.</p></blockquote><p>Here we give an example of a Markov chain that has null recurrentstates. Consider the following markov chain whose states are thepositive integers.</p><figure><img src="/2023/07/12/Markov-Chains/image.png" alt="Fig. 1. An example of a Markov chain that has null recurrent states"><figcaption aria-hidden="true">Fig. 1. An example of a Markov chain thathas null recurrent states</figcaption></figure><p>Starting at state 1, the probability of not having returned to state1 within the first <span class="math inline">\(t\)</span> steps is <span class="math display">\[\prod_{j=1}^{t} \frac{j}{j+1} = \frac{1}{t+1}.\]</span> The probability of never returning to state 1 from state 1 is0, and state 1 is recurrent. Thus, the probability of the first timetransition to state <span class="math inline">\(1\)</span> occurs attime <span class="math inline">\(t\)</span> is <span class="math display">\[r_{1,1}^{t} = \frac{1}{t} \cdot \frac{1}{t+1} = \frac{1}{t(t+1)}.\]</span> The expected number of steps until the first return to state 1when starting at state 1 is <span class="math display">\[\mu_{1} = \sum_{t = 1}^{\infty} t \cdot r_{1,1}^{t} = \sum_{t =1}^{\infty} \frac{1}{t+1} = \infty.\]</span> State 1 is recurrent but null recurrent.</p><blockquote><p>Lemma 2. In a finite Markov chain: 1. at least one state isrecurrent; and 2. all recurrent states are positive recurrent.</p></blockquote><p>Thus, all states of a finite, irreducible Markov chain are positiverecurrent.</p><h3 id="periodic-vs-aperiodic-states">Periodic vs Aperiodic states</h3><blockquote><p>Definition 7. A state <span class="math inline">\(j\)</span> in adiscrete time Markov chain is <strong>periodic</strong> if there existsan integer <span class="math inline">\(k&gt;1\)</span> such that <span class="math inline">\(P(X_{t+s}= j | X_t = j) = 0\)</span> unless <span class="math inline">\(s\)</span> is divisible by <span class="math inline">\(k\)</span>. A discrete time Markov chain isperiodic if any state in the chain is periodic. A state or chain that isnot periodic is <strong>aperiodic</strong>.</p></blockquote><p>A state <span class="math inline">\(i\)</span> is periodic means thatfor <span class="math inline">\(s = k, 2k, 3k,...\)</span>, <span class="math inline">\(P(X_{t+s}= j | X_t = j) &gt; 0\)</span>.</p><p><strong>NB: k &gt; 1</strong></p><h3 id="ergodic">Ergodic</h3><blockquote><p>Definition 8. An <strong>aperiodic</strong>, <strong>positiverecurrent</strong> state is an <strong>ergodic</strong> state. A Markovchain is ergodic if all its states are ergodic.</p></blockquote><blockquote><p>Corollary 1. Any finite, irreducible, and aperiodic Markov chain isan ergodic chain.</p></blockquote><h3 id="stationary-distribution">Stationary distribution</h3><p>Consider the two-state “broken printer” Markov chain:</p><figure><img src="/2023/07/12/Markov-Chains/2023-07-22-11-00-52.png" alt="Transition diagram for the two-state broken printer chain"><figcaption aria-hidden="true">Transition diagram for the two-statebroken printer chain</figcaption></figure><p>There are two state (0 and 1) in this Markov chain, and assume thatthe initial distribution is <span class="math display">\[P(X_0 = 0) = \frac{\beta}{\alpha+\beta}, \qquad P(X_0 = 1) =\frac{\alpha}{\alpha+\beta}.\]</span> Then, according to the transition probability matrix <span class="math inline">\(P\)</span>, after one step, the distribution is<span class="math display">\[\begin{align*}P(X_1 = 0) &amp;= P(X_0 = 0)P(X_1 = 0 | X_0 = 0) + P(X_0 = 1)P(X_1 = 0 |X_0 = 1) \\&amp;= \frac{\beta}{\alpha+\beta} \cdot (1-\alpha) +\frac{\alpha}{\alpha+\beta} \cdot \beta = \frac{\beta}{\alpha+\beta}, \\P(X_1 = 1) &amp;= P(X_0 = 0)P(X_1 = 1 | X_0 = 0) + P(X_0 = 1)P(X_1 = 1 |X_0 = 1) \\&amp;= \frac{\beta}{\alpha+\beta} \cdot \alpha +\frac{\alpha}{\alpha+\beta} \cdot (1-\beta) =\frac{\alpha}{\alpha+\beta}.\end{align*}\]</span> Apparently, the distribution of <span class="math inline">\(X_1\)</span> is the same as the initialdistribution. Similarly, we can prove that the distribution of <span class="math inline">\(X_t\)</span> is the same as the initialdistribution for any <span class="math inline">\(t\)</span>. Here, <span class="math inline">\(\pi = (\frac{\beta}{\alpha+\beta},\frac{\alpha}{\alpha+\beta})\)</span> is called <strong>stationarydistribution</strong>.</p><blockquote><p>Definition 9. A probability distribution <span class="math inline">\(\pi = (\pi_i)\)</span>, <span class="math inline">\(\sum_{i \in \Omega} \pi_i = 1\)</span>(<strong>rowvector</strong>) on the state space <span class="math inline">\(\Omega\)</span> is called a <strong>stationarydistribution</strong> (or an equilibrium distribution) for the Markovchain with transition probability matrix <span class="math inline">\(P\)</span> if <span class="math inline">\(\pi =\pi P\)</span>, equivalently, <span class="math inline">\(\pi_j =\sum_{i \in \Omega}\pi_i P_{i,j}\)</span> for all <span class="math inline">\(j \in \Omega\)</span>.</p></blockquote><ul><li><p>One interpretation of the stationary distribution: if we startedoff a <strong>thousand</strong> Markov chains, choosing each startingposition to be state <span class="math inline">\(i\)</span> withprobability <span class="math inline">\(\pi_i\)</span>, then(roughly)<strong><span class="math inline">\(1000 \pi_j\)</span></strong> of themwould be in state <span class="math inline">\(j\)</span> at any time inthe future – but not necessarily the same ones each time.</p></li><li><p>If a chain ever reaches a stationary distribution then itmaintains that distribution for all future time, and thus a stationarydistribution represents a steady state or an equilibrium in the chain’sbehavior.</p></li></ul><h4 id="finding-a-stationary-distribution">Finding a stationarydistribution</h4><p>Consider the following no-claims discount Markov chain with statespace <span class="math inline">\(\Omega = \{1,2,3\}\)</span> andtransition matrix <span class="math display">\[P =\begin{pmatrix}\frac{1}{4} &amp; \frac{3}{4} &amp; 0\\\frac{1}{4} &amp; 0 &amp; \frac{3}{4}\\0 &amp; \frac{1}{4} &amp; \frac{3}{4}\end{pmatrix}\]</span></p><ul><li><p>Step 1: Assume $= {_1, _2, _3} $ is a stationary distribution.According to the definition 9 of stationary distribution, we need tosolve the following equations: <span class="math display">\[\begin{align*}\pi_1 &amp;= \frac{1}{4}\pi_1 + \frac{1}{4}\pi_2, \\\pi_2 &amp;= \frac{3}{4}\pi_1 + \frac{1}{4}\pi_3, \\\pi_3 &amp;= \frac{3}{4}\pi_2 + \frac{3}{4}\pi_3.\end{align*}\]</span> Adding the normalising condition <span class="math inline">\(\pi_1 + \pi_2 + \pi_3 = 1\)</span>, we get fourequations in three unknown parameters.</p></li><li><p>Step 2: Choose one of the parameters, say <span class="math inline">\(\pi_1\)</span>, and solve for the other twoparameters in terms of <span class="math inline">\(\pi_1\)</span>. Weget <span class="math display">\[\pi_1 = \frac{1}{4}\pi_1 + \frac{1}{4}\pi_2 \Rightarrow \pi_2 = 3\pi_1,\qquad \pi_3 = 3\pi_2 = 9\pi_1.\]</span></p></li><li><p>Step 3: Combining with the normalising condition, we get <span class="math display">\[\pi_1 + 3\pi_1 + 9\pi_1 = 1 \Rightarrow \pi_1 = \frac{1}{13}, \qquad\pi_2 = \frac{3}{13}, \qquad \pi_3 = \frac{9}{13}.\]</span> Finally, we get the stationary distribution <span class="math inline">\(\pi = (\frac{1}{13}, \frac{3}{13},\frac{9}{13})\)</span>.</p></li></ul><h4 id="existence-and-uniqueness">Existence and uniqueness</h4><p>Given a Markov chaine, how can we know whether it has a stationarydistribution? If it has, is it unique? At this part, we will answerthese questions.</p><p>Some notations: - Hitting time to hit the state <span class="math inline">\(j\)</span>: <span class="math inline">\(H_{j} =\min \{ t \in \{0, 1, 2,...\}: X_t = j\}\)</span>. Note that here weinclude time <span class="math inline">\(t = 0\)</span>.</p><ul><li>Hitting probability to hit the state <span class="math inline">\(j\)</span> staring from state <span class="math inline">\(i\)</span>: <span class="math inline">\(h_{i,j} =P(X_t = j, \text{for some} \ t \geq 0 | X_0 = i) = P(H_{j} &lt; \infty |X_0 = i) = \sum_{t \geq 0} r_{i,j}^{t}\)</span>.</li></ul><p>Note that this is different from <span class="math inline">\(r_{i,j}^{t}\)</span>, which denotes theprobability that the chain, starting at state <span class="math inline">\(i\)</span>, the <strong>first</strong> timetransition to state <span class="math inline">\(j\)</span><strong>occurs at time <span class="math inline">\(t\)</span></strong>.</p><p>We also have <span class="math display">\[h_{i,j} =\begin{cases}\sum_{k \in \Omega}P_{i,k}h_{k,j} &amp; , &amp; \text{if} \quad j \ne i,\\1 &amp; , &amp; \text{if} \quad  j = i.\end{cases}\]</span> - Expected hitting time: <span class="math inline">\(\eta_{i,j} = E(H_{j} | X_0 = i) = \sum_{t \geq 0}t \cdot r_{i,j}^{t}\)</span>. The expected time until we hit state <span class="math inline">\(j\)</span> starting from state <span class="math inline">\(i\)</span>. We also have <span class="math display">\[\eta_{i,j} =\begin{cases}1 + \sum_{k \in \Omega}P_{i,k}\eta_{k,j} &amp; , &amp; if j \ne i, \\0 &amp; , &amp; if j = i.\end{cases}\]</span> (For the first case, we add 1 because we need to consider thefirst step from state <span class="math inline">\(i\)</span> to state<span class="math inline">\(k\)</span>.)</p><ul><li>Return time: <span class="math inline">\(M_i = \min \{ t \in \{1,2,...\}: X_t = i\}\)</span>. It is different from <span class="math inline">\(H_{i}\)</span>, as we exclude time <span class="math inline">\(t = 0\)</span>. It is the first time that thechain returns to state <span class="math inline">\(i\)</span> after<span class="math inline">\(t = 0\)</span>.</li><li>Return probability: <span class="math inline">\(m_{i} = P(X_t = i  \\text{for some} \ n \geq 1 | X_0 = i) = P(M_i &lt; \infty | X_0 = i) =\sum_{t&gt;1}r_{i,i}^{t}.\)</span></li><li>Expected return time: <span class="math inline">\(\mu_{i} = E(M_i |X_0 = i) = \sum_{t \geq 1} t \cdot r_{i,i}^{t}\)</span>. The expectedtime until we return to state <span class="math inline">\(i\)</span>starting from state <span class="math inline">\(i\)</span>. <span class="math display">\[m_{i} = \sum_{j \in \Omega} P_{i,j}h_{j,i},  \qquad \mu_{i} = 1 +\sum_{j \in \Omega} P_{i,j}\eta_{j,i}.\]</span></li></ul><table style="width:24%;"><colgroup><col style="width: 23%"></colgroup><tbody><tr><td>&gt; Theorem 1. Consider an irreducible Markov chain (<strong>finiteor infinite</strong>), &gt; (1) if it is <strong>positiverecurrent</strong>, <span class="math inline">\(\exists\)</span> anunique stationary distribution <span class="math inline">\(\pi\)</span>,such that <span class="math inline">\(\pi_i =\frac{1}{\mu_{i}}\)</span>. &gt; (2) if it is <strong>nullrecurrent</strong> or <strong>transient</strong>, no stationarydistribution exists.</td></tr><tr><td>Remark: If the chain is <strong>finite</strong> irreducible, it mustbe positive recurrent, thus it has an unique stationarydistribution.</td></tr><tr><td>Remark: If the Markov chain is not irreducible, we can decompose thestate space into several communicating classes. Then, we can considereach communicating class separately. - If none of the classes arepositive recurrent, then no stationary distribution exists. - If exactlyone of the classes is positive recurrent (and therefore closed), thenthere exists a unique stationary distribution, supported only on thatclosed class. - If more the one of the classes are positive recurrent,then many stationary distributions will exist.</td></tr><tr><td>Now, we give the proof of Theorem 1. We first prove that if a Markovchain is irreducible and positive recurrent, then there<strong>exists</strong> a stationary distribution. Next, we will provethe stationary distribution is <strong>unique</strong>. Since the secondpart with the null recurrent or transitive Markov chains is lessimportant and more complicated, we will omit it. If you are interestedin it, you can refer to the book <a href="https://www.statslab.cam.ac.uk/~james/Markov/">Markov Chains</a>by James Norris.</td></tr><tr><td>Proof. (1) Suppose that <span class="math inline">\((X_0, X_1...)\)</span> a recurrent Markov chain, which can be positive recurrentor null recurrent. Then we can desigh a stationary distribution asfollows. (If we can desigh a stationary distribution, then it must beexisted.)</td></tr><tr><td>Let <span class="math inline">\(\nu_i\)</span> be the expectednumber of visits to <span class="math inline">\(i\)</span> before wereturn back to <span class="math inline">\(k\)</span>, <span class="math display">\[\begin{align*}\nu_i &amp;= \mathbb{E}(\# \text{visits to $i$ before returning to } k |X_0 = k) \\&amp;= \mathbb{E}\sum_{t=1}^{M_k} P(X_t = i | X_0 = k) \\&amp;= \mathbb{E}\sum_{t = 0}^{M_k - 1} P(X_t = i | X_0 = k)\end{align*}\]</span> The last equation holds because of $ P(X_0 = i | X_0 = k) = 0$and $ P(X_{M_k} = i | X_0 = k) = 0$.</td></tr><tr><td>If we want design a stationary distribution, it must statisfy <span class="math inline">\(\pi P = \pi\)</span> and <span class="math inline">\(\sum_{i \in \Omega}\pi_i = 1\)</span>.</td></tr><tr><td>(a) We first prove that <span class="math inline">\(\nu P =\nu\)</span>. <span class="math display">\[\begin{align*}\sum_{i \in \Omega} \nu_i P_{i,j} &amp;= \mathbb{E}\sum_{i \in \Omega}\sum_{t = 0}^{M_k - 1} P(X_t = i, X_{t+1} = j | X_0 = k) \\&amp;= \mathbb{E}\sum_{t = 0}^{M_k - 1}  \sum_{i \in \Omega}  P(X_t = i,X_{t+1} = j | X_0 = k) \\&amp;=  \mathbb{E} \sum_{t = 0}^{M_k - 1} P(X_{t+1} = j | X_0 = k) \\&amp;= \mathbb{E} \sum_{t = 1}^{M_k } P(X_{t} = j | X_0 = k) \\&amp;= \mathbb{E} \sum_{t = 0}^{M_k - 1} \nu_i \\&amp;= \nu_j.\end{align*}\]</span> (b) Next, what we need to do is to normalize <span class="math inline">\(\nu\)</span> to get a stationary distribution. Wehave <span class="math display">\[\sum_{i \in \Omega} \nu_i = \sum_{i \in \Omega} \mathbb{E} \sum_{t =0}^{M_k - 1} P(X_t = i | X_0 = k) =\mathbb{E} \sum_{t = 0}^{M_k -1}  \sum_{i \in \Omega}  P(X_t = i | X_0 = k) = E(M_k | X_0 = i) =\mu_k.\]</span> Thus, we can define <span class="math inline">\(\pi_i =\nu_i/\mu_k\)</span>, <span class="math inline">\(\pi = \{\pi_i, i \in\Omega\}\)</span> is one of the stationary distribution.</td></tr><tr><td>(2) Next, we prove that if a Markov chain is irreducible andpositive recurrent, then the stationary distribution is<strong>unique</strong> and is given by <span class="math inline">\(\pi_j = \frac{1}{\mu_j}\)</span>.</td></tr><tr><td>Given a stationary distribution <span class="math inline">\(\pi\)</span>, if we prove that for all <span class="math inline">\(i\)</span>, <span class="math inline">\(\pi_j ==\frac{1}{\mu_j}\)</span>, then we prove that the stationary distributionis unique.</td></tr><tr><td>Remember that the expected hitting time: <span class="math display">\[\eta_{i,j} = 1 + \sum_{k \in \Omega}P_{i,k}\eta_{k,j},  j \ne i  \qquad(eq:1)\]</span> We multiply both sides of (eq:1) by <span class="math inline">\(\pi_i\)</span> and sum over <span class="math inline">\(i (i \ne j)\)</span> to get <span class="math display">\[\sum_{i \ne j} \pi_i \eta_{i,j} = \sum_{i \ne j} \pi_i + \sum_{i \ne j}\sum_{k \in \Omega} \pi_i P_{i,k}\eta_{k,j}\]</span> Since <span class="math inline">\(\eta_{j,j} = 0\)</span>, wecan rewrite the above equation as <span class="math display">\[\sum_{i \in \Omega} \pi_i \eta_{i,j} = \sum_{i \ne j} \pi_i + \sum_{i\ne j} \sum_{k \in \Omega} \pi_i P_{i,k}\eta_{k,j}. \qquad (eq:2)\]</span></td></tr><tr><td>(The above equality lacks <span class="math inline">\(j\)</span>,and we also want to design <span class="math inline">\(\pi_j =1/\mu_j\)</span>.) Remember that the expected return time: <span class="math display">\[ \mu_{j} = 1 + \sum_{i \in \Omega}P_{j,i}\eta_{i,j}. \qquad (eq:3) \]</span> We multiply both sides of(eq:2) by <span class="math inline">\(\pi_j\)</span> to get <span class="math display">\[\pi_j \mu_{j} =\pi_j +  \sum_{k \in \Omega} \pi_j P_{j,k}\eta_{k,j}\qquad (eq:4)\]</span> Adding (eq:2) and (eq:4), we get <span class="math display">\[\begin{align*}\sum_{i \in \Omega} \pi_i \eta_{i,j} + \pi_j \mu_{j} &amp;= \sum_{i \in\Omega} \pi_i + \sum_{i \in \Omega} \sum_{k \in \Omega} \pi_iP_{i,k}\eta_{k,j} \\&amp;= 1 + \sum_{k \in \Omega} \sum_{i \in \Omega}  \pi_iP_{i,k}\eta_{k,j} \\&amp;= 1 +  \sum_{k \in \Omega} \pi_k \eta_{k,j}  \qquad (\text{since}\sum_{i \in \Omega} \pi_i P_{i,k} = \pi_k) \\\end{align*}\]</span> <strong>Since the Markov chain is irreducible and positiverecurrent, that means all states belong to a communication class and theexpected return time of each state is finite. Thus, the space <span class="math inline">\(\Omega\)</span> is a finite dimensionalspace.</strong> We can substract <span class="math inline">\(\sum_{k \in\Omega} \pi_k \eta_{k,j}\)</span> and $_{i } <em>i </em>{i,j} $ (equal)from both sides of the above equation to get <span class="math display">\[\pi_j \mu_{j}=1,\]</span> which means <span class="math inline">\(\pi_j =1/\mu_j\)</span>. Similarly, we can prove that <span class="math inline">\(\pi_i = 1/\mu_i\)</span> for all <span class="math inline">\(i \in \Omega\)</span>.</td></tr></tbody></table><blockquote><p>Theorem 2 (Limit theorem) Consider an irreducible, aperiodic Markovchain (maybe infinite), we have <span class="math inline">\(\lim\limits_{t \to \infty} P_{i,j}^{t} =\frac{1}{\mu_{j}}\)</span>. Spectially, (1) Suppose the Markov chain ispositive recurrent. Then <span class="math inline">\(\lim\limits_{t \to\infty} P_{i,j}^{t} = \pi_j = \frac{1}{\mu_{j}}\)</span>. (2) Supposethe Markov chain is null recurrent or transient. Then there is no limiteprobability.</p></blockquote><ul><li>Three conditions for convergence to an equilibrium probabilitydistribution: irreducibility, aperiodicity, and positive recurrence. Thelimit probability <span class="math display">\[P =\begin{pmatrix}\pi_1 &amp; \pi_2 &amp; \cdots &amp; \pi_N\\\pi_1 &amp; \pi_2 &amp; \cdots &amp; \pi_N\\\vdots &amp; \vdots &amp; \ddots &amp; \vdots\\\pi_1 &amp; \pi_2 &amp; \cdots &amp; \pi_N\\\end{pmatrix}\]</span> where each row is identical.</li></ul><table style="width:7%;"><colgroup><col style="width: 6%"></colgroup><tbody><tr><td>Define <span class="math inline">\(V_{i,j}^{t} = |\{ n &lt; t | X_n= j\}|\)</span>. <span class="math inline">\(V_{i,j}^{t}\)</span> is thenumber of visits to state <span class="math inline">\(j\)</span> beforetime <span class="math inline">\(t\)</span> starting from state <span class="math inline">\(i\)</span>. Then we can interpret <span class="math inline">\(V_{i,j}^{t}/t\)</span> as the proportion of timeup to time <span class="math inline">\(t\)</span> spent in state <span class="math inline">\(j\)</span>.</td></tr><tr><td>&gt; Theorem 3 [Ergodic theorem] Consider an irreducible Markovchain, we have <span class="math inline">\(\lim\limits_{t \to \infty}V_{i,j}^{t}/t = \frac{1}{\mu_{j}}\)</span> <strong>almostsurely</strong>. Spectially, &gt; (1) Suppose the Markov chain ispositive recurrent. Then <span class="math inline">\(\lim\limits_{t \to\infty}  V_{i,j}^{t}/t = \pi_j = \frac{1}{\mu_{j}}\)</span><strong>almost surely</strong>. &gt; (2) Suppose the Markov chain isnull recurrent or transient. Then $ V_{i,j}^{t}/t $ <strong>almostsurely</strong> for all <span class="math inline">\(j\)</span>.</td></tr><tr><td><strong>almost surely</strong> means that the convergenceprobability of the event is 1.</td></tr></tbody></table><blockquote><p>Theorem 4[Detailed balance condition]. Consider a finite,irreducible, and ergodic Markov chain with transition matrix <span class="math inline">\(P\)</span>. If there are nonnegative numbers <span class="math inline">\(\bar{\pi} = (\pi_0, \pi_1, ..., \pi_n)\)</span>such that <span class="math inline">\(\sum_{i=0}^{n} \pi_i = 1\)</span>and if, for any pair of states <span class="math inline">\(i,j\)</span>, <span class="math display">\[\pi_i P_{i,j} = \pi_{j} P_{j,i},\]</span> then <span class="math inline">\(\bar{\pi}\)</span> is thestationary distribution corresponding to <span class="math inline">\(P\)</span>.</p></blockquote><p>Proof. <span class="math display">\[\sum_{i} \pi_i P_{i,j} = \sum_{i}\pi_{j} P_{j,i} = \pi_{j}\]</span> Thus, <span class="math inline">\(\bar{\pi} =\bar{\pi}P\)</span>. Since this is a finite, irreducible, and ergodicMarkov chain, <span class="math inline">\(\bar{\pi}\)</span> must be theunique stationary distribution of the Markov chain.</p><p>Remark: Theorem 2 is a sufficient but not necessary condition.</p><h2 id="reference">Reference</h2><ul><li>Mitzenmacher, M., &amp; Upfal, E. (2005). Probability and Computing.Cambridge University Press.</li><li><a href="https://mpaldridge.github.io/math2750/S09-recurrence-transience.html">Recurrenceand transience</a></li><li><a href="https://mpaldridge.github.io/math2750/S07-classes.html">Classstructure</a></li><li><a href="https://mpaldridge.github.io/math2750/S10-stationary-distributions.html">Stationarydistributions</a></li><li>Stirzaker, David. <a href="https://www.ctanujit.org/uploads/2/5/3/9/25393293/_elementary_probability.pdf">ElementaryProbability</a></li></ul>]]></content>
      
      
      <categories>
          
          <category> Stochastic Process </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Markov Chains </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Gradient Descent, Stochastic Gradient Descent, Variance Reduction</title>
      <link href="/2021/02/27/GD/"/>
      <url>/2021/02/27/GD/</url>
      
        <content type="html"><![CDATA[<h1 id="svrg2013-accelerating-stochastic-gradient-descent-using-predictive-variance-reduction">[SVRG2013]Accelerating Stochastic Gradient Descent using Predictive VarianceReduction</h1><h2 id="introduction">Introduction</h2><p>考虑如下优化问题 <span class="math display">\[\min P(\omega) = \frac{1}{n}\psi_{i}(\omega)\]</span></p><ul><li>如果采用平方损失，最小二乘回归；</li><li>考虑正则项，令 <span class="math inline">\(\psi_{i}(\omega) = \ln(1+ \exp(-\omega^{T}x_{i}y_{i})) + 0.5\lambda\omega^{T}\omega, y_{i} \in\{-1,1\}\)</span>，regularized logistic regression。</li></ul><p>梯度下降算法更新过程： <span class="math display">\[\omega^{(t)} = \omega^{(t-1)} - \eta_{t}\nabla P(\omega^{(t-1)}) =\omega^{(t-1)} - \frac{\eta_{t}}{n}\sum_{i=1}^{n}\nabla\psi_{i}(\omega^{(t-1)})\]</span></p><p>但是，在每一步，GD都需要计算 <span class="math inline">\(n\)</span>个一阶偏导，计算量大。所以，一个改进就是随机梯度下降SGD：在每一步迭代时，随机从<span class="math inline">\(\{1,...,n\}\)</span> 中抽取 <span class="math inline">\(i_{t}\)</span>，然后 <span class="math display">\[\omega^{(t)} = \omega^{(t-1)} - \eta_{t}\nabla\psi_{i_{t}}(\omega^{(t-1)})\]</span></p><p>期望 <span class="math inline">\(E[\omega^{(t)}|\omega^{(t-1)}]\)</span>同梯度更新的结果一致。 SGD的更一般表达形式为 <span class="math display">\[\omega^{(t)} = \omega^{(t-1)} - \eta_{t}g_{t}(\omega^{(t-1)},  \xi_{t})\]</span></p><p><span class="math inline">\(\xi_{t}\)</span> 为一个依赖于 <span class="math inline">\(\omega^{(t-1)}\)</span> 的随机变量， 并且期望<span class="math inline">\(E[g_{t}(\omega^{(t-1)},  \xi_{t})|\omega^{(t-1)}]= \nabla P(\omega^{(t-1)})\)</span>。</p><p>SGD的优势就是每步迭代只需要计算一个梯度，因此计算成本是GD的 <span class="math inline">\(\frac{1}{n}\)</span>。但是，SGD的一个缺点就是随机性引入了方差：虽然<span class="math inline">\(g_{t}(\omega^{(t-1)},  \xi_{t})\)</span>的期望等于梯度 <span class="math inline">\(\nablaP(\omega^{(t-1)})\)</span>，但是每个<span class="math inline">\(g_{t}(\omega^{(t-1)},  \xi_{t})\)</span>是不同的。方差的出现导致收敛速度变慢。对于SGD，由于随机取样带来的方差，一般都会要求其步长 <span class="math inline">\(\eta_{t} = O(1/t)\)</span>，从而得到一个sub-linear 收敛率 <span class="math inline">\(O(1/t)\)</span>。</p><ul><li>GD: 每步迭代计算慢，收敛快。</li><li>SGD：每步迭代计算快，收敛慢。</li></ul><p>为了改进SGD，一些学者开始设计算法以减少方差，从而可以使用较大的步长<span class="math inline">\(\eta_{t}\)</span>。有一些算法被提出来，比如：SAG(stochasticaveragegradient)，SDCA。但是这两个算法需要存储所有的梯度，一些情况下不太实际。因此作者提出了一个新的算法，该算法不需要存储所有的梯度信息，并且有较快的收敛速度，可以应用于非凸优化问题。</p><h2 id="stochastic-variance-reduced-gradient-svrg">Stochastic VarianceReduced Gradient (SVRG)</h2><p>为了保证收敛，SGD的步长必须衰减到0，从而导致收敛率变慢。需要较小步长的原因就是SGD的方差。作者提出一个解决方案。每进行<span class="math inline">\(m\)</span> 次SGD迭代后，记录当前参数 <span class="math inline">\(\tilde{\omega}\)</span> 以及平均梯度： <span class="math display">\[\tilde{\mu} = \nabla P(\tilde{\omega}) = \frac{1}{n}\sum_{i=1}^{n}\nabla \psi_{i}(\tilde{\omega}).\]</span></p><p>然后接下来的更新为：</p><p><span class="math display">\[\omega^{(t)} = \omega^{(t-1)} - \eta_{t}(\nabla\psi_{i_{t}}(\omega^{(t-1)}) - \nabla \psi_{i_{t}}(\tilde{\omega}) +\tilde{\mu})\]</span></p><p>注意到： <span class="math display">\[E[\omega^{(t)}|\omega^{(t-1)}] = \omega^{(t-1)} - \eta_{t}\nablaP(\omega^{(t-1)})\]</span></p><p>算法如下：</p><p><img src="/2021/02/27/GD/SVRG.png"></p><p>更新步骤中梯度的方差是减小的。当 <span class="math inline">\(\tilde{\omega}\)</span> 和 <span class="math inline">\(\omega^{(t)}\)</span> 收敛到最优参数 <span class="math inline">\(\omega_{*}\)</span>，<span class="math inline">\(\tilde{\mu} \to 0\)</span>， <span class="math inline">\(\nabla\psi_{i}(\tilde{\omega}) \to \nabla\psi_{i}(\omega_{*})\)</span>，有 <span class="math display">\[\nabla\psi_{i}(\omega^{(t-1)}) - \nabla\psi_{i}(\tilde{\omega}) + \tilde{\mu} \to \nabla\psi_{i}(\omega^{(t-1)}) -\nabla\psi_{i}(\omega_{*}) \to 0\]</span></p><p>SVRG的学习率不需要衰减，因此能有较快的收敛速度。作者提到，参数 <span class="math inline">\(m\)</span> 应该 <span class="math inline">\(O(n)\)</span>， 比如对于凸问题：<span class="math inline">\(m = 2n\)</span>，非凸问题：<span class="math inline">\(m = 2n\)</span>。</p><h1 id="saga2015-saga-a-fast-incremental-gradient-method-with-support-for-non-strongly-convex-composite-objectives">[SAGA2015]SAGA: A Fast Incremental Gradient Method With Support for Non-StronglyConvex Composite Objectives</h1><p>考虑最小化函数： <span class="math display">\[f(x) = \frac{1}{n}\sum_{i=1}^{n}f_{i}(x)\]</span></p><p>作者提出了一个叫做SAGA的算法，在目标函数为强凸函数时，SAGA的收敛速度优于SAG和SVRG。算法如下：</p><p><img src="/2021/02/27/GD/SAGA.png"></p><p>本文给出了variance reduction算法的一个解释： <img src="/2021/02/27/GD/SAGA1.png"></p><p>几种算法比较： <img src="/2021/02/27/GD/SAGA2.png"></p><h1 id="variance-reduced-stochastic-gradient-descent-with-neighbors-2015">[VarianceReduced Stochastic Gradient Descent with Neighbors 2015]</h1><h1 id="katyusha-2017-katyusha-the-first-direct-acceleration-of-stochastic-gradient-methods">[Katyusha2017] Katyusha: The First Direct Acceleration of Stochastic GradientMethods</h1><p>Nesterov's momentum通常用于加速梯度下降算法，但是，对于随机梯度下降，Nesterov's momentum可能无法对算法进行加速，即使优化目标为凸函数。因此，针对SGD，作者提出Katyusha 算法，借助于动量Katyusha momentum实现加速SGD。</p><p>考虑如下优化问题：</p><p><span class="math display">\[\min_{x \in \mathbb{R}^{d}} \{F(x) = f(x) + \psi(x) =\frac{1}{n}\sum_{i=1}^{n} f_{i}(x) + \psi(x)\}\]</span> 其中 <span class="math inline">\(f(x) =\frac{1}{n}\sum_{i=1}^{n} f_{i}(x)\)</span> 为凸函数，并且是 <span class="math inline">\(n\)</span> 个凸函数的有限平均。<span class="math inline">\(\psi(x)\)</span>为凸函数，可为近端函数。大多数假设 <span class="math inline">\(\psi(x)\)</span> 为 <span class="math inline">\(\sigma\)</span>-strongly，并且 <span class="math inline">\(f_{i}(x)\)</span> L-smooth。</p><p>作者提出一个可以求解上述优化问题的加速随机梯度下降算法-Katyusha：</p><p><img src="/2021/02/27/GD/ka.png"></p><p>其中，<span class="math inline">\(\tilde{x}\)</span> 为snapshotpoint，每经过 <span class="math inline">\(n\)</span>次迭代更新一次。<span class="math inline">\(\tilde{\nabla}_{k+1}\)</span> 为variance reduction中的梯度形式。<span class="math inline">\(\tau_{1}\)</span>，<span class="math inline">\(\tau_{2} \in[0,1]\)</span> 为两个动量参数，<span class="math inline">\(\alpha = \frac{1}{3\tau_{1}L}\)</span>。</p><h2 id="our-new-technique-katyusha-momentum">Our New Technique –Katyusha Momentum</h2><p>最novel的部分是 <span class="math inline">\(x_{k+1}\)</span>的更新步骤，是 <span class="math inline">\(y_{k}\)</span>, <span class="math inline">\(z_{k}\)</span> 以及 <span class="math inline">\(\tilde{x}\)</span> 的凸组合。理论建议参数 <span class="math inline">\(\tau_{2} = 0.5\)</span>，<span class="math inline">\(\tau_{1} = \min\{\sqrt{n\sigma/L},0.5\}\)</span>。</p><p>对于传统的加速梯度下降算法，<span class="math inline">\(x_{k+1}\)</span> 仅仅是 <span class="math inline">\(y_{k}\)</span> 和 <span class="math inline">\(z_{k}\)</span> 的凸组合，<span class="math inline">\(z_{k}\)</span>起到了一个“动量”的作用，即将历史加权的梯度信息添加到 <span class="math inline">\(y_{k+1}\)</span> 上。比如假设 <span class="math inline">\(\tau_{2} = 0, \tau_{1} = \tau\)</span>, <span class="math inline">\(x_{0} = y_{0} = z_{0}\)</span>，我们可以得到：</p><p><img src="/2021/02/27/GD/ka1.png"></p><p>由于 <span class="math inline">\(\alpha\)</span> 通常大于 <span class="math inline">\(1/3L\)</span>，上述递推过程意味着随着迭代进行，梯度<span class="math inline">\(\tilde{\nabla}_{t}\)</span>的贡献越高。比如，<span class="math inline">\(\tilde{\nabla}_{1}\)</span> 的权重不断增大 (<span class="math inline">\(\frac{1}{3L} &lt; ((1-\tau)\frac{1}{3L} +\tau\alpha) &lt; ((1 - \tau)^{2}\frac{1}{3L} + (1 - (1 -\tau)^{2})\alpha)\)</span>)。这就是一阶加速方法的核心思想。</p><p>在Katyusha算法中，作者认为 <span class="math inline">\(\tilde{x}\)</span> 同等重要，它能保证 <span class="math inline">\(x_{k+1}\)</span> 不要太远离 <span class="math inline">\(\tilde{x}\)</span>。<span class="math inline">\(\tilde{x}\)</span> 的添加可以看作是一个 “negativemomentum”，使 <span class="math inline">\(x_{k+1}\)</span> back to <span class="math inline">\(\tilde{x}\)</span>，抵消一部分前面迭代时的positive momentum”。</p><p>当 <span class="math inline">\(\tau_{1} = \tau_{2} = 0.5\)</span>时，Katyusha同SVRG几乎一致。</p><h1 id="l-svrg-l-katyusha-2019-dont-jump-through-hoops-and-remove-those-loops-svrg-and-katyusha-are-betterwithout-the-outer-loop">[L-SVRG,L-Katyusha 2019] Don’t Jump Through Hoops and Remove Those Loops: SVRGand Katyusha are BetterWithout the Outer Loop</h1><p>SVRG和Katyusha算法的共同关键结构就是两者都包含一个外层循环 (outerloop)。最初先在outerloop上使用所有样本计算梯度，然后计算出来的结果再用于内层循环 (innerloop)，结合新的随机梯度信息，构造variance-reduced梯度估计量。作者指出，由于SVRG和Katyusha算法都包括一个outerloop，所以存在一些问题，比如：算法很难分析；人们需要决定内部循环的次数。对于SVRG，理论上内部循环的最优次数取决于<span class="math inline">\(L\)</span>和 <span class="math inline">\(\mu\)</span>，但是 <span class="math inline">\(\mu\)</span>通常未知。由于这些问题存在，人们只能选择次优的inner loopsize，通常设置内部循环次数为 <span class="math inline">\(O(n)\)</span>或者 <span class="math inline">\(n\)</span>。</p><p>在这篇论文中，作者将外层循环 (outer loop)丢弃，在每次迭代时采用掷硬币技巧决定是否计算梯度，从而解决了上述问题。作者证明，新提出的算法和原始两个算法具有同样的理论性质。</p><p><img src="/2021/02/27/GD/loopless1.jpg"> <img src="/2021/02/27/GD/loopless1.jpg"></p><h1 id="l-svrg-l-katyusha-2019-l-svrg-and-l-katyusha-with-arbitrary-sampling">[L-SVRG,L-Katyusha 2019] L-SVRG and L-Katyusha with Arbitrary Sampling</h1><h1 id="参考文献">参考文献</h1><ul><li>Johnson, R. and Zhang, T. Accelerating stochastic gradient descentusing predictive variance reduction. In Advances in Neural InformationProcessing Systems 26, pp. 315–323, 2013a.<br></li><li>Defazio, A., Bach, F., and Lacoste-Julien, S. SAGA: a fastincremental gradient method with support for non-strongly convexcomposite objectives. In Advances in Neural Information ProcessingSystems, pp. 1646–1654, 2014.<br></li><li>Hofmann, T., Lucchi, A., Lacoste-Julien, S., and McWilliams, B.Variance reduced stochastic gradient descent with neighbors. In Advancesin Neural Information Processing Systems, pp.2305–2313, 2015.<br></li><li>Allen-Zhu, Z. Katyusha: The first direct acceleration of stochasticgradient methods. In Proceedings of the 49th Annual ACM SIGACT Symposiumon Theory of Computing, pp.1200–1205. ACM,2017.</li><li>Kovalev, D., Horváth, S., and Richtárik, P. Don’t jump through hoopsand remove those loops: SVRG and Katyusha are better without the outerloop. In Proceedings of the 31st International Conference on AlgorithmicLearning Theory, 2020.</li><li>Qian, X., Qu, Z., and Richtárik, P. L-SVRG and L-Katyusha witharbitrary sampling. arXiv preprint arXiv:1906.01481, 2019a.</li></ul>]]></content>
      
      
      <categories>
          
          <category> Machine Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> GD </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>ADMMdecentralized</title>
      <link href="/2021/02/03/ADMMdecentralized/"/>
      <url>/2021/02/03/ADMMdecentralized/</url>
      
        <content type="html"><![CDATA[<ul><li><a href="#abstract">Abstract</a></li><li><a href="#introduction">Introduction</a></li><li><a href="#preliminaries">Preliminaries</a><ul><li><a href="#notations-and-problem-setting">Notations and ProblemSetting</a></li></ul></li><li><a href="#model-propagation">Model Propagation</a><ul><li><a href="#asynchronous-gossip-algorithm">Asynchronous GossipAlgorithm</a></li></ul></li><li><a href="#collaborative-learning">Collaborative Learning</a><ul><li><a href="#problem-formulation">Problem Formulation</a></li><li><a href="#asynchronous-gossip-algorithm-1">Asynchronous GossipAlgorithm</a></li></ul></li><li><a href="#experiments">Experiments</a><ul><li><a href="#collaborative-linear-classification">Collaborative LinearClassification</a></li></ul></li><li><a href="#参考文献">参考文献</a></li></ul><!-- /TOC --><h1 id="abstract">Abstract</h1><p>考虑点对点的协作网络。本文解决的问题是：每个节点如何与具有相似目标的其他节点进行通信来改善本地模型？作者介绍了两种完全去中心化的算法，一种是受标签传播的启发，旨在平滑预先训练好的局部模型；第二种方法，节点基于本地数据核相邻节点进行迭代更新来共同学习和传播。</p><h1 id="introduction">Introduction</h1><p>数据不断产生，当前从数据中提取信息的主要方式是收集所有用户的个人数据于一个服务器上，然后进行数据挖掘。但是，中心化的方式存在一些问题，比如说一些用户拒绝提供个人数据，带宽和设备花费问题。即使一些算法允许数据分布在用户设备上，通常需要中心端来进行聚合和协调。</p><p>在本文中，作者考虑完全去中心化的点对点网络。不同于那些求解全局模型的算法，本文关注于每个节点可以根据自身目标函数学习一个个性化模型。作者假设网络结构已知，该网络结构能够反映出不同节点的相似度（如果两个节点具有相似的目标函数，那么这两个节点在网络中是邻居），每个节点只知道与其直接相邻的节点。一个节点不仅可以根据自身数据学习模型，还可以结合它的邻居。假设每个节点只知道相邻节点的信息，不知道整个网络结构。</p><p>作者提出两个算法。第一个是 modelpropagation：首先，每个节点先基于自己的局部数据学习到模型参数，然后，结合整个网络结构，平滑这些参数。第二个是collaborativelearning，这个算法更加灵活，它通过优化一个模型参数正则化（平滑）和局部模型准确性上的折中问题。作者基于分布式的ADMM算法提出一个异步gossip算法。</p><h1 id="preliminaries">Preliminaries</h1><h2 id="notations-and-problem-setting">Notations and ProblemSetting</h2><p>考虑 <span class="math inline">\(n\)</span> 个节点 <span class="math inline">\(V = [n] = \{1,...,n\}\)</span>。凸的损失函数 <span class="math inline">\(l: \mathbb{R}^{p} \times \mathcal{X} \times\mathcal{Y}\)</span>，节点 <span class="math inline">\(i\)</span>的目标是学习模型参数 <span class="math inline">\(\theta_{i} \in\mathbb{R}^{p}\)</span>，使得关于未知分布 <span class="math inline">\(\mu_{i}\)</span> 的期望损失 <span class="math inline">\(E_{(x_{i}, y_{i})\sim \mu_{i}}l(\theta_{i}; x_{i},y_{i})\)</span> 很小。节点 <span class="math inline">\(i\)</span> 具有<span class="math inline">\(m_{i}\)</span> 个来自分布 <span class="math inline">\(\mu_{i}\)</span> 的 i.i.d 的训练样本 <span class="math inline">\(S_{i} = \{(x_{i}^{j},y_{i}^{j})\}_{j=1}^{m_{i}}\)</span>。允许不同节点的样本量相差很大。每个节点可以最小化局部损失函数得到<span class="math inline">\(\theta_{i}^{sol}\)</span>:</p><p><span class="math display">\[\theta_{i}^{sol} \in \argmin_{\theta \in \mathbb{R}^{p}} L_{i}(\theta) =\sum_{j=1}^{m_{i}} l(\theta;x_{i}^{j}, y_{i}^{j}).\]</span></p><p>我们目标是通过结合其他节点信息，进一步改善上述模型。考虑一个加权网络结构<span class="math inline">\(G = (V, E)\)</span>，具有 <span class="math inline">\(V\)</span> 个节点，<span class="math inline">\(E\subseteq V \times V\)</span> 为无向边。定义 <span class="math inline">\(W \in \mathbb{R}^{n \times n}\)</span> 为由 <span class="math inline">\(G\)</span> 得到的对称非负加权矩阵，如果 <span class="math inline">\((i,j) \ne E\)</span> or <span class="math inline">\(i = j\)</span>， <span class="math inline">\(W_{ij} =0\)</span>。本文假设权重矩阵已知。定义对角阵 <span class="math inline">\(D\in \mathbb{R}^{n \times n}\)</span>，<span class="math inline">\(D_{ii} = \sum_{j=1}^{n} W_{ij}\)</span>。节点<span class="math inline">\(i\)</span> 的邻域 ：<span class="math inline">\(\mathcal{N}_{i} = \{j \ne i: W_{ij} &gt;0\}\)</span>。</p><h1 id="model-propagation">Model Propagation</h1><p>假设每个节点通过最小化局部损失函数得到各自的模型 <span class="math inline">\(\theta_{i}^{sol}\)</span>。由于每个节点上的模型都是在不同大小数据集上考虑得到，作者使用<span class="math inline">\(c_{i} \in (0,1]\)</span>定义每个节点模型的可信度。 <span class="math inline">\(c_{i}\)</span>的值应该和节点 <span class="math inline">\(i\)</span>的样本量大小呈正相关，可以设置为 <span class="math inline">\(c_{i} =\frac{m_{i}}{\max_{j} m_{j}}\)</span>。如果 <span class="math inline">\(m_{i}=0\)</span>，可以设置为一个小量。</p><p>定义 <span class="math inline">\(\Theta = [\theta_{1};\theta_{2};...;\theta_{n}] \in \mathbb{R}^{n \timesp}\)</span>，我们要优化的目标函数为：</p><figure><img src="https://cdn.jsdelivr.net/gh/sugar-xue/BlogImages@main/Picbed1612018569.jpg" alt="1612018569"><figcaption aria-hidden="true">1612018569</figcaption></figure><p>第一项二次函数用来平滑相邻节点的参数，当两个节点间权重越大时，节点间参数越相近；第二项的目的是使具有较高置信度的模型的参数不要太远离各自模型上的参数。具有较低置信度的模型的参数被允许具有较大的偏差，容易被相邻节点影响。<span class="math inline">\(D_{ii}\)</span> 的目的是为了normalization。</p><p><img src="https://cdn.jsdelivr.net/gh/sugar-xue/BlogImages@main/Picbed1612018994(1).png" alt="1612018994(1)"><br><img src="https://cdn.jsdelivr.net/gh/sugar-xue/BlogImages@main/Picbed1612019022(1).png" alt="1612019022(1)"></p><p>计算 (4)需要知道整个网络的信息以及所有节点的独立模型信息，这对于节点而言是未知的，因为每个节点只知道相邻节点的信息。因此，作者提出下面的迭代形式：</p><figure><img src="https://cdn.jsdelivr.net/gh/sugar-xue/BlogImages@main/Picbed1612019342(1).png" alt="1612019342(1)"><figcaption aria-hidden="true">1612019342(1)</figcaption></figure><p>作者证明，无论初始值 <span class="math inline">\(\Theta(0)\)</span>取何值，上述迭代序列收敛到 (4)。(5) 式可以进一步分解为</p><figure><img src="https://cdn.jsdelivr.net/gh/sugar-xue/BlogImages@main/Picbed1612019480(1).png" alt="1612019480(1)"><figcaption aria-hidden="true">1612019480(1)</figcaption></figure><p>考虑一个同步计算：在每一步，每个节点都和其所有相邻节点进行通信，收集它们当前参数，然后使用它们的参数更新上式。同步更新会导致很大的延迟，因为任何节点都必须等剩余节点更新完后才能进行下一步更新。并且，每一步，所有节点都需要和其邻居节点进行通信，降低了算法的效率。所以作者提出一个异步算法。</p><h2 id="asynchronous-gossip-algorithm">Asynchronous GossipAlgorithm</h2><p>在异步设置中，每个节点都有一个局部clock ticking at times of rate 1Poisson process.由于节点都是独立同分布的，所以相当于在每一步时等概率激活每个节点。</p><p>在时间 <span class="math inline">\(t\)</span>时，每个节点都存有相邻节点的信息。以数学形式表示，考虑矩阵 <span class="math inline">\(\tilde{\Theta}_{i}(t) \in \mathbb{R}^{n \timesp}\)</span>，第 <span class="math inline">\(i\)</span> 行 <span class="math inline">\(\tilde{\Theta}_{i}^{i}(t) \in\mathbb{R}^{p}\)</span> 为节点 <span class="math inline">\(i\)</span>在时刻 <span class="math inline">\(t\)</span> 的模型参数，<span class="math inline">\(\tilde{\Theta}_{i}^{j}(t) \in \mathbb{R}^{p} (j\ne i)\)</span> 为节点 <span class="math inline">\(i\)</span>储存的关于邻居节点 <span class="math inline">\(j\)</span> 的lastknowledge. 对于 <span class="math inline">\(j \notin \mathcal{N}_{i}\bigcup \{i\}\)</span>，<span class="math inline">\(\forall t &gt;0\)</span>，<span class="math inline">\(\tilde{\Theta}_{i}^{j}(t) =0\)</span>。令<span class="math inline">\(\tilde{\Theta} =[\tilde{\Theta}_{1}^{T}, ...,\tilde{\Theta}_{n}^{T}] \in\mathbb{R}^{n^{2} \times p}\)</span>。</p><p>如果在时间 <span class="math inline">\(t\)</span> 时，节点 <span class="math inline">\(i\)</span> wakes up，执行如下步骤：</p><ul><li><p>communication: 节点 <span class="math inline">\(i\)</span>随机选择一个邻居节点 <span class="math inline">\(j \in\mathcal{N}_{i}\)</span>，(先验概率 <span class="math inline">\(\pi_{i}^{j}\)</span>)，节点 <span class="math inline">\(i\)</span> 和节点 <span class="math inline">\(j\)</span> 同时更新它们的参数： <span class="math display">\[\tilde{\Theta}_{i}^{j}(t+1) = \tilde{\Theta}_{j}^{j}(t) \qquad\tilde{\Theta}_{j}^{i}(t+1) = \tilde{\Theta}_{i}^{i}(t),\]</span></p></li><li><p>update: 基于当前信息，节点 <span class="math inline">\(i\)</span>和节点 <span class="math inline">\(j\)</span> 更新自己的模型参数： <span class="math display">\[\tilde{\Theta}_{l}^{l}(t+1) = (\alpha +\bar{\alpha}c_{l})^{-1}(\alpha\sum_{k \in \mathcal{N}_{l}}\frac{W_{lk}}{D_{ll}}\tilde{\Theta}_{l}^{k}(t+1) +\bar{\alpha}c_{l}\theta^{sol}_{l}) \quad(l \in \{i,j\}).\]</span></p><p>网络中的其他变量保持不变。作者提出的算法属于 gossipalgorithms，每个节点每次最多只和一个邻居节点通信。</p><p>作者证明，上述算法可以收敛到使每个节点具有最优参数。</p><figure><img src="https://cdn.jsdelivr.net/gh/sugar-xue/BlogImages@main/Picbed1612340701.jpg" alt="1612340701"><figcaption aria-hidden="true">1612340701</figcaption></figure></li></ul><h1 id="collaborative-learning">Collaborative Learning</h1><p>上述算法先在局部节点上进行学习，然后在进行网络通信。在这部分，作者提出了一个使节点可以同时进行基于局部数据和邻居节点信息更新模型参数的算法。相较于前面的算法，该算法通信成本较高，但是估计精度高于前者。</p><h2 id="problem-formulation">Problem Formulation</h2><p>优化目标：</p><figure><img src="https://cdn.jsdelivr.net/gh/sugar-xue/BlogImages@main/Picbed1612341031(1).png" alt="1612341031(1)"><figcaption aria-hidden="true">1612341031(1)</figcaption></figure><p>注意到，这里的置信度通过 <span class="math inline">\(\mathcal{L}_{i}\)</span> 体现，因为 <span class="math inline">\(\mathcal{L}_{i}\)</span> 为局部节点 <span class="math inline">\(i\)</span> 上所有观测的损失函数和。</p><p>一般情况下，上述问题没有解析解，作者提出一个分散式迭代算法进行求解。</p><h2 id="asynchronous-gossip-algorithm-1">Asynchronous GossipAlgorithm</h2><p>作者基于ADMM提出了一个异步分散式算法。本文的目的不是寻找一个consensus解，因为我们的目标是为了学习到每个节点的personalized model.作者通过将问题 (7)进行变换为一个部分consensus问题，使用ADMMD进行求解。</p><p>令 <span class="math inline">\(\Theta_{i} \in\mathbb{R}^{(|\mathcal{N_{i}}|+1)\times p}\)</span> 为变量 <span class="math inline">\(\theta_{j} \in \mathbb{R}^{p}(j \in\mathcal{N_{i}} \bigcup \{i\})\)</span> 的集合。定义 <span class="math inline">\(\theta_{j}\)</span> 为 <span class="math inline">\(\Theta_{i}^{j}\)</span>。优化问题(7)重新写为：</p><figure><img src="https://cdn.jsdelivr.net/gh/sugar-xue/BlogImages@main/Picbed1612341938(1).png" alt="1612341938(1)"><figcaption aria-hidden="true">1612341938(1)</figcaption></figure><p>在这个目标函数中，所有的节点相互依赖，因为它们共享一个优化变量 <span class="math inline">\(\in\Theta\)</span>。为了使用ADMM，需要将各个节点的优化变量独立，对于每个节点<span class="math inline">\(i\)</span>，定义一个local copy <span class="math inline">\(\tilde{\Theta}_{i} \in\mathbb{R}^{(|\mathcal{N_{i}}|+1)\times p}\)</span>，添加等式约束：<span class="math inline">\(\tilde{\Theta}_{i}^{i} =\tilde{\Theta}_{j}^{i}\)</span>，对于所有的 <span class="math inline">\(i \in [n], j \in \mathcal{N}_{i}\)</span>。</p><figure><img src="https://cdn.jsdelivr.net/gh/sugar-xue/BlogImages@main/Picbed1612342502(1).png" alt="1612342502(1)"><figcaption aria-hidden="true">1612342502(1)</figcaption></figure><p>增广拉格朗日乘子：</p><figure><img src="https://cdn.jsdelivr.net/gh/sugar-xue/BlogImages@main/Picbed1612342580(1).png" alt="1612342580(1)"><figcaption aria-hidden="true">1612342580(1)</figcaption></figure><p>算法如下，假设时刻 <span class="math inline">\(t\)</span> 时节点<span class="math inline">\(i\)</span> wakes up，选取邻居节点 <span class="math inline">\(j \in \mathcal{N}_{i}\)</span>，定义 <span class="math inline">\(e = (i,j)\)</span>，</p><figure><img src="https://cdn.jsdelivr.net/gh/sugar-xue/BlogImages@main/Picbed1612342728(1).png" alt="1612342728(1)"><figcaption aria-hidden="true">1612342728(1)</figcaption></figure><figure><img src="https://cdn.jsdelivr.net/gh/sugar-xue/BlogImages@main/Picbed1612342765(1).png" alt="1612342765(1)"><figcaption aria-hidden="true">1612342765(1)</figcaption></figure><h1 id="experiments">Experiments</h1><h2 id="collaborative-linear-classification">Collaborative LinearClassification</h2><p>考虑100个节点，每个节点的目标是建立一个线性分类模型 in <span class="math inline">\(\mathbb{R}^{p}\)</span>。为了方便可视化，每个节点的真实参数位于2维子空间：将其参数看作是<span class="math inline">\(\mathbb{R}^{p}\)</span>空间中的向量，前两项从正态分布中随机产生，剩余项为0。两个节点 <span class="math inline">\(i\)</span> 和节点 <span class="math inline">\(j\)</span> 的相似度通过参数距离的高斯核定义，定义<span class="math inline">\(\phi_{ij}\)</span>为两个真实参数在单位圆上投影的夹角，<span class="math inline">\(W_{ij} =\exp(\cos \phi_{ij} - 1)/\sigma\)</span>，<span class="math inline">\(\sigma =0.1\)</span>。权重为负值的将被忽略。每个节点具有随机的训练样本，样本的标签为二元标签，由线性分类模型产生。以概率0.05随机使标签反转，以产生噪音数据。每个节点的损失函数为hinge损失：<span class="math inline">\(l(\theta;(x_{i}, y_{i})) = \max(0,1-y_{i}\theta^{T}x_{i})\)</span>。作者评估了模型在100个测试样本上的预测精度。</p><figure><img src="https://cdn.jsdelivr.net/gh/sugar-xue/BlogImages@main/Picbed1612343843(1).png" alt="1612343843(1)"><figcaption aria-hidden="true">1612343843(1)</figcaption></figure><h1 id="参考文献">参考文献</h1><ul><li>Vanhaesebrouck, P., Bellet, A. &amp; Tommasi, M.. (2017).Decentralized Collaborative Learning of Personalized Models overNetworks. Proceedings of the 20th International Conference on ArtificialIntelligence and Statistics, in PMLR 54:509-517</li></ul>]]></content>
      
      
      <categories>
          
          <category> Distributed Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> FL </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Lower Bounds and Optimal Algorithms for Personalized Federated Learning</title>
      <link href="/2021/01/26/AL2SGD/"/>
      <url>/2021/01/26/AL2SGD/</url>
      
        <content type="html"><![CDATA[<ul><li><a href="#introduction">Introduction</a></li><li><a href="#contributions">Contributions</a></li><li><a href="#lower-complexity-bounds">Lower complexity bounds</a><ul><li><a href="#lower-complexity-bounds-on-the-communication">Lowercomplexity bounds on the communication</a></li><li><a href="#lower-complexity-bounds-on-the-local-computation">Lowercomplexity bounds on the local computation</a></li></ul></li><li><a href="#优化算法">优化算法</a><ul><li><a href="#accelerated-proximal-gradient-descent-apgd-for-federated-learning">AcceleratedProximal Gradient Descent (APGD) for Federated Learning</a></li><li><a href="#beyond-proximal-oracle-inexact-apgd-iapgd">Beyond proximaloracle: Inexact APGD (IAPGD)</a></li><li><a href="#accelerated-l2sgd">Accelerated L2SGD+</a></li></ul></li><li><a href="#experiments">Experiments</a></li><li><a href="#参考文献">参考文献</a></li></ul><!-- /TOC --><h1 id="introduction">Introduction</h1><p>作者在前文考虑了一个新的优化问题： <span class="math display">\[\min_{\mathbf{x_{1},...,x_{n}} \in \mathbb{R}^{d}} \{ F(\mathbf{x}) :=f(\mathbf{x}) + \lambda \psi(\mathbf{x})\}\]</span> <span class="math display">\[f(\mathbf{x}):= \frac{1}{n}\sum_{i=1}^{n}f_{i}(\mathbf{x_{i}}), \quad\psi(\mathbf{x}):= \frac{1}{2n}\sum_{i=1}^{n}\|\mathbf{x_{i}} -\mathbf{\bar{x}}\|^{2}\]</span></p><p>Remark：该问题的最优解 <span class="math inline">\(\mathbf{x}^{*} =[\mathbf{x}_{1}^{*},..., \mathbf{x}_{n}^{*}] \in\mathbb{R}^{nd}\)</span> 可以被表示为 <span class="math inline">\(\mathbf{x}^{*}_{i} = \mathbf{\bar{x}}^{*} -\frac{1}{\lambda}\nabla f_{i}(\mathbf{x}_{i}^{*})\)</span>，其中 <span class="math inline">\(\mathbf{\bar{x}}^{*} = \frac{1}{n}\sum_{i=1}^{n}\mathbf{x}_{i}^{*}\)</span>，该形式与MAML相似。</p><h1 id="contributions">Contributions</h1><p>在这篇论文中，作者给出了求解上述优化问题的通信和局部计算复杂度（迭代次数）的最低界限，并且给出了几种能够达到最低界限的算法。</p><ul><li><p>lower bound on the communication complexity.作者证明对于任意一个满足一定假设条件的算法，会有一个L-smooth, <span class="math inline">\(\mu\)</span>-strongly convex 局部目标函数 <span class="math inline">\(f_{i}\)</span> 至少需要通信 <span class="math inline">\(O(\sqrt{\frac{\min\{L,  \lambda\}}{\mu}}\log\frac{1}{\epsilon})\)</span>轮才能得到最优解 <span class="math inline">\(\epsilon\)</span>邻域内的解。</p></li><li><p>lower complexity bound on the number of local oracle calls.作者证明对于局部近端梯度下降，至少需要迭代<span class="math inline">\(O(\sqrt{\frac{\min\{L,  \lambda\}}{\mu}}\log\frac{1}{\epsilon})\)</span>次；对于局部梯度下降，至少需要进行 <span class="math inline">\(O(\sqrt{\frac{L}{\mu}}\log\frac{1}{\epsilon})\)</span>次迭代；若每个目标函数为 <span class="math inline">\(m\)</span>个有限和形式（<span class="math inline">\(\tilde{L}\)</span>-smooth)，至少需要 <span class="math inline">\(O((m +\sqrt{\frac{m\tilde{L}}{\mu}})\log\frac{1}{\epsilon})\)</span>次。</p></li><li><p>作者讨论了不同的用于求解上述优化问题的算法，这些算法在不同设定下可以达到最优通信复杂度和最优局部梯度复杂度。。首先是加速近端梯度下降算法(APGD)，作者考虑两种不同的应用方式，第一种是：函数<span class="math inline">\(f\)</span> 采用梯度下降，<span class="math inline">\(\lambda \psi\)</span>采用近端梯度下降，第二种是反过来。对于第一种情况，当 <span class="math inline">\(L \leq \lambda\)</span>时，我们可以实现最优通信复杂度和局部梯度复杂度 <span class="math inline">\(O(\sqrt{\frac{L}{\mu}}\log\frac{1}{\epsilon})\)</span>；对于第二种情况，当<span class="math inline">\(L \geq \lambda\)</span>时，我们可以得到最优通信复杂度和局部近端复杂度 <span class="math inline">\(O(\sqrt{\frac{\lambda}{\mu}}\log\frac{1}{\epsilon})\)</span>。受一篇论文启发，作者提到局部近端可以由局部加速梯度下降 (Local AGD) 近似(inexactly) 得到，当目标函数为有限和形式，还可以采用Katyusha算法近似得到。Local AGD 可以得到 <span class="math inline">\(O(\sqrt{\frac{\lambda}{\mu}}\log\frac{1}{\epsilon})\)</span>的通信复杂度，以及 <span class="math inline">\(\tilde{O}(\sqrt{\frac{L+\lambda}{\mu}})\)</span>的局部梯度复杂度，当 <span class="math inline">\(L \geq\lambda\)</span>（取决于对数因子）时，两者都能达到最优。同样，当局部采用 Katyusha时，我们可以得到通信复杂度 <span class="math inline">\(O(\sqrt{\frac{\lambda}{\mu}}\log\frac{1}{\epsilon})\)</span>和局部梯度复杂度 <span class="math inline">\(\tilde{O}(m\sqrt{\frac{\lambda}{\mu}} + \sqrt{m\frac{\tilde{L}}{\mu}})\)</span>，前者当 <span class="math inline">\(L\geq \Lambda\)</span> 时能达到最优，后者当 <span class="math inline">\(m\lambda \leq \tilde{L}\)</span>（取决于对数因子）时达到最优。</p></li><li><p>作者提出了加速的L2SGD+算法-AL2SGD+，该算法可以实现最优通信复杂 度<span class="math inline">\(O(\sqrt{\frac{\min\{\tilde{L},  \lambda\}}{\mu}}\log\frac{1}{\epsilon})\)</span>，以及局部梯度复杂度<span class="math inline">\(O((m + \sqrt{\frac{m(\tilde{L} +\lambda)}{\mu}})\log\frac{1}{\epsilon})\)</span>，当 <span class="math inline">\(\lambda \leq \tilde{L}\)</span>时最优。但是，两者无法同时实现最优。</p><p><img src="/2021/01/26/AL2SGD/t1.jpg"></p></li></ul><h1 id="lower-complexity-bounds">Lower complexity bounds</h1><h2 id="lower-complexity-bounds-on-the-communication">Lower complexitybounds on the communication</h2><p><img src="/2021/01/26/AL2SGD/3.1.png"> <img src="/2021/01/26/AL2SGD/3.11.png"></p><h2 id="lower-complexity-bounds-on-the-local-computation">Lowercomplexity bounds on the local computation</h2><p><img src="/2021/01/26/AL2SGD/3.2.png"></p><h1 id="优化算法">优化算法</h1><h2 id="accelerated-proximal-gradient-descent-apgd-for-federated-learning">AcceleratedProximal Gradient Descent (APGD) for Federated Learning</h2><p>首先介绍非加速版本的近端梯度下降算法(PGD): <img src="/2021/01/26/AL2SGD/1.png"></p><p>根据另一篇论文，有两种不同的方式可以将梯度下降算法应用到上述优化问题上。最直接的方式是令<span class="math inline">\(h = f\)</span>，<span class="math inline">\(\phi =\lambda\psi\)</span>，那么可以得到如下更新步骤： <img src="/2021/01/26/AL2SGD/2.png"></p><p>另一种方式是令 <span class="math inline">\(h(\mathbf{x}) = \lambda\phi(\mathbf{x}) + \frac{\mu}{2n}\|\mathbf{x}\|^{2}\)</span>， <span class="math inline">\(\phi(\mathbf{x}) = f(\mathbf{x}) -\frac{\mu}{2n}\|\mathbf{x}\|^{2}\)</span>。由此得到的更新过程如下： <img src="/2021/01/26/AL2SGD/3.png"></p><p>同FedProx算法一致。</p><p>由于上述两种情况下，每次迭代都需要进行一轮通信，相应的通信复杂度次优。但是可以结合动量算法，程序(6)可以结合Nesterov'smomentum，能够得到最优通信复杂度，以及最优局部近端复杂度（当 <span class="math inline">\(\lambda \leqL)\)</span>，该算法定义为APGD1，具体如下：</p><p><img src="/2021/01/26/AL2SGD/a2.png"></p><p>将更新过程(5)和动量结合，可以得到最优通信复杂度以及最优局部近端复杂度（当<span class="math inline">\(\lambda \geqL）\)</span>。将该算法定义为APGD2，具体如下：</p><p><img src="/2021/01/26/AL2SGD/a3.png"></p><h2 id="beyond-proximal-oracle-inexact-apgd-iapgd">Beyond proximaloracle: Inexact APGD (IAPGD)</h2><p>在多数情况下，如果采用局部近端操作，每一步迭代时都需要得到子问题的精确解，这是不实际的。因此，作者提出了一个针对(6)的加速非精确的算法，每个节点只需要进行局部梯度运算（AGD, Katyusha)：</p><p><img src="/2021/01/26/AL2SGD/a1.png"></p><h2 id="accelerated-l2sgd">Accelerated L2SGD+</h2><p>作者给出L2SGD+算法的一个加速版本-AL2SGD+。作者指出AL2SGD+算法不过是L-Katyusha算法与非均匀抽样的结合。</p><p><img src="/2021/01/26/AL2SGD/a4.png"></p><h1 id="experiments">Experiments</h1><p>在第一个实验中，作者比较了当局部损失为有限和形式时，算法IAPGD+Katyusha、AL2SGD+以及L2SGD+的收敛速度。结果如下图：</p><p><img src="/2021/01/26/AL2SGD/5.jpg"></p><p>对于通信轮数，IAPGD+Katyusha和AL2SGD+都显著优于L2SGD+；对于局部计算次数，AL2SGD+表现最优，IAPGD+Katyusha不如L2SGD+。</p><p>第二个实验中，作者研究了数据异质性对算法的影响，结果如下图所示。可以看出，数据异质性不影响算法的收敛速度，各个算法的表现同第一个实验相似。</p><p><img src="/2021/01/26/AL2SGD/6.png"></p><p>在第三个实验中，作者比较了APGD算法的两种变形：APGD1和APGD2。作者不断改变参数<span class="math inline">\(\lambda\)</span>的取值，其余参数保持不变。在理论上，APGD2算法应该不受参数 <span class="math inline">\(\lambda\)</span> 影响，而APGD1 算法的收敛率会随着<span class="math inline">\(\lambda\)</span> 而增加 (<span class="math inline">\(\sqrt{\lambda}\)</span>)。 当 <span class="math inline">\(\lambda \leq L = 1\)</span>时，APGD1是最优选择；当<span class="math inline">\(\lambda &gt; L = 1\)</span> 时，APGD2应该是最优选择。实验结果如下图所示，结果与理论一致。</p><p><img src="/2021/01/26/AL2SGD/7.png"></p><h1 id="参考文献">参考文献</h1><ul><li>Filip Hanzely (KAUST) · Slavomír Hanzely (KAUST) · Samuel Horváth(King Abdullah University of Science and Technology)· Peter Richtarik(KAUST). Lower Bounds and Optimal Algorithms for Personalized FederatedLearning.arXiv e-prints.https://ui.adsabs.harvard.edu/abs/2020arXiv201002372H</li></ul>]]></content>
      
      
      <categories>
          
          <category> Distributed Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> FL </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Federated Learning of a Mixture of Global and Local Models</title>
      <link href="/2021/01/26/L2SGD/"/>
      <url>/2021/01/26/L2SGD/</url>
      
        <content type="html"><![CDATA[<ul><li><a href="#introduction">Introduction</a><ul><li><a href="#11-federated-learning">1.1 Federated learning</a></li></ul></li><li><a href="#contributions">Contributions</a></li><li><a href="#新的优化问题">新的优化问题</a></li><li><a href="#l2gd-loopless-local-gd">L2GD: Loopless Local GD</a><ul><li><a href="#收敛理论">收敛理论</a></li><li><a href="#收敛率优化">收敛率优化</a></li></ul></li><li><a href="#loopless-local-sgd-with-variance-reduction">Loopless LocalSGD with Variance Reduction</a><ul><li><a href="#问题设置">问题设置</a></li><li><a href="#理论">理论</a></li></ul></li><li><a href="#experiments">Experiments</a></li><li><a href="#附录">附录</a><ul><li><a href="#experimental-setup-and-further-experiments">ExperimentalSetup and further experiments</a></li><li><a href="#其余算法">其余算法</a></li></ul></li><li><a href="#参考文献">参考文献</a></li></ul><!-- /TOC --><h1 id="introduction">Introduction</h1><h2 id="federated-learning">1.1 Federated learning</h2><p>联邦学习的目标函数： <span class="math display">\[  \min_{\mathbf{x} \in \mathbb{R}^{d}}\frac{1}{n}\sum_{i=1}^{n}f_{i}(\mathbf{x})  \]</span> 其中 <span class="math inline">\(n\)</span>表示参与训练的节点个数，<span class="math inline">\(\mathbf{x} \in\mathbb{R}^{d}\)</span>为全模型优化变量。 <span class="math inline">\(f_{i}(\mathbf{x})\)</span>为节点<span class="math inline">\(i\)</span>上的损失函数。</p><h1 id="contributions">Contributions</h1><ul><li>提出了新的FL优化形式，尝试学习全局模型和局部模型的混合。</li><li>给出了新的优化形式的理论性质。作者证明了最优局部模型以<span class="math inline">\(O(1/\lambda)\)</span>收敛到传统的全局模型；作者证明了在局部模型上得到的损失不高于全局模型上的损失(定理3.1)；作者指出局部模型的最优解等于所有局部模型最优解的平均值减去对应局部模型上损失函数的一阶梯度，这一点和MAML一致。</li><li>Loopless LGD：作者提出了一个随机梯度算法 — Loopless Local GradientDescent(L2GD)（算法1）来解决提出的优化问题。该算法不是一个标准的SGD，它可以看作是一个关于损失函数和惩罚项的不均匀抽样。当抽到损失函数部分时，每个节点执行一次随机梯度下降；当抽到惩罚项时，进行信息聚合。</li><li>收敛理论。假设函数 <span class="math inline">\(f_{i}\)</span> 为<span class="math inline">\(L-smooth\)</span>，并且为 <span class="math inline">\(\mu-strong \, convex\)</span>，可以得到抽样概率<span class="math inline">\(p^{*} = \frac{\lambda}{\lambda +L}\)</span>，固定期望局部更新次数为 <span class="math inline">\(1 +\frac{L}{\lambda}\)</span>，作者证明通信 (communication)复杂度为（通信次数上界）为 <span class="math inline">\(\frac{2\lambda}{\lambda +L}\frac{L}{\mu}\log\frac{1}{\epsilon}\)</span>。当 <span class="math inline">\(\lambda \to 0\)</span>时，通信次数非常小；当$$时，根据新优化问题得到的解收敛到全局模型最优解，并且L2GD算法的通信上界为 <span class="math inline">\(O(\frac{L}{\mu}\log\frac{1}{\epsilon})\)</span>。</li><li>推广。部分连接，局部SGD，variancereduction（variance来自三部分：非均匀抽样，部分连接，从节点样本随机抽样）。</li><li>可用于异质数据。</li><li>经验表现不错。</li></ul><h1 id="新的优化问题">新的优化问题</h1><p><span class="math display">\[\min_{\mathbf{x_{1},...,x_{n}} \in \mathbb{R}^{d}} \{ F(\mathbf{x}) :=f(\mathbf{x}) + \lambda \psi(\mathbf{x})\}\]</span> <span class="math display">\[f(\mathbf{x}):= \frac{1}{n}\sum_{i=1}^{n}f_{i}(\mathbf{x_{i}}), \quad\psi(\mathbf{x}):= \frac{1}{2n}\sum_{i=1}^{n}\|\mathbf{x_{i}} -\mathbf{\bar{x}}\|^{2}\]</span></p><ul><li>Local model (<span class="math inline">\(\lambda = 0\)</span>)</li><li>Mixed model (<span class="math inline">\(\lambda \in (0,\infty)\)</span>)</li><li>Global model (<span class="math inline">\(\lambda =\infty\)</span>)</li></ul><h1 id="l2gd-loopless-local-gd">L2GD: Loopless Local GD</h1><p>在这一部分中，作者给出一个算法求解上述优化问题，该算法可以看作是一个非均匀SGD，要么抽取<span class="math inline">\(\nabla f\)</span>，要么抽取 <span class="math inline">\(\nabla \psi\)</span> 估计 <span class="math inline">\(\nabla F\)</span>。令 <span class="math inline">\(0 &lt; p &lt; 1\)</span>，定义一个随机梯度如下：<span class="math display">\[G(\mathbf{x}):= \begin{cases} \frac{\nabla f(\mathbf{x})}{1-p}, &amp;\text {概率 $1-p$} \\ \frac{\lambda \nabla \psi(\mathbf{x})}{p}, &amp;\text{概率 $p$ } \end{cases}\]</span> 显然，<span class="math inline">\(G(\mathbf{x})\)</span>为<span class="math inline">\(\nabla F(\mathbf{x})\)</span>的无偏估计量。每步的更新为: <span class="math display">\[\mathbf{x}^{k+1} = \mathbf{x}^{k} - \alpha G(\mathbf{x}).\]</span></p><figure><img src="/2021/01/26/L2SGD/l2gd.png" alt="Algorithm 1"><figcaption aria-hidden="true">Algorithm 1</figcaption></figure><p><span class="math inline">\(\textbf{Lemma 4.2}\)</span> 经过 <span class="math inline">\(k\)</span> 步迭代后，期望的通信次数为 <span class="math inline">\(p(1-p)k\)</span>。</p><h2 id="收敛理论">收敛理论</h2><p>作者首先证明梯度估计量<span class="math inline">\(G(\mathbf{x})\)</span>的期望具有光滑性质，然后证明了算法L2GD的收敛性质。（<span class="math inline">\(\mathbf{x(\lambda)}\)</span>为最优解，定理4.4表明，L2GD算法只能收敛到最优解邻域。） <img src="/2021/01/26/L2SGD/4.3.png"></p><h2 id="收敛率优化">收敛率优化</h2><p>作者给出最优抽样概率 <span class="math inline">\(p^{*} =\frac{\lambda}{L + \lambda}\)</span>，步长 <span class="math inline">\(\alpha\)</span> 要满足 <span class="math inline">\(\frac{\alpha\lambda}{np} \leq\frac{1}{2}\)</span>. <img src="/2021/01/26/L2SGD/4.4.png"></p><h1 id="loopless-local-sgd-with-variance-reduction">Loopless Local SGDwith Variance Reduction</h1><p>L2GD算法仅线性收敛到最优解的邻域，无法收敛到最优解。假设每个子目标函数具有有限和形式，作者提出了一个算法L2SGD+，在每个节点上进行随机梯度下降，并且具有线性收敛速度。L2SGD是一个具有variancereduction 的局部SGD算法，关于SGD的variance reduction，见另一篇博客：SGDwith variance reduction.</p><h2 id="问题设置">问题设置</h2><p>假设 <span class="math inline">\(f_{i}\)</span> 具有有限和结构：<span class="math display">\[f_{i} = \frac{1}{m}\sum_{j=1}^{m}f_{i,j}(\mathbf{x}_{i})\]</span></p><p>那么目标函数变为： <span class="math display">\[F(\mathbf{x}) =\frac{1}{n}\sum_{i=1}^{n}(\frac{1}{m}\sum_{i=1}^{m}f_{i,j}(\mathbf{x}_{i}))+ \lambda\frac{1}{2n}\sum_{i=1}^{n}\|\mathbf{x}_{i} -\mathbf{\bar{x}}\|^{2}\]</span></p><p><img src="/2021/01/26/L2SGD/l2sgd+.png"></p><p>L2SGD算法仅在两次抽样不同时才会发生通信，经过 <span class="math inline">\(k\)</span> 次迭代后，需要进行 <span class="math inline">\(p(1-p)k\)</span>次聚合平均。但是，L2SGD算法还需要通信控制变量 <span class="math inline">\(\mathbf{J_{i}I,  \Psi_{i}}\)</span>，因此通信次数变为原来的3倍。在附录中，作者给出了一个高效的L2SGD+，不需要通信控制变量。</p><h2 id="理论">理论</h2><p>作者给出了L2SGD算法的理论性质，并且给出最优抽样概率 <span class="math inline">\(p^{*}\)</span>。</p><p><img src="/2021/01/26/L2SGD/5.1.jpg"> <img src="/2021/01/26/L2SGD/5.2.png"> <img src="/2021/01/26/L2SGD/5.3.png"></p><h1 id="experiments">Experiments</h1><p>作者考虑Logistic回归问题，数据为LibSVM data(Chank &amp; Lin,2011)。数据首先进行normalized，以使得 <span class="math inline">\(f_{ij}\)</span>为1-smooth。步长根据定理5.2确定。每个数据集被划分为不同个数的节点，具体参数设置如下：<img src="/2021/01/26/L2SGD/table1.png"></p><p>作者考虑三种算法：L2SGD+, L2SGD(L2GD with local SGD), L2SGD2(L2GDwith local subsampling and control variates constructed for <span class="math inline">\(\Psi\)</span>)。根据理论分析，L2SGD+线性收敛到最优解，而L2SGD和L2SGD2收敛到最优解邻域。</p><p>作者考虑了两种数据分割方式。对于homogeneous data,首先将观测样本随机打乱，然后按照打乱后的数据划分到不同节点上；对于heterogeneousdata,首先根据观测样本的标签将样本排序，然后将排序后的数据依次划分到不同节点上(the worst-case heterogeneity)。</p><p><img src="/2021/01/26/L2SGD/figure3.png"></p><p>结果表明 - L2SGD+ (Full variance reduction)可以收敛到最优解，而L2SGD(without variance reduction)和 L2SGD2(with partial variancereduction) 只收敛到最优解邻域。 - 进行variancereduction是非常有必要的。它可以保证较快的全局收敛。 -数据异质性对算法收敛性没有影响。</p><h1 id="附录">附录</h1><h2 id="experimental-setup-and-further-experiments">Experimental Setupand further experiments</h2><ul><li>参数 <span class="math inline">\(p\)</span>如何影响算法L2SGD+的收敛速度</li><li>参数 <span class="math inline">\(\lambda\)</span>如何影响算法L2SGD+的收敛速度</li></ul><h2 id="其余算法">其余算法</h2><ul><li><p>Local GD with variance reduction</p><p>当每个节点采用梯度下降算法，且考虑variance reduction时，</p><p><img src="/2021/01/26/L2SGD/b1.png"> <img src="/2021/01/26/L2SGD/a3.png"></p></li><li><p>Efficient implementation of L2SGD+考虑到L2SGD+需要通信控制变量，增加了通信次数。作者给出了一个高效的版本，不需要通信控制变量，<span class="math inline">\(k\)</span>次迭代只需要通信 <span class="math inline">\(p(1-p)k\)</span>次。</p><p><img src="/2021/01/26/L2SGD/a4.png"></p></li><li><p>Local SGD with variance reduction – general method在这部分中，作者给出了一个使用性更广的版本。每个节点上目标函数可以包含一个非光滑正则项：</p><p><img src="/2021/01/26/L2SGD/b3.png"></p><p>另外，该版本算法允许从所有节点中任意抽样，允许节点结构任意（比如节点数据集大小，目标函数光滑程度，每个节点抽样方式任意）。</p><p><img src="/2021/01/26/L2SGD/a5.png"></p></li><li><p>Local stochastic algorithms</p><p>在这部分中，作者给出两个简单算法，不考虑variance reduction的LocalSGD(算法6)以及只考虑部分variance reduction的Local SGD (算法7)。</p><p><img src="/2021/01/26/L2SGD/a6.png"></p><p><img src="/2021/01/26/L2SGD/a7.png"></p></li></ul><h1 id="参考文献">参考文献</h1><ul><li>Hanzely, F. , &amp; Richtárik, Peter. (2020). Federated learning ofa mixture of global and local models.</li></ul>]]></content>
      
      
      <categories>
          
          <category> Distributed Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> FL </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>FLreview</title>
      <link href="/2021/01/21/FLreview/"/>
      <url>/2021/01/21/FLreview/</url>
      
        <content type="html"><![CDATA[<h1 id="aaai21-personalized-cross-silo-federated-learning-on-non-iid-data">[AAAI21]Personalized Cross-Silo Federated Learning on Non-IID Data</h1><p>该算法的目标函数为： <img src="https://cdn.jsdelivr.net/gh/sugar-xue/BlogImages@main/Picbed1613893223.png" alt="1613893223"></p><p>第二项<span class="math inline">\(A(\|\omega_{i} -\omega_{j}\|^{2})\)</span>的作用是使不同节点进行信息交流。该函数的定义如下：</p><figure><img src="https://cdn.jsdelivr.net/gh/sugar-xue/BlogImages@main/Picbed1614603445(1).png" alt="1614603445(1)"><figcaption aria-hidden="true">1614603445(1)</figcaption></figure><p>作者提出了一个求解上述目标函数的算法-FedAMP，具体如下：</p><figure><img src="https://cdn.jsdelivr.net/gh/sugar-xue/BlogImages@main/Picbed1613893256(1).png" alt="1613893256(1)"><figcaption aria-hidden="true">1613893256(1)</figcaption></figure><p>注意到，函数<span class="math inline">\(A(\cdot)\)</span>中的变量为<span class="math inline">\(\|\omega_{i} -\omega_{j}\|^{2}\)</span>，由于是子模型参数距离二范数的平方，在式(3)进行求导时，会出现<span class="math inline">\((\omega_{i} -\omega_{j})\)</span>项，进而式(3)可以表示为模型参数 <span class="math inline">\(\omega_{1}^{k-1},...,\omega_{m}^{k-1}\)</span>的线性组合：<img src="https://cdn.jsdelivr.net/gh/sugar-xue/BlogImages@main/Picbed1613893464(1).png" alt="1613893464(1)"></p><p>我们可以将 <span class="math inline">\(u_{i}\)</span> 看作是节点<span class="math inline">\(i\)</span>在云端子模型的参数，可以聚合各个节点的参数<span class="math inline">\(\omega_{1}^{k-1},...,\omega_{m}^{k-1}\)</span>信息。计算得到 <span class="math inline">\(u_{i}^{k}\)</span> 后，我们可以根据公式（4）在节点<span class="math inline">\(i\)</span> 上更新 <span class="math inline">\(\omega_{i}^{k}\)</span>:</p><figure><img src="https://cdn.jsdelivr.net/gh/sugar-xue/BlogImages@main/Picbed1613893769(1).png" alt="1613893769(1)"><figcaption aria-hidden="true">1613893769(1)</figcaption></figure><p>借助于<span class="math inline">\(u_{i}\)</span>聚合其他节点的参数，节点<span class="math inline">\(i\)</span>可以获取其他节点的信息。在云端优化完<span class="math inline">\(A(W)\)</span>后，对于每个节点，再利用式(6)优化损失函数<span class="math inline">\(F_{i}(w)\)</span>。</p><figure><img src="https://cdn.jsdelivr.net/gh/sugar-xue/BlogImages@main/Picbed1613893796(1).png" alt="1613893796(1)"><figcaption aria-hidden="true">1613893796(1)</figcaption></figure><figure><img src="https://cdn.jsdelivr.net/gh/sugar-xue/BlogImages@main/Picbed1613893828(1).png" alt="1613893828(1)"><figcaption aria-hidden="true">1613893828(1)</figcaption></figure><p>在聚合其他节点参数时，式(5)中不同节点参数的权重为 <span class="math display">\[\xi_{i,j} = \alpha_{k}A'(\|\omega_{i}^{k-1} -\omega_{j}^{k-1}\|^{2}), \quad (i \ne j)\]</span> 根据定义1，在<span class="math inline">\([0,\infty)\)</span>上，<span class="math inline">\(A\)</span>是一个increasing and concave函数，函数A的导数<span class="math inline">\(A'\)</span>在<span class="math inline">\((0, \infty)\)</span>上为non-negative andnon-increasing 函数，所以<span class="math inline">\(A'(\|\omega_{i}^{k-1} -\omega_{j}^{k-1}\|^{2})\)</span>相当于一个相似度函数，如果两个节点的参数<span class="math inline">\(w_{i}^{k-1}\)</span>和<span class="math inline">\(w_{j}^{k-1}\)</span>的欧氏距离小，那么这两个节点的相似度要高，对应到<span class="math inline">\(u_{i}^{k}\)</span>和<span class="math inline">\(u_{j}^{k}\)</span>中，它们的权重更高，因而<span class="math inline">\(u_{i}^{k}\)</span>和<span class="math inline">\(u_{j}^{k}\)</span>更接近，进一步，<span class="math inline">\(w_{i}^{k}\)</span>和<span class="math inline">\(w_{j}^{k}\)</span>更接近。</p><h1 id="aaai21-tornadoaggregate-accurate-and-scalable-federated-learning-via-the-ring-based-architecture">[AAAI21]TornadoAggregate: Accurate and Scalable Federated Learning via theRing-Based Architecture</h1><p>在这篇文章中，作者提出一种可以提高精度和稳定性的聚合方式，并且讨论了当前已有的各种聚合方式。</p><figure><img src="https://cdn.jsdelivr.net/gh/sugar-xue/BlogImages@main/Picbed1613894143.png" alt="1613894143"><figcaption aria-hidden="true">1613894143</figcaption></figure><p>作者指出STAR 这种全局聚合结构的稳定性差，相较而言，RING结构通过移除全局聚合，解决了STAR稳定性差的问题。但是，RING结构在FL中不切实际，假设共 <span class="math inline">\(|N|\)</span>个节点，RING需要进行的通信轮数是 STAR结构的 <span class="math inline">\(|N|\)</span> 倍数。除此之外，作者也总结讨论了其他已有聚合结构：STAR-stars, STAR-rings,RING-stars, RING-rings。</p><p>作者基于RING结构提出了两种新的聚合结构，通过减少RING结构带来的方差，提高了稳定性和精度。</p><h1 id="icml20-fedboost-communication-efficient-algorithms-for-federated-learning">[ICML20]FedBoost: Communication-Efficient Algorithms for Federated Learning</h1><p>作者借助集成的思想以减少FL中的通信成本。一些预先训练好的弱模型可以通过可获得的公共数据集训练。假设我们有<span class="math inline">\(q\)</span> 个已经训练好的弱模型 <span class="math inline">\(H =(h_{1},...,h_{q})\)</span>，本文的目标是学习组合权重 <span class="math inline">\(\alpha = \{ \alpha_{1}, ...,\alpha_{q}\}\)</span>，从而得到 <span class="math inline">\(\sum_{k=1}^{q} \alpha_{k}h_{k}\)</span>使得损失最小化。</p><figure><img src="https://cdn.jsdelivr.net/gh/sugar-xue/BlogImages@main/Picbed1613895221(1).png" alt="1613895221(1)"><figcaption aria-hidden="true">1613895221(1)</figcaption></figure><h1 id="icml20-fetchsgd-communication-efficient-federated-learning-with-sketching">[ICML20]FetchSGD: Communication-Efficient Federated Learning with Sketching</h1><p>作者提出一个新的算法，算法思想为：在每一轮，每个节点基于自己的局部信息计算得到一个梯度，然后在进行聚合前，作者使用一种叫做CountSketch的数据结构对梯度进行压缩。中心端保留momentum和error accumulationCount Sketches，每轮更新的权重参数根据error accumulationsketch得到。</p><h1 id="icml20-federated-learning-with-only-positive-labels">[ICML20]Federated Learning with Only Positive Labels</h1><h1 id="icml20-from-local-sgd-to-local-fixed-point-methods-for-federated-learning">[ICML20]From Local SGD to Local Fixed-Point Methods for Federated Learning</h1><h1 id="nips20-lower-bounds-and-optimal-algorithms-for-personalized-federated-learning">[NIPS20]Lower Bounds and Optimal Algorithms for Personalized Federatedlearning</h1><p>L2SGD # [NIPS20] Federated Bayesian Optimization # [NIPS20] FederatedMulti-Task Learning MOCHA # [NIPS20] FedSplit: An algorithmic frameworkfor fast federated optimization作者首先讨论了两种已有算法FedSGD和FedProx算法，作者证明这两种算法都不具有可行的收敛理论保证，因为它们得到的稳定点都不是它们预先要求解的目标函数的解。因此，作者提出FedSplit算法，该算法得到的稳定点是优化问题的最优解。</p><figure><img src="https://cdn.jsdelivr.net/gh/sugar-xue/BlogImages@main/Picbed1613897029.png" alt="1613897029"><figcaption aria-hidden="true">1613897029</figcaption></figure><h1 id="nips20-an-efficient-framework-for-clustered-federated-learning">[NIPS20]An Efficient Framework for Clustered Federated Learning</h1><p>作者提出一个迭代的聚类算法，论文假设所有的节点都能够被划分为若干类。由于每个节点所属类别未知，该算法可以交替估计每个节点所属的类别，并且通过梯度下降优化模型参数。论文中的算法可以解决数据分布的异质性问题。但是需要预先给定聚类个数<span class="math inline">\(k\)</span>。 <img src="https://cdn.jsdelivr.net/gh/sugar-xue/BlogImages@main/Picbed1613897504(1).png" alt="1613897504(1)"></p><h1 id="nips20-group-knowledge-transfer-federated-learning-of-large-cnns-at-the-edge">[NIPS20]Group Knowledge Transfer: Federated Learning of Large CNNs at theEdge</h1><p>作者提出一种新的交替最小化算法，该算法在每个节点上先训练较小的CNN网络，然后通过信息迁移训练一个较大的中心端CNN网络。<img src="https://cdn.jsdelivr.net/gh/sugar-xue/BlogImages@main/Picbed1613898317(1).jpg" alt="1613898317(1)"></p><p>上图展示了每个节点有一个特征提取器和分类器，可以在单个节点上进行模型训练。进行局部训练后，每个节点生成同样的张量，将其特征输出到中心端进行训练，然后借助于最小化预测标签和真实标签的KD损失函数训练参数。为了提升节点模型的表现，中心端会将其预测的标签发送给每个节点，然后每个节点可以基于其预测标签和中心端预测结果的损失函数训练子模型。</p><p><img src="https://cdn.jsdelivr.net/gh/sugar-xue/BlogImages@main/Picbed1613898899(1).png" alt="1613898899(1)"> # [NIPS20] Personalized Federated Learning withMoreau Envelopes为了解决异质性问题，作者考虑给每个节点的损失函数添加正则项： <span class="math display">\[f_{i}(\theta_{i}) + \frac{\lambda}{2}\|\theta_{i} - w\|^{2}，\]</span></p><p>优化问题表示为： <img src="https://cdn.jsdelivr.net/gh/sugar-xue/BlogImages@main/Picbed1613899301(1).jpg" alt="1613899301(1)"></p><h1 id="nips20-tackling-the-objective-inconsistency-problem-in-heterogeneous-federated-optimization">[NIPS20]Tackling the Objective Inconsistency Problem in Heterogeneous FederatedOptimization</h1><p>大多数论文在分析算法的收敛性时，往往会假设每个节点上进行局部更新的次数相同，它们的工作表明算法能够达到全局目标函数的稳定点。事实上，论文指出当不同节点局部更新次数不一致时，算法收敛到的稳定点不是原始目标函数的最优解，而是另一个目标函数。</p><p>解决这个问题的最简单想法就是固定每个节点的局部更新次数，在进行新的一轮迭代前，要等所有节点进行迭代完才能开始。这种方法能够保证目标函数的一致性，但是会带来训练成本。一些算法比如FedProx,VRLSGD以及SCAFFOLD用于处理non-IID问题，可以减少目标函数的不一致问题，但是要么有较慢的收敛速度，要么需要额外的通信成本和内存。</p><p>本文作者提出FedNova算法，可以保证目标函数的一致性问题。</p><h1 id="nips20-throughput-optimal-topology-design-for-cross-silo-federated-learning">[NIPS20]Throughput-Optimal Topology Design for Cross-Silo FederatedLearning</h1><h1 id="nips20-federated-principal-component-analysis">[NIPS20]Federated Principal Component Analysis</h1><h1 id="nips20-ensemble-distillation-for-robust-model-fusion-in-federated-learning">[NIPS20]Ensemble Distillation for Robust Model Fusion in Federated Learning</h1><h1 id="nips20-differentially-private-federated-linear-bandits">[NIPS20]Differentially-Private Federated Linear Bandits</h1><h1 id="nips20-inverting-gradients---how-easy-is-it-to-break-privacy-in-federated-learning">[NIPS20]Inverting Gradients - How easy is it to break privacy in federatedlearning?</h1><h1 id="nips20-distributionally-robust-federated-averaging">[NIPS20]Distributionally Robust Federated Averaging</h1><h1 id="iclr20-fair-resource-allocation-in-federated-learning">[ICLR20]FAIR RESOURCE ALLOCATION IN FEDERATED LEARNING</h1><p>作者提出 q-FFL算法，目的是解决FL中的公平问题：不同节点上的精度均匀。通过最小化一个加权的损失函数，具有较高损失的节点具有较高的权重。</p><figure><img src="https://cdn.jsdelivr.net/gh/sugar-xue/BlogImages@main/Picbed1613901146(1).png" alt="1613901146(1)"><figcaption aria-hidden="true">1613901146(1)</figcaption></figure><p>目标函数：</p><figure><img src="https://cdn.jsdelivr.net/gh/sugar-xue/BlogImages@main/Picbed1613901254(1).png" alt="1613901254(1)"><figcaption aria-hidden="true">1613901254(1)</figcaption></figure><p>具体算法略。 # [ICLR20] DIFFERENTIALLY PRIVATE META-LEARNING</p><h1 id="iclr20-dba-distributed-backdoor-attacks-against-federated-learning">[ICLR20]DBA: DISTRIBUTED BACKDOOR ATTACKS AGAINST FEDERATED LEARNING</h1><h1 id="iclr20-generative-models-for-effective-ml-on-private-decentralized-datasets">[ICLR20]GENERATIVE MODELS FOR EFFECTIVE ML ON PRIVATE, DECENTRALIZEDDATASETS</h1><h1 id="iclr20-attack-resistant-federated-learning-with-residual-based-reweighting">[ICLR20]ATTACK-RESISTANT FEDERATED LEARNING WITH RESIDUAL-BASED REWEIGHTING</h1>]]></content>
      
      
      <categories>
          
          <category> Distributed Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> FL </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>

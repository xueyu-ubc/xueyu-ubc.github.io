<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>Transformer, KV Cache, MHA, MQA, GQA, BERT, ViT, Swin Transformer</title>
      <link href="/2024/08/26/Transformer/"/>
      <url>/2024/08/26/Transformer/</url>
      
        <content type="html"><![CDATA[<h1 id="attention-is-all-you-need">Attention is all you need</h1><h2 id="comparison-with-rnn-and-cnn">Comparison with RNN and CNN</h2><p>RNN: 对于给定一个序列，从左向右进行计算。对于第<span class="math inline">\(t\)</span>个词，会对应一个隐藏状态向量<span class="math inline">\(h_t\)</span>。该隐藏状态向量<span class="math inline">\(h_t\)</span>是由前一个词的隐藏状态向量<span class="math inline">\(h_{t-1}\)</span>和当前位置<span class="math inline">\(t\)</span>的输入词决定的。因此，历史信息可以通过隐藏状态<span class="math inline">\(h_{t-1}\)</span>传送到当下。</p><ul><li><p>优点：可以处理时序信息。</p></li><li><p>缺点：（1）由于是序列计算，无法进行并行计算，计算性能较差。（2）如果时序很长，历史信息可以无法有效传输到后面。虽然可以设置较大的<span class="math inline">\(h_{t}\)</span>缓解该问题，但是存储<span class="math inline">\(h_t\)</span>会提升内存的需求。</p></li></ul><p>CNN:</p><ul><li><p>优点：具有多个输出通道(多个卷积核)，每个输出通道可以识别不同的模式。</p></li><li><p>缺点：对于较长的序列，卷积核只能观察到距离较近的像素点，否则需要进行多层卷积操作。</p></li></ul><h2 id="model-architecture">Model Architecture</h2><p>当前的时序模型主要是encoder-decoder的架构。对于一个序列表示<span class="math inline">\((x_1, ...,x_n)\)</span>，encoder将该序列映射为一个连续表征 <span class="math inline">\(\mathbf z = (\mathbf z_1,..., \mathbfz_n)\)</span>，其中 <span class="math inline">\(\mathbf z_i \inR^d\)</span>, <span class="math inline">\(d\)</span>为隐藏向量维度。对于encoder输出的<span class="math inline">\(\mathbfz\)</span>，decoder <strong>依次</strong> 生成输出序列 <span class="math inline">\((y_1, ..., y_m)\)</span>。</p><p>注意，对于encoder而言，可以看到整个输入句子。但是，对于decoder而言，无法观察到序列后面的词，因此词是按照自回归模式一个一个生成的，<strong>过去时刻的输出也作为当前时刻的输入</strong>。</p><p><img src="/2024/08/26/Transformer/architecture.jpg"></p><h3 id="encoder-and-decoder-stacks">Encoder and Decoder Stacks</h3><p>Encoder: 包含<span class="math inline">\(N=6\)</span>个layer，每个layer具有两个sub-layers，其中第一个子层是一个multi-headself-attention，第二个子层是一个position-wise fully connectedfeed-forward network(MLP)。对于每个子层，使用残差连接+layernorm，即Layernorm(<span class="math inline">\(x\)</span> + sublayer(<span class="math inline">\(x\)</span>))。每层的输出维度统一为<span class="math inline">\(d_{model}=512\)</span>。</p><p>Decoder: <span class="math inline">\(N=6\)</span>个layers。每个layer具有三个sub-layers，并且每个子层都使用残差连接+layernorm。对于第一个子层的self-attention，由于不能获取之后的输入，因此使用maskedMHA。</p><h3 id="attention">Attention</h3><p>Query: 需要查询的内容向量；</p><p>Key: 可以认为是用于被查询的关键信息向量；</p><p>Value: 通过将 query和key进行匹配对比，可以获得不同value的权重，然后基于该权重对value进行加权获得输出向量。</p><p>Scaled dot-product attention: <span class="math display">\[\begin{aligned}\text{Attention}(Q,K,V) = \text{softmax}(\frac{QK^T}{\sqrt{d_k}}V),\end{aligned}\]</span> 其中，query Q，key K 以及value V是等长的，都是 <span class="math inline">\(d_k\)</span>.</p><p>对于encoder，使用<strong>self-attention</strong>，query, key andvalue都是来自input embedding投影得到。</p><p>对于decoder, 使用<strong>masked self-attention</strong> 和<strong>cross-attention</strong>。对于cross-attention，key和value来自encoder的输出，query是来自decoder的下一刻的输入得到。通过计算query和key的相似度，对value进行加权得到输出。</p><h2 id="position-wise-feed-forward-networks"><strong>Position-wise</strong>feed-forward networks</h2><p>对于attention 外的sub-layers, 对于每一个position的输入使用<strong>同一个</strong>MLP进行映射： <span class="math display">\[\begin{aligned}\text{FFN}(x) = \text{max}(0, xW_1 + b_1)W_2 + b_2.\end{aligned}\]</span> 其中, <span class="math inline">\(x\)</span>是一个 512的向量，inner-layer has dimensionality <span class="math inline">\(d_{ff}=2048\)</span>，输出也是一个 512 向量。</p><h2 id="为什么需要除以sqrtd_k">为什么需要除以<span class="math inline">\(\sqrt{d_k}\)</span></h2><p><a href="https://mp.weixin.qq.com/s/h-24XRdJDDZDg65LTjXA0w">ref1</a></p><ol type="1"><li>当维度<span class="math inline">\(d_k\)</span>比较大时，点积的大小会增大，元素的相对距离增大，进行softmax操作时，会推动softmax函数往仅有很小的梯度的方向靠拢，导致softmax函数容易导致梯度消失问题。</li><li>假设Q和K的均值为0，方差为1。它们的矩阵乘积将有均值为0，方差为<span class="math inline">\(d_k\)</span>。因此，<span class="math inline">\(d_k\)</span>的平方根被用于缩放（而非其他数值），因为，Q和K的矩阵乘积的均值本应该为0，方差本应该为1，这样会获得一个更平缓的softmax。</li><li>也可以使用其他缩放方式，只要能做到每层参数的梯度保持在训练敏感的范围内，不要太大，不要太小。那么这个网络就比较好训练。</li></ol><h2 id="mask-self-attention">Mask self-attention</h2><p>为了不看到 <span class="math inline">\(t\)</span>时刻之后的内容，对于点积矩阵的上半部分添加一个较小的数字，比如<span class="math inline">\(-1e10\)</span>，这样经过softmax函数后对应位置会变成零。</p><h2 id="mha">MHA</h2><p><img src="/2024/08/26/Transformer/mha.jpg">对原始的Q,K，V，先通过一个linear layer映射到低维向量，然后进行scaleddot-productattention操作，得到h个输出向量，再将h个输出向量进行拼接，最后再通过一个linearlayer回到<span class="math inline">\(d_{model}\)</span>维度。</p><p>直接进行dot-product时，没有什么需要学习的参数。而使用MHA时，linearlayer的投影参数<span class="math inline">\(W^Q, W^K,W^V\)</span>是需要学习的，因此可以学习到不同的模式信息。</p><p>计算公式： <span class="math display">\[\begin{aligned}\text{MultiHead}(Q,K,V) &amp;= \text{concat}(\text{head}_1,\text{head}_2,...,\text{head}_h)W^O,\\\text{head}_i &amp;= \text{Attention}(QW_i^Q, KW^K_i, VW^V_i),\end{aligned}\]</span> 其中, <span class="math inline">\(W_i^Q \in\mathbb{R}^{d_{model} \times d_k}\)</span>, <span class="math inline">\(W_i^K \in \mathbb{R}^{d_{model} \timesd_k}\)</span>, <span class="math inline">\(W_i^V \in\mathbb{R}^{d_{model} \times d_v}\)</span>, <span class="math inline">\(W_i^O \in \mathbb{R}^{hd_v \timesd_{model}}\)</span>. In this paper, they set <span class="math inline">\(h=8\)</span>, <span class="math inline">\(d_k =d_v = d_{model}/h\)</span>.</p><h2 id="kv-cache-原理-mha-mqa-gqa">KV Cache 原理, MHA, MQA, GQA</h2><p><a href="https://mp.weixin.qq.com/s/mKdliGu4WhUx4PHatBpewA">ref1</a></p><p><a href="https://www.linsight.cn/3dc22f96.html">ref2</a></p><h2 id="positional-encoding">Positional Encoding</h2><p>对于rnn而言，当前时刻的输入包含了上一时刻的输出，依次传递序列信息。而attention是考虑所有词之间的关联性，权重与序列信息无关，并没有将序列/位置信息考虑进去。如果将句子的词打乱，语义可能有所不同，但是attention无法捕捉这种情况。在transformer中，通过将position进行encoding记录时序信息，然后和词的embedding相加作为输入。</p><p><img src="/2024/08/26/Transformer/positional.png"> <span class="math display">\[\begin{aligned}PE(pos, 2i) &amp;= sin(pos/10000^{2i/d_{model}}) \\PE(pos, 2i+1) &amp;= cos(pos/10000^{2i/d_{model}}) \\\end{aligned}\]</span></p><p>pos is the index of the word in the sentence. (0-30) <span class="math inline">\(2i\)</span> and <span class="math inline">\(2i+1\)</span> is the index of the column, d_modelis the number of columns, it is a hyper-parameter(120). For eachword(token), we encode it to a vector with dimension d_model accordingto its position.</p><p>Here we use denominator <span class="math inline">\(10000^{2i/d_{model}}\)</span> to make sure thepositional encoding is different for different tokens. The sin and cosare periodic functions, if we don't use the denominator, then thepositional encoding could be same for different tokens.</p><ul><li>If there are two different sentences with the same size, will thepositional encodings be the same?? yes.</li></ul><h2 id="complexity">Complexity</h2><p><img src="/2024/08/26/Transformer/complexity.jpg"></p><h1 id="bert-bidirectional-encoder-representations-from-transformers">BERT:Bidirectional Encoder Representations from Transformers</h1><ul><li>GPT: 单向，使用过去的信息预测未来。</li><li>ELMo: 基于rnn的架构，双向rnn,在用到一些下游任务时，需要对架构进行调整。</li><li>BERT: 相较于gpt, 可以使用左右侧信息，进行双向预测。相较于ELMo,基于transformer架构，结构简单，只需要修改最上层。</li></ul><p>Bert结合了ELMo的双向性和gpt的transformer架构，将预测未来变成<strong>完形填空</strong>。</p><h2 id="framework">Framework</h2><p>Bert主要包括两部分，pre-training andfine-tuning。在pre-training阶段，模型在一个没有进行标注的数据上进行训练。在fine-tuning阶段，用同一个Bert模型，但是模型首先被初始化为预训练的权重，然后再在有标注的数据上进行微调。每一个下游任务都会创建一个不同的模型进行微调。</p><p><img src="/2024/08/26/Transformer/bert.jpg"></p><p>Model architecture: a multi-layer bidirectional transformerencoder.</p><p>主要包括三个参数： 1. number of layers/ transformer blocks, i.e.,<span class="math inline">\(L\)</span>. 2. hidden dimension, i.e., <span class="math inline">\(H\)</span> (<span class="math inline">\(d_{model}\)</span>). 3. the number of attentionheads, i.e., <span class="math inline">\(A\)</span>.</p><p>两个模型： 1. Bert <span class="math inline">\(_{base}\)</span>:<span class="math inline">\(L=12, H=768, A=12\)</span>, total parametersis <span class="math inline">\(110M\)</span>. 2. Bert <span class="math inline">\(_{large}\)</span>: <span class="math inline">\(L=24, H=1024, A=16\)</span>, total parameters is<span class="math inline">\(340M\)</span>.</p><table style="width:58%;"><colgroup><col style="width: 58%"></colgroup><tbody><tr><td style="text-align: left;"><strong><em>如何根据超参数设置计算所需要训练的参数量？</em></strong></td></tr><tr><td style="text-align: left;">对于transformer架构，输入是字典（句子）的大小，这里假设为<span class="math inline">\(30k\)</span>。通过嵌入层得到输出，输出维度为<span class="math inline">\(H\)</span> (<span class="math inline">\(d_{model}\)</span>)。输出的embedding会喂给transformer blocks，transformer block中包括两部分，分别是self-attention和 mlp。 对于self-attention，dot-production是没有学习参数的，但是对于MHA，会对Q, K, V分别通过一个linearlayer映射到低维向量，然后进行scaled dot-product attention操作，得到<span class="math inline">\(A\)</span>个输出向量，再将<span class="math inline">\(A\)</span>个输出向量进行拼接，最后再通过一个linearlayer回到<span class="math inline">\(H\)</span> (<span class="math inline">\(d_{model}\)</span>)维度。在MHA中，头的个数乘以低维投影的维度=<span class="math inline">\(H\)</span> (<span class="math inline">\(d_{model}\)</span>)，因此低维投影部分的参数量为 $3 H H<span class="math inline">\(。这里乘以 3 的原因是Q, K,V分别通过一个linearlayer进行投影操作。同样，对于得到的低维投影向量进行拼接后还会进行一次投影，可学习参数量是\)</span>HH<span class="math inline">\(。因此，一个self-attention层的可学习参数量为\)</span>4 H H = 4 H^2$ （观察上文中的MHA结构图，可以发现有 4个linear模块）。接下来是 mlp,mlp具有两个全连接层，第一个全连接层的输入输出是<span class="math inline">\(H \times4H\)</span>，第二个全连接层的输入输出是<span class="math inline">\(4H\times H\)</span>，总共为 <span class="math inline">\(8H^2\)</span>。因此，一个transformerblock的可学习参数总共为 <span class="math inline">\(12H^2\)</span>。</td></tr><tr><td style="text-align: left;">假设模型有 <span class="math inline">\(L\)</span>个blocks，那么该模型的可学习参数总量为<span class="math inline">\(30k \times H + L \times H^2 \times12\)</span>。</td></tr><tr><td style="text-align: left;">对于Bert <span class="math inline">\(_{base}\)</span>，<span class="math inline">\(L=12, H=768, A=12\)</span>，根据公式计算得到：$30k+ 12 ^2 = 108,514,656 110M $。</td></tr></tbody></table><p>输入输出：</p><p>对于transformer而言，输入是一个序列对，编码器和解码器会分别输入一个序列。Bert只有一个编码器，输入是一个序列，可以包含两个句子。</p><p>To make BERT handle a variety of downstream tasks, our inputrepresentation is able to unambiguously represent both a single sentenceand a pair of sentences (e.g., ⟨ Question, Answer ⟩) in one tokensequence. Throughout this work, a “sentence” can be an arbitrary span ofcontiguous text, rather than an actual linguistic sentence. A “sequence”refers to the input token sequence to BERT, which may be a singlesentence or two sentences packed together.</p><h1 id="vit">ViT</h1><p>paper: AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGERECOGNITION AT SCALE</p><p><img src="/2024/08/26/Transformer/vit.jpg"></p><h1 id="swin-transformer">Swin Transformer</h1><h1 id="references">References:</h1><ul><li><p><a href="https://github.com/mli/paper-reading">李沐,Transformer论文精读</a></p></li><li><p><a href="http://nlp.seas.harvard.edu/2018/04/03/attention.html">TheAnnotated Transformer</a></p></li><li><p><a href="https://arxiv.org/pdf/1706.03762">Attention is all youneed</a></p></li><li><p><a href="https://jalammar.github.io/illustrated-transformer/" class="uri">https://jalammar.github.io/illustrated-transformer/</a></p></li><li><p><a href="https://arxiv.org/pdf/2010.11929">ViT</a></p></li><li><p><a href="https://arxiv.org/pdf/2103.14030">Swin Transformer:Hierarchical Vision Transformer using Shifted Windows</a></p></li></ul>]]></content>
      
      
      <categories>
          
          <category> Foundation Model </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Transformer </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Generative Models</title>
      <link href="/2023/11/29/Generative-Models/"/>
      <url>/2023/11/29/Generative-Models/</url>
      
        <content type="html"><![CDATA[<h1 id="generative-models">Generative Models</h1><h2 id="autoregressive-models-gan-flow-based-models-vae">AutoregressiveModels, GAN, Flow-based Models, VAE</h2><p>GAN: refer to <a href="https://lilianweng.github.io/lil-log/2017/08/20/from-GAN-to-WGAN.html">FromGAN to WGAN</a></p><p>VAE: refer to <a href="https://lilianweng.github.io/lil-log/2018/08/12/from-autoencoder-to-beta-vae.html">FromAutoencoder to Beta-VAE</a></p><p>Flow-based Models: refer to <a href="https://lilianweng.github.io/lil-log/2018/10/13/flow-based-deep-generative-models.html">Flow-basedDeep Generative Models</a></p><p>Autoregressive Models: refer to <a href="https://lrjconan.github.io/UBC-EECE571F-DL-Structures/">AutoregressiveModels</a></p><p>Reference: <a href="https://deepgenerativemodels.github.io/syllabus.html">CS236 - Fall2023 Deep Generative Models</a></p><p>Reference: <a href="https://lrjconan.github.io/UBC-EECE571F-DL-Structures/">EECE 571F(2023 Winter Term 1): Deep Learning with Structures</a></p><h2 id="energy-based-models-ebms">Energy-based Models (EBMs)</h2><h3 id="parameterizing-probability-distributions">Parameterizingprobability distributions</h3><p>In generating models, we want to learn a probability distribution<span class="math inline">\(p_{\theta}(x)\)</span>, which closelymatches the true data distribution <span class="math inline">\(p_{data}(x)\)</span>. The probability shouldsatisfy the following two conditions:</p><ul><li><p>non-negative: <span class="math inline">\(p_{\theta}(x) \geq0\)</span>.</p></li><li><p>sum to one: <span class="math inline">\(\int p_{\theta}(x)dx =1\)</span> or <span class="math inline">\(\sum_{x}p(x) =1\)</span>.</p></li></ul><p>It's not hard to choose a non-negative function, for example, givenany function <span class="math inline">\(f_{\theta}(x)\)</span>, we canchoose <span class="math inline">\(g_{\theta}(x) = f_{\theta}(x)^2,g_{\theta} = \exp(f_{\theta}(x)), g_{\theta}(x) =|f_{\theta}(x)|\)</span>, etc. However, <span class="math inline">\(g_{\theta}(x)\)</span> might not sum to one. Thesolution is to normalize <span class="math inline">\(g_{\theta}(x)\)</span> by dividing the sum of<span class="math inline">\(g_{\theta}(x)\)</span> over all possible<span class="math inline">\(x\)</span>. <span class="math display">\[p_{\theta}(x) = \frac{g_{\theta}(x)}{\sum_{x}g_{\theta}(x)} =\frac{g_{\theta}(x)}{\int g_{\theta}(x) \text{d}x} =\frac{g_{\theta}(x)}{Z(\theta)},\]</span> where <span class="math inline">\(Z(\theta)\)</span> is calledthe Partition function / Normalization constant.</p><p>Example:</p><ul><li><p>Gaussian: <span class="math inline">\(g_{(\mu, \sigma)}(x) =e^{-\frac{(x-\mu)^2}{2 \sigma^2}}\)</span>, volume is <span class="math inline">\(Z(\mu, \sigma) = \int e^{-\frac{(x-\mu)^2}{2\sigma^2}} \text{d}x = \sqrt{2 \pi \sigma^2}\)</span>.</p></li><li><p>Exponential: <span class="math inline">\(g_{\lambda}(x) =e^{-\lambda x}\)</span>, volume is <span class="math inline">\(Z(\lambda) = \int_0^{\infty} e^{-\lambda x}\text{d}x = 1/\lambda\)</span>.</p></li><li><p>Exponential family: <span class="math inline">\(g_{\theta}(x) =h(x) e^{\theta^T T(x)}\)</span>, volume is <span class="math inline">\(Z(\theta) = \int h(x) e^{\theta^T T(x)}\text{d}x\)</span>.</p></li><li><p>Beta, Poisson, Gamma, Dirichlet, etc.</p></li></ul><p>Generally, we can choose <span class="math inline">\(g_{\theta}(x)\)</span> so that <span class="math inline">\(Z(\theta)\)</span> is analytically. But how aboutusing the models that <span class="math inline">\(Z(\theta)\)</span> isnot easy to compute analytically?</p><h3 id="energy-based-models">Energy-based Models</h3><p>EBMs has the following form: <span class="math display">\[p_{\theta}(x) = \frac{1}{\int \exp(f_{\theta}(x))\text{d}x}e^{f_{\theta}(x)} = \frac{1}{Z(\theta)} e^{f_{\theta}(x)}.\]</span></p><p>Why do we choose <span class="math inline">\(f_{\theta}(x)\)</span>as the form of <span class="math inline">\(e^{f_{\theta}(x)}\)</span>?</p><ul><li>We want to capture large variations in probability. We usually touse log-probability.</li><li>Exponential families. Many distributions can be written in thisform.</li><li>Some physical meaning. <span class="math inline">\(-f_{\theta}(x)\)</span> is called the energy.</li></ul><p>Pros:</p><ul><li>We can use any function <span class="math inline">\(f_{\theta}(x)\)</span> to parameterize theprobability distribution.</li><li>Stable training.</li><li>Relatively high sample quality.</li></ul><p>Cons:</p><ul><li>Sampling from <span class="math inline">\(p_{\theta}(x)\)</span> ishard.</li><li>Evaluating and optimizing likelihood is <span class="math inline">\(p_{\theta}(x)\)</span> is hard.</li><li>Curse of dimensionality. Computing <span class="math inline">\(Z(\theta)\)</span> numerically scalesexponentially with the dimensionality of <span class="math inline">\(x\)</span>.</li></ul><h4 id="ebms-with-discrete-observable-variables-and-discrete-latent-variables-restricted-boltzmann-machinerbm">EBMswith Discrete Observable Variables and Discrete Latent Variables:Restricted Boltzmann machine(RBM)</h4><p>Suppose we have binary visible units <span class="math inline">\(x\)</span>, binary hidden units(latent variables)<span class="math inline">\(h\)</span>, the energy function is: <span class="math display">\[E(x, h) = - a^T x - b^T h - x^T W h\]</span> where <span class="math inline">\(a, b, W\)</span> areparameters.</p><p>The probability distribution is: <span class="math display">\[p(x, h) = \frac{1}{Z} e^{-E(x, h)} = \frac{1}{Z} e^{a^T x + b^T h + x^TW h}\]</span> where <span class="math inline">\(Z = \sum_{x, h} e^{-E(x,h)}\)</span>.</p><p><img src="/2023/11/29/Generative-Models/bipartite_model.jpg"> Whyrestricted? - Only one layer of hidden units. - No connections betweenhidden units.</p><p>Bipartite graph: conditional independence <span class="math display">\[\begin{aligned}p(x|h) &amp;= \prod_{i=1}^D p(x_i|h) \\p(h|x) &amp;= \prod_{j=1}^H p(h_j|x)\end{aligned}\]</span></p><p>Formally, we have <span class="math display">\[\begin{aligned}p(x|h = \tilde{h}) \propto \exp(- E_{\theta}(x, h = \tilde{h})) \propto\exp(-\tilde{a}^Tx) = \prod_{i} \exp(-\tilde{a}_i x_i)\end{aligned}\]</span></p><h4 id="inference-gibbs-sampling">Inference: Gibbs Sampling</h4><p>In inference, we want to compute the maximum a posterior(MAP) <span class="math inline">\(p(h|x)\)</span> and computing the marginals <span class="math inline">\(p(x)\)</span>.</p><ul><li>Due to the conditional independence, we can compute <span class="math inline">\(p(h|x)\)</span> in parallel.</li><li>But, the marginal <span class="math inline">\(p(x)\)</span> isintractable. We need to use Markov Chain Monte Carlo(MCMC) to samplefrom <span class="math inline">\(p(x)\)</span>.</li></ul><p>Gibbs sampling is a special case of MCMC. It can draw samples from<span class="math inline">\(p(x_1, x_2,...,x_n)\)</span> by iterativelysampling from the conditional distributions <span class="math inline">\(p(x_i|x_1, x_2,...,x_{i-1},x_{i+1},...,x_n)\)</span>.</p><p>In RBM, we do not iterative over individual variables. Instead, we doblock-Gibbs sampling, i.e., sampling a block of variables conditioned onthe other block.</p><blockquote><p>Given initial sample <span class="math inline">\((x^{(0)},h^{(0)})\)</span>,</p><p>for <span class="math inline">\(t = 1, 2, ..., T\)</span>:</p><p><span class="math display">\[\begin{aligned}h^{(t)} &amp;\sim p(h|x = x^{(t-1)}), \\x^{(t)} &amp;\sim p(x|h = h^{(t)}),\end{aligned}\]</span></p><p>Return <span class="math inline">\((x^{(T)}, h^{(T)})\)</span>.</p></blockquote><p>For both <span class="math inline">\(p(h|x)\)</span> and <span class="math inline">\(p(x|h)\)</span>, we can sampling in parallel.</p><p>Remark: <strong>the Gibbs sampler can generate random variables froma (marginal) distribution indirectly.</strong> After sampling manyiterations, <span class="math inline">\((x^{(T)}, h^{(T)})\)</span>follows the distribution <span class="math inline">\(p(x, h)\)</span>,<span class="math inline">\(x^{(T)}\)</span> follows the marginaldistribution <span class="math inline">\(p(x)\)</span>, and <span class="math inline">\(h^{(T)}\)</span> follows the marginal distribution<span class="math inline">\(p(h)\)</span>. For more detials:</p><p><a href="https://uh.edu/~cmurray/courses/econ_7395/Explaining%20the%20Gibbs%20Sampler.pdf">Explainingthe Gibbs sampler</a></p><p><a href="https://edisciplinas.usp.br/pluginfile.php/7733433/mod_resource/content/1/aula9slidesT.pdf#:~:text=%E2%80%9CThe%20Gibbs%20sampler%20is%20a,this%20scheme%20may%20seem%20mysterious.">MarkovChain Monte Carlo.Gibbs Sampler.</a></p><h4 id="learning-contrastive-divergence">Learning: ContrastiveDivergence</h4><p>In RBMs, we want to learn the parameters <span class="math inline">\(\theta\)</span> by maximizing the summedlog-likelihood of the training data <span class="math inline">\(\logp_{\theta}(x)\)</span>. The problem is that the partition function <span class="math inline">\(Z(\theta)\)</span> is intractable. Contrastivedivergence(CD) is a method to approximate the gradient of thelog-likelihood.</p><p>Since <span class="math display">\[\begin{aligned}    \frac{\partial p_{\theta}(x)}{\partial \theta} &amp;=\frac{1}{p_{\theta}(x)} \frac{\partial p_{\theta}(x)}{\partial \theta}\\    &amp;= \frac{1}{p_{\theta}(x)} \frac{\partial \int p_{\theta}(x,h)\text{d}h}{\partial \theta} \\    &amp;= \frac{1}{p_{\theta}(x)} \int \frac{\partial p_{\theta}(x,h)}{\partial \theta}\text{d}h \\    &amp;=  \frac{1}{p_{\theta}(x)} \int \frac{\frac{1}{Z}\exp(-E_{\theta}(x, h))}{\partial \theta} \text{d}h \\    &amp;= \frac{1}{p_{\theta}(x)} \int (-\frac{1}{Z^2}\exp(-E_{\theta}(x, h)) \frac{\partial Z}{\partial \theta} - \frac{1}{Z}\exp(-E_{\theta}(x, h)) \frac{\partial E_{\theta}(x, h)}{\partial\theta}) \text{d}h \\    &amp;= -\frac{1}{p_{\theta}(x)} \int \frac{1}{Z}\frac{\partial  Z}{\partial \theta} p_{\theta}(x, h) \text{d}h  -\frac{1}{p_{\theta}(x)}  \int  \frac{\partial E_{\theta}(x, h)}{\partial\theta} p_{\theta}(x, h) \text{d}h \\    &amp;= - \int \frac{1}{Z} \frac{\partial  Z}{\partial \theta}p_{\theta}(h | x) \text{d}h  - \int \frac{\partial E_{\theta}(x,h)}{\partial \theta} p_{\theta}(h | x) \text{d}h  \\    &amp;= - \frac{1}{Z} \frac{\partial  Z}{\partial \theta} - \int\frac{\partial E_{\theta}(x, h)}{\partial \theta} p_{\theta}(h | x)\text{d}h  \\    &amp;= - \frac{1}{Z} \frac{\partial  \int \int  \exp(-E_{\theta}(x,h)) \text{d}x \text{d}h}{\partial \theta} -\mathbb{E}_{p_{\theta}(h|x)}[\frac{\partial E_{\theta}(x, h)}{\partial\theta}] \\    &amp;=  -\int \int   (-\frac{\partial E_{\theta}(x, h)}{\partial\theta}) p_{\theta}(x,h) \text{d}x \text{d}h -\mathbb{E}_{p_{\theta}(h|x)}[\frac{\partial E_{\theta}(x, h)}{\partial\theta}] \\    &amp;= \mathbb{E}_{p_{\theta}(h|x)}[-\frac{\partial E_{\theta}(x,h)}{\partial \theta}] - \mathbb{E}_{p_{\theta}(x, h)}[-\frac{\partialE_{\theta}(x, h)}{\partial \theta}]  \\\end{aligned}\]</span> Here we don't know the distribution <span class="math inline">\(p_{\theta}(h|x)\)</span>. Maximizing the summedlog-likelihood of the training data <span class="math inline">\(\logp_{\theta}(x)\)</span> is equivalent to minimizing the KL divergencebetween the real data distribution <span class="math inline">\(p_{data}(x)\)</span> and the model distribution<span class="math inline">\(p_{\theta}(x)\)</span>: <span class="math display">\[\begin{aligned}    \min_{\theta} \text{KL}(p_{data}(x) || p_{\theta}(x)) =\min_{\theta} \int p_{data}(x) \log p_{data}(x) \text{d}x - \intp_{data}(x) \log p_{\theta}(x) \text{d}x.\end{aligned}\]</span> Since the entropy of <span class="math inline">\(p_{data}(x)\)</span> is: <span class="math display">\[\begin{aligned}    H(p_{data}(x)) &amp;= - \int p_{data}(x) \log p_{data}(x) \text{d}x\end{aligned}\]</span> The cross-entropy of <span class="math inline">\(p_{data}(x)\)</span> and <span class="math inline">\(p_{\theta}(x)\)</span> is: <span class="math display">\[\begin{aligned}    H(p_{data}(x), p_{\theta}(x)) &amp;= -\int p_{data}(x) \logp_{\theta}(x) \text{d}x \\\end{aligned}\]</span> And <span class="math inline">\(H(p_{data}(x), p_{\theta}(x))= H(p_{data}(x)) + \text{KL}(p_{data}(x) || p_{\theta}(x))\)</span>.</p><p>The entropy of <span class="math inline">\(p_{data}(x)\)</span> is aconstant, so minimizing the KL divergence is equivalent to minimizingthe cross-entropy of <span class="math inline">\(p_{data}(x)\)</span>and <span class="math inline">\(p_{\theta}(x)\)</span>, which isequivalent to maximizing： <span class="math display">\[\begin{aligned}    \max_{\theta} \int p_{data}(x) \log p_{\theta}(x) \text{d}x.\end{aligned}\]</span></p><p>We can use stochastic gradient ascent to maximize the above equation.The gradient is: <span class="math display">\[\begin{aligned}    \frac{\partial}{\partial \theta} \int p_{data}(x) \log p_{\theta}(x)\text{d}x &amp;= \int p_{data}(x) \frac{\partial}{\partial \theta} \logp_{\theta}(x) \text{d}x \\    &amp;= \mathbb{E}_{p_{\theta}(h|x)p_{data}(x)}[-\frac{\partialE_{\theta}(x, h)}{\partial \theta}] - \mathbb{E}_{p_{\theta}(x,h)}[-\frac{\partial E_{\theta}(x, h)}{\partial \theta}].\end{aligned}\]</span> We can use Monte Carlo to approximate the above equation.</p><ul><li>For the first expectation <span class="math inline">\(\mathbb{E}_{p_{\theta}(h|x)p_{data}(x)}[-\frac{\partialE_{\theta}(x, h)}{\partial \theta}]\)</span>, we can first sample <span class="math inline">\(x\)</span> from <span class="math inline">\(p_{data}(x)\)</span> (we don't know thedistribution of real data, but we have training data.), then sample<span class="math inline">\(h\)</span> from <span class="math inline">\(p_{\theta}(h|x)\)</span>.</li><li>For the second expectation <span class="math inline">\(\mathbb{E}_{p_{\theta}(x, h)}[-\frac{\partialE_{\theta}(x, h)}{\partial \theta}]\)</span>, we can use<strong>finite-step</strong> Gibbs sampler.</li></ul><p>In this way, we don't need to compute the partition function <span class="math inline">\(Z(\theta)\)</span>. This method is calledContrastive Divergence(CD).</p><h4 id="ebms-with-continuous-observable-variables-and-discrete-latent-variables-grbms">EBMswith Continuous Observable Variables and Discrete Latent Variables:GRBMs</h4><p>Here, we consider continuous observable variables <span class="math inline">\(v\)</span> and binary units (latent variables)<span class="math inline">\(h\)</span>. The energy function is: <span class="math display">\[E_{\theta}(v, h) = \frac{1}{2}(\frac{v-\mu}{\sigma})^T(\frac{v-\mu}{\sigma}) - (\frac{v}{\sigma^2}) Wh - b^T h.\]</span> The conditional independence still holds: <span class="math display">\[\begin{aligned}p(v|h) &amp;= \mathcal{N}(v | Wh + \mu, \text{diag}(\sigma^2)) \\p(h_j = 1|v) &amp;= [\text{Sigmoid}(W^T \frac{v}{\sigma^2} + b)]_j\end{aligned}\]</span></p><h3 id="modern-ebms">Modern EBMs</h3><h4 id="ebms-with-learnable-energy-functions">EBMs with Learnable EnergyFunctions</h4><p>For RBMs, we designed the energy function in advance, and it impliedconditional independence. But, in general, it's hard to design theenergy function in advance.Thus, we want to learn the energy function<span class="math inline">\(E_{\theta}(x)\)</span> from data.</p><p>One way is to use deep neural networks to parameterize the energyfunction <span class="math inline">\(E_{\theta}(x)\)</span>. Forexample, we can use U-Net architecture：</p><p><img src="/2023/11/29/Generative-Models/u-net.png"></p><p>The energy obtained by the energy function is a scalar and the outputof the U-Net is a tensor. Thus, we need to design some readout choicesto get the scalar energy. For example, <span class="math display">\[\begin{aligned}    E_{\theta}(x) &amp;= x^T f_{\theta}(x), \\    E_{\theta}(x) &amp;= (x - f_{\theta}(x))^2, \\    E_{\theta}(x) &amp;= f_{\theta}(x)^2 .\\\end{aligned}\]</span> Empirically, the first choice is better.</p><h4 id="inference-langevin-monte-carlo">Inference: Langevin MonteCarlo</h4><p>After learning the energy function <span class="math inline">\(E_{\theta}(x)\)</span>, how to sample from <span class="math display">\[p_{\theta}(x) =\frac{1}{Z}\exp(-E_{\theta}(x))\]</span></p><p>One way is to use Langevin Monte Carlo(LMC). The stochasticdifferential equation(SDE) of LMC is: <span class="math display">\[\begin{aligned}    \text{d}x = \nabla \log p_{\theta} (x) \text{d}t + \sqrt{2} \text{d}B_t\end{aligned}\]</span> where <span class="math inline">\(B_t\)</span> is a standardBrownian motion. The first term <span class="math inline">\(\nabla \logp_{\theta} (x) \text{d}t\)</span> is called the drift term, whichdominates the movement of the particle. The second term <span class="math inline">\(\sqrt{2} \text{d} B_t\)</span> is called thediffusion term, which includes the stochasticity of the process.</p><p>One can prove Langevin diffusion is irreducible, strong Feller, andaperiodic. Thus, <strong>the stationary distribution of the Langevindiffusion is <span class="math inline">\(p_{\theta}(x)\)</span>, and wecan use Langevin diffusion to sample from <span class="math inline">\(p_{\theta}(x)\)</span>.</strong></p><p>To turn the Langevin diffusion into a sampling algorithm, we need todiscretize the SDE. The simplest way is to use Euler-Maruyamadiscretization: <span class="math display">\[\begin{aligned}    \text{d}x &amp;= \nabla \log p_{\theta} (x) \text{d}t + \sqrt{2}\text{d} B_t \\    x_{t+\eta} &amp;= x_t + \nabla \log p_{\theta} (x_t) (t+\eta - t) +\sqrt{2} (B_{t+\eta} - B_t) \\    &amp;= x_t + \eta \nabla \log p_{\theta} (x_t) + \sqrt{2 \eta}\epsilon, \quad \epsilon \sim N(0, I)\end{aligned}\]</span> where <span class="math inline">\(\eta\)</span> is the stepsize.</p><ul><li>If we ignore the noise term, we are using gradient ascent tomaximize the density, this means we are trying to fing the 'mode' of<span class="math inline">\(x\)</span>. The 'mode' is the mean of thedistribution. In order to generate more samples, we add noise term.</li></ul><p>Sampling algorithm: - Given initial sample <span class="math inline">\(x^{(0)}\)</span> and step size <span class="math inline">\(\eta\)</span>. - for <span class="math inline">\(t= 1, 2, ..., T\)</span>: <span class="math inline">\(x^{(t)} = x^{(t-1)}+ \eta \nabla \log p_{\theta} (x^{(t-1)}) + \sqrt{2 \eta} \epsilon,\quad \epsilon \sim N(0, I)\)</span> - Return <span class="math inline">\(x^{(T)}\)</span>.</p><p>In EBMs, the score function <span class="math inline">\(\nabla \logp_{\theta} (x)\)</span> is the derivative of the energy function <span class="math inline">\(\log p_{\theta} (x)\)</span> with respect to <span class="math inline">\(x\)</span>, and <span class="math display">\[\nabla_x \log p_{\theta} (x) = \nabla_x (-E_{\theta}(x) - \log Z)=-\nabla E_{\theta}(x).\]</span> here, <span class="math inline">\(Z\)</span> doesn't depend on<span class="math inline">\(x\)</span>.</p><p>Noticed that there is another score function <span class="math inline">\(\nabla \log p_{\theta} (x)\)</span>, which is thederivative of the probability distribution <span class="math inline">\(\log p_{\theta} (x)\)</span> with respect to <span class="math inline">\(\theta\)</span>.</p><h4 id="learning-contrastive-divergence-1">Learning: ContrastiveDivergence</h4><p>Similar to RBMs, we can use contrastive divergence to update <span class="math inline">\(\theta\)</span>. The gradient is <span class="math display">\[\int p_{data} \frac{\partial \log p_{\theta}(x)}{\partial \theta}\text{d} x = \mathbb{E}_{p_{data}(x)}[- \frac{\partialE_{\theta}(x)}{\partial \theta}] -\mathbb{E}_{p_{\theta}(x)}[-\frac{\partial E_{\theta}(x)}{\partial\theta}].\]</span></p><p>For the second expectation, we can use Langevin Monte Carlo samplingto sample from <span class="math inline">\(p_{\theta}(x)\)</span>, andthen estimate the expectation.</p><h4 id="score-matching">Score Matching</h4><p>In contrastive divergence, <strong>at each trainingiteration</strong>, we use Langevin Monte Carlo to sample from <span class="math inline">\(p_{\theta}(x)\)</span>, and then estimate theexpectation. However, the Langevin Monte Carlo sampling is notefficient, expecially in high-dimensional space. Thus, we need to trainthe model without sampling.</p><p>Score matching is a method to train the model without sampling. Theidea is to minimize the difference between the score function <span class="math inline">\(\nabla \log p_{\theta} (x)\)</span> and the scorefunction of the data distribution <span class="math inline">\(\nabla\log p_{data} (x)\)</span>.</p><p>The (stein) score function is: <span class="math display">\[s_{\theta}(x) = \nabla \log p_{\theta} (x) = -\nabla E_{\theta}(x),\]</span> which is independent of the partition function <span class="math inline">\(Z(\theta)\)</span> and needs <strong>the pdf isdifferentiable</strong>.</p><p><strong>Fisher divergence</strong> between two distributions <span class="math inline">\(p(x)\)</span> and <span class="math inline">\(q(x)\)</span> is: <span class="math display">\[D_F(p(x), q(x)) = \frac{1}{2} \mathbb{E}_{x \sim p(x)}[||\nabla_x \logp(x) - \nabla_x \log q(x)||_2^2].\]</span> Score matching is to minimize the Fisher divergence between<span class="math inline">\(p_{\theta}(x)\)</span> and <span class="math inline">\(p_{data}(x)\)</span>: <span class="math display">\[\begin{aligned}    \min_{\theta} D_F(p_{\theta}(x), p_{data}(x)) &amp;= \min_{\theta}\frac{1}{2} \mathbb{E}_{x \sim p_{data}(x)}[||\nabla_x \log p_{data}(x)- s_{\theta}(x)||_2^2] \\    &amp;= \min_{\theta} \frac{1}{2} \mathbb{E}_{x \simp_{data}(x)}[||\nabla_x \log p_{data}(x) - (-\nabla_x E_{\theta}(x))||_2^2]\end{aligned}\]</span> Since we don't know the real data distribution, we need todeal with <span class="math inline">\(\nabla_x \logp_{data}(x)\)</span>. Assume that $ p_{data}(x)$ decays to 0sufficiently rapidly as <span class="math inline">\(x \rightarrow \pm\infty\)</span>, one can derive the following equation: <span class="math display">\[\begin{aligned}   &amp;\frac{1}{2} \mathbb{E}_{x \sim p_{data}(x)}[||\nabla_x \logp_{data}(x) - \nabla_x \log p_{\theta}(x)||_2^2] \\    =&amp;\mathbb{E}_{x \sim p_{data}(x)}[\frac{1}{2} ||\nabla_x \logp_{\theta}(x)||^2_2 + \text{tr}(\nabla^2_x \log p_{\theta}(x))] +\text{const},\end{aligned}\]</span> where <span class="math inline">\(tr(\nabla^2_x \logp_{\theta}(x))\)</span> is the trace of the Hessian matrix of <span class="math inline">\(\log p_{\theta}(x)\)</span>. Therefore, we can usemonte carlo to estimate the above loss: <span class="math display">\[\begin{aligned}   &amp;\mathbb{E}_{x \sim p_{data}(x)}[\frac{1}{2} ||\nabla_x \logp_{\theta}(x)||^2_2 + \text{tr}(\nabla^2_x \log p_{\theta}(x))], \\   =&amp; \frac{1}{n} \sum_{i=1}^n [\frac{1}{2} ||\nabla_x \logp_{\theta}(x_i)||^2_2 + \text{tr}(\nabla^2_x \log p_{\theta}(x_i))] \\   =&amp; \frac{1}{n} \sum_{i=1}^n [\frac{1}{2} ||\nablaE_{\theta}(x_i)||^2_2 + \text{tr}(\nabla^2_x \log p_{\theta}(x_i))] \\\end{aligned}\]</span> Then, we can use stochastic gradient descent to minimize theabove loss.</p><p>Note: computing the trace of the Hessian matrix <span class="math inline">\(\text{tr}(\nabla^2_x \log p_{\theta}(x))\)</span>is expensive.</p><p>Conclusions:</p><ul><li>we have used two distances for training EBMs:<ul><li>KL divergence, which is equal to maximum likelihood. (contrastivedivergence).</li><li>Fisher divergence, which is equal to score matching.</li></ul></li><li>Energy-based models are very felxible probabilistic models withintracable partition functions.</li><li>Sampling is hard and requires MCMC.</li><li>Computing the likelihood is hard.</li><li>Comparing the likelihood/probability of two different points istractable.</li><li>Contrastive divergence is a good approximation to maximumlikelihood. But, it needs sampling for each iteration.</li><li>Sampling free methods: score matching, noise contrastive estimation,adversarial optimization, etc.</li></ul><h2 id="score-based-models">Score-Based Models</h2><p>How to represent probability distribution function <span class="math inline">\(p(x)\)</span> in different models:</p><ul><li><p>GAN: min-max loss</p></li><li><p>Autoregressive models: <span class="math inline">\(p_{\theta}(x)= \prod_{i=1}^{d} p_{\theta}(x_i | x_{&lt;i})\)</span></p></li><li><p>Flow-based models: <span class="math inline">\(p_{\theta}(x) =p(z) |\det(J_{f_{\theta}}(x))|\)</span>, <span class="math inline">\(z =f_{\theta}(x)\)</span>.</p></li><li><p>VAE: use ELBO obj and latent variables</p></li><li><p>EBMs: <span class="math inline">\(p_{\theta}(x) =\frac{1}{Z}\exp(-E_{\theta}(x))\)</span></p></li></ul><p>Pros: except for GAN, these models are maximizing the likelihood.</p><p>Cons: They need special atchitectures or surrogate losses.</p><p>Remember that the score function is: <span class="math display">\[s_{\theta}(x) = \nabla \log p_{\theta} (x).\]</span> As shown in the following figure, score function is thegradient of the log probability function <span class="math inline">\(\log p_{\theta}(x)\)</span> and the direction ofthe score function is the vector to the mode of the distribution. Thismeans that the score function directly models the vector field ofgradients.</p><p><img src="/2023/11/29/Generative-Models/score.jpg"></p><p>Score matching is not limited to EBMs. We can use score matching totrain other models, such as autoregressive models, flow-based models,etc.</p><p><img src="/2023/11/29/Generative-Models/scorebased-models.jpg"></p><p>We want to train a score-based model <span class="math inline">\(s_{\theta}\)</span> to estimate the score <span class="math inline">\(\nabla_{x} \log p_{data}(x)\)</span>, we use theaverage Euclidean distance between the score function <span class="math inline">\(s_{\theta}(x)\)</span> and the score <span class="math inline">\(\nabla_{x} \log p_{data}(x)\)</span> over thewhole space as the loss function: <span class="math display">\[\begin{aligned}    \frac{1}{2} \mathbb{E}_{x \sim p_{data}(x)}[||s_{\theta}(x) -\nabla_{x} \log p_{data}(x)||_2^2],  (\text{Fisher divergence})\end{aligned}\]</span> which is equal to minimize: <span class="math display">\[\begin{aligned}    \mathbb{E}_{x \sim p_{data}(x)}[\frac{1}{2} ||s_{\theta}(x) ||^2_2 +\text{tr}(\nabla_x s_{\theta}(x))],  (\text{Score matching})\end{aligned}\]</span></p><p>We need to compute the value of the score function <span class="math inline">\(s_{\theta}(x)\)</span> and the trace of theJacobian matrix <span class="math inline">\(\text{tr}(\nabla_xs_{\theta}(x))\)</span>. Thus, the score model must be efficient toevaluate. Since the score models is not scalable, computing the trace ofthe Jacobian matrix <span class="math inline">\(\text{tr}(\nabla_xs_{\theta}(x))\)</span> in the backpropagation is order of <span class="math inline">\(O(d)\)</span>, where <span class="math inline">\(d\)</span> is the dimension of <span class="math inline">\(x\)</span>. We need to find an efficient way totrain the score model.</p><h3 id="denoising-score-matching">Denoising Score Matching</h3><p>Consider the perturbed distribution: <span class="math display">\[q_{\sigma}(\tilde{x}|x) = \mathcal{N}(\tilde{x}|x, \sigma^2 I), \qquadq_{\sigma}(\tilde{x}) = \int p(x)q_{\sigma}(\tilde{x}|x) \text{d}x.\]</span> Instead of estimating <span class="math inline">\(\nabla_x\log q_{\theta}(x)\)</span>, we can estimate <span class="math inline">\(\nabla_{\tilde{x}} \logq_{\sigma}(\tilde{x})\)</span>. It's easier to estimate and when thenoise level is small, <span class="math inline">\(q_{\sigma}(\tilde{x})\approx p(\tilde{x})\)</span>.</p><p>Therefore, we can use denoising score matching to match the score ofa noise-perturbed distribution: <span class="math display">\[\begin{aligned}    &amp;\frac{1}{2}E_{\tilde{x} \simq_{\sigma}}[\|\nabla_{\tilde{x}}\log q_{\sigma}(\tilde{x}) -s_{\theta}(\tilde{x})\|_2^2] \\    =&amp; \frac{1}{2} E_{x \sim p_{data}(x), \tilde{x} \simq_{\sigma}(\tilde{x}|x)}[\|s_{\theta}(\tilde{x})- \nabla_{\tilde{x}}\log q_{\sigma}(\tilde{x}|x)\|_2^2] + \text{const}.\end{aligned}\]</span> In this form, we don't need to compute the trace of theJacobian matrix <span class="math inline">\(\text{tr}(\nabla_xs_{\theta}(x))\)</span>. Since <span class="math inline">\(q_{\sigma}(\tilde{x}|x) = \mathcal{N}(\tilde{x}|x,\sigma^2 I)\)</span>, <span class="math inline">\(\nabla_{\tilde{x}}\log q_{\sigma}(\tilde{x}|x) = -\frac{\tilde{x} - x}{\sigma^2}\)</span>.It's more efficient to optimize for high dimensional data.</p><p>Con: notice that, we use score function to estimate thenoise-perturbed distribution, which means <strong>we cannot estimate thescore of the clean data</strong>.</p><h4 id="denoising">Denoising</h4><p>Denoising: after training a score model, we can use langevin MCsampling to get noise samples from <span class="math inline">\(q_{\sigma}(\tilde{x})\)</span>. According toTweedie's formula: <span class="math display">\[E_{x \sim p(x|\tilde{x})}[x] = \tilde{x} + \sigma^2 \nabla_x \logq_{\sigma}(\tilde{x}) \approx \tilde{x} + \sigma^2s_{\theta}(\tilde{x}).\]</span> [remember <span class="math inline">\(s_{\theta}(\tilde{x}) =\nabla_{\tilde{x}}\log q_{\sigma}(\tilde{x})\)</span>]</p><p>Langevin MCMC: from scores to samples:</p><ul><li>Given initial sample <span class="math inline">\(x^{(0)}\)</span>.</li><li>for <span class="math inline">\(t = 1, 2, ..., T\)</span>:</li><li><span class="math inline">\(\qquad z^{(t)} \sim \mathcal{N}(0,I)\)</span></li><li><span class="math inline">\(\qquad\tilde{x}^{(t)} =\tilde{x}^{(t-1)} + \frac{\epsilon}{2} \nabla s_{\theta}(\tilde{x}) +\sqrt{\epsilon} z^t\)</span></li><li>Return <span class="math inline">\(\tilde{x}^{(T)}\)</span>.</li></ul><p>If noise <span class="math inline">\(\epsilon \to 0\)</span> and<span class="math inline">\(T \to \infty\)</span>, <span class="math inline">\(\tilde{x}^{(T)} \sim p_{data}(x)\)</span>.</p><h4 id="multi-scale-noise-perturbation">Multi-scale NoisePerturbation</h4><p>When using the denoising score matching, we only use the observedsamples to estimate the scores. Thus, the estimated scores in lowdensity regions are not accurate. Moreover, langevin MCMC converges veryslowly.</p><p><img src="/2023/11/29/Generative-Models/low_density.png"></p><p>One way to improve the accuracy of the estimated scores in lowdensity regions is to increase the noise level <span class="math inline">\(\sigma\)</span>. As shown in the following figure,the estimated scores in low density regions are more accurate when thenoise level <span class="math inline">\(\sigma\)</span> is large.</p><p><img src="/2023/11/29/Generative-Models/add_noise.png"></p><ul><li>High noise provides useful directional information for Langevindynamics.</li><li>But perturbed density no longer approximates the true datadensity.</li></ul><p>Multi-scale noise perturbation: perturb data with different levels ofnoise simulteanously, and aggregate the information from all noiselevels.</p><p><img src="/2023/11/29/Generative-Models/multi_noise.png"></p><p>If the noise levle <span class="math inline">\(\sigma\)</span> islarge, the perturbed data quality is worse, but the estimated score ismore close to the perturbed scores. It's a trade-off between the dataquality and estimated score accuracy.</p><p>When the noise is small, the perturbed distribution is close to theoriginal data distribution, but the estimation errors in low densityregions are still high.</p><p>When we add larger and larger noise, the estimation score is close tothe perturbed data score, but the perturbed data score differs from theoriginal data score.</p><p>In order to achieve the best data quality and estimation accuracy atthe same time, we should consider all perturbations jointly instead offocusing on only one perturbation.</p><p><strong>Training procedure</strong>: Assume we have <span class="math inline">\(L\)</span> noise levels <span class="math inline">\(\sigma_1, \sigma_2, ..., \sigma_L\)</span> andcorresponding perturbed data distributions <span class="math inline">\(q_{\sigma_1}(\tilde{x}), q_{\sigma_2}(\tilde{x}),..., q_{\sigma_L}(\tilde{x})\)</span>. For each perturbed datadistribution, we can easily sample from them, and use score estimationto estimate the corresponding scores. However, this method requires alarge number of separate score models to be learned independently, whichis very costly, exspecially when the number of noise levels <span class="math inline">\(L\)</span> is large.</p><p>One way is to train a single conditional score network for all noiselevels. The score model will take <span class="math inline">\(\sigma\)</span> as an input. This model is namedthe <strong>Noise Conditional Score Network</strong>.</p><p><img src="/2023/11/29/Generative-Models/noise-score.png"></p><p>The loss function is a weighted combination of denoising scorematching loss with different noise levels: <span class="math display">\[\begin{aligned}    &amp;\frac{1}{L}\sum_{l=1}^L \lambda(\sigma_i) E_{\tilde{x} \simq_{\sigma_i}}[\|\nabla_{\tilde{x}}\log q_{\sigma_i}(\tilde{x}) -s_{\theta}(\tilde{x}, \sigma_i)\|_2^2]  \\    =&amp; \frac{1}{L}\sum_{l=1}^L \lambda(\sigma_i)    E_{x \sim p_{data}(x), z \sim N(0, I)}[\|s_{\theta}(x+\sigma_i z,\sigma_i) + \frac{z}{\sigma_i}\|_2^2] + \text{const}.\end{aligned}\]</span></p><p>[compute the loss in parallel?]</p><p>About the weighting function <span class="math inline">\(\lambda(\sigma_i)\)</span>, we can set <span class="math inline">\(\lambda(\sigma_i) = \sigma_i^2\)</span>.</p><p>About choosing the noise level <span class="math inline">\(\sigma_i\)</span>:</p><ul><li><p>The largest noise level <span class="math inline">\(\sigma_1\)</span> approximates the maximumpairwise distance between data points.</p></li><li><p>The smallest noise level <span class="math inline">\(\sigma_L\)</span> should be small enough so thatthe noise in final samples is negligible.</p></li><li><p>Adjacent noise scales should have sufficient overlap tofacilitate transitioning across noise scales in annealed Langevindynamics. One way is to use geometric sequence: <span class="math display">\[\frac{\sigma_1}{\sigma_2} = \frac{\sigma_2}{\sigma_3} = ... =\frac{\sigma_{L-1}}{\sigma_L},  \quad \sigma_1 &gt; \sigma_2 &gt; ...&gt; \sigma_L.\]</span></p></li></ul><p><strong>Sampling procedure</strong>: we use annealed Langevindynamics to sample from the noise conditional score network using scoresof different noise levels.</p><p>We first use Langevin dynamics to sample from the most perturbed datadistribution. Then, the resulting samples will be used as initialsamples for sampling from the next noise level. We continue in thisfashion and finally use Langevin dynamics to sample from the leastperturbed data distribution.</p><p><img src="/2023/11/29/Generative-Models/annealed.png"></p><p><strong><span class="math inline">\(s_{\theta}\)</span> is shared ateach iteration, since we only have one network.</strong></p><p>Conclusions:</p><ul><li>Gradients of distributions (scores) can be estimated easily</li><li>Flexible architecture choices — no need to benormalized/invertible</li><li>Stable training — no minimax optimization</li><li>Better or comparable sample quality to GANs</li><li>Exact likelihood computation</li></ul><h2 id="introduction-about-diffusion-model">Introduction about diffusionmodel</h2><p>Reference: <a href="https://lilianweng.github.io/posts/2021-07-11-diffusion-models/">Whatare Diffusion Models?</a></p><h3 id="ddpm-denoising-diffusion-probabilistic-models">DDPM: DenoisingDiffusion Probabilistic Models</h3><p>Diffusion model is a generative model:</p><p><img src="/2023/11/29/Generative-Models/image.png"></p><ul><li>diffusion process: add noise to a real image, finally we get a noiseimage.</li><li>reverse process: from noise image to generate real image.</li></ul><ol type="1"><li><p>training phase from a real image datasets ---&gt; throughdiffusion process ---&gt; noise images ---&gt; through reverse process---&gt; real images</p></li><li><p>inference phase</p></li></ol><p>sampling noise images from a gaussian distribution, then use thepre-trained reverse process to generate images.</p><h4 id="diffusion-process">Diffusion process</h4><p>add noise to a clean image <span class="math inline">\(X_0\)</span>and then we get noisy image <span class="math inline">\(X_1, X_2, ...,X_T\)</span>.</p><p>Now, we focus on the process from image <span class="math inline">\(X_{t-1}\)</span> to <span class="math inline">\(X_t\)</span>. <span class="math display">\[X_t = \sqrt {1 - \beta_t}X_{t-1} + \sqrt {\beta_t} Z_{t}, \quad Z_t \simN(0, I)\]</span> &gt;Remark: the noise scale <span class="math inline">\(\beta_t\)</span> will be increased gradually. $_t$ increases from <span class="math inline">\(10^{-4}\)</span> to <span class="math inline">\(2*10^{-2}\)</span> linearly. <span class="math inline">\(T = 2000\)</span>.</p><p>Let <span class="math inline">\(1 - \beta_t = \alpha_t\)</span>, thenwe have <span class="math display">\[X_t = \sqrt{\alpha_t}X_{t-1} + \sqrt{1 - \alpha_t} Z_t \\X_{t-1} = \sqrt{\alpha_{t-1}}X_{t-2} + \sqrt{1 - \alpha_{t-1}} Z_{t-1}\\\]</span> Combine these two equlities, we have <span class="math display">\[\begin{aligned}X_t &amp;= \sqrt{\alpha_t}X_{t-1} + \sqrt{1 - \alpha_t} Z_t \\&amp;= \sqrt{\alpha_t}(\sqrt{\alpha_{t-1}}X_{t-2} + \sqrt{1 -\alpha_{t-1}} Z_{t-1})+ \sqrt{1 - \alpha_t} Z_t \\&amp;= \sqrt{\alpha_t \alpha_{t-1}}X_{t-2} + \sqrt{\alpha_t(1 -\alpha_{t-1})} Z_{t-1}+ \sqrt{1 - \alpha_t} Z_t \\&amp; = \sqrt{\alpha_t \alpha_{t-1}}X_{t-2} + + \sqrt{1 - \alpha_t\alpha_{t-1}} Z,   \quad Z \sim N(0, I) \\&amp;= ... \\&amp;= \sqrt{\alpha_t \alpha_{t-1}...\alpha_1}X_{0} + + \sqrt{1 -\alpha_t \alpha_{t-1}... \alpha_{1}} Z \\&amp;= \sqrt{\bar{\alpha}_t}X_{0} + + \sqrt{1 - \bar{\alpha}_t}Z,   \qquad \bar{\alpha}_t = \prod_{i = 1}^{t}\alpha_i.\end{aligned}\]</span></p><h4 id="the-relation-between-ddpm-and-sde">The relation between DDPM andSDE</h4><p>DDPM: <span class="math inline">\(x_i = \sqrt{1 - \beta_i} x_{i-1} +\sqrt{\beta_i}z_{i-1}\)</span></p><p>SDE: <span class="math inline">\(\text{d}x = f(x, t)\text{d}t + g(x,t)\text{d}w\)</span>,</p><p>the solution is <span class="math display">\[x_t = x_0 + \int_0^t f(x, t)dt + \int_0^t g(x, t)dw\]</span> where <span class="math inline">\(w\)</span> is a Brownianmotion.</p><p>How to get the mean and convariance of <span class="math inline">\(x_t\)</span> ?</p><p>We can use FPK equation. See more on <a href="https://users.aalto.fi/~ssarkka/course_s2014/handout3.pdf">FPKequation</a>.</p><p>The problem is the mean and covariance of <span class="math inline">\(x_t\)</span> is dependent on the expectation of<span class="math inline">\(p(x(t))\)</span>, which we don't know.</p><h4 id="ddpm-variance-preserving-sde">DDPM / Variance preservingSDE</h4><p><span class="math display">\[x_i = \sqrt{1 - \beta_i} x_{i-1} +\sqrt{\beta_i}z_{i-1}\]</span></p><p>Let <span class="math inline">\(\beta(t=i/N) = N \beta_i, X(t = i/N)= x_i, Z(i/N) = z_i, \Delta t = 1/N\)</span>.</p><p>Then we have <span class="math display">\[\begin{aligned}    X(t + \Delta t) &amp;= \sqrt{1 - \beta(t+\Delta t)\Delta t}X(t) +\sqrt{\beta(t+\Delta t)\Delta t}Z(t) \\    &amp; \approx X(t)  - \frac{1}{2} \beta(t) \Delta t X(t) +\sqrt{\beta(t)\Delta t}Z(t) \\\end{aligned}\]</span></p><p><span class="math display">\[\text{d} x = - \frac{1}{2} \beta(t) x(t) dt + \sqrt{\beta(t)} \text{d}w,\]</span> where <span class="math inline">\(w\)</span> is a Brownianmotion.</p><p>The mean and covariance of <span class="math inline">\(x_t\)</span>is <span class="math display">\[\begin{aligned}    \mathbb{E}[x(t)] &amp;= \mathbb{E}[x(0)]e^{-\frac{1}{2}\int_0^t\beta(s)ds} \\    \text{cov}[x(t)] &amp;= I -  I \times e^{-\frac{1}{2}\int_0^t\beta(s)ds} \leq I.\end{aligned}\]</span></p><h4 id="score-based-model-variance-exploding-sde">SCORE-based model /Variance Exploding SDE</h4><p><span class="math display">\[x_i = x_{i-1} + \sqrt{\sigma_{i}^2 -\sigma^2_{i-1}}z_{i-1}\]</span></p><p>Let <span class="math inline">\(X(t = i/N) = x_i\)</span>, <span class="math inline">\(\sigma(t = i/N) = \sigma_i, Z(t = 1/N) = z_i,\Delta t = 1/N\)</span>.</p><p><span class="math display">\[\begin{aligned}    X(t + \Delta t) &amp;= X(t) + \sqrt{\sigma(t+\Delta t)^2 -\sigma(t)^2}Z(t) \\    &amp; \approx X(t) + \sqrt{\frac{\Delta \sigma(t)^2}{\Delta t}\Delta t}Z(t) \\\end{aligned}\]</span> where <span class="math inline">\(w\)</span> is a Brownianmotion. <span class="math display">\[d x = \sqrt{\frac{d \sigma(t)^2}{dt}} d w\]</span></p><p>The mean and covariance of <span class="math inline">\(x_t\)</span>is <span class="math display">\[\begin{aligned}    \mathbb{E}[x(t)] &amp;= \mathbb{E}[x(0)]  \\    \text{cov}[x(t)] &amp;= \sigma^2(t) I.\end{aligned}\]</span> Here, <span class="math inline">\(\sigma^2(t)\)</span> isnon-decreasing variance. Thus, the variance will be exploded.</p><h2 id="evaluating-generative-models">Evaluating Generative Models</h2><h3 id="model-families">Model families</h3><ul><li><p>Probability density/mass functions</p><ul><li>Autoregressive models: <span class="math inline">\(p_{\theta}(x) =\prod_{i=1}^{d} p_{\theta}(x_i | x_{&lt;i})\)</span>.</li><li>Normalizing flow models: <span class="math inline">\(p_{\theta}(x) =p(z) |\det(J_{f_{\theta}}(x))|\)</span>, <span class="math inline">\(z =f_{\theta}(x)\)</span>.</li><li>Latent variable models(VAEs): <span class="math inline">\(p_{\theta}(x) = \int p_{\theta}(x|z)p(z)\text{d}z\)</span>.</li><li>Energy-based models: <span class="math inline">\(p_{\theta}(x) =\frac{1}{Z}\exp(-E_{\theta}(x))\)</span>.</li></ul></li><li><p>Sample generation processes</p><ul><li>GANs: <span class="math inline">\(x = G_{\theta}(z)\)</span>, <span class="math inline">\(z \sim p(z)\)</span>.</li></ul></li><li><p>Score functions</p><ul><li>Score-based models: <span class="math inline">\(s_{\theta}(x) =\nabla_x \log p_{\theta} (x)\)</span>.</li></ul></li></ul><h3 id="distances-of-probability-distributions">Distances of probabilitydistributions</h3><ul><li><p>KL divergence(maximum likelihood): <span class="math inline">\(D_{KL}(p||q) = \int p(x) \log \frac{p(x)}{q(x)}\text{d}x\)</span></p><ul><li>Autoregressive models.</li><li>Normalizing flow models.</li><li>ElBO in VAEs.</li><li>Contrastive divergence in EBMs.</li></ul></li><li><p>f-divergences, Wasserstein distances</p><ul><li>GANs (f-GANs, WGANs)</li></ul></li><li><p>Fisher divergence(score matching): denoising score matching,sliced score matching</p><ul><li>Score-based models</li><li>Energy-based models</li></ul></li><li><p>Noise-contrastive estimation</p><ul><li>Energy-based models</li></ul></li></ul><h3 id="evaluation---density-estimation">Evaluation - Densityestimation</h3><p>We can use likelihood as a matric for density estimation:</p><ul><li>Split dataset into train, validation, test sets.</li><li>Learn model <span class="math inline">\(p_{\theta}(x)\)</span> usingthe train set.</li><li>Tune hyperparameters using the validation set.</li><li>Evaluate likelihood on the test set: <span class="math inline">\(\mathbb{p_{data}}[\logp_{\theta}(x)]\)</span>.</li></ul><p>However, the likelihood is intractable for many models. Not allmodels have tractable likelihoods. For example, GANs, VAEs, EBMs,etc.</p><p>For VAEs, we can compare the ELBO to log-likelihood. But, how aboutGANs and EBMs?</p><p>In general, unbiased estimation of probability density functions fromsamples is impossible. We can use approximation methods, such as kerneldensity estimation.</p><h4 id="kernel-density-estimation">Kernel density estimation:</h4><p>Given an intractable density model <span class="math inline">\(p_{\theta}(x)\)</span> and limited samples <span class="math inline">\(S = \{x_i\}_{i=1}^n\)</span>, we can use kerneldensity estimation to estimate the density function <span class="math inline">\(p_{\theta}(x)\)</span>. For a new data point <span class="math inline">\(x\)</span>, we can estimate the density of <span class="math inline">\(x\)</span> as: <span class="math display">\[\hat{p}_{\theta}(x) = \frac{1}{n} \sum_{i \in S} K(\frac{x -x_i}{\sigma}),\]</span> where <span class="math inline">\(K\)</span> is a kernelfunction, <span class="math inline">\(\sigma\)</span> is thebandwidth.</p><ul><li>Gaussian kernel: <span class="math inline">\(K(x) =\frac{1}{\sqrt{2\pi}}\exp(-\frac{1}{2}x^2)\)</span>.</li><li>A kernel is a function that satisfies the following properties:<ul><li><span class="math inline">\(K(x) \geq 0\)</span>.</li><li><span class="math inline">\(\int K(x) \text{d}x = 1\)</span>.</li><li><span class="math inline">\(K(x) = K(-x)\)</span>.</li></ul></li><li>Bandwidth <span class="math inline">\(\sigma\)</span> controls thesmoothness of the density estimate.<ul><li>Small <span class="math inline">\(\sigma\)</span>:undersmoothed</li><li>Large <span class="math inline">\(\sigma\)</span>: oversmoothed</li><li><span class="math inline">\(\sigma\)</span> is a hyperparameter. Wecan use cross-validation to choose <span class="math inline">\(\sigma\)</span>.</li></ul></li><li>KDE is very unreliable in higher dimensions.</li></ul><h3 id="importance-sampling-for-latent-variable-models">Importancesampling for latent variable models</h3><p>For likelihood <span class="math inline">\(p(x)\)</span>, we can uselikehood weighting to estimate the likelihood: <span class="math display">\[p(x)= \mathbb{E}_{p(z)}[p(x|z)].\]</span></p><p>Monte Carlo sampling is one way to estimate the expectation. However,if <span class="math inline">\(p(z)\)</span> is far from <span class="math inline">\(p(z|x)\)</span>, the variance of the likehoodweighting is very large. For example, the probability of <span class="math inline">\(p(z)\)</span> is very small, but the probabilityof <span class="math inline">\(p(z|x)\)</span> is very large at someregions. Thus, we need another distribution <span class="math inline">\(q(z)\)</span>, which is close to <span class="math inline">\(p(z|x)\)</span>, to estimate the expectation.</p><p>Importance sampling is another way to estimate the expectation. Theidea is to sample from a proposal distribution <span class="math inline">\(q(z)\)</span>, then <span class="math display">\[\begin{aligned}\mathbb{E}_{p(z)}[p(x|z)] = \int p(z) p(x|z) \text{d}z = \int q(z)\frac{p(z)}{q(z)} p(x|z) \text{d}z = \mathbb{E}_{q(z)}[\frac{p(z)}{q(z)}p(x|z)].\end{aligned}\]</span> Then, we can use Monte Carlo sampling to estimate theexpectation.</p><p>Pros:</p><ul><li>Still unbiased.</li><li>We can choose <span class="math inline">\(q(z)\)</span> to havelower variance. One can prove that <span class="math inline">\(q(z)\)</span> should be high where <span class="math inline">\(|p(z)p(x|z)|\)</span> is high.</li></ul><p>Cons:</p><ul><li>We need to choose a good proposal distribution <span class="math inline">\(q(z)\)</span>.</li><li>It's unreliable in high dimensions.</li></ul><p>When to use importance sampling?</p><ul><li><span class="math inline">\(p(z)\)</span> is difficult to samplefrom.</li><li>We can evaluate <span class="math inline">\(p(z)\)</span>.</li><li><span class="math inline">\(q(x)\)</span> is easy to evaluate andsample from.</li><li>We can choose <span class="math inline">\(q(z)\)</span> to be highwhere <span class="math inline">\(|p(z)p(x|z)|\)</span> is high.</li></ul><p>Annealed importance sampling is another method to estimate thelikelihood.</p><h3 id="sample-quality">Sample quality</h3><h4 id="inception-score-is">Inception score (IS)</h4><p>Inception score is a metric for evaluating the quality of generatedimages. The idea is to use a pretrained Inception network to classifythe generated images.</p><p>Assumption 1: we are evaluating sample quality for generative modelstrained on labeled datasets.</p><p>Assumption 2: We have a good probabilistic classifier <span class="math inline">\(c(y|x)\)</span> for predicting the label <span class="math inline">\(y\)</span> of any point <span class="math inline">\(x\)</span>.</p><p>(A classifier can be trained on a large dataset, such asImageNet.)</p><p>We want a good generative model to satisfy two properties:</p><ul><li><p>Sharpness: the generated images should be sharp.</p><p><img src="/2023/11/29/Generative-Models/sharpness.png"></p><p><span class="math display">\[S = \exp(E_{x \sim p}[\int c(y | x)\logc(y|x) \text{d}y])\]</span> High sharpness implies classifier isconfident in making predictions for generated images, and <span class="math inline">\(c(y|x)\)</span> has low entropy.</p></li><li><p>Diversity: the generated images should be diverse.</p><p><img src="/2023/11/29/Generative-Models/diversity.png"></p><p><span class="math display">\[D = \exp(E_{x \sim p}[\int c(y|x) \logc(y) \text{d} y]), \qquad c(y) = E_{x \sim p} [c(y|x)]\]</span></p><p>High diversity implies the generated images are diverse, and <span class="math inline">\(c(y)\)</span> has high entropy.</p></li></ul><p>Inception score combines these two properties: <span class="math display">\[IS = D \times S.\]</span></p><p>Higher IS implies better sample quality.</p><h4 id="frechet-inception-distance-fid">Frechet inception distance(FID)</h4><p>Inception score only considers the samples from <span class="math inline">\(p_{\theta}(x)\)</span>, but ignores the real datadistribution <span class="math inline">\(p_{data}(x)\)</span>.</p><p>FID is a metric for evaluating the quality of generated images. Theidea is to use a pretrained Inception network to extract features fromthe generated images and real images. Then, we can compute the Frechetdistance between the two feature distributions.</p><ul><li>Let <span class="math inline">\(\mathcal{G}\)</span> be thegenerated samples and <span class="math inline">\(\mathcal{T}\)</span>be the test dataset.</li><li>Compute feature representations <span class="math inline">\(F_{\mathcal{G}}\)</span> and <span class="math inline">\(F_{\mathcal{T}}\)</span>.</li><li>Fit a multivariate Gaussian to <span class="math inline">\(F_{\mathcal{G}}\)</span> and <span class="math inline">\(F_{\mathcal{T}}\)</span>. Let <span class="math inline">\(\mu_{\mathcal{G}}\)</span> and <span class="math inline">\(\mu_{\mathcal{T}}\)</span> be the mean vectors and<span class="math inline">\(\Sigma_{\mathcal{G}}\)</span> and <span class="math inline">\(\Sigma_{\mathcal{T}}\)</span> be the covariancematrices.</li><li>FID is defined as the Wasserstein-2 distance between the twoGaussians: <span class="math display">\[FID(\mathcal{G}, \mathcal{T}) = ||\mu_{\mathcal{G}} -\mu_{\mathcal{T}}||_2^2 + \text{tr}(\Sigma_{\mathcal{G}} +\Sigma_{\mathcal{T}} -2(\Sigma_{\mathcal{G}}\Sigma_{\mathcal{T}})^{1/2}).\]</span></li></ul><p>Lower FID implies better sample quality.</p><h4 id="kernel-inception-distance-kid">Kernel inception distance(KID)</h4><p>Maximum mean discrepancy (MMD) is a two-sample test statistic thatmeasures the distance between two distributions by computing differencesin their moments. Using the kernel trick, we can compute the MMD betweentwo distributions: <span class="math display">\[MMD(p, q) = E_{x, x' \sim p} [K(x, x')] + E_{y, y' \sim q}[K(y, y')] - 2E_{x \sim p, y \sim q} [K(x, y)].\]</span></p><p>Kernel inception distance (KID) is a metric for evaluating thequality of generated images. The idea is to use a pretrained Inceptionnetwork to extract features from the generated images and real images.Then, we can compute the MMD between the two feature distributions.</p><p>FID VS. KID:</p><ul><li>FID can only be positive, and it's biased, KID is unbiased.</li><li>The computation time of FID is <span class="math inline">\(O(n)\)</span>, but the computation time of KID is<span class="math inline">\(O(n^2)\)</span>.</li></ul><h3 id="evaluation---latent-representations">Evaluation - latentrepresentations</h3><p>What is a good latent representation? For downstream tasks, we canevaluate the quality of latent representations by evaluating theperformance of the downstream tasks, such as reconstruction,classification, etc.</p><p>For unsupervised learning, there is no one-size-fits-all metric forevaluating the quality of latent representations. We can use thefollowing metrics to evaluate the quality of latent representations:</p><h4 id="clustering">clustering</h4><p>Representations that can be grouped into clusters are potentiallyuseful. For example, the representations of a generated model for MNISTcan be grouped into different clusters, where each cluster correspondsto one or more digits.</p><p>For labelled datasets, there are many evaluation metrics. The lablesare only used for evaluation, not for clustering.</p><pre><code>from sklearn.metrics.cluster import completeness_score, homogeneity_score, v_measure_score</code></pre><h4 id="compression-or-reconstruction">compression orreconstruction</h4><p>Latent representations can be evaluated based on the maximumcompression they can achieve without significant loss in reconstructionquality.</p><p>Some metrics: Mean Squared Error (MSE), Peak signal-to-noise ratio(PSNR), Structural similarity (SSIM), etc.</p><h4 id="disentanglement">disentanglement</h4><p>We want representations that disentangle independent andinterpretable factors of variation in the observed data.</p><p>Some quantitative metrics:</p><ul><li>Beta-VAE metric: accuracy of a linear classifier that predicts afixed factor of variation.</li><li>Factor-VAE, Mutual Information Gap (MIG), SAP score, DCIdisentanglement, Modularity, etc.</li></ul><h2 id="reference">Reference</h2><ul><li><p><a href="https://users.aalto.fi/~asolin/sde-book/sde-book.pdf">SDEBOOK</a></p></li><li><p><a href="https://arxiv.org/abs/2006.11239">Denoising DiffusionProbabilistic Models. Jonathan et al.</a></p></li><li><p><a href="https://yang-song.net/blog/2021/score/">GenerativeModeling by Estimating Gradients of the Data Distribution</a></p></li><li><p><a href="https://lilianweng.github.io/posts/2021-07-11-diffusion-models/">Whatare Diffusion Models?</a></p></li><li><p><a href="https://deepgenerativemodels.github.io/syllabus.html">CS236 - Fall2023 Deep Generative Models</a></p></li><li><p><a href="https://lrjconan.github.io/UBC-EECE571F-DL-Structures/">EECE 571F(2023 Winter Term 1): Deep Learning with Structures</a></p></li><li><p><a href="https://arxiv.org/abs/2101.03288">How to Train YourEnergy-Based Models. Yang Song and Durk Kingma.</a></p></li><li><p><a href="https://glizen.com/radfordneal/ftp/ais-rev.pdf">Importancesampling</a></p></li></ul>]]></content>
      
      
      <categories>
          
          <category> Generative Model </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Diffusion </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>The Effect of Data Centering on PCA Models</title>
      <link href="/2023/11/07/The%20Effect%20of%20Data%20Centering%20on%20PCA%20Models/"/>
      <url>/2023/11/07/The%20Effect%20of%20Data%20Centering%20on%20PCA%20Models/</url>
      
        <content type="html"><![CDATA[<p>A common misconception in PCA is that PC 1 is the mean of the datawhen the data are not centered. It was shown in this paper that PC 1 isnot the mean of the data but it can point in the direction of the mean.The extent to which PC 1 points in the direction of the mean depends onhow far away the data set mean is from the origin. More details can befound in the paper [1].</p><h2 id="references">References</h2><p>[1] <a href="https://eigenvector.com/wp-content/uploads/2020/06/EffectofCenteringonPCA.pdf">TheEffect of Data Centering on PCA Models</a></p>]]></content>
      
      
      <categories>
          
          <category> Statistic </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Statistic </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Monte Carlo Gradient Estimation in Machine Learning</title>
      <link href="/2023/10/21/Monte-Carlo-Gradient-Estimation/"/>
      <url>/2023/10/21/Monte-Carlo-Gradient-Estimation/</url>
      
        <content type="html"><![CDATA[<h1 id="monte-carlo-methods-and-stochastic-optimisation">Monte CarloMethods and Stochastic Optimisation</h1><p>The mean-value analysis problem: <span class="math display">\[\mathcal{F}(\theta) := \int p(x; \theta) f(x; \phi) \text{d}x =\mathbb{E}_{p(x; \theta)} f(x; \phi),  \enspace (1)\]</span> where <span class="math inline">\(f(x; \phi)\)</span> is thecost with parameters <span class="math inline">\(\phi\)</span>, <span class="math inline">\(p(x;\theta)\)</span> is the measure, a probabilitydistribution that is continuous in its domain and differentiable with<span class="math inline">\(\theta\)</span>.</p><p>This objective is very common in variational inference, reinforcementlearning, etc.</p><p>To learn the distribution parameter <span class="math inline">\(\theta\)</span>, we need to consider the gradient:<span class="math display">\[\eta := \nabla_{\theta} \mathcal{F}(\theta) = \nabla_{\theta}\mathbb{E}_{p(x; \theta)} f(x; \phi). \enspace  (2)\]</span> <span class="math inline">\(\eta\)</span> is called thesensitivity analysis of <span class="math inline">\(\mathcal{F}\)</span>.</p><p>Problems:</p><ul><li>not able to evaluate the expectation in closed form;</li><li><span class="math inline">\(x\)</span>: high-dimensional, <span class="math inline">\(\theta\)</span>: high-dimensional;</li><li>the cost funtion may not be differential or a black-boxfunction.</li></ul><h2 id="monte-carlo-estimators">Monte Carlo Estimators</h2><p>We can numerically evaluate the integral by first drawing<strong>independent</strong> samples <span class="math inline">\(\hat{x}^{(1)}, ..., \hat{x}^{(N)}\)</span> fromthe distribution <span class="math inline">\(p(x; \theta)\)</span>, andthen computing the averaged of the function evaluated at these samples:<span class="math display">\[\bar{\mathcal{F}}_N = \frac{1}{N} \sum_{n=1}^{N} f(\hat{x}^{(n)}),\qquad \hat{x}^{(n)} \sim p(x; \theta), \quad n = 1,..., N.\]</span> <span class="math inline">\(\bar{\mathcal{F}}_N\)</span> is arandom variable and is called Monte Carlo estimator of eq. (1).</p><p>Remark 1. As long as we can write an integral in the form of eq. (1)(a product of a function and a distribution that we can easily samplefrom), we can apply the Monte Carlo method.</p><h3 id="four-properties">Four Properties</h3><ul><li><p>Consistency. As <span class="math inline">\(N \to\infty\)</span>, the estimator <span class="math inline">\(\bar{\mathcal{F}}_N \to \mathbb{E}_{p(x; \theta)}f(x; \phi)\)</span>. This can be easily satisfied according to thestrong law of large number.</p></li><li><p>Unbiasedness. <span class="math display">\[\mathbb{E}_{p(x; \theta)} [\bar{\mathcal{F}}_N] = \mathbb{E}_{p(x;\theta)} [\frac{1}{N} \sum_{n=1}^{N} f(\hat{x}^{(n)})] = \frac{1}{N}\sum_{n=1}^{N} \mathbb{E}_{p(x; \theta)} [f(\hat{x}^{(n)})] =\mathbb{E}_{p(x; \theta)}[f(x)].\]</span> (if we repeat the estimation process many times, the estimateis centered on the the actual value of the integral on average)</p></li><li><p>Minimum variance. If we consider two <strong>unbiased</strong>estimators using the same number of sampling <span class="math inline">\(N\)</span>, we will prefer the estimator that haslower variance. [<strong>if MC estimator has the minimumvariance?</strong>]</p></li><li><p>Computational efficiency. for example: a computational costlinear in the number of parameters, can be computed inparallel.</p></li></ul><h3 id="the-central-role-of-gradient-estimation-eq.-2">The central roleof Gradient Estimation eq. (2)</h3><p>some examples in different areas.</p><h4 id="variational-inference">Variational Inference</h4><p>Variational inference is a general method for approximating complexand unknown distributions by the closest distribution within a tractablefamily. Consider a generic probabilistic model <span class="math inline">\(p(x|z)p(z)\)</span> that defines a generativeprocess in which observed data <span class="math inline">\(x\)</span> isgenerated from a set of unobserved variables z using a data distribution<span class="math inline">\(p(x|z)\)</span> and a prior distribution<span class="math inline">\(p(z)\)</span>. The posterior distribution ofthis generative process <span class="math inline">\(p(z|x)\)</span> isunknown, and is approximated by a variational distribution <span class="math inline">\(q(z|x; \theta)\)</span> with variationalparameters <span class="math inline">\(\theta\)</span>. The objective is<span class="math display">\[\max_{\theta, \phi} \mathbb{E}_{q(z|x;\theta)} [\log p(x|z;\phi) - \log\frac{q(z|x;\theta)}{p(z)}].\]</span> Optimising the distribution <span class="math inline">\(q\)</span> requires the gradient of the objectivewith respect to the variational parameters <span class="math inline">\(\theta\)</span>: <span class="math display">\[\eta = \nabla_{\theta}\mathbb{E}_{q(z|x;\theta)} [\log p(x|z;\phi) -\log \frac{q(z|x;\theta)}{p(z)}].\]</span></p><h4 id="reinforcement-learning">Reinforcement Learning</h4><p>Model-free policy search is an area of reinforcement learning wherewe learn a policy|a distribution over actions|that on average maximisesthe accumulation of long-term rewards. Through interaction in anenvironment, we can generate trajectories <span class="math inline">\(\tau = (s_1, a_1, s_2, a_2, ... , s_T , a_T)\)</span> that consist of pairs of states st and actions at for timeperiod <span class="math inline">\(t = 1, ... , T\)</span>. A policy islearnt by following the policy gradient:</p><p><span class="math display">\[\eta = \nabla_{\theta} \mathbb{E}_{p(\tau;\theta)} [\sum_{t=0}^{T}\gamma^t r(s_t,a_t)]\]</span> The cost is the return over the trajectory, which is aweighted sum of rewards obtained at each time step <span class="math inline">\(r(s_t, a_t)\)</span>, with the discount facthor<span class="math inline">\(\gamma \in [0, 1]\)</span>. The measure isthe joint distribution over states and actions <span class="math inline">\(p(\tau;\theta) = \prod_{t=0}^{T-1} [p(s_{t+1}|s_t,a_t)p(a_t|s_t; \theta)]p(a_T |s_T ; \theta)\)</span>.</p><h2 id="intuitive-analysis-of-gradient-estimators">Intuitive Analysis ofGradient Estimators</h2><p>The gradients <span class="math inline">\(\nabla_{\theta}\mathbb{E}_{p(x;\theta)}[f(x)]\)</span> can be computed in two ways:</p><ul><li><p>Derivatives of Measure. The gradient can be computed bydifferentiation of the measure <span class="math inline">\(p(x;\theta)\)</span>. Gradient estimators in this class include the scorefunction estimator and the measure-valued gradient.</p></li><li><p>Derivatives of Paths. The gradient can be computed bydifferentiation of the cost <span class="math inline">\(f(x)\)</span>,which encodes the pathway from parameters <span class="math inline">\(\theta\)</span>, through the random variable <span class="math inline">\(x\)</span>, to the cost value, such as thepathwise gradient, harmonic gradient estimators and finite dfferences,and Malliavin-weighted estimators.</p></li></ul><p>We focus on three classes of gradient estimators: the score function,pathwise and measure-valued gradient estimators. <strong>All threeestimators satisfy two desirable properties: consistent and unbiased;but they differ in their variance behaviour and in their computationalcost.</strong></p><h3 id="intuitive-comparision">Intuitive comparision</h3><p>Consider the stochastic gradient problem (2) that uses Gaussianmeasures for three simple families of cost functions, quadratics,exponentials and cosines: <span class="math display">\[\eta = \nabla_{\theta} \int \mathcal{N}(x | \mu, \sigma^2) f(x; k)\text{d}x; \quad \theta \in \{\mu, \sigma \}; \quad f \in \{ (x-k)^2,\exp(-kx^2), \cos(kx)\}.\]</span></p><p><img src="/2023/10/21/Monte-Carlo-Gradient-Estimation/figure2.png"></p><p><img src="/2023/10/21/Monte-Carlo-Gradient-Estimation/figure3.png"></p><p>The computational cost:</p><ul><li><p>Both of the score-function and pathwise estimators can becomputed using a single sample in the Monte Carlo estimator (N = 1),even for multivariate distributions, making them computationallycheap.</p></li><li><p>The measure-valued derivative estimator will require 2Devaluations of the cost function for D dimensional parameters, and forthis the reason will typically not be preferred in high-dimensionalsettings.</p></li><li><p>If the cost function is not differentiable, then the pathwisegradient will not be applicable.</p></li></ul><p>Several criteria to be judged when choosing an unbiased gradientestimator:</p><ul><li>computational cost;</li><li>implications on the use of differentiable and non-differentiablecost functions;</li><li>the change in behaviour as the cost itself changes;</li><li>the availability of effective variance reduction techniques toachieve low variance.</li></ul><h1 id="score-function-gradient-estimators-likelihood-ratio-method-reinforce-estimator">ScoreFunction Gradient Estimators (likelihood ratio method, REINFORCEestimator)</h1><h2 id="score-function">Score function</h2><p>The score function is the derivative of the log-probability of thedistribution <span class="math inline">\(\nabla_{\theta} \log p(x;\theta)\)</span> with respect to its parameters <span class="math inline">\(\theta\)</span>: <span class="math display">\[\nabla_{\theta} \log p(x; \theta) = \frac{\nabla_{\theta} p(x;\theta)}{p(x; \theta)}.\]</span></p><p>properties: 1, Its expectation is zero: <span class="math display">\[\mathbb{E}_{p(x; \theta)} [\nabla_{\theta} \logp(x; \theta)] = \int p(x; \theta) \frac{\nabla_{\theta} p(x;\theta)}{p(x; \theta)} \text{d}x =  \nabla_{\theta} \int p(x; \theta)\text{d}x = \nabla_{\theta} 1 = 0.\]</span></p><p>2, Its variance is the Fisher information matrix.</p><p>Using the score function, we can derive a general-purpose estimatorfor the sensitivity analysis of eq. (2): <span class="math display">\[\begin{aligned}\eta &amp;= \nabla_{\theta} \mathbb{E}_{p(x; \theta)} [f(x)] \\&amp;= \nabla_{\theta} \int p(x; \theta) f(x) \text{d}x \\&amp;= \int f(x) \nabla_{\theta} p(x; \theta) \text{d}x \\&amp;= \int p(x; \theta) f(x) \nabla_{\theta} \log p(x; \theta)\text{d}x \\&amp;= \mathbb{E}_{p(x; \theta)} [f(x) \nabla_{\theta} \log p(x;\theta)].\end{aligned}\]</span> The form is what we need - a product of a distribution we cansample from and a function we can evaluate. Then, use the Monte Carloestimator, we have <span class="math display">\[\bar{\eta} = \frac{1}{N} \sum_{n=1}^{N} f(\hat{x}^{(n)}) \nabla_{\theta}\log p(\hat{x}^{(n)}; \theta), \quad \hat{x}^{(n)} \sim p(x; \theta).\]</span></p><p><strong>Notice that, in the third line, we have exchanged the orderof the integral and the derivative. We will discuss the validity of thisexchange later.</strong></p><h2 id="estimator-properties">Estimator Properties</h2><h3 id="unbiasedness">Unbiasedness</h3><p>When the interchange between differentiation and integration isvalid, we will obtain an <strong>unbiased</strong> estimator of thegradient. Intuitively, since differentiation is a process of limits, thevalidity of the interchange will relate to the conditions for which itis possible to exchange limits and integrals, in such cases most oftenrelying on the use of the <strong>dominated convergence theorem or theLeibniz integral rule</strong> (Flanders, 1973; Grimmett and Stirzaker,2001). The interchange will be valid if the following conditions aresatisfied:</p><ol type="a"><li><p>The measure <span class="math inline">\(p(x; \theta)\)</span> iscontinuously differentiable with respect to <span class="math inline">\(\theta\)</span>;</p></li><li><p>The product <span class="math inline">\(f(x) p(x;\theta)\)</span> is both integrable and differentiable for <span class="math inline">\(\theta\)</span>;</p></li><li><p>There exists an integrable function <span class="math inline">\(g(x)\)</span> such that <span class="math inline">\(\sup_{\theta} \|f(x) \nabla_{\theta} p(x; \theta)\|_1 \leq g(x)\)</span> for <span class="math inline">\(\forallx\)</span>.</p></li></ol><p>These assumptions usually hold in machine learning applications.</p><h3 id="abosolute-continuity">Abosolute Continuity</h3><ul><li><p>Example (Bounded support). Consider the score-function estimatorfor a cost <span class="math inline">\(f(x) = x\)</span> anddistribution <span class="math inline">\(p(x; \theta) = \frac{1}{\theta}1_{0 &lt; x &lt; \theta}\)</span>, which is differential in <span class="math inline">\(\theta\)</span> when <span class="math inline">\(x\in (0, \theta)\)</span>; the score function is <span class="math inline">\(\nabla_{\theta} \log p(x; \theta) =-\frac{1}{\theta}\)</span>. The true gradient is: <span class="math inline">\(\nabla_{\theta} \mathbb{E}_{p(x;\theta)} [x] =\nabla_{\theta} (\frac{1}{\theta} \int_{0}^{\theta} \frac{x^2}{2}) =\frac{1}{2}\)</span>. The score-funtion gradient is: <span class="math inline">\(\mathbb{E}_{p(x;\theta)} [x \frac{-1}{\theta}] =-\frac{\theta/2}{\theta} = -\frac{1}{2}\)</span>.</p><p>Why the score-function estimator fails to provide the correctgradient? The reason is that the measure is not absolutely continuouswith respect to <span class="math inline">\(\theta\)</span> at theboundary of the support.</p></li></ul><p>Let's explain the absolute continuity in detail. <span class="math display">\[\begin{aligned}  \nabla_{\theta} \mathbb{E}_{p(x;\theta)}[f(x)] &amp;= \int\nabla_{\theta} p(x;\theta) f(x) \text{d}x \\&amp;= \int \lim_{h \to 0} \frac{p(x; \theta + h) - p(x;\theta)}{h} f(x)\text{d}x \\&amp;= \lim_{h \to 0} \frac{1}{h} \int p(x; \theta) \frac{p(x; \theta +h) - p(x;\theta)}{p(x;\theta)} f(x) \text{d}x\\&amp;= \lim_{h \to 0} \frac{1}{h} \int p(x; \theta) (\frac{p(x; \theta +h)}{p(x;\theta)}-1) f(x)  \text{d}x\\&amp;= \lim_{h \to 0} \frac{1}{h}(\mathbb{E}_{p(x;\theta)}[\omega(\theta, h) f(x)] -\mathbb{E}_{p(x;\theta)}[f(x)])   \end{aligned}\]</span> where the ratio <span class="math inline">\(\omega(\theta, h):= \frac{p(x; \theta+h)}{p(x;\theta)}\)</span>. The estimator makes animplicit assumption of absolute continuity, where <strong>we require<span class="math inline">\(p(x; \theta+h) &gt; 0\)</span> for allpoints where <span class="math inline">\(p(x; \theta) &gt;0\)</span>.</strong> Not all distributions of interest satisfy thisproperty, and failures of absolute continuity can result in a biasedgradient.</p><h3 id="estimator-variance">Estimator Variance</h3><p>Define the estimator mean as <span class="math inline">\(\mu(\theta):= \mathbb{E}_{p(x;\theta)}[\bar{\eta}_N]\)</span>, for <span class="math inline">\(N=1\)</span>, The variance of the score functionestimator is: <span class="math display">\[\begin{aligned}\text{Var}_{p(x;\theta)}(\bar{\eta}_1) &amp;=\mathbb{E}_{p(x;\theta)}[(f(x) \nabla_{\theta} \log p(x; \theta))^2] -\mu(\theta)^2,\end{aligned}\]</span> or <span class="math display">\[\begin{aligned}\text{Var}_{p(x;\theta)}(\bar{\eta}_1) &amp;= \lim_{h \to 0} \frac{1}{h}\mathbb{E}_{p(x;\theta)}[(\omega(\theta, h) - 1)^2f(x)^2] -\mu(\theta)^2, \\&amp; \geq \sup_{h} \frac{(\mu(\theta + h) -\mu(\theta))^2}{\mathbb{E}_{p(x;\theta)}[\omega(\theta, h) - 1]^2}.\end{aligned}\]</span> Three sources of variance:</p><ul><li><p>importance ratio <span class="math inline">\(\omega(\theta,h)\)</span> (the need for absolute continuity);</p></li><li><p>The dimensionality of the parameters <span class="math inline">\(x\)</span>;</p></li><li><p>The variance of the cost function <span class="math inline">\(f(x)\)</span>.</p></li></ul><p><img src="/2023/10/21/Monte-Carlo-Gradient-Estimation/figure4.jpg"></p><h3 id="computational-cost">Computational Cost</h3><p>The computational cost of the score function estimator is low, it isthe order of <span class="math inline">\(O(N(D+L))\)</span> for <span class="math inline">\(N\)</span> samples, <span class="math inline">\(D\)</span> dimensional distributional parameters<span class="math inline">\(\theta\)</span> and <span class="math inline">\(L\)</span> is the cost of evaluating the costfunction.</p><h3 id="conclusion">Conclusion</h3><ul><li><p>The score funtion only need the final value of the cost in itscomputation and it makes no assumptions about the internal structure ofthe cost function. Any type of cost function can be used.</p></li><li><p>The measure must be differentiable with respect to the parameters<span class="math inline">\(\theta\)</span>, and we can easily samplefrom the measure. It is applicable to both discrete and continuousdistributions.</p></li><li><p>The estimator can be implemented using only a single sample,making it computationally efficient.</p></li><li><p>There are too many sources of variance, we can use some variancereduction techniques to reduce the variance.</p></li></ul><h1 id="pathwise-gradient-estimators">Pathwise Gradient Estimators</h1><h2 id="sampling-paths-sampling-process">Sampling Paths (samplingprocess)</h2><p>For continuous distribution, the alternative way to generate samples<span class="math inline">\(\hat{x}\)</span> from the distribution <span class="math inline">\(p(x; \theta)\)</span> is to sample from a simplerbase distribution <span class="math inline">\(p(\epsilon)\)</span> whichis independent of the parameters <span class="math inline">\(\theta\)</span>, and then transform the samplesthrough a deterministic <strong>path</strong> <span class="math inline">\(g(\epsilon;  \theta)\)</span>: <span class="math display">\[\hat{x} \sim p(x; \theta) \quad \equiv \quad \hat{x} = g(\epsilon;\theta), \quad \epsilon \sim p(\epsilon).\]</span></p><p>This transformation is described by the rule for the change ofvariables for probability: <span class="math display">\[p(x; \theta) = p(\epsilon) |\nabla_{\epsilon} g(\epsilon;\theta)|^{-1}.\]</span></p><h3 id="one-liners-reparameterization-trick">One-liners[Reparameterization Trick]</h3><p>One whidely-known example is sampling from a multivariate Gaussiandistribution <span class="math inline">\(p(\bf x; \bf \theta) =\mathcal{N}(\mathbf x|\mathbf \mu, \mathbf \Sigma)\)</span>:</p><ol type="1"><li>First sample from a standard Gaussian distribution <span class="math inline">\(p(\mathbf \epsilon) = \mathcal{N}(\mathbf\epsilon|\mathbf 0, \mathbf I)\)</span>;</li><li>Then transform the samples through the local-scale transformation<span class="math inline">\(g(\epsilon; \theta) = \mu + \mathbf L\epsilon\)</span>, where <span class="math inline">\(\mathbf{LL}^T =\mathbf \Sigma\)</span>.</li></ol><p><span class="math display">\[\hat{x} = \mu + \mathbf L \epsilon, \quad \epsilon \sim\mathcal{N}(\mathbf 0, \mathbf I), \quad \mathbf L \mathbf L^T = \mathbf\Sigma.\]</span></p><p><img src="/2023/10/21/Monte-Carlo-Gradient-Estimation/figure7.png"></p><p>Many such transformations exist for common distributions, includingDirichlet, Gamma, and many others. These types of transformations arecalled <strong>one-liners</strong> because they can be implemented in asingle line of code.</p><p>The expectation of eq. (1) is then: <span class="math display">\[\begin{aligned}\mathbb{E}_{p(x; \theta)} [f(x)] = \mathbb{E}_{p(\epsilon)}[f(g(\epsilon; \theta))]\end{aligned}\]</span> It is often used in Monte Carlo methods, and is called<strong>reparameterisation</strong> trick.</p><h2 id="gradient-estimators">Gradient Estimators</h2><p>Assume that we have a distribution <span class="math inline">\(p(x;\theta)\)</span> with known <strong>differentiable</strong> samplingpath <span class="math inline">\(g(\epsilon; \theta)\)</span> and basedistribution <span class="math inline">\(p(\epsilon)\)</span>. Thegradient estimator for the sensitivity analysis of eq. (2) is: <span class="math display">\[\begin{aligned}\eta &amp;= \nabla_{\theta} \mathbb{E}_{p(x; \theta)} [f(x)] \\&amp;= \nabla_{\theta} \int p(\epsilon) f(g(\epsilon; \theta))\text{d}\epsilon \\&amp;= \mathbb{E}_{p(\epsilon)}[\nabla_{\theta} f(g(\epsilon;\theta))]\\\bar{\eta} &amp;= \frac{1}{N} \sum_{n=1}^{N} \nabla_{\theta}f(g(\hat{\epsilon}^{(n)}; \theta)), \quad \hat{\epsilon}^{(n)} \simp(\epsilon).   \qquad (3)\end{aligned}\]</span></p><h3 id="decoupling-sampling-and-gradient-computation">DecouplingSampling and Gradient Computation</h3><p>The pathwise estimator (3) is limited to those distributions forwhich we simultaneously have a differential path and use this same pathto generate samples. But, <strong>sampling from a distribution may notprovide a differentiable path</strong>. Thus, we can expand theapplicability of the pathwise gradient by <strong>decoupling</strong>these two processes.</p><p>The pathwise estimator can be rewritten in a more general form:</p><p><span class="math display">\[\begin{aligned}  \eta &amp;= \nabla_{\theta} \mathbb{E}_{p(\mathbf x;\theta)}[f(\mathbfx)] \\  &amp;= \mathbb{E}_{p(\epsilon)}[\nabla_{\theta} f(\mathbf x)|_{\mathbfx = g(\epsilon;\theta)}] \\  &amp;= \int p(\epsilon) \nabla_{\mathbf x} f(\mathbf x)|_{\mathbf x =g(\epsilon;\theta)} \nabla_{\theta} g(\epsilon; \theta) \text{d}\epsilon \\  &amp;= \int p(\mathbf x;\theta) \nabla_{\mathbf x} f(\mathbfx)\nabla_{\theta} \mathbf x \text{d} \mathbf x \\  &amp;= \mathbb{E}_{p(\mathbf x;\theta)}[\nabla_{\mathbf x} f(\mathbfx) \nabla_{\theta} \mathbf x].\end{aligned}\]</span></p><p>One way to compute <span class="math inline">\(\nabla_{\theta}\mathbf x\)</span> is to use <span class="math inline">\(\nabla_{\theta}g(\epsilon; \theta)\)</span>, but this form is not alwaysconvenient.Another way to compute <span class="math inline">\(\nabla_{\theta} \mathbf x\)</span> is to use theinverse of the path <span class="math inline">\(g^{-1}(x;\theta)\)</span>. <span class="math inline">\(g^{-1}(x; \theta)\)</span>can be thought as the 'standardisation path' of the random variable--that is the transformation that removes the dependence of the samplingon the distribution parameters, standardising it to a zero mean unitvariance-like form.</p><p>Consider the equation <span class="math inline">\(\epsilon =g^{-1}(x; \theta)\)</span>, evaluating the total derivative on bothsides: <span class="math display">\[\begin{aligned}  \nabla_{\theta} \epsilon &amp;= \nabla_{\theta} g^{-1}(x;\theta) \\  0 &amp;= \nabla_{x} g^{-1}(x;\theta) \nabla_{\theta} x +\nabla_{\theta} g^{-1}(x;\theta) \\  \text{thus,} \nabla_{\theta} x &amp;= - (\nabla_{x}g^{-1}(x;\theta))^{-1} \nabla_{\theta} g^{-1}(x; \theta).\end{aligned}\]</span></p><p>In this form, we can apply pathwise gradient estimator to a far widerset of distributions and paths, such as for the Beta, Gamma, andDirichlet distributions.</p><ul><li>Example (Univariate distributions). For univariate distribution<span class="math inline">\(p(x;\theta)\)</span>, we can use thesampling path given by the inverse CDF: <span class="math inline">\(x =g(\epsilon;\theta) = F^{-1}(\epsilon; \theta)\)</span>, where <span class="math inline">\(\epsilon \sim \mathcal{U}[0, 1]\)</span>.Computing the derivative <span class="math inline">\(\nabla_{\theta} x =\nabla_{\theta} F^{-1} (\epsilon; \theta)\)</span> is often complicatedand expensive. We can obtain an alternative expression by consideringthe inverse path <span class="math inline">\(g^{-1}(x;\theta) = F(x;\theta)\)</span>, we have: <span class="math display">\[\nabla_{\theta} x = -\frac{\nabla_{\theta} F(x;\theta)}{\nabla_x F(x;\theta)} = - \frac{\nabla_{\theta} F(x;\theta)}{p(x; \theta)}.\]</span></li></ul><h3 id="bias-and-variance">Bias and variance</h3><p>When deriving the pathwise estimator, we exploited an interchangebetween differentiation and integration. If this interchange is valid,then the estimator is <strong>unbiased</strong>.</p><p>The variance of the pathwise estimator can be shown to be bounded bythe squared Lipschitz constant of the cost function <span class="math inline">\(f(x)\)</span>. (1) The variance bounds that existare independent of the dimensionality of the parameters <span class="math inline">\(\theta\)</span>, which means we can getlow-variance gradient estimates, even in high-dimensional space. (2) Asthe cost funtion becomes highly-variable, i.e., the Lipschitz constantincreases, the variance of the pathwise estimator can be higher thanthat of the score function methods.</p><p>The pathwise estimator will not always have lower variance whencompared to other methods since the variance is bounded by the Lipschitzconstant of the cost function.</p><h3 id="computational-cost-1">Computational cost</h3><p>The pathwise gradient estimator is restricted to differentiable costfunctions, which is a limitation when compared to the score functionestimator. Rapid convergence can be obtained even when using only asingle sample to compute the gradient, as is often done in practice.There is a trade-off between the number of samples used and theLipschitz constant of the cost function, and may require more samples tobe used for functions with higher Lipschitz constants. Thisconsideration is why we will find that regularisation that promotessmoothness of the functions we learn is important for successfulapplications.</p><p>The computational cost of the pathwise estimator is the same as thescore function estimator and is of the order <span class="math inline">\(O(N(D+L))\)</span> for <span class="math inline">\(N\)</span> samples, <span class="math inline">\(D\)</span> dimensional distributional parameters<span class="math inline">\(\theta\)</span> and <span class="math inline">\(L\)</span> is the cost of evaluating the costfunction and its gradient.</p><h3 id="conclusion-1">Conclusion</h3><ul><li>The pathwise estimator is only applicable to differentiable costfunctions.</li><li>When using the pathwise estimator, we do not need to know themeasure <span class="math inline">\(p(x; \theta)\)</span>, but we needto know the deterministic and differentiable sampling path <span class="math inline">\(g(\epsilon; \theta)\)</span> and the basedistribution <span class="math inline">\(p(\epsilon)\)</span>.</li><li>The estimator can be implemented using only a single sample, makingit computationally efficient.</li><li>We might need to control the smoothness of the cost function toensure that the variance of the estimator is low, and may need to employvariance reduction techniques.</li></ul><h3 id="gumbel-softmax-estimator">Gumbel softmax estimator</h3><h2 id="measure-valued-gradient-estimators">Measure-Valued GradientEstimators</h2><h3 id="weak-derivatives-measure-valued-derivatives">Weak Derivatives(measure-valued derivatives)</h3><p>Consider the derivative of a density function <span class="math inline">\(p(x; \theta)\)</span> with respect to a singleparameter <span class="math inline">\(\theta_i\)</span>, with <span class="math inline">\(i\)</span> the index on the set of distributionalparameters. The derivative <span class="math inline">\(\nabla_{\theta_i}p(x;\theta)\)</span> is not a density, since it may have negative valuesand does not integrate to one. Using the properties of signed measures,we can always decompose this derivative into a difference of twodensities, each multiplied by a constant: <span class="math display">\[\nabla_{\theta_i} p(x;\theta) = c_{\theta_i}^+ p^+(x;\theta) -c_{\theta_i}^- p^-(x;\theta),\]</span> where <span class="math inline">\(p^+, p^-\)</span> aredensities, referred to as the positive and negative parts of <span class="math inline">\(p\)</span>. By integrating both sides of theequation, we can obtain the constants <span class="math inline">\(c_{\theta_i}^+, c_{\theta_i}^-\)</span>: <span class="math display">\[\begin{aligned}  &amp;\int \nabla_{\theta_i} p(x;\theta) \text{d}x = \nabla_{\theta_i}\int p(x;\theta) \text{d}x  = 0; \\  &amp;\int c_{\theta_i}^+ p^+(x;\theta) - c_{\theta_i}^- p^-(x;\theta)\text{d}x = c_{\theta_i}^+ - c_{\theta_i}^- .\end{aligned}\]</span> Thus, we have: <span class="math display">\[c_{\theta_i}^+ = c_{\theta_i}^- := c_{\theta_i}\]</span> The decomposition of the derivative becomes: <span class="math display">\[\nabla_{\theta_i} p(x;\theta) = c_{\theta_i} (p^+(x;\theta) -p^-(x;\theta)).\]</span> The triple <span class="math inline">\((c_{\theta_i}, p^+,p^-)\)</span> is called the i-th <strong>weak derivative</strong> of<span class="math inline">\(p\)</span> with respect to <span class="math inline">\(\theta_i\)</span>.</p><p>For multivariate parameters <span class="math inline">\(\theta\)</span>, each dimension has onetriple.</p><p><img src="/2023/10/21/Monte-Carlo-Gradient-Estimation/figure5.png"></p><ul><li>The derivative is weak because we do not require the density to bedifferentiable.</li><li>The weak derivative is not unique, but always exists and can beobtained using the Hahn-Jordan decomposition of a signed measure intotwo measures that have complementary support. see <a href="https://www.math.uwaterloo.ca/~beforres/PMath451/Course_Notes/Chapter4.pdf">signedmeasure</a>.</li></ul><h3 id="deriving-the-estimator">Deriving the estimator</h3><p>For D-dimensional parameters <span class="math inline">\(\theta\)</span>, we can write the gradient of theexpectation as: <span class="math display">\[\begin{aligned}\eta_i &amp;= \nabla_{\theta_i} \mathbb{E}_{p(x;\theta)}[f(x)] =\nabla_{\theta_i} \int p(x;\theta) f(x) \text{d}x \\&amp;= \int \nabla_{\theta_i} p(x;\theta) f(x) \text{d}x \\&amp;= \int c_{\theta_i} (p_i^+(x;\theta) - p_i^-(x;\theta)) f(x)\text{d}x \\&amp;= c_{\theta_i} (\mathbb{E}_{p_i^+(x;\theta)}[f(x)] -\mathbb{E}_{p_i^-(x;\theta)}[f(x)]). \\\bar{\eta}_{i, N} &amp;=  \frac{c_{\theta_i}}{N} (\sum_{n=1}^{N}f(\hat{x}^{+(n)}) - \sum_{n=1}^{N} f(\hat{x}^{-(n)})), \quad\hat{x}^{+(n)} \sim p_i^+(x;\theta), \quad \hat{x}^{-(n)} \simp_i^-(x;\theta).\end{aligned}\]</span> The positive and negative components may be differentdepending on which parameter of the measure the derivative is taken withrespect to. The constant <span class="math inline">\(c_{\theta_i}\)</span> will also change dependingthe parameter being differentiated.</p><ul><li>Example (Bernoulli measure-valued gradient). Consider the Bernoullidistribution <span class="math inline">\(p(x;\theta) = \theta^x(1-\theta)^{1-x}\)</span>, with <span class="math inline">\(x \in \{0,1\}\)</span> and <span class="math inline">\(\theta \in [0, 1]\)</span>.By taking the derivative with respect to <span class="math inline">\(\theta\)</span>, we have: <span class="math display">\[\begin{aligned}  \nabla_{\theta} \int p(x;\theta) f(x) \text{d}x&amp;=  \nabla_{\theta} (\theta f(1) + (1-\theta) f(0))\\  &amp;= f(1) - f(0).\end{aligned}\]</span> By weak derivative, we have: <span class="math display">\[\begin{aligned}\nabla_{\theta} \int p(x;\theta) f(x) \text{d}x &amp;= \int\nabla_{\theta} p(x;\theta) f(x) \text{d}x \\&amp;= \int \delta_1 f(x) - \delta_0 f(x) \text{d}x \\&amp;= f(1) - f(0).\end{aligned}\]</span> which is the same as the original gradient.</li></ul><h4 id="vector-case">Vector case</h4><p>If the measure is a factorised distribution <span class="math inline">\(p(x;\theta) = \prod_d p(x_d | \theta_d)\)</span>,then the positive component and negative component of the weakderivative will itself factorise across the dimensions. For the positivecomponent, this decomposition will be <span class="math inline">\(p^+_i(x;\theta) =p(x_{-i})p^+_i(x_i;\theta_i)\)</span>, which is the product of themarginal distribution <span class="math inline">\(p(x_{-i})\)</span> andthe positive component of the weak derivative with respect to <span class="math inline">\(\theta_i\)</span>. The negative component will be<span class="math inline">\(p^-_i(x;\theta) =p(x_{-i})p^-_i(x_i;\theta_i)\)</span>, which is the product of themarginal distribution <span class="math inline">\(p(x_{-i})\)</span> andthe negative component of the weak derivative with respect to <span class="math inline">\(\theta_i\)</span>.</p><h3 id="estimator-properties-1">Estimator Properties</h3><h4 id="domination">Domination</h4><p>Remember in the score function estimator, we need the measure to beabsolutely continuous with respect to <span class="math inline">\(\theta\)</span>. We explored one example where wewere unable to ensure domination, because no bounding constant appliesat the boundaries of the domain. For weak derivatives, we can alwaysensure the correctness of the interchange between differentiation andintegration： the fundamental property of weak derivatives states thatif the triple <span class="math inline">\((c, p^+, p^-)\)</span> is theweak derivative of <span class="math inline">\(p(x;\theta)\)</span>,then for every <strong>bounded continuous</strong> function <span class="math inline">\(f(x)\)</span>, we have: <span class="math display">\[\nabla_{\theta} \int f(x)  p(x;\theta) \text{d}x = c_{\theta} [\int f(x)p^+(x;\theta) \text{d}x - \int f(x) p^-(x;\theta) \text{d}x].\]</span></p><ul><li>Example (Bounded support). Consider the measure-valued estimator fora cost function <span class="math inline">\(f(x) = x\)</span> anddistribution <span class="math inline">\(p(x; \theta) = \frac{1}{\theta}1_{\{0 &lt; x &lt; \theta\}}\)</span>, which is differential in <span class="math inline">\(\theta\)</span> when <span class="math inline">\(x\in (0, \theta)\)</span>; The measure-valued derivative is <span class="math display">\[\begin{aligned}    \nabla_{\theta} \int f(x) \mathcal{U}_{[0, \theta]}(x) \text{d}x&amp;= \nabla_{\theta} (\frac{1}{\theta} \int_{0}^{\theta} f(x)\text{d}x) = \frac{1}{\theta} f(\theta) - \frac{1}{\theta^2}\int_{0}^{\theta} f(x) \text{d} x \\    &amp;= \frac{1}{\theta} (\int f(x) \delta_{\theta}(x) \text{d}x -\int f(x) \mathcal{U}_{[0, \theta]}(x) \text{d}x) \\\end{aligned}\]</span></li></ul><p>The measure-valued derivative is given by the triple <span class="math inline">\((\frac{1}{\theta}, \delta_{\theta},\mathcal{U}_{[0, \theta]})\)</span>. For specific cost function <span class="math inline">\(f(x) = x\)</span>, we have:</p><p>The true gradient is: <span class="math inline">\(\nabla_{\theta}\mathbb{E}_{p(x;\theta)} [x] = \nabla_{\theta} (\frac{1}{\theta}\int_{0}^{\theta} \frac{x^2}{2}) = \frac{1}{2}\)</span>. Themeasure-valued gradient is: <span class="math inline">\(\frac{1}{\theta}(\mathbb{E}_{\delta_{\theta}} [x] - \mathbb{E}_{\mathcal{U}_{[0,\theta]}[x]}) = \frac{1}{\theta} (\theta - \frac{\theta}{2}) =\frac{1}{2}.\)</span></p><h4 id="bias-and-variance-1">Bias and variance</h4><p>For bounded and continuous cost functions <span class="math inline">\(f\)</span>, by using the fundamental property ofweak derivatives, the measure-valued gradient estimator is<strong>unbiased</strong>.</p><p>The variance of the measure-valued gradient estimator is: <span class="math display">\[\begin{aligned}\text{Var}_{p(x;\theta)}[\bar{\eta}_1] =\text{Var}_{p^+(x;\theta)}[f(x)] + \text{Var}_{p^-(x;\theta)}[f(x)] - 2\text{Cov}_{p^+(x';\theta)p^-(x;\theta)} [f(x'), f(x)].\end{aligned}\]</span></p><ul><li>The variance depends on the choice of decomposition of the weakderivative into positive and negative components.</li><li>If the random variables can be 'coupled' in some way, where theyshare the same underlying source of randomness, this will reduce thevariance of the gradient estimator by increasing the covariance term.The most common way is to sample the variables <span class="math inline">\(x'\)</span> and <span class="math inline">\(x\)</span> using common random numbers. Anotherway is to use variance reduction techniques.</li><li>Figure 5 shows that the measure valued estimator is not sensitive tothe dimensionality of the parameters <span class="math inline">\(\theta\)</span>. It is however sensitive to themagnitude of the function.</li></ul><p><img src="/2023/10/21/Monte-Carlo-Gradient-Estimation/figure6.png"></p><h4 id="computational-cost-2">Computational cost</h4><p>Measure-valued gradients are much more computationally expensive thanthe score-function or pathwise gradients. This is because the gradientwe computed is the gradient for a single parameter: for every parameterwe require two evaluations of the cost function to compute its gradient.It is this structure of adapting the underlying sampling distributionsfor each parameter that leads to the low variance of the estimator butat the same time makes its application to high-dimensional parameterspaces prohibitive.</p><p>The computational cost of the measure-valued gradient estimator isthe order of <span class="math inline">\(O(2NDL)\)</span> for <span class="math inline">\(N\)</span> samples, <span class="math inline">\(D\)</span> dimensional distributional parameters<span class="math inline">\(\theta\)</span> and <span class="math inline">\(L\)</span> is the cost of evaluating the costfunction.</p><h3 id="conclusion-2">Conclusion</h3><ul><li>The measure-valued estimator can be used with any type of costfunction, differentiable or not. As long as we can evaluate the costfunction repeatedly for different inputs.</li><li>It is applicable to both discrete and continuous distributions.</li><li>Computationally expensice in high-dimensional parameter spaces.</li><li>We need methods to sample from the positive and negativemeasures.</li><li>Using the weak derivative <strong>requires manual derivation of thedecomposition at first</strong>, although for many common distributionsthe weak-derivative decompositions are known.</li></ul><h2 id="variance-reduction-techniques">Variance ReductionTechniques</h2><p>The gradient variance is one of the principal sources of performanceissues. This paper introduces four common methods to reduce the varianceof gradient estimators: large-samples, coupling, conditioning, andcontrol variates.</p><h3 id="large-samples">Large-Samples</h3><p>The simplest way to reduce the variance of the gradient estimator isto use more samples. The variance of an estimators will shrinks as <span class="math inline">\(O(\frac{1}{N})\)</span>, where <span class="math inline">\(N\)</span> is the number of samples. However, thecomputational cost will increase linearly with the number of samples.The computational cost can be reduces by parallelising the computationof the gradient across multiple processors. Sometimes, increasing thenumber of Monte Carlo samples will not be an option, such as when thecost function involves a real-world experiment or interaction with auser.</p><h3 id="coupling-and-common-random-numbers">Coupling and Common randomnumbers</h3><p>When consider the difference between two expectations of a function<span class="math inline">\(f(x)\)</span> under different butclosely-related distributions <span class="math inline">\(p_1(x)\)</span> and <span class="math inline">\(p_2(x)\)</span>: <span class="math display">\[\begin{aligned}\eta = \mathbb{E}_{p_1(x)}[f(x)] - \mathbb{E}_{p_2(x)}[f(x)]\end{aligned}\]</span></p><p>The direct method to compute the difference is to estimate eachexpectation separately using Monte Carlo sampling, and then compute thedifference between the two estimates： <span class="math display">\[\begin{aligned}\bar{\eta}_{ind} = \frac{1}{N} \sum_{n=1}^{N} f(\hat{x}_1^{(n)}) -\frac{1}{N} \sum_{n=1}^{N} f(\hat{x}_2^{(n)}),\end{aligned}\]</span> where <span class="math inline">\(\hat{x}_1^{(n)} \simp_1(x)\)</span> and <span class="math inline">\(\hat{x}_2^{(n)} \simp_2(x)\)</span>.</p><p>We can achieve a simple form of variance reduction by coupling <span class="math inline">\(\hat{x}_1^{(n)}\)</span> and <span class="math inline">\(\hat{x}_2^{(n)}\)</span>, so that each pair <span class="math inline">\((\hat{x}_1^{(n)}, \hat{x}_2^{(n)})\)</span> issampled from some joint distribution <span class="math inline">\(p_{12}(x_1, x_2)\)</span> with marginals <span class="math inline">\(p_1(x)\)</span> and <span class="math inline">\(p_2(x)\)</span>. The variance of the coupledestimator is: <span class="math display">\[\begin{aligned}\text{Var}_{p_12(x_1, x_2)}[\bar{\eta}_{cpl}] &amp;=\text{Var}_{p_12(x_1, x_2)}[f(x_1) - f(x_2)] \\&amp;= \text{Var}_{p_1(x_1)}[f(x_1)] + \text{Var}_{p_2(x_2)}[f(x_2)] - 2\text{Cov}_{p_12(x_1, x_2)}[f(x_1), f(x_2)] \\&amp;= \text{Var}_{p_1(x_1)p_2(x_2)}[\bar{\eta}_{ind}] - 2\text{Cov}_{p_1(x_1)}[f(x_1), f(x_2)].\end{aligned}\]</span> Thus, to reduce the variance we need to choose a coupling<span class="math inline">\(p_{12}(x_1, x_2)\)</span> such that <span class="math inline">\(f(x_1)\)</span> and <span class="math inline">\(f(x_2)\)</span> are positively correlated. Themost common way to achieve this is to use <strong>common randomnumbers</strong> when <span class="math inline">\(p_1(x_1)\)</span> and<span class="math inline">\(p_2(x_2)\)</span> are close or in a relatedfamily of distributions. This means that the random numbers used togenerate <span class="math inline">\(x_1\)</span> and <span class="math inline">\(x_2\)</span> are the same. For example, in theunivariate case, we can first sample <span class="math inline">\(u \sim\mathcal{U}[0,1]\)</span> and then apply the inverse CDF transformationto obtain <span class="math inline">\(x_1 = F_{p_1}^{-1}(u)\)</span> and<span class="math inline">\(x_2 = F_{p_2}^{-1}(u)\)</span>.</p><ul><li>Coupling may not always reduce the variance of the measure valuedestimator, depending on the cost function.</li></ul><h3 id="conditioning">Conditioning</h3><p>Rao-Blackwellisation is a variance reduction technique thatprobabilistically conditions our estimator on a subset of dimensions andintegrates out the remaining dimensions.</p><p>Assume that the dimensions <span class="math inline">\(\{1,...,D\}\)</span> of <span class="math inline">\(x\)</span> are partitionedinto a set of dimensions <span class="math inline">\(\mathcal{S}\)</span> and its complement <span class="math inline">\(\mathcal{S}^c = {1,...,D}\ \mathcal{S}\)</span>.The expectation <span class="math inline">\(g(x_{\mathcal{S}^c}) =\mathbb{E}_{p(x_{\mathcal{S}})}[f(x)|x_{\mathcal{S}^c}]\)</span>. We canestimate <span class="math inline">\(\mathbb{E}_{p(x)}[f(x)]\)</span> byperforming Monte Carlo integration over the dimensions <span class="math inline">\(\mathcal{S}^c\)</span>: <span class="math display">\[\begin{aligned}\bar{g}_{N} &amp;= \frac{1}{N} \sum_{n=1}^{N}g(\hat{x}_{\mathcal{S}^c}^{(n)}) \\&amp;= \frac{1}{N} \sum_{n=1}^{N}\mathbb{E}_{p(x_{\mathcal{S}})}[f(x)|\hat{x}_{\mathcal{S}^c}^{(n)}],  \quad\hat{x}_{\mathcal{S}^c}^{(n)} \sim p(x_{\mathcal{S}^c}).\end{aligned}\]</span></p><p>By law of total expectation <span class="math inline">\(\text{Var}(Y)= \mathbb{E}[\text{Var}(Y | X)] + \text{Var}(\mathbb{E}[Y |X])\)</span>, we have: <span class="math display">\[\begin{aligned}\text{Var}_{p(x)}[f(x)] &amp;=\mathbb{E}_{p(x_{\mathcal{S}^c})}[\text{Var}_{p(x_{\mathcal{S}})}[f(x)|x_{\mathcal{S}^c}]]+\text{Var}_{p(x_{S^c})}[\mathbb{E}_{p(x_{\mathcal{S}})}[f(x)|x_{\mathcal{S}^c}]]\\&amp;=\mathbb{E}_{p(x_{\mathcal{S}^c})}[\text{Var}_{p(x_{\mathcal{S}})}[f(x)|x_{\mathcal{S}^c}]]+ \text{Var}_{p(x_{\mathcal{S}^c})}[g(x_{\mathcal{S}^c})] \\&amp; \geq \text{Var}_{p(x_{\mathcal{S}^c})}[g(x_{\mathcal{S}^c})]\end{aligned}\]</span></p><p>Thus, for unconditional one <span class="math inline">\(\bar{f}_N =\frac{1}{N}\sum_{n=1}^{N} f(\hat{x}^{(n)})\)</span>, we have: <span class="math display">\[\begin{aligned}\text{Var}_{p(x)}[\bar{f}_N] = \frac{1}{N} \text{Var}_{p(x)}[f(x)] \geq\frac{1}{N}  \text{Var}_{p(x_{\mathcal{S}^c})}[g(x_{\mathcal{S}^c})] =\text{Var}_{p(x_{\mathcal{S}^c})}[\bar{g}_{N}].\end{aligned}\]</span></p><ul><li>Conditional estimator has lower variance than the unconditionalestimator.</li><li>This technique is useful in practice only if we can compute theconditional expectation <span class="math inline">\(g(x_{\mathcal{S}^c})\)</span> efficiently.</li></ul><h2 id="control-variates">Control Variates</h2><p>Since all the gradient estimators have the same form <span class="math inline">\(\mathbb{E}_{p(x;\theta)}[f(x)]\)</span>, we willfocus on this general form. The strategy is to replace the function<span class="math inline">\(f(x)\)</span> in the expectation with asubstitute function <span class="math inline">\(\tilde{f}(x)\)</span>whose expectation is the same as <span class="math inline">\(\mathbb{E}_{p(x;\theta)}[f(x)]\)</span>, but whosevariance is lower.</p><p>If we have a function <span class="math inline">\(h(x)\)</span> witha known expectation <span class="math inline">\(\mathbb{E}_{p(x;\theta)}[h(x)]\)</span>, then wecan construct a new function <span class="math display">\[\tilde{f}(x) =f(x) - \beta(h(x)-\mathbb{E}_{p(x;\theta)}[h(x)]).\]</span> Here <span class="math inline">\(h(x)\)</span> is a control variate. <span class="math inline">\(\beta\)</span> is a coefficient that affects thestrength of the control variate. Then we can get a control variateestimator: <span class="math display">\[\begin{aligned}\bar{\eta}_{N} &amp;= \frac{1}{N} \sum_{n=1}^{N}\tilde{f}(\hat{x}^{(n)}) \\&amp;= \bar{f} - \beta (\bar{h} - \mathbb{E}_{p(x;\theta)}[h(x)]).\end{aligned}\]</span></p><h3 id="bias-consistency-and-variance">Bias, consistency andvariance</h3><ol type="1"><li><p>Unbiasedness. The control variate estimator is unbiased： <span class="math display">\[\begin{aligned}\mathbb{E}_{p(x;\theta)}[\bar{\eta}_{N}] &amp;=\mathbb{E}_{p(x;\theta)}[\bar{f} - \beta (\bar{h} -\mathbb{E}_{p(x;\theta)}[h(x)]) ]\\&amp;= \mathbb{E}_{p(x;\theta)}[\bar{f}] \\&amp;= \mathbb{E}_{p(x;\theta)}[f(x)].\end{aligned}\]</span></p></li><li><p>Consistency. The control variate estimator is consistent: <span class="math display">\[\lim_{N \to \infty} \bar{\eta}_{N} =\mathbb{E}_{p(x;\theta)}[\tilde{f}(x)] = \mathbb{E}_{p(x;\theta)}[f(x)].\]</span></p></li><li><p>Variance. The variance of the control variate estimator is (N=1):<span class="math display">\[\begin{aligned}\text{Var}_{p(x;\theta)}[\tilde{f}] &amp;= \text{Var}_{p(x;\theta)}[f -\beta (h - \mathbb{E}_{p(x;\theta)}[h(x)]) ]\\&amp;= \text{Var}_{p(x;\theta)}[f] + \beta^2 \text{Var}_{p(x;\theta)}[h]- 2 \beta \text{Cov}_{p(x;\theta)}[f, h].\end{aligned}\]</span> By minimising the right-hand side of the equation with respectto <span class="math inline">\(\beta\)</span>, we can obtain the optimalvalue of <span class="math inline">\(\beta\)</span>: <span class="math display">\[\beta^* = \frac{\text{Cov}_{p(x;\theta)}[f,h]}{\text{Var}_{p(x;\theta)}[h]} =\sqrt{\frac{\text{Var}_{p(x;\theta)}[f]}{\text{Var}_{p(x;\theta)}[h]}}\text{Corr}(f, h).\]</span> Using the optimal value of <span class="math inline">\(\beta\)</span>, the potential variance reductionis: <span class="math display">\[\begin{aligned}\frac{\text{Var}_{p(x;\theta)}[\tilde{f}]}{\text{Var}_{p(x;\theta)}[f]}= \frac{\text{Var}_{p(x;\theta)}[f - \beta (h -\mathbb{E}_{p(x;\theta)}[h(x)])]}{\text{Var}_{p(x;\theta)}[f]} = 1 -\text{Corr}(f, h)^2 \leq 1.\end{aligned}\]</span></p></li></ol><ul><li>The stronger the correlation between <span class="math inline">\(f\)</span> and <span class="math inline">\(h\)</span>, the greater the potential variancereduction.</li><li>In practice, the optimal <span class="math inline">\(\beta^*\)</span> will not be known and it can beestimated using the same <span class="math inline">\(N\)</span> samples.But the samples used to estimate <span class="math inline">\(\bar{h}\)</span> will introduce a bias because<span class="math inline">\(\bar{\beta}_N\)</span> and <span class="math inline">\(\bar{h}\)</span> will no longer be independent. Inpractice, thi bias is often negligible and can be controlled since itdecreases quickly as the number of samples <span class="math inline">\(N\)</span> increases.</li></ul><h3 id="multiple-and-non-linear-controls">Multiple and Non-linearControls</h3><h3 id="designing-control-variates">Designing Control Variates</h3><ol type="1"><li><p>Baselines. One simple way to reduce the variance of ascore-function gradient estimator is to use the score function itself asa control variate, since its expectation under the measure is zero. Themodified estimator is: <span class="math display">\[\begin{aligned}\bar{\eta}_{N} &amp;= \frac{1}{N} \sum_{n=1}^{N} (f(\hat{x}^{(n)}) -\beta)\nabla_{\theta} \log p(\hat{x}^n;\theta), \hat{x}^{(n)} \simp(x;\theta) \\\end{aligned}\]</span> In reinforcement learning, <span class="math inline">\(\beta\)</span> is called a baseline and it can beestimated with a running average of the cost. While this approach iseasier to implement than optimising <span class="math inline">\(\beta\)</span> to minimise variance, it is notoptimal and does not guarantee lower variance compared to the vanillascore-function estimator.</p></li><li><p>Bounds. We can use bounds on the cost function <span class="math inline">\(f\)</span> as ways of specifying the form of thecontrol variate <span class="math inline">\(h\)</span>. This isintuitive because it maintains a correlation between <span class="math inline">\(f\)</span> and <span class="math inline">\(h\)</span>, and if chosen well, may be easilyintegrable against the measure and available in closed form. Thisapproach requires more knowledge of the cost function, since we willneed to characterise the cost analytically in some way to bound it. Ingeneral, unless the bounds used are tight, they will not be effective ascontrol variates, since the gap between the bound and the true functionis not controllable and will not necessarily give the information neededfor variance reduction.</p></li><li><p>Delta method. The delta method is a way of constructing a controlvariate by using the Taylor expansion of the cost function. Thisrequires a cost function that is differentiable so that we can computethe second-order Taylor expansion, but can be an effective and verygeneral approach for variance reduction that allows easy implementation.It can be used for variance reduction in both the score-functionestimator (Paisley et al., 2012) and the pathwise estimator(Miller etal., 2017).</p></li></ol><ul><li><p>Example. Define <span class="math inline">\(\gamma(x)\)</span> isthe gradient of the cost function <span class="math inline">\(f(x)\)</span>, <span class="math inline">\(H(x)\)</span> is the Hessian of the cost function<span class="math inline">\(f(x)\)</span>. The second-order Taylorexpansion of a cost function expand around point <span class="math inline">\(\mu\)</span> and its derivate are <span class="math display">\[\begin{aligned}  h(x) &amp;= f(\mu) + (x - \mu)^T \gamma(\mu) + \frac{1}{2} (x - \mu)^TH(\mu) (x - \mu), \\  \nabla_{x} h(x) &amp;= \gamma(\mu)^T + (x - \mu)^T H(\mu).\end{aligned}\]</span></p><p>We can use this expansion directly as a control variate for thescore-function estimator:</p></li></ul><p><span class="math display">\[\begin{aligned}\bar{\eta}_{SF} &amp;= \nabla_{\theta} \mathbb{E}_{p(x;\theta)}[f(x)] \\&amp;= \nabla_{\theta} \mathbb{E}_{p(x;\theta)}[f(x) - \beta^T h(x)] +\beta^T \nabla_{\theta} \mathbb{E}_{p(x;\theta)}[h(x)] \\&amp;= \mathbb{E}_{p(x;\theta)}[(f(x) - \beta^T h(x))\nabla_{\theta}\log p(x;\theta)] + \beta^T \nabla_{\theta}\mathbb{E}_{p(x;\theta)}[h(x)].\end{aligned}\]</span> In the Gaussian mean-field variational inference that Paisleyet al. (2012) consider, the second term is known in closed-form andhence does not require Monte Carlo approximation. <span class="math inline">\(\beta\)</span> is a multivariate controlcoefficient and is estimated separately.</p><p>For the pathwise estimator, using the sampling path <span class="math inline">\(x = g(\epsilon; \theta)\)</span>, we have: <span class="math display">\[  \begin{aligned}    \bar{\eta}_{PW} &amp;= \nabla_{\theta}\mathbb{E}_{p(x;\theta)}[f(x)] \\    &amp;= \nabla_{\theta} \mathbb{E}_{p(x;\theta)}[f(x) - \beta^T h(x)]+ \beta^T \nabla_{\theta} \mathbb{E}_{p(x;\theta)}[h(x)] \\    &amp;= \nabla_{\theta} \mathbb{E}_{p(x;\theta)}[f(g(\epsilon;\theta)) - \beta^T h(g(\epsilon; \theta))] + \beta^T \nabla_{\theta}\mathbb{E}_{p(x;\theta)}[h(x)] \\    &amp;= \mathbb{E}_{p(\epsilon)}[\nabla_{x}f(x) \nabla_{\theta}g(\epsilon; \theta) - \beta \nabla_x h(x)\nabla_{\theta} g(\epsilon;\theta)] + \beta^T \nabla_{\theta} \mathbb{E}_{p(x;\theta)}[h(x)].  \end{aligned}  \]</span> Assume that the final term is known in closed-form and doesnot require stochastic approximation.</p><h2 id="guidance-in-choosing-gradient-estimators">Guidance in ChoosingGradient Estimators</h2><p>The authors provide some guidance in choosing gradientestimators.</p><ul><li><p>If our estimation problem involves continuous functions andmeasures that are continuous in the domain, then using the pathwiseestimator is a good default. It is relatively easy to implement and itsdefault implementation, without additional variance reduction, willtypically have variance that is low enough so as not to interfere withthe optimisation.</p></li><li><p>If the cost function is not differentiable or is a black-boxfunction then the score-function or the measure-valued gradients areavailable. If the number of parameters is low, then the measure-valuedgradient will typically have lower variance and would be preferred. Butif we have a high-dimensional parameter set, then the score-functionestimator should be used.</p></li><li><p>If we have no control over the number of times we can evaluate ablack-box cost function, effectively only allowing a single evaluationof it, then the score function is the only estimator of the three wereviewed that is applicable.</p></li><li><p>The score-function estimator should, by default, always beimplemented with at least some basic variance reduction. The simplestoption is to use a baseline control variate estimated with a runningaverage of the cost value. • When using the score-function estimator,some attention should be paid to the dynamic range of the cost functionand its variance, and ways found to keep its value bounded within areasonable range, e.g. by transforming the cost so that it is zero mean,or using a baseline.</p></li><li><p>For all estimators, track the variance of the gradients ifpossible and address high variance by using a larger number of samplesfrom the measure, decreasing the learning rate, or clipping the gradientvalues. It may also be useful to restrict the range of some parametersto avoid extreme values, e.g. by clipping them to a desiredinterval.</p></li><li><p>The measure-valued gradient should be used with some couplingmethod for variance reduction. Coupling strategies that exploitrelationships between the positive and negative components of thedensity decomposition, and which have shared sampling paths, are knownfor the commonly-used distributions.</p></li><li><p>If we have several unbiased gradient estimators, a convexcombination of them might have lower variance than any of the individualestimators.</p></li><li><p>If the measure is discrete on its domain then the score-functionor measure-valued gradient are available. The choice will again dependon the dimensionality of the parameter space.</p></li><li><p>In all cases, we strongly recommend having a broad set of teststo verify the unbiasedness of the gradient estimator whenimplemented.</p></li></ul><h1 id="reference">Reference</h1><p>(https://pages.stat.wisc.edu/~shao/stat609/stat609-07.pdf)</p><p>https://bochang.me/blog/posts/measure-val-grad/</p><p><a href="https://www.math.uwaterloo.ca/~beforres/PMath451/Course_Notes/Chapter4.pdf">signedmeasure</a></p>]]></content>
      
      
      <categories>
          
          <category> Machine Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Monte Carlo Gradient Estimator </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Introduction to Deep Learning</title>
      <link href="/2023/09/11/deep-learning-with-structures/"/>
      <url>/2023/09/11/deep-learning-with-structures/</url>
      
        <content type="html"><![CDATA[<h2 id="brief-intro-to-deep-learning">Brief Intro to Deep Learning</h2><p>Deep learning: - Data: large datasets, e.g., ImageNet, etc.; - Model:deep neural networks, e.g., ResNet-152, etc.; - Learning algorithm:backpropagation, i.e., stochastic gradient descent (SGD).</p><p>In recurrent neural networks (RNNs): <strong>same</strong> neuralnetwork gets reused many times. <span class="math display">\[h^t = F(x^t, h^{t-1}, W).\]</span></p><p>Back propagation: - Loss is always a scalar value; - For the backwardpropogation, the gradient is in the form: <span class="math inline">\(J^T v\)</span>, where <span class="math inline">\(v\)</span> is a vector, and <span class="math inline">\(J\)</span> is a Jacobian matrix, <span class="math inline">\(T\)</span> means transpose; - For the forwardpropogation, the gradient is in the form: <span class="math inline">\(Jv\)</span>. - In BF process, the shape of a Jacobian matrix is <span class="math inline">\(m \times n\)</span>, where <span class="math inline">\(m\)</span> is the dimension of output, and <span class="math inline">\(n\)</span> is the dimension of input.</p><p>Consider Vector-by-Matrix Gradients:</p><p>Case 1: <span class="math inline">\(\bm{z} =\mathbf{W}\bm{x}\)</span>, where <span class="math inline">\(\bm x \in\mathbb{R}^{n \times 1}\)</span>, <span class="math inline">\(\mathbf{W}\in \mathbb{R}^{m \times n}\)</span>, <span class="math inline">\(\bm z\in \mathbb{R}^{m \times 1}\)</span>. <span class="math display">\[\begin{aligned}\frac{\partial \bm{z}}{\partial \bm{x}} &amp;= \mathbf{W}, \\\end{aligned}\]</span></p><p>Case 2： <span class="math inline">\(\bm{z} = \bm{x}\mathbf{W}\)</span>, where <span class="math inline">\(\bm x \in\mathbb{R}^{1 \times m}\)</span>, <span class="math inline">\(\mathbf{W}\in \mathbb{R}^{m \times n}\)</span>, <span class="math inline">\(\bm z\in \mathbb{R}^{1 \times n}\)</span>. <span class="math display">\[\begin{aligned}\frac{\partial \bm{z}}{\partial \bm{x}} &amp;= \mathbf{W}^T, \\\end{aligned}\]</span></p><p>Case 3: <span class="math inline">\(\bm z = \bm x\)</span>, then<span class="math inline">\(\frac{\partial \bm z}{\partial \bm x} =\mathbf{I}\)</span>.</p><p>Case 4: <span class="math inline">\(\mathbf{X} \in \mathbb{R}^{m\times n}\)</span>, then <span class="math inline">\(\frac{\partialscalar}{\partial \mathbf{X}}  \in \mathbb{R}^{m \times n}\)</span>, justas the same as the shape of <span class="math inline">\(\mathbf{X}\)</span>.</p><p>Case 5: <span class="math inline">\(\mathbf{Z} =\mathbf{X}\mathbf{W}\)</span>, where <span class="math inline">\(\mathbf{X} \in \mathbb{R}^{m \times n}\)</span>,<span class="math inline">\(\mathbf{W} \in \mathbb{R}^{n \timesw}\)</span>. Assume <span class="math inline">\(\frac{\partialLoss}{\partial \mathbf{Z}} = \mathbf{\delta} \in \mathbb{R}^{m \timesw}\)</span>. <span class="math display">\[\begin{aligned}\frac{\partial Loss}{\partial \mathbf{X}} &amp;= \mathbf{\delta}\mathbf{W}^T, \\\frac{\partial Loss}{\partial \mathbf{W}} &amp;= \mathbf{X}^T\mathbf{\delta}.\end{aligned}\]</span></p><p><img src="/2023/09/11/deep-learning-with-structures/bp.jpg"></p><p>Here we consider the simple example: <span class="math display">\[\begin{aligned}\bm h_1 = \bm x \mathbf{W}_1,  \qquad (1 \times 4) = (1 \times 3) \times(3 \times 4)\\\hat{\bm y} = \bm h_1 \mathbf{W}_2, \qquad (1 \times 2) = (1 \times 4)\times (4 \times 2)\\L = \|\hat{\bm y}\|_2^2. \qquad (1 \times 1) = (1 \times 2) \times (2\times 1)\end{aligned}\]</span> Then, for the backward propagation process, we have: <span class="math display">\[\begin{aligned}\frac{\partial L}{\partial \hat{\bm y}} &amp;= 2 \hat{\bm y}, \qquad (1\times 2) = (1 \times 2) \\\frac{\partial L}{\partial \mathbf{W}_2} &amp;=  (\frac{\partial\hat{\bm y}}{\partial \mathbf{W}_2})^T \frac{\partial L}{\partial\hat{\bm y}}= \bm h_1^T  \frac{\partial L}{\partial \hat{\bm y}} = 2 \bmh_1^T \hat{\bm y}, \qquad (4 \times 2) = (4 \times 1) \times (1 \times2)\\\frac{\partial L}{\partial \bm h_1} &amp;=  \frac{\partial L}{\partial\hat{\bm y}}\frac{\partial \hat{\bm y}}{\partial \bm h_1}=\frac{\partialL}{\partial \hat{\bm y}} \mathbf{W}_2^T = 2 \hat{\bm y} \mathbf{W}_2^T, \qquad (1 \times 4) = (1 \times 2) \times (2 \times 4)\\\frac{\partial L}{\partial \mathbf{W}_1} &amp;=  (\frac{\partial \bmh_1}{\partial \mathbf{W}_1})^T [\frac{\partial L}{\partial \hat{\bmy}}\frac{\partial \hat{\bm y}}{\partial \bm h_1}] = \bm x^T\frac{\partial L}{\partial \bm h_1} = 2 \bm x^T \hat{\bm y}\mathbf{W}_2^T, \qquad (3 \times 4) = (3 \times 1) \times (1 \times 4)\\\frac{\partial L}{\partial \bm x} &amp;=  \frac{\partial L}{\partial\hat{\bm y}}\frac{\partial \hat{\bm y}}{\partial \bm h_1} \frac{\partial\bm h_1}{\partial \bm x} = 2\hat{\bm y} \mathbf{W}_2^T \mathbf{W}_1^T,\qquad (1 \times 3) = (1 \times 2) \times (2 \times 4) \times (4 \times3)\end{aligned}\]</span></p><p>We can get similar results if <span class="math inline">\(\bm x \in\mathbb{R}^{3 \times 1}\)</span>.</p><h2 id="invariant-and-equivariant">Invariant and Equivariant:</h2><p><strong>Invariant</strong>: A mathematical object (or a class ofmathematical objects) remains unchanged after operations ortransformations of a certain type are applied to the objects <span class="math inline">\(F(g(x)) = F(x)\)</span>, e.g., max pooling;</p><ul><li>Symmetry Group: all transformations under which the object isinvariant</li></ul><p><strong>Equivariant</strong>: Applying a transformation and thencomputing the function produces the same result as computing thefunction and then applying the transformation <span class="math inline">\(F(g(x)) = g(F(x))\)</span>.</p><ul><li><p>Convolution is translation equivariant, i.e., Conv(Shift(X)) =Shift(Conv(X))!</p></li><li><p>Global pooling gives you shift-invariance!</p></li></ul><h3 id="permutation-invariance">Permutation Invariance</h3><p>Birkhoff Polytope: <span class="math display">\[B_n = \{P \in \mathbb{R}^{n \times n} | \forall i, j, P_{ij} \geq 0,\sum_{i=1}^n P_{ij} = 1, \sum_{j=1}^n P_{ij} = 1\}\]</span> This type of matrix is Doubly Stochastic Matrix.</p><p>Birkhoff–von Neumann Theorem: 1. Birkhoff Polytope is the convex hullof permutation matrices 2. Permutation matrices = Vertices of BirkhoffPolytope (S_n):</p><p><img src="/2023/09/11/deep-learning-with-structures/Birkhoff.png"></p><p>Assume <span class="math inline">\(\mathbf{X} \in \mathbb{R}^{n\times 3}\)</span>, permutation matrix <span class="math inline">\(\mathbf{P} \in \mathbb{R}^{n \times n}\)</span>.<span class="math inline">\(\bm Y \in \mathbb{R}^{1 \times K}\)</span>is the output probability of classes.</p><p>Permutation Invariance : <span class="math inline">\(\bm Y =f(\mathbf{PX})\)</span>, <span class="math inline">\(\forall \mathbf{P}\in S_n\)</span>.</p><p>Assume <span class="math inline">\(\mathbf{H} \in \mathbb{R}^{n\times d}\)</span> is the representations, and <span class="math inline">\(\mathbf{H} = f(\mathbf{X})\)</span>, then,</p><p>Permutation Equivariance: <span class="math inline">\(\mathbf{PH} =\mathbf{P}f(\mathbf{X}) = f(\mathbf{PX})\)</span>, where <span class="math inline">\(\mathbf{P} \in S_n\)</span> is a permutationmatrix.</p><ul><li>equivariant first, then move to invariant.</li></ul><p>Valid set funtions: the function is invariant to the order of theinput.</p><p>Theorem: A function <span class="math inline">\(f\)</span> operatingon a set <span class="math inline">\(X\)</span> having elements from acountable <strong>universe</strong>. If <span class="math inline">\(f\)</span> is a valid set function, then thereexists a function <span class="math inline">\(g\)</span> such that <span class="math inline">\(f(X) = \rho(\sum_{x \in X} \phi(x))\)</span>.</p><p>Proof: For the mapping: <span class="math inline">\(f(X) \toR\)</span>, the domain of <span class="math inline">\(f\)</span> is allsubsets in <span class="math inline">\(X\)</span>. For example: if <span class="math inline">\(X = \{a, b, c\}\)</span>, then the domain is a<strong>power set</strong> <span class="math inline">\(\{\phi, a, b, c,\{a, b\}, \{a, c\}, \{b, c\}, \{a, b, c\}\}\)</span>.</p><p>Sufficiency: summation is permutation invariant!</p><p>Necessity: find an unique representation of any set and then mapit:</p><p>why choose 4: base 3,4 ... are ok, but base 2 is not. Since base 2cannot guarantee the uniqueness of the representation.</p><h2 id="deep-learning-for-sequences">Deep learning for Sequences</h2><p>Applications:</p><ul><li><p>Language Model: <span class="math inline">\(P(x^{t+1}| x^{t},\cdots, x^1)\)</span>, where <span class="math inline">\(x^{t+1}\)</span> is the word we want topredict.</p></li><li><p>Machine Translation.</p></li></ul><p>Key challenges:</p><ul><li>Variable length input and output;</li><li>Order change may be crucial for cognition;</li><li>complex statistical dependencies (e.g. long-range ones).</li></ul><h3 id="transformer">Transformer:</h3><p><img src="/2023/09/11/deep-learning-with-structures/transformer.png"> Theoutput representation of the final encoder is the input of eachdecoder.</p><p><img src="/2023/09/11/deep-learning-with-structures/encoder_decoder.png">The decoder includes self-attention and encoder-decoder attention. Forthe self-attention, it uses <strong>masked multi-headattention</strong>, why????</p><p><strong>How to encode the input sequence?</strong></p><p><strong>Input embedding:</strong> Construct the one-hot vector foreach word. N words, each word will be mapping to a D dimension vector.Then, we can get a NxD matrix. D is the hyper-parameter.</p><ul><li><p>Will the input embedding manners affect the performance of themodel?</p><p>In general, we use the same vocabulary dictionary for the inputembedding.</p></li></ul><p><strong>positional encoding:</strong> <img src="/2023/09/11/deep-learning-with-structures/positional.png"> <span class="math display">\[\begin{aligned}PE(pos, 2i) &amp;= sin(pos/10000^{2i/d_{model}}) \\PE(pos, 2i+1) &amp;= cos(pos/10000^{2i/d_{model}}) \\\end{aligned}\]</span></p><p>pos is the index of the word in the sentence. (0-30) <span class="math inline">\(2i\)</span> and <span class="math inline">\(2i+1\)</span> is the index of the column, d_modelis the number of columns, it is a hyper-parameter(120). For eachword(token), we encode it to a vector with dimension d_model accordingto its position.</p><p>Here we use denominator <span class="math inline">\(10000^{2i/d_model}\)</span> to make sure thepositional encoding is different for different tokens. The sin and cosare periodic functions, if we don't use the denominator, then thepositional encoding could be same for different tokens.</p><ul><li>If there are two different sentences with the same size, will thepositional encodings be the same?? yes.</li></ul><p><strong>Self attention:</strong> <span class="math inline">\(X \inR^{tokes \times dim}\)</span>, <span class="math inline">\(W_Q \inR^{dim \times dim}\)</span>, <span class="math inline">\(Q = XW_Q \inR^{tokens \times dim}\)</span>.</p><p><span class="math inline">\(softmax(\frac{QK^T}{\sqrt{dim}}) \inR^{tokens \times tokens}\)</span> is the <strong>attentionmatrix</strong>, the dimension must be the same as the number oftokens.</p><p>we apply softmax in individual row, then the output of softmax is<span class="math inline">\(tokens \times tokens\)</span>.</p><p>The final output dimension is <span class="math inline">\(tokens\times dim\)</span>.</p><ul><li><p>Why does it need to divide by <span class="math inline">\(\sqrt{dim}\)</span>?</p><p>To keep the variance of each entry <span class="math inline">\(QK^T[i,k]\)</span> to be 1. <strong>[approach to1]</strong>. if we don't preserve the variance, then the gradient willbe larger and larger, and the model will be unstable.</p></li></ul><p><strong>Multi-head attention</strong> it can capture differentdependency of the input sequence. One choice is to input the same inputembedddings for each attention head, and then aggregate the output ofeach attention head. Another choice is to split the input embeddingsinto different parts, and then input different parts to differentattention heads. (The problem is not a convex problem, thus the weightsof each attention head may be different.)</p><p><strong>Layer normalization</strong> <img src="/2023/09/11/deep-learning-with-structures/layernorm.png"> it isapplied to each row of the output of multi-head attention. It is similarto batch normalization, but it is applied to each row, not each column.we want to learn <span class="math inline">\(\gamma\)</span> and <span class="math inline">\(\beta\)</span>, because we want to learn thedistribution of each row.</p><p><strong>Masked multi-head attention in decoder</strong> The decodermust be autoregressive. We need to input the previous words to predictthe next word and prevent attending from future. For a attention matrix,we can mask the upper triangle, then the values masked are zeros. But,if we mask the upper triangle, the sum of each row is not equal to 1,thus we need to adjust the attention matrix. Normally, the attentionmatrix is: <span class="math display">\[\begin{aligned}A = softmax(\frac{QK^T}{\sqrt{dim}})\end{aligned}\]</span> The dimension of the attention matrix is <span class="math inline">\(outputtokens \times outputtokens\)</span>. If wemask <span class="math inline">\(A_{ij}\)</span>, then the input ofsoftmax function will be: <span class="math display">\[\begin{aligned}A_{ij} = \frac{\exp \frac{ \sum_k Q_{ik}K_{jk} -\infty}{\sqrt{dim}}}{\sum \exp()}\end{aligned}\]</span></p><p>Shifted right: we already generate one token, and we want to predictthe next token, then we always focus on the right part of the outputsequence.</p><p><strong>Cross attention</strong> it is used in the decoder. The inputof the decoder is the output of the encoder. The encoder-decoderattention is similar to the self attention, but the query is the outputof the decoder, and the key and value are the output of the encoder.Here the output of encoder is the embaddings, and the docoder cangenerate the key and value from the embeddings. Cross attention cancapture the relationship between the input sentence and the outputsentence.</p><h2 id="graph-neural-networks-message-passing-models">Graph NeuralNetworks Message Passing Models</h2><p>Graph: multi-edges, nodes have types, edges have types.</p><ul><li>connectivity: adjacency list <span class="math inline">\(G = (V,E)\)</span> and adjacency matrix <span class="math inline">\(A\)</span>.<span class="math inline">\(|V| = n, |E| = m\)</span>. <span class="math inline">\(A \in \mathbb{R}^{n \times n}\)</span>.</li><li>features: node features <span class="math inline">\(X\)</span>, edgefeatures, graph features.</li></ul><p>if you want to permute a graph, then you need to<strong>left</strong> multiply the permutation matrix to the adjacencymatrix (change rows) and also right multiply the transpose permutationmatrix to the adjacency matrix(change columns): <span class="math display">\[\begin{aligned}A' = PAP^T\end{aligned}\]</span></p><p>For a graph <span class="math inline">\(A_1\)</span>, if there existsa permutation matrix <span class="math inline">\(P\)</span>, such that<span class="math inline">\(A_2 = PA_1P^T\)</span>, then <span class="math inline">\(A_1\)</span> and <span class="math inline">\(A_2\)</span> are graph isomorphic.</p><p>For a graph <span class="math inline">\(A\)</span>, if there exists apermutation matrix <span class="math inline">\(P\)</span>, such that<span class="math inline">\(A = PAP^T\)</span>, then <span class="math inline">\(A\)</span> is graph automorphism.</p><p>Given graph data <span class="math inline">\((A, X)\)</span> and<span class="math inline">\(f(A, X) \in \mathbb{R}^{n \timesd}\)</span>:</p><ul><li><p>invariance: <span class="math inline">\(f(PAP^T, PX)=f(A,X)\)</span>, <span class="math inline">\(\forall P \inS_n\)</span>.</p></li><li><p>equivariance: <span class="math inline">\(f(PAP^T, PX) = Pf(A,X)\)</span>, <span class="math inline">\(\forall P \inS_n\)</span>.</p></li></ul><p>Key challenges:</p><ul><li>unordered neighbors;</li><li>variable size of neighbors;</li><li>varying graph partitions.</li></ul><h3 id="message-passing-in-gnns">Message Passing in GNNs</h3><p>Feedforward networks: layers do not share message passing module.[don't share weights]. Usually, we call it 'layers'.</p><p>Recurrent networks: layers share message passing module.[reuseweights]. Usually, we call it 'steps' instead of 'layers'.</p><p>Even if we increase the number of nodes and edges, the model canstill work.</p><p><strong>ONLY USE ONE NETWORK FOR ALL NEIGHBORS</strong>, same acrossedges. if we use different networks for each neighbor, changing thenumber of nodes will affect the model. The model is not invariant to thenumber of nodes.</p><p>For undirected graph, the message passing is symmetric, i.e., m_ij =m_ji. It doesn't related to the order of nodes. For directed graph, m_ijmay not equal to m_ji.</p><p>We can also use transformers or gcn or LSTM to implement the messagepassing. Be careful that LSTM is not permutation invariant.</p><p>Tips: parallel message passing: compute messages for all nodes/edgesand compute updates for all nodes in parallel. Use dense operators onGPUs.</p><p>privacy and robust to attack in gnn.</p><p>we can also use transformers in encoding graph structure as anattention mask.</p><h2 id="graph-convolutional-networks">Graph Convolutional Networks</h2><p>laplace operator is the eigenfuction, why?</p><p>For <strong>undirected</strong> graph, Graph Laplacian: <span class="math inline">\(L = D - A\)</span>, where <span class="math inline">\(D\)</span> is the degree matrix, <span class="math inline">\(A\)</span> is the adjacency matrix. Laplacianmatrix can compute the difference between the node and its neighbors. Itis symmetric, diagonally dominant, positive semi-definite(eigenvaluesare nonnegative), and the number of zero eigenvalues is the number ofconnected components.</p><ul><li>Translation group;</li><li>Roto-translation group: SO(n): <span class="math inline">\(Q \in\mathbb{R}^{n \times n}, Q^TQ = Q Q^T = I, det(Q) = 1.\)</span></li></ul><p><span class="math inline">\(g = (X, R_\theta), g' = (X',R_{\theta'})\)</span></p><p><span class="math inline">\(g \cdot g' = g \cdot g' (x_0) =g(R_{\theta'}x_0 + X') = R_{\theta}R_{\theta'}x_0 +R_{\theta}X' + X = (R_{\theta'}X' + X, R_{\theta +\theta'})\)</span></p><ul><li>Scale-translation group.</li><li>Affine group.</li></ul><p>cross-correlations: <span class="math inline">\(f \star g (x) = \intf(x'-x)g(x')dx'\)</span>, in mathmetics, the order ofconvolution is inverse to the order in DL.</p><h2 id="autoregressive-models">Autoregressive Models</h2><p>Autoregressive model： <span class="math display">\[\begin{aligned}P(x_1, \cdots, x_n) = \prod_{i=1}^n P(x_i|x_1, \cdots, x_{i-1}) =\prod_{i=1}^n P(x_i|x_{&lt;i})\end{aligned}\]</span></p><p>For images, each <span class="math inline">\(x_i\)</span> is a pixelvalue, e.g., {0,...,255}. n = height * width. Each term <span class="math inline">\(P(x_i|x_{&lt;i})\)</span> can be modeled by asingle CNN/RNN/....</p><p>Why do we consider the same model for each term? Otherwise, thenumber of model is O(n).</p><p>PixelCNNs: conditioned on the pixels above and to the left of thepixel being predicted. At each step, we will mask the pixels below andright of the pixel being predicted. Then we use convolution on theimage, but it will yield high computation cost.</p><p>PixelRNNs: vectorize the image as a sequence of pixels, and then useRNN to model the sequence.</p><p>Masked Filter: we mask the filter to make sure the convolution isautoregressive. But it will yield blind spots.</p><p>blind spots: the model cannot see the pixels below and right of thepixel being predicted.</p><p>resovle blind spots: use a stack of masked convolutions.</p><p>The cons of softmax: if the number of dimension is very large, thenthe softmax will be very small. Thus we use the discretized mixturelogistic distribution: <span class="math display">\[\begin{aligned}P(x) = \sum_{k=1}^K \pi_k \sigma(\frac{x - \mu_k}{s_k})\end{aligned}\]</span> where <span class="math inline">\(\sigma\)</span> is thesigmoid function, <span class="math inline">\(\pi_k\)</span> is theweight of the <span class="math inline">\(k\)</span>-th component, <span class="math inline">\(\mu_k\)</span> is the mean of the <span class="math inline">\(k\)</span>-th component, <span class="math inline">\(s_k\)</span> is the scale of the <span class="math inline">\(k\)</span>-th component.</p><p>Due to the sequential nature of autoregressive sampling, it isslow.</p><ul><li><p>will autoregressive model focus more on the nearby locations?yes, because we use masked convolutions.</p></li><li><p>For directed graphs, we can generate the lower triangle of theadjacency matrix, and then generate the upper triangle.</p></li></ul><h2 id="generative-adversarial-networks-gans">Generative AdversarialNetworks (GANs)</h2><p>Generative models: generate data from noise.</p><p>Min-max loss: <span class="math display">\[\begin{aligned}\min_{\theta} \max_{\phi}\mathbb{E}_{x \sim p_{data}(x)}[\logD_{\phi}(x)] + \mathbb{E}_{z \sim p_z(z)}[\log(1 -D_{\phi}(G_{\theta}(z)))]\end{aligned}\]</span> where <span class="math inline">\(D\)</span> is thediscriminator, <span class="math inline">\(G\)</span> is the generator,<span class="math inline">\(x\)</span> is the real data, <span class="math inline">\(z\)</span> is the noise, <span class="math inline">\(p_{data}\)</span> is the distribution of the realdata, <span class="math inline">\(p_z\)</span> is the distribution ofthe noise.</p><p>The fake images are generated from noise (normal distribution), wecan not get the distribution of fake images. Why? Because the mappingfrom noise to fake images is not invertible. Thus, we cannot get thelikelihood of the fake images.</p><p>GAN is also called likihood-free model. We cannot get the likelihoodof the fake images.</p><p>For GANs, we can get images through one forward pass, but forautoregressive model, we need to generate images pixel in a sequentialmanner.</p><p>The output of the discriminator is a scalar, the probability of theinput image being real. The output of the generator is an image.</p><p>Fix generator, the optimal discriminator is: <span class="math display">\[\begin{aligned}D_{\phi}^*(x) = \frac{p_{data}(x)}{p_{data}(x) + p_{G_{\theta}}(x)}\end{aligned}\]</span> where <span class="math inline">\(p_{G_{\theta}}(x)\)</span>is the distribution of the fake images. When <span class="math inline">\(p_{G_{\theta}}(x) = p_{data}(x)\)</span>, then<span class="math inline">\(D_{\phi}^*(x) = 0.5\)</span>. However, wedon't know the distribution of the fake images and the distribution ofthe real images.</p><p>inner-loop is the discriminator, outer-loop is the generator. why?focus more on the generator, because the discriminator is easy to train??</p><h2 id="reference">Reference</h2><p><a href="https://lrjconan.github.io/UBC-EECE571F-DL-Structures/">EECE571F (2023 Winter Term 1): Deep Learning with Structures</a></p><p>http://cs231n.stanford.edu/slides/2018/cs231n_2018_ds02.pdf</p><p><a href="https://jalammar.github.io/illustrated-transformer/" class="uri">https://jalammar.github.io/illustrated-transformer/</a></p><p><a href="https://uvagedl.github.io/">UvA - An Introduction to GroupEquivariant Deep Learning</a></p>]]></content>
      
      
      <categories>
          
          <category> Graph </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Deep Learning with Structures </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Basic knowledge</title>
      <link href="/2023/09/06/Basic%20knowledge/"/>
      <url>/2023/09/06/Basic%20knowledge/</url>
      
        <content type="html"><![CDATA[<h2 id="graph-convolutional-autoencoders-with-co-learning-of-graph-structure-and-node-attributes">Graphconvolutional autoencoders with co-learning of graph structure and nodeattributes</h2><p>In this paper, they design the special graph encoder and decoder forthe tasks undertaken by the graph autoencoders. The task of the encoderis to embed the nodes into a new space, and then the latentrepresentation of each node is close to its neighbors[the encoder is alow-pass filter]. The decoder restores the original space from theembedded space by making the latent representation of each node awayfrom its neighbors[the decoder is a high-pass filter].</p><p>In this paper, they encode both the graph structure and the nodeattributes in the latent space with an improved GCN, which is a<strong>completely low-pass graph filter</strong>. Then, to reconstructthe node attributes X , they design a new <strong>high-pass graphdecoder</strong>. At the same time, we use the inner product layer toreconstruct the graph structure information. Last, the graph encoder andtwo sub-decoders are jointly optimized in a unified framework in such away that each can be beneficial to the other and finally lead to abetter graph embedding.</p><h3 id="normalized-adjacency-matrix-and-laplacian-matrices">Normalizedadjacency matrix and Laplacian matrices</h3><p>1, The normalized adjacency matrix is defined as: <span class="math display">\[\hat{A} = D^{-1/2}A D^{-1/2},\]</span> where<span class="math inline">\(A\)</span> is the adjacency matrix of graph<span class="math inline">\(G\)</span>. <span class="math inline">\(D =diag(d)\)</span>, <span class="math inline">\(d(i)\)</span> is thedegree of node <span class="math inline">\(i\)</span>.</p><p>2, The normalized Laplacian matrix is defined as: <span class="math display">\[L_s = I - \hat{A} = I - D^{-1/2}AD^{-1/2}.\]</span> Note that <span class="math inline">\(L_s = I -\hat{A} = D^{-1/2}(D - A) D^{-1/2} = D^{-1/2}L D^{-1/2}\)</span>, where<span class="math inline">\(L = D - A\)</span> is the unnormalizedLaplacian matrix of graph <span class="math inline">\(G\)</span>.</p><p>For the largest eigenvalue <span class="math inline">\(\lambda^s\)</span> of <span class="math inline">\(A\)</span> and the maximum degree <span class="math inline">\(\Delta\)</span> of a node in a graph, we have $d_{avg} ^s $. Normalizing the adjacency matrix can make its largesteigenvalue 1.</p><p>3, Let <span class="math inline">\(\alpha_1 \geq \alpha_2 \geq ...\geq \alpha_n\)</span> be the eigenvalues of <span class="math inline">\(\hat{A}\)</span>, <span class="math inline">\(\lambda^s_1 \leq \lambda^s_2 \leq ... \leq\lambda^s_n\)</span> be the eigenvalues of <span class="math inline">\(L_s\)</span>, then <span class="math display">\[ 1= \alpha_1 \geq ... \geq \alpha_n \geq -1, \quad 0=\lambda^s_1 \leq ...\leq \lambda^s_n \leq 2.\]</span></p><h3 id="graph-convolutional-networks">Graph convolutional networks</h3><p>GCN generalizes the convolutional neural networks on non-Euclideandomains. It uses the first-order approximation of Chebyshev polynomials:<span class="math display">\[g_{\theta} \star x \approx \theta (I_N + D^{-1/2}AD^{-1/2})X.\]</span> The spectral radius of <span class="math inline">\((I_N +D^{-1/2}AD^{-1/2})\)</span> is 2, and repeated application of thisoperator will cause numerical instabilities. To solve this problem, GCNuses a renormalization trick by adding a self-loop to each node, whichis equivalent to adding the identity matrix <span class="math inline">\(I_N\)</span> to the adjacency matrix <span class="math inline">\(A\)</span>: <span class="math inline">\(\tilde{A}= A + I\)</span>, the associated degree matrix <span class="math inline">\(\tilde{D} = D + I\)</span>. The new symmetricallynormalized matrix is <span class="math inline">\(\tilde{A}_{GCN} =\tilde{D}^{-1/2} \tilde{A} \tilde{D}^{-1/2}\)</span>. The one-layer GCNis <span class="math display">\[Z^{(m+1)} = \sigma(\tilde{A}_{GCN}Z^{(m)}W^{(m)}),\]</span> where <span class="math inline">\(Z^{(m)}\)</span> is thelatent representation matrix learned by the <span class="math inline">\(m\)</span>-th layer, <span class="math inline">\(Z^{(0)} = X\)</span>.</p><h3 id="graph-signal-processing">Graph signal processing</h3><p>In graph signal processing , the eigenvalues and eigenvectors of thegraph Laplacian correspond to the frequencies and Fourier basis.</p><p>The graph laplacian is defined as <span class="math inline">\(L =D-A\)</span>. By eigen-decomposition, <span class="math inline">\(L = U\Lambda U^{-1}\)</span>, where <span class="math inline">\(\Lambda =diag(\lambda_1, ..., \lambda_n)\)</span>, <span class="math inline">\(U= (u_1, u_2, ..., u_n)\)</span>. The eigenvalues <span class="math inline">\(\lambda_i, i \in [n]\)</span> can be considered tobe frequencies, and the associated eigenvectors <span class="math inline">\(u_i, i \in [n]\)</span> can be considered to be aFourier basis.</p><p>A graph signal <span class="math inline">\(f\)</span> can bedecomposed into a linear combination of basis signals <span class="math inline">\(u_i\)</span>: <span class="math display">\[f = Uc = \sum_{i=1}^n c_i u_i,\]</span> where <span class="math inline">\(c = (c_1, ...,c_n)^T\)</span>, <span class="math inline">\(c_i\)</span> is thecoefficient of <span class="math inline">\(u_i\)</span>, the magnitudeof <span class="math inline">\(c_i\)</span> represents the importance of<span class="math inline">\(u_i\)</span> in <span class="math inline">\(f\)</span>.</p><p>The smoothness of the basis signal <span class="math inline">\(u_i\)</span> is measured by the correspondingeigenvalue <span class="math inline">\(\lambda_i\)</span>. The smallerthe eigenvalue <span class="math inline">\(\lambda_i\)</span>, thesmoother the basis signal <span class="math inline">\(u_i\)</span>.<span class="math display">\[\sum_{e_{j,k} \in E} a_{j,k}[u_i(j) - u_i(k)]^2 = u_i^T L u_i =\lambda_i u_i^T u_i = \lambda_i.\]</span></p><p>The basic idea of graph filtering is to design a proper graph filterto produce the required signals for the downstream tasks. A graph filteris a function that takes a graph signal as input and <strong>outputs anew signal</strong>. A linear graph filter can be represented as amatrix <span class="math inline">\(G \in \mathbb{R}^{N \timesN}\)</span>, which is defined as <span class="math display">\[G = U p(\Lambda) U^{-1},\]</span> where <span class="math inline">\(p(\Lambda) =diag(p(\lambda_1), ..., p(\lambda_n))\)</span>. <span class="math inline">\(p(\cdot)\)</span> is the frequency responsefunction.</p><p>The output signal can be written as <span class="math display">\[y = Gf = U p(\Lambda) U^{-1} Uc = U p(\Lambda) c = \sum_{i=1}^np(\lambda_i) c_i u_i.\]</span></p><p>Definition 1 (completely low-pass graph filter). A completelylow-pass graph filter is a graph filter whose frequency responsefunction <span class="math inline">\(p(\cdot): \mathbb{R} \to\mathbb{R}^{+}\)</span> is a decreasing function with <span class="math inline">\(\lambda\)</span>.</p><ul><li>According to definition 1, the completely low-pass graph filterobtains a smooth graph output signal <span class="math inline">\(y\)</span> that consists of mostly low-frequencybasis signals, and as a result, the latent representation of each nodeis close to its neighbors.</li></ul><p>Definition 2 (completely high-pass graph filter). A completelyhigh-pass graph filter is a graph filter whose frequency responsefunction <span class="math inline">\(p(\cdot): \mathbb{R} \to\mathbb{R}^{+}\)</span> is an increasing function with <span class="math inline">\(\lambda\)</span>.</p><p>According to definition 2, the completely high-pass graph filterobtains an unsmooth graph output signal <span class="math inline">\(y\)</span> that consists of mostly high-frequencybasis signals, which makes the latent representation of each node faraway from its neighbors.</p><p>For GCN, the graph filter of GCN is <span class="math display">\[\tilde{A}_{GCN} = \tilde{D}^{-1/2} \tilde{A} \tilde{D}^{-1/2} = I - L_s= U (I - \Lambda^s) U^{-1}.\]</span> The frequency response function of GCN is <span class="math inline">\(p(\lambda^s_i) = 1 - \lambda_i^s\)</span>. Sincethe range of <span class="math inline">\(\lambda_i^s\)</span> is <span class="math inline">\([0, 2]\)</span>, the frequency response functionof GCN is a decreasing function with <span class="math inline">\(\lambda_i^s\)</span>. GCN is completely low-passgraph filter when <span class="math inline">\(\lambda_i^s \in [0,1]\)</span>, but not in <span class="math inline">\([1, 2]\)</span>.When <span class="math inline">\(\lambda_i^s \in [1, 2]\)</span>, <span class="math inline">\(p(\lambda^s_i)\)</span> will take a negative valuethat will introduce noise and undermine the performance. Thus, GCN isnot a completely low-pass graph filter.</p><h3 id="reference">Reference</h3><p>Jie Wang, Jiye Liang, Kaixuan Yao, Jianqing Liang, Dianhui Wang,Graphconvolutional autoencoders with co-learning of graph structure and nodeattributes,Pattern Recognition,Volume 121,2022,108215,ISSN 0031-3203, <a href="https://doi.org/10.1016/j.patcog.2021.108215" class="uri">https://doi.org/10.1016/j.patcog.2021.108215</a>.</p><p><a href="https://people.orie.cornell.edu/dpw/orie6334/Fall2016/lecture7.pdf" class="uri">https://people.orie.cornell.edu/dpw/orie6334/Fall2016/lecture7.pdf</a></p><h2 id="the-difference-between-adam-and-adamw">The difference betweenAdam and AdamW</h2><p><a href="https://towardsdatascience.com/why-adamw-matters-736223f31b5d" class="uri">https://towardsdatascience.com/why-adamw-matters-736223f31b5d</a></p><h2 id="why-regularization-can-reduce-overfitting">Why regularizationcan reduce overfitting?</h2><p><a href="http://neuralnetworksanddeeplearning.com/chap3.html#regularization" class="uri">http://neuralnetworksanddeeplearning.com/chap3.html#regularization</a></p><h2 id="cosine-decay-schedule-with-warm-up-period">Cosine decay schedulewith warm up period</h2><p>Loshchilov and Hutter, SGDR: Stochastic Gradient Descent with WarmRestarts. ICLR 2017. <a href="https://arxiv.org/abs/1608.03983" class="uri">https://arxiv.org/abs/1608.03983</a></p><p><a href="https://scorrea92.medium.com/cosine-learning-rate-decay-e8b50aa455b" class="uri">https://scorrea92.medium.com/cosine-learning-rate-decay-e8b50aa455b</a></p>]]></content>
      
      
      <categories>
          
          <category> Graph </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Graph </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Kolmogorov-Smirnov statistic</title>
      <link href="/2023/08/27/KS_test/"/>
      <url>/2023/08/27/KS_test/</url>
      
        <content type="html"><![CDATA[<h1 id="kolmogorov-smirnov-statistic">Kolmogorov-Smirnov statistic</h1><p>Consider any distribution <span class="math inline">\(D\)</span> on<span class="math inline">\(\mathbb{R}\)</span>, and CDF <span class="math inline">\(F(t) = \mathbb{P}_{x \sim D}(x \leqt)\)</span>.</p><p>Let <span class="math inline">\(X = (x_j)_{i \in [n]}\)</span> be<span class="math inline">\(n\)</span> samples drawn from <span class="math inline">\(D\)</span>.</p><p>Def: The empirical CDF (eCDF) of <span class="math inline">\(X\)</span> is defined as <span class="math inline">\(\hat{F}_n(t) = \frac{1}{n} \sum_{j=1}^n\mathbb{1}_{x_j \leq t}\)</span>.</p><p>Def: The Kolmogorov-Smirnov statistic is defined as <span class="math inline">\(D_n = \sup_{t \in \mathbb{R}} \{|\hat{F}_n(t) -F(t)|\}\)</span>.</p><p>Theorem [DKW, 1956]: <span class="math inline">\(\mathbb{P}(D_n &gt;\epsilon) \leq c e^{-2n\epsilon^2}\)</span>, where <span class="math inline">\(c\)</span> is a constant.</p><p>Theorem [Massart, 1990]: <span class="math inline">\(\mathbb{P}(D_n&gt; \epsilon) \leq 2 e^{-2n\epsilon^2}\)</span>.</p><p>Theorem [Harvey, 2020]: <span class="math inline">\(\mathbb{P}(D_n&gt; \epsilon) \leq \frac{4}{\epsilon}e^{-\frac{1}{2}n\epsilon^2}\)</span>.</p>]]></content>
      
      
      <categories>
          
          <category> Statistic </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Statistic </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Q&amp;A</title>
      <link href="/2023/08/27/Q&amp;A/"/>
      <url>/2023/08/27/Q&amp;A/</url>
      
        <content type="html"><![CDATA[<h1 id="排序">排序</h1><p>Q 1, Why does sorting a collection have (at least) <span class="math inline">\(O(n log n)\)</span> complexity? n is the length ofthe collection.</p><p>ans: <span class="math inline">\(O(n log n)\)</span> is the optimalvalue for a comparison sort.[https://theartofmachinery.com/2019/01/05/sorting_is_nlogn.html].</p><p>Q 2, Quicksort, Heapsort and Mergesort. What are the differencesbetween them? What are the time complexities of them?</p><p>ans: [https://www.geeksforgeeks.org/quick-sort-vs-merge-sort/]</p><p>[https://www.interviewcake.com/concept/java/heapsort#:~:text=Heapsort%20is%20similar%20to%20selection,into%20a%20max%20heap%20%E2%86%B4]</p><p>The worst case complexity of quick sort is <span class="math inline">\(O(n^2)\)</span> as there is need of lot ofcomparisons in the worst condition. whereas In merge sort, worst caseand average case has same complexities <span class="math inline">\(O(nlog n)\)</span>. Heap sort runs in <span class="math inline">\(O(n logn)\)</span> time.</p><p>Space efficient. Heap sort takes <span class="math inline">\(O(1)\)</span> extra space. Merge sort requires atemporary array <span class="math inline">\(O(n)\)</span>.</p>]]></content>
      
      
      <categories>
          
          <category> Basic </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Itô Calculus and Stochastic Differential Equations</title>
      <link href="/2023/08/24/Heuristic%20Solutions%20of%20SDEs/"/>
      <url>/2023/08/24/Heuristic%20Solutions%20of%20SDEs/</url>
      
        <content type="html"><![CDATA[<h1 id="the-stochastic-integral-of-itô">The Stochastic Integral ofItô</h1><p>A stochastic differential equation can be transformed into a vectordifferential equation of the form <span class="math display">\[\begin{aligned}\frac{\text{d}x}{\text{d} t}  &amp;= f(x, t) + \mathbb{L}(X, t) w(t), \\\end{aligned}\]</span> where <span class="math inline">\(w(t)\)</span> is a whiteGaussian noise with zero mean. Since <span class="math inline">\(w(t)\)</span> is discontinuous, we can not use theordinary differential equation to solve the above equation. Fortunately,we can reduce the problem to definition of a new king of integral, thestochastic integral of Itô.</p><p>We can integrate the SDE from initial time <span class="math inline">\(t_0\)</span> to final time <span class="math inline">\(t\)</span>: <span class="math display">\[\begin{aligned}x(t) - x(t_0) &amp;= \int_{t_0}^{t} f(x, t) \text{d} t + \int_{t_0}^{t}\mathbb{L}(x, t) w(t) \text{d} t. \\\end{aligned}\]</span> The first integral with respect to time on the right-hand sidecan be solved by Riemann integral or Lebesgue integral. The secondintegral is the problem we need to solve. We will first discuss thereason why we can not use the Riemann integral, Lebesgue integral andStieltjes integral to solve the second integral.</p><p>First, it cannot be solved by Riemann integral. The Riemann integralis defined as <span class="math display">\[\int_{t_0}^{t} \mathbb{L}(X, t) w(t) \text{d} t = \lim_{n \to \infty}\sum_{k=1}^{n} \mathbb{L}(x(t_k^{*}), t_k^{*}) w(t_k^{*}) (t_{k+1} -t_k),\]</span> where <span class="math inline">\(t_0 &lt; t_1 &lt; ...&lt;t_n= t\)</span>, and <span class="math inline">\(t_k^{*} \in [t_k,t_{k+1}]\)</span>. In Riemann integral, the upper and lower bounds ofthe integral are defined as the selections of <span class="math inline">\(t_k^*\)</span> such that the integral is maximizedand minimized. If the upper bound and lower bound converge to the samevalue, the Riemann integral exists. However, the white Gaussian noise isdiscontinuous and not bounded, it can take arbitrarily small and largevalues at every finite interval, so the upper and lower bounds of theintegral are not convergent. Therefore, the Riemann integral does notexist.</p><p>For Stieltjes integral, we need to interpret the increment <span class="math inline">\(w(t) \text{d}t\)</span> as an increment of anotherprocess <span class="math inline">\(\beta(t)\)</span>, thus the intergalbecomes <span class="math display">\[\int_{t_0}^{t} \mathbb{L}(x(t), t) w(t) \text{d} t = \int_{t_0}^{t}\mathbb{L}(x(t), t) \text{d} \beta(t).\]</span> Here <span class="math inline">\(\beta(t)\)</span> is aBrownian motion. Brownian motion is a continuous process. However, theBrownian motion is not differentiable, so the Stieltjes integral doesnot converge.</p><p>Both Stieltjes and Lebesgue integrals are defined as limits of theform <span class="math display">\[\int_{t_0}^{t} \mathbb{L}(x(t), t) \text{d} \beta = \lim_{n \to \infty}\sum_{k=1}^{n} \mathbb{L}(x(t_k^{*}), t_k^{*}) (\beta(t_{k+1}) -\beta(t_k)),\]</span> where <span class="math inline">\(t_0 &lt; t_1 &lt; ...&lt;t_n= t\)</span>, and <span class="math inline">\(t_k^{*} \in [t_k,t_{k+1}]\)</span>. Both of these definitions would require the limit tobe independent of the position on the interval <span class="math inline">\(t_k^{*} \in [t_k, t_{k+1}]\)</span>. However, inthis case, the limit is not independent of the position on the interval<span class="math inline">\(t_k^{*} \in [t_k, t_{k+1}]\)</span>, so theStieltjes and Lebesgue integrals do not exist.</p><h2 id="definition-of-the-stochastic-integral-of-itô">Definition of theStochastic Integral of Itô</h2><p>For Itô integral, it fixed the choice of <span class="math inline">\(t_k^{*}\)</span> to be <span class="math inline">\(t_k\)</span>, thus the limit becomes unique. TheItô integral is defined as <span class="math display">\[\int_{t_0}^{t} \mathbb{L}(x(t), t) \text{d} \beta = \lim_{n \to \infty}\sum_{k=1}^{n} \mathbb{L}(x(t_k), t_k) (\beta(t_{k+1}) - \beta(t_k)).\]</span></p><p>The SDE can be defined to be the Itô integral of the form <span class="math display">\[\begin{aligned}x(t) - x(t_0) &amp;= \int_{t_0}^{t} f(x(t), t) \text{d} t +\int_{t_0}^{t} \mathbb{L}(x, t) \text{d} \beta(t).\end{aligned}\]</span></p><p>The differential form is <span class="math display">\[\begin{aligned}\text{d} x &amp;= f(x, t) \text{d} t + \mathbb{L}(x, t) \text{d}\beta(t).\end{aligned}\]</span> or <span class="math display">\[\begin{aligned}\frac{\text{d} x}{\text{d} t } &amp;= f(x, t) + \mathbb{L}(x, t) \frac{\text{d} \beta(t)}{\text{d} t}.\end{aligned}\]</span></p><ul><li>Why don't we consider more general SDEs of the form <span class="math display">\[\begin{aligned}\frac{\text{d} x}{\text{d} t } &amp;= f(x(t), w(t), t),\end{aligned}\]</span> where the white noise <span class="math inline">\(w(t)\)</span> enters the system through anonlinear transformation. We can not rewrite this equation as astochastic integral with respect to a Brownian motion, and thus wecannot define the mathematical meaning of this equation.</li></ul><h2 id="itô-formula">Itô Formula</h2><p>Consider the stochastic integral <span class="math display">\[\int_{t_0}^{t} \beta(t) \text{d} \beta(t),\]</span> where <span class="math inline">\(\beta(t)\)</span> is astandard Brownian motion with zero mean and diffusion constant <span class="math inline">\(q = 1\)</span>. Based on the definition of the Itôintegral, we have <span class="math display">\[\begin{aligned}\int_{t_0}^{t} \beta(t) \text{d} \beta(t) &amp;= \lim_{n \to \infty}\sum_{k=1}^{n} \beta(t_k) (\beta(t_{k+1}) - \beta(t_k)) \\&amp;= \lim_{n \to \infty} \sum_{k=1}^{n} [-\frac{1}{2}(\beta(t_{k+1}) -\beta(t_k))^2 +　\frac{1}{2}(\beta^2(t_{k+1}) - \beta^2(t_k))]\\&amp;= -\frac{1}{2}t + \frac{1}{2}\beta^2(t).\end{aligned}\]</span> where <span class="math inline">\(0 = t_0 &lt; t_1 &lt; ...&lt; t_n = t\)</span> and <span class="math inline">\(\lim_{n \to\infty} \sum_{k=1}^{n} (\beta(t_{k+1}) - \beta(t_k))^2\)</span>. That isbecause <span class="math inline">\(\beta(t_{k+1}) - \beta(t_k) \simN(0, t_{k+1} - t_k) \sim N(0, \frac{t}{n})\)</span>.</p><p>So the Itô differential of <span class="math inline">\(\beta^2(t)/2\)</span> is <span class="math display">\[\begin{aligned}\text{d} \frac{\beta^2(t)}{2} &amp;= \beta(t) \text{d}\beta(t) +\frac{1}{2} \text{d}t.\end{aligned}\]</span> It is not the same as the ordinary differential of <span class="math inline">\(\beta^2(t)/2\)</span>: <span class="math display">\[\begin{aligned}\frac{\text{d} \beta^2(t)}{2} &amp;= \beta(t) \text{d}\beta(t).\end{aligned}\]</span> That is because the Itô integral fixes the choice of <span class="math inline">\(t_k^{*}\)</span> to be <span class="math inline">\(t_k\)</span>.</p><p>Theorem Itô formula: Let <span class="math inline">\(x(t)\)</span> bean Itô process(note: <span class="math inline">\(x(t)\)</span> is avector process) which is the solution of an SDE of the form <span class="math display">\[\begin{aligned}\text{d} x &amp;= f(x, t) \text{d} t + \mathbb{L}(x, t) \text{d}\beta(t),\end{aligned}\]</span> where <span class="math inline">\(\beta(t)\)</span> is aBrownian motion. Consider an arbitrary <strong>scalar</strong> function<span class="math inline">\(\phi(x(t), t)\)</span> of the process, theItô SDE of <span class="math inline">\(\phi\)</span> is <span class="math display">\[\begin{aligned}\text{d} \phi &amp;= \frac{\partial \phi}{\partial t} \text{d} t +\sum_{i}\frac{\partial \phi}{\partial x_i} \text{d} x_i + \frac{1}{2}\sum_{i,j}\frac{\partial^2 \phi}{\partial x_i \partial x_j} \text{d} x_i\text{d} x_j \\&amp;= \frac{\partial \phi}{\partial t} \text{d} t + (\nabla \phi)^T\cdot \text{d} x + \frac{1}{2} tr\{ \nabla \nabla^T \phi\} \text{d} x\text{d} x^T\end{aligned}\]</span> provided that the required partial derivatives exist, wherethe mixed partial derivatives are combined according to the rules <span class="math display">\[\begin{aligned}\text{d} \beta \text{d} t &amp;= 0, \\\text{d} t \text{d} \beta &amp;= 0, \\\text{d} \beta \text{d} \beta^T &amp;= Q \text{d} t.\end{aligned}\]</span> (Q is the diffusion matrix(covariance matrix) of the Brownianmotion). It can be derived from the Taylor expansion of <span class="math inline">\(\phi(x(t), t)\)</span>. Usually, in deterministiccase, we could ignore the second-order, we have <span class="math display">\[\begin{aligned}\text{d} \phi &amp;= \frac{\partial \phi}{\partial t} \text{d} t +\frac{\partial \phi}{\partial x} \text{d} x.\end{aligned}\]</span> In stochastic case, because <span class="math inline">\(\text{d} \beta \text{d} \beta^T = Q \text{d}t\)</span>, which is order one, the <span class="math inline">\(\text{d}x \text{d} x^T\)</span> is potentially of order one, so we need toconsider the second-order term.</p><ul><li>Here the Itô formula is derived for a scalar function <span class="math inline">\(\phi(x(t), t)\)</span>. However, for vectorfunction, it works for each of the components of a vector-valuedfunction separately and thus also includes the vector case.</li></ul><p>Example: We can apply the Itô formula to the function <span class="math inline">\(\phi(x(t), t) = x^2(t)/2\)</span>, with <span class="math inline">\(x(t) = \beta(t)\)</span>, where <span class="math inline">\(\beta(t)\)</span> is a standard Brownian motion(q=1). The Itô SDE of <span class="math inline">\(\phi\)</span> is <span class="math display">\[\begin{aligned}\text{d} \phi &amp;= \beta \text{d} \beta + \frac{1}{2} \text{d} \beta\text{d} \beta \\&amp;= \beta \text{d} \beta +\frac{1}{2} \text{d} t.\end{aligned}\]</span></p><h1 id="reference">Reference</h1><p>Simo Särkkä and Arno Solin (2019). Applied Stochastic DifferentialEquations. Cambridge University Press.</p>]]></content>
      
      
      <categories>
          
          <category> Generative Model </category>
          
      </categories>
      
      
        <tags>
            
            <tag> SDE </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Gaussian processes</title>
      <link href="/2023/08/06/gaussianprocess/"/>
      <url>/2023/08/06/gaussianprocess/</url>
      
        <content type="html"><![CDATA[<h1 id="jointly-gaussian-random-variables">Jointly Gaussian randomvariables</h1><p>Definition: Random variables (RV) <span class="math inline">\(X_1,..., X_n\)</span> are jointly Gaussian if any linear combination of themis Gaussian.</p><p>RV <span class="math inline">\(X = [X_1, ..., X_n]^{T}\)</span> isGaussian <span class="math inline">\(\leftrightarrows\)</span> Given anyscalars <span class="math inline">\(a_1,... a_n\)</span>, the RV <span class="math inline">\(Y = a_1 X_1 + a_2 X_2 + ... + a_n X_n\)</span> isGaussian distributed.</p><h2 id="pdf-of-jointly-gaussian-rvs-in-n-dimensions">Pdf of jointlyGaussian RVs in n dimensions</h2><p>Let <span class="math inline">\(X \in \mathbb{R}^n\)</span>, <span class="math inline">\(\mu = \mathbb{E}[X]\)</span>,</p><p>covariance matrix <span class="math display">\[C:= \mathbb{E}[(X -\mu)(X - \mu)^T] =\begin{pmatrix}    \sigma_{11}^2 &amp; \sigma_{12}^2 &amp; \cdots &amp; \sigma_{1n}^2\\    \sigma_{21}^2 &amp; \sigma_{22}^2 &amp; \cdots &amp; \sigma_{2n}^2\\    \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\    \sigma_{n1}^2 &amp; \sigma_{n2}^2 &amp; \cdots &amp; \sigma_{nn}^2\end{pmatrix}\]</span> Then, the pdf of RV <span class="math inline">\(X\)</span> canbe defined as <span class="math display">\[p(x) = \frac{1}{(2\pi)^{n/2} \text{det}^{1/2}(C)} \exp(-\frac{1}{2}(x -\mu)^T C^{-1} (x - \mu)).\]</span></p><ul><li><span class="math inline">\(C\)</span> is invertible</li><li>We can verify all linear combinations is Gaussian.</li><li>To fully specify the probability distribution of a Gaussian vector<span class="math inline">\(X\)</span>, the mean vector <span class="math inline">\(\mu\)</span> and covariance matrix <span class="math inline">\(C\)</span> suffice.</li></ul><h1 id="gaussian-processes">Gaussian processes</h1><p>Gaussian processes (GP) generalize Gaussian vectors to<strong>infinite</strong> dimensions.</p><p>Definition. <span class="math inline">\(X(t)\)</span> is a GP if anylinear combination of values <span class="math inline">\(X(t)\)</span>is Gaussian. That is, for arbitrary <span class="math inline">\(n &gt;0\)</span>, times <span class="math inline">\(t_1, ..., t_n\)</span> andconstants <span class="math inline">\(a_1, ..., a_n\)</span>, <span class="math inline">\(Y = a_1 X(t_1) + a_2 X(t_2) + ... + a_nX(t_n)\)</span> is Gaussian distributed.</p><ul><li><p>Time index <span class="math inline">\(t\)</span> can becontinuous or discrete.</p></li><li><p>Any linear functional of <span class="math inline">\(X(t)\)</span> is Gaussian distributed. Forexample, the integral <span class="math inline">\(Y = \int_{t_1}^{t_2}X(t) \text{d}t\)</span> is Gaussian distributed.</p></li></ul><h2 id="jointly-pdf-in-a-gaussian-process">Jointly pdf in a Gaussianprocess</h2><p>Consider times <span class="math inline">\(t_1,..., t_n\)</span>, themean value <span class="math inline">\(\mu(t_i)\)</span> is <span class="math display">\[\mu(t_i) = \mathbb{E}[X(t_i)].\]</span></p><p>The covariance between values at time <span class="math inline">\(t_i\)</span> and <span class="math inline">\(t_j\)</span> is <span class="math inline">\(C(t_i,t_j) := \mathbb{E}[(X(t_i) - \mu(t_i))(X(t_j) -\mu(t_j))^T]\)</span>.</p><p>The covariance matrix for values <span class="math inline">\(X(t_1),..., X(t_n)\)</span> is <span class="math display">\[C(t_1,..., t_n) =\begin{pmatrix}    C_{t_1, t_1} &amp; C_{t_1, t_2} &amp; \cdots &amp; C_{t_1, t_n}\\    C_{t_2, t_1} &amp; C_{t_2, t_2} &amp; \cdots &amp; C_{t_2, t_n} \\    \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\    C_{t_n, t_1} &amp; C_{t_n, t_2} &amp; \cdots &amp; C_{t_n, t_n}\end{pmatrix}\]</span>.</p><p>The jointly pdf of <span class="math inline">\(X(t_1),...,X(t_n)\)</span> is <span class="math inline">\(N([\mu(t_1), ...,\mu(t_n)]^T, C(t_1,..., t_n))\)</span>.</p><h2 id="mean-value-and-autocorrelation-functions">Mean value andautocorrelation functions</h2><p>To specify a Gaussian process, we only need to specify:</p><ul><li><p>Mean value function: <span class="math inline">\(\mu(t) =\mathbb{E}[x(t)]\)</span>.</p></li><li><p>Autocorrelation function (symmetric): <span class="math inline">\(R(t_1, t_2) = \mathbb{E}[X(t_1)X(t_2)]\)</span>.</p></li></ul><p>The autocovariance <span class="math inline">\(C(t_1, t_2) = R(t_1,t_2) - \mu(t_1) \mu (t_2)\)</span>.</p><p>More general, we consider GP with <span class="math inline">\(\mu(t)= 0\)</span>. [define new process <span class="math inline">\(Y(t) =X(t) - \mu(t)\)</span>]. In this case, <span class="math inline">\(C(t_1, t_2) = R(t_1, t_2)\)</span>.</p><p>All probs. in a GP can be expressed in terms of <span class="math inline">\(\mu(t)\)</span> and <span class="math inline">\(R(t, t)\)</span>. <span class="math display">\[p(x_t) = \frac{1}{\sqrt{2\pi (R(t,t) - \mu^2(t))}} \exp(- \frac{(x_t -\mu(t))^2}{2(R(t,t) - \mu^2(t))}).\]</span></p><h2 id="conditional-probabilities-in-a-gp">Conditional probabilities ina GP</h2><p>Consider a zero-mean GP <span class="math inline">\(X(t)\)</span>,two times <span class="math inline">\(t_1\)</span> and <span class="math inline">\(t_2\)</span>. The covariance matrix is <span class="math display">\[C = \begin{pmatrix}  R(t_1, t_1) &amp; R(t_1, t_2) \\  R(t_1, t_2) &amp; R(t_2, t_2)\end{pmatrix}\]</span></p><p>The jointly pdf of <span class="math inline">\(X(t_1)\)</span> and<span class="math inline">\(X(t_2)\)</span> is <span class="math display">\[p(x_{t_1}, x_{t_2}) = \frac{1}{2\pi \text{det}^{1/2} C}\exp(-\frac{1}{2}[x_{t_1}, x_{t_2}]^T C^{-1} [x_{t_1}, x_{t_2}])\]</span></p><p>The conditional pdf of <span class="math inline">\(X(t_1)\)</span>given <span class="math inline">\(X(t_2)\)</span> is <span class="math display">\[p_{X(t_1)| X(t_2)}(x_{t_1} | x_{t_2}) = \frac{p(x_{t_1},x_{t_2})}{p(x_{t_2})}. \qquad (1)\]</span></p><h1 id="brownian-motion-process-a.k.a-wiener-process">Brownian motionprocess (a.k.a Wiener process)</h1><p>Definition. A Brownian motion process (a.k.a Wiener process)satisfies</p><ol type="1"><li><p><span class="math inline">\(X(t)\)</span> is normally distributedwith zero mean and variance <span class="math inline">\(\sigma^2t\)</span>, <span class="math display">\[X(t) \sim N(0, \sigma^2t).\]</span></p></li><li><p>Independent increments. For all times <span class="math inline">\(0 &lt; t_1 &lt; t_2 &lt; \cdots &lt; t_n\)</span>,the random variables <span class="math inline">\(X(t_1), X(t_2) -X(t_1), ..., X(t_n) - X(t_{n-1})\)</span> are independent.</p></li><li><p>Stationary increments. Probability distribution of increment<span class="math inline">\(X(t+s) - X(s)\)</span> is the same asprobability distribution of <span class="math inline">\(X(t)\)</span>.<span class="math display">\[[X(t+s) - X(s)]  \sim N(0, \sigma^2t).\]</span></p></li></ol><ul><li>Brownian motion is a Markov process.</li><li>Brownian motion is a Gaussian process.</li></ul><h2 id="mean-and-autocorrelation-of-brownian-motion">Mean andautocorrelation of Brownian motion</h2><p>1, Mean funtion <span class="math inline">\(\mu(t) = \mathbb{E}[X(t)]= 0\)</span>.</p><p>2, Autocorrelation of Brownian motion <span class="math inline">\(R(t_1, t_2) = \sigma^2 \min\{t_1,t_2\}\)</span>.</p><p>Proof. Assume <span class="math inline">\(t_1 &lt; t_2\)</span>, thenautocorrelation <span class="math inline">\(R(t_1, t_2) =\mathbb{E}[X(t_1)X(t_2)] = \sigma^2 t_1\)</span>.</p><p>If <span class="math inline">\(t_1 &lt; t_2\)</span>, according toconditional expectations, we have <span class="math display">\[\begin{aligned}  R(t_1, t_2) = \mathbb{E}[X(t_1)X(t_2)] &amp;=\mathbb{E}_{X(t_1)}[\mathbb{E}_{X(t_2)}[X(t_1)X(t_2) | X(t_1)]] \\  &amp;= \mathbb{E}_{X(t_1)}[X(t_1)\mathbb{E}_{X(t_2)}[X(t_2) | X(t_1)]]\end{aligned}\]</span> According to equation (1), the condition distribution of <span class="math inline">\(X(t_2)\)</span> given <span class="math inline">\(X(t_1)\)</span> is <span class="math display">\[[X(t_2) | X(t_1)] \sim N(X(t_1), \sigma^2 (t_2 - t_1)),\]</span> thus, <span class="math inline">\(\mathbb{E}_{X(t_2)}[X(t_2) |X(t_1)] = X(t_1)\)</span>.</p><p><span class="math display">\[\begin{aligned}  R(t_1, t_2) = \mathbb{E}[X(t_1)X(t_2)] &amp;=\mathbb{E}_{X(t_1)}[\mathbb{E}_{X(t_2)}[X(t_1)X(t_2) | X(t_1)]] \\  &amp;= \mathbb{E}_{X(t_1)}[X(t_1) X(t_1)] \\  &amp;= \mathbb{E}_{X(t_1)}[X^2(t_1)] = \sigma^2 t_1.\end{aligned}\]</span></p><p>Similarly, if <span class="math inline">\(t_2 &lt; t_1\)</span>,<span class="math inline">\(R(t_1, t_2) = \sigma^2 t_2\)</span>.</p><h2 id="brownian-motion-with-drift-bmd">Brownian motion with drift(BMD)</h2><p>For Brownian motion, it is an unbiased random walk. Walker stepsright or left with the same probability <span class="math inline">\(1/2\)</span> for each direction (onedimension).</p><p>For BMD, it is a biased random walk. Walker steps right or left withdifferent probs.</p><p>For example, consider time interval <span class="math inline">\(h\)</span>, step size <span class="math inline">\(\sigma \sqrt{h}\)</span>, <span class="math display">\[p(X(t+h) = x + \sigma \sqrt{h} | X(t) = x) = \frac{1}{2} (1 +\frac{\mu}{\sigma} \sqrt{h}).\]</span> <span class="math display">\[p(X(t+h) = x - \sigma \sqrt{h} | X(t) = x) = \frac{1}{2} (1 -\frac{\mu}{\sigma} \sqrt{h}).\]</span></p><ul><li><p><span class="math inline">\(\mu &gt; 0\)</span>, biased to theright. <span class="math inline">\(\mu &lt; 0\)</span>, biased to theleft.</p></li><li><p><span class="math inline">\(h\)</span> needs to be small enoughto make <span class="math inline">\(|\frac{\mu}{\sigma} \sqrt{h} | \leq1\)</span>.</p></li></ul><p>In this BMD case, <span class="math inline">\(x(t) \sim N(\mu t,\sigma^2 t)\)</span>.</p><ul><li>Independent and stationary increments.</li></ul><p>(We omit the proof. More details can be found at <a href="https://www.hajim.rochester.edu/ece/sites/gmateos/ECE440/Slides/block_5_stationary_processes_part_a.pdf">Gaussianprocess</a> ).</p><h2 id="geometric-brownian-motion-gbm">Geometric Brownian motion(GBM)</h2><p>Definition. Suppose that <span class="math inline">\(Z(t)\)</span> isa standard Brownian motion <span class="math inline">\(Z(t) \sim N(0,t)\)</span>. Parameters <span class="math inline">\(\mu \in\mathbb{R}\)</span> and <span class="math inline">\(\sigma \in (0,\infty)\)</span>. Let <span class="math display">\[X(t) = \exp[(\mu - \frac{\sigma^2}{2})t + \sigma Z(t)], \qquad t \geq 0.\qquad (2)\]</span> The stochastic process <span class="math inline">\(\{X(t): t\geq 0\}\)</span> is geometric Brownian motion with drift parameter<span class="math inline">\(\mu\)</span> and volatility parameter <span class="math inline">\(\sigma\)</span>.</p><ul><li>The process is always positive, one of the reasons that geometricBrownian motion is used to model financial and other processes that<strong>cannot be negative</strong>.</li><li>For the stochastic process</li></ul><p><span class="math display">\[(\mu - \frac{\sigma^2}{2})t + \sigma Z(t) \sim N((\mu -\frac{\sigma^2}{2})t , \sigma^2 t),  \]</span></p><p>it is a BMD with drift parameter <span class="math inline">\(\mu -\sigma^2/2\)</span> and scale parameter <span class="math inline">\(\sigma\)</span>. Thus, the geometric Brownianmotion is just the exponential of this BMD process.</p><ul><li><p>Here <span class="math inline">\(X(0) = 1\)</span>, the processstarts at 1. For GBM starting at <span class="math inline">\(X(0) =x_0\)</span>, the process is <span class="math display">\[X(t) = x_0 \exp[(\mu - \frac{\sigma^2}{2})t + \sigma Z(t)], \qquad t\geq 0.\]</span></p></li><li><p>GBM is not a Gaussian process.</p></li></ul><p>From the definition of GBM (2), we can have the followingdifferential equation: <span class="math display">\[\begin{aligned}  \frac{\text{d}X}{\text{d} t} &amp;= \exp[(\mu - \frac{\sigma^2}{2})t +\sigma Z(t)][(\mu - \frac{\sigma^2}{2}) + \sigma\frac{\text{d}Z}{\text{d} t}] \\  &amp;= X [(\mu - \frac{\sigma^2}{2}) + \sigma\frac{\text{d}Z}{\text{d} t}] \\  &amp;= X \tilde{\mu} + \sigma X \frac{\text{d}Z}{\text{d} t},  \qquad(\tilde{\mu} := \mu - \frac{\sigma^2}{2})\end{aligned}\]</span> thus, Geometric Brownian motion <span class="math inline">\(X(t)\)</span> satisfies the stochasticdifferential equation <span class="math display">\[\begin{aligned}\frac{\text{d}X}{\text{d} t}  &amp;= X \tilde{\mu} + \sigma X\frac{\text{d}Z}{\text{d} t},  \\  \text{d}X &amp; = X \tilde{\mu} {\text{d} t} + \sigma X \text{d}Z.\end{aligned}\]</span></p><p>The second equation is the Black–Scholes model. In the Black–Scholesmodel, <span class="math inline">\(X(t)\)</span> is the stock price.</p><ul><li><a href="https://stats.libretexts.org/Bookshelves/Probability_Theory/Probability_Mathematical_Statistics_and_Stochastic_Processes_(Siegrist)/18%3A_Brownian_Motion/18.04%3A_Geometric_Brownian_Motion">GeometricBrownian motion</a></li></ul><h1 id="white-gaussian-process">White Gaussian process</h1><p>Definition. A white Gaussian noise (WGN) process <span class="math inline">\(W(t)\)</span> is a GP with</p><ol type="1"><li><p>zero mean: <span class="math inline">\(\mu(t) = \mathbb{E}[W(t)]= 0\)</span> for all <span class="math inline">\(t\)</span>.</p></li><li><p>Delta function antocorrelation: <span class="math inline">\(R(t_1, t_2) = \sigma^2 \delta(t_1 -t_2)\)</span>.</p></li></ol><p>Here the Dirac delta is often thought as a function that is 0everywhere and infinite at 0. <span class="math display">\[\delta(t) =\begin{cases}\infty, &amp; t=0 \\0, &amp; t\ne 0\end{cases}.\]</span> The Dirac delta is actually a distribution, a generalizationof functions, and it is defined through the integral of its product withan arbitrary function <span class="math inline">\(f(t)\)</span>. <span class="math display">\[\int_{a}^{b} f(t)\delta(t) \text{d} t=\begin{cases}f(0), &amp; a &lt; 0 &lt; b \\0, &amp; \text{otherwise}\end{cases}.\]</span></p><!-- since the autocorrelation function of W(t) is not really a function (it involves theDirac delta), WGN cannot model any real physical phenomena. Nonetheless, it is a convenientabstraction to generate processes that can model real physical phenomena. --><p>Properties of white Gaussian noise:</p><ol type="1"><li><p>For <span class="math inline">\(t_1 \ne t_2\)</span>, <span class="math inline">\(W(t_1)\)</span> and <span class="math inline">\(W(t_2)\)</span> are uncorrelated. <span class="math display">\[\mathbb{E}[W(t_1)W(t_2)] = R(t_1, t_2) = 0, \qquad t_1 \ne t_2.\]</span> This means <span class="math inline">\(W(t)\)</span> atdifferent times are independent.</p></li><li><p>WGN has infinite variance (large power). <span class="math display">\[\mathbb{E}[W^2(t)] = R(t, t) = \sigma^2 \delta(0) = \infty.\]</span></p></li></ol><ul><li>WGN is discontinuous almost everywhere.</li><li>WGN is unbounded and it takes arbitrary large positive and negativevalues at any finite interval.</li></ul><h2 id="white-gaussian-noise-and-brownian-motion">White Gaussian noiseand Brownian motion</h2><p>Remember that the Brownian motion is a solution to the differentialequation: <span class="math display">\[\frac{\text{d} X(t)}{\text{d}t} = W(t).\]</span> <strong>Why <span class="math inline">\(\frac{\text{d}X(t)}{\text{d}t}\)</span> is called white noise ?</strong></p><p>Proof. Assume <span class="math inline">\(X(t)\)</span> is theintegral of a WGN process <span class="math inline">\(W(t)\)</span>,i.e., <span class="math inline">\(X(t) = \int_{0}^{t} W(u) \text{d}u\)</span>.</p><p>Since integration is linear functional and <span class="math inline">\(W(t)\)</span> is a GP, <span class="math inline">\(X(t)\)</span> is also a GP.</p><p>A Gaussian process can be uniquely specified by its Mean valuefunction and Autocorrelation function.</p><ol type="1"><li>The mean function: <span class="math display">\[\mu(t) = \mathbb{E}[\int_{0}^{t} W(u) \text{d} u] = \int_{0}^{t}\mathbb{E} [W(u)] \text{d} u = 0.\]</span><br></li><li>The autocorrelation <span class="math inline">\(R_{X}(t_1,t_2)\)</span> with <span class="math inline">\(t_1 &lt; t_2\)</span>:<span class="math display">\[\begin{aligned}  R_{X}(t_1, t_2) &amp;= \mathbb{E}[(\int_{0}^{t_1} W(u_1) \text{d}u_1)(\int_{0}^{t_2} W(u_2) \text{d} u_2)] \\  &amp;= \mathbb{E}[\int_{0}^{t_1} \int_{0}^{t_2} W(u_1)  W(u_2)\text{d} u_1 \text{d} u_2] \\  &amp;= \int_{0}^{t_1} \int_{0}^{t_2} \mathbb{E}[W(u_1)  W(u_2)]\text{d} u_1 \text{d} u_2 \\  &amp;= \int_{0}^{t_1} \int_{0}^{t_2} \sigma^2 \delta(u_1 - u_2)\text{d} u_1 \text{d} u_2 \\  &amp;= \int_{0}^{t_1} \int_{0}^{t_1} \sigma^2 \delta(u_1 - u_2)\text{d} u_1 \text{d} u_2 + \int_{0}^{t_1} \int_{t_1}^{t_2} \sigma^2\delta(u_1 - u_2) \text{d} u_1 \text{d} u_2 \\  &amp;= \int_{0}^{t_1} \int_{0}^{t_1} \sigma^2 \delta(u_1 - u_2)\text{d} u_1 \text{d} u_2 + 0\\  &amp;= \int_{0}^{t_1} \sigma^2 \text{d} u_1 \\  &amp;= \sigma^2 t_1.\end{aligned}\]</span> If <span class="math inline">\(t_2 &lt; t_1\)</span>, we canobtain <span class="math inline">\(R_{X}(t_1, t_2) = \sigma^2t_2\)</span>. Thus, <span class="math inline">\(R_{X}(t_1, t_2) =\sigma^2 \min \{t_1, t_2\}\)</span>.</li></ol><p>The mean function and autocorrelation function are the same asBrownian motion!</p><p>Because a Gaussian process can be uniquely determined by its meanvalue function and autocorrelation function. We can conclude</p><ul><li>The integral of WGN is a Brownian motion process.</li><li>The derivative of Brownian motion is WGN.</li></ul><h1 id="reference">Reference</h1><p><a href="https://www.hajim.rochester.edu/ece/sites/gmateos/ECE440/Slides/block_5_stationary_processes_part_a.pdf">Gaussianprocess</a></p><p><a href="https://stats.libretexts.org/Bookshelves/Probability_Theory/Probability_Mathematical_Statistics_and_Stochastic_Processes_(Siegrist)/18%3A_Brownian_Motion/18.04%3A_Geometric_Brownian_Motion">GeometricBrownian motion</a></p><p><a href="http://www.columbia.edu/~ks20/FE-Notes/4700-07-Notes-GBM.pdf">GeometricBrownian motion</a></p><p><a href="https://www.seas.upenn.edu/~ese3030/homework/week_11/week_11_white_gaussian_noise.pdf">WhiteGaussian noise</a></p>]]></content>
      
      
      <categories>
          
          <category> Stochastic Process </category>
          
      </categories>
      
      
        <tags>
            
            <tag> SDE </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Ordinary Differential Equation</title>
      <link href="/2023/07/19/ODE/"/>
      <url>/2023/07/19/ODE/</url>
      
        <content type="html"><![CDATA[<h1 id="ordinary-differential-equation-ode">Ordinary DifferentialEquation (ODE)</h1><h2 id="the-defination-of-ode">The defination of ODE</h2><p>An ODE is an equation in which the unknown quantity is a function,and it also involves derivatives of the unknown function.</p><p>For example, the forced spring-mass system: <span class="math display">\[\frac{d^2 x(t)}{d t^2} + \gamma \frac{d x(t)}{dt} + v^2 x(t) =w(t).  \qquad (1)\]</span> In this equation:</p><ul><li><p><span class="math inline">\(v\)</span> and <span class="math inline">\(\gamma\)</span> are constants that determine theresonant angular velocity and damping of the spring.</p></li><li><p><span class="math inline">\(w(t)\)</span> is a given functionthat may or may not depend on time.</p></li><li><p>position variable <span class="math inline">\(x\)</span> iscalled dependent variable.</p></li><li><p>time <span class="math inline">\(t\)</span> is called independentvariable.</p></li><li><p>This equation is <strong>second order</strong>. It contains thesecond derivative and doesn't have higher-order terms.</p></li><li><p>This equation is <strong>linear</strong>. <span class="math inline">\(x(t)\)</span> is linearly. There is no terms like<span class="math inline">\(x^2(t), \log x(t)\)</span>...</p></li><li><p>This equation is <strong>inhomogeneous</strong>. Because itcontains forcing term <span class="math inline">\(w(t)\)</span>.</p></li></ul><h2 id="the-solution-to-ode">The solution to ODE</h2><p>It can be divided to two categories: - particular solution: afunction that satisfies the differential equation and does not containany arbitrary constants.</p><ul><li>general solution: a function that satisfies the differentialequation and contains free constants.</li></ul><p>To exactly solve the differential equation, it is necessary tocombine the general solution with some initial conditions (i.e., <span class="math inline">\(x(t_0)\)</span>, <span class="math inline">\(\frac{d x(t)}{dt} |_{t_0}\)</span>) or some other(boundary) conditions of the differential equation.</p><h2 id="different-formulation-of-ode">Different formulation of ODE</h2><p>It is common to omit the time <span class="math inline">\(t\)</span>,so equation (1) can also be writen is this form: <span class="math display">\[\frac{d^2 x}{d t^2} + \gamma \frac{d x}{dt} + v^2 x = w.  \]</span></p><p>Sometimes, time derivatives are also denoted with dots over thevariable, for example:</p><p><span class="math display">\[\ddot{x} + \gamma \dot x + v^2 x = w.  \qquad (2)\]</span></p><h2 id="state-space-form-of-the-differential-equation-first-order-vector-differential-equation">State-spaceform of the differential equation (first-order vector differentialequation)</h2><ul><li><p><strong>order</strong>: the order of a differential equation isthe order of the highest derivative that appears in theequation.</p></li><li><p>Order <span class="math inline">\(N\)</span> ODE can convert toOrder 1 vector ODE i.e., if we define a state variable <span class="math inline">\(\vec{x} = (x_1 = x, x_2 = \dot{x})\)</span>, thenwe can convert equation（2） to <span class="math display">\[\begin{pmatrix}\frac{d x_1 (t)}{dt} \\\frac{d x_2 (t)}{dt}\end{pmatrix}=\begin{pmatrix}0 &amp; 1 \\-v^2 &amp; -\gamma\end{pmatrix}\begin{pmatrix}x_1 (t) \\x_2 (t)\end{pmatrix} +\begin{pmatrix}0 \\1\end{pmatrix}w(t) \qquad (3)\]</span></p></li></ul><p>Define <span class="math inline">\(\frac{d \vec{x}(t)}{dt} =\begin{pmatrix}\frac{d x_1 (t)}{dt} \\\frac{d x_2 (t)}{dt}\end{pmatrix},\)</span> <span class="math inline">\(f(\vec{x}(t)) =\begin{pmatrix}0 &amp; 1 \\-v^2 &amp; -\gamma\end{pmatrix}\begin{pmatrix}x_1 (t) \\x_2 (t)\end{pmatrix}\)</span> and <span class="math inline">\(\mathbf{L} =\begin{pmatrix}0 \\1\end{pmatrix}.\)</span></p><p>Equation (3) can be seen to be a special case of this form: <span class="math display">\[\frac{d \vec{x}(t)}{dt} = f(\vec{x}(t), t) + \mathbf{L}(\vec{x}(t),t)\vec{w}(t)\]</span> where the vector-valued function <span class="math inline">\(\vec{x}(t) \in R^D\)</span> is the state of thesystem. <span class="math inline">\(f(\cdot, \cdot)\)</span> and <span class="math inline">\(\mathbf{L}(\cdot, \cdot)\)</span> are arbitraryfunctions. <span class="math inline">\(\vec{w}(t)\)</span> is some(vector-valued) forcing function, driving function, or input to thesystem.</p><ul><li><p>The first-order vector differential equation representation of an<span class="math inline">\(n\)</span>th-order differential equation isoften called the state-space form of the differential equation.</p></li><li><p>The theory and solution methods for first-order vectordifferential equations are easier to analyze.</p></li><li><p>And, <span class="math inline">\(n\)</span> th order differentialequations can (almost) always be converted into equivalent <span class="math inline">\(n\)</span>-dimensional vector-valued first-orderdifferential equations.</p></li></ul><h2 id="linear-odes">Linear ODEs</h2><p>Equation (3) is also a special case of the <strong>lineardifferential equations</strong>: <span class="math display">\[\frac{d \vec{x}(t)}{dt} = \mathbf{F}(t) \vec{x}(t) + \mathbf{L}\vec{w}(t).\]</span> where <span class="math inline">\(\mathbf{F}(t)\)</span> is amatrix-valued function of time. <span class="math inline">\(\mathbf{L}\)</span> is a matrix. <span class="math inline">\(\vec{w}(t)\)</span> is a vector-valued function oftime.</p><ul><li><strong>homogeneous</strong>: the equation is homogeneous if theforcing function <span class="math inline">\(\vec{w}(t)\)</span> is zerofor all <span class="math inline">\(t\)</span>.</li><li><strong>time-invariant</strong>: the equation is time-invariant if<span class="math inline">\(\mathbf{F}(t)\)</span> is constant for all<span class="math inline">\(t\)</span>.</li></ul><p>In the following sections, we first start with the simple scalarlinear time-invariant homogeneous differential equation with fixedinitial condition at <span class="math inline">\(t = 0\)</span>. Then,we will consider the multidimensional generalization of this equation.Besides, we also consider the linear time-invariant inhomogeneousdifferential equations. Finally, we will consider more generaldifferential equations.</p><h4 id="solutions-of-linear-time-invariant-differential-equations">Solutionsof Linear Time-Invariant Differential Equations</h4><p>Consider the <strong>scalar</strong> linear<strong>homogeneous</strong> differential equation with fixed initialcondition at <span class="math inline">\(t = 0\)</span>: <span class="math display">\[\frac{d x(t)}{dt} = F x(t),  x(0) = \text{given}, \qquad (4)\]</span> where <span class="math inline">\(F\)</span> is aconstant.</p><p>This equation can be solved by separation of variables: <span class="math display">\[\frac{d x(t)}{x(t)} = F dt.\]</span> Integrating the left-hand side from <span class="math inline">\(x(0)\)</span> to <span class="math inline">\(x(t)\)</span>, and right-hand side from <span class="math inline">\(0\)</span> to <span class="math inline">\(t\)</span>, we get <span class="math display">\[\log \frac{x(t)}{x(0)} = F t.\]</span> Combining the initial condition, we get <span class="math display">\[x(t) = x(0) e^{F t}.\]</span></p><p>Another way to solve this equation is to by intergrating both sidesof equation (4) from <span class="math inline">\(0\)</span> to <span class="math inline">\(t\)</span>. We get <span class="math display">\[x(t) = x(0) + \int_0^t F x(\tau) d \tau.\]</span> The we can substitute the solution back into the right-handside of the equation, and we get <span class="math display">\[\begin{aligned}x(t) &amp;= x(0) + \int_0^t F x(\tau) d \tau \\&amp;= x(0) + \int_0^t F [x(0) + \int_0^{\tau} F x(\tau') d\tau'] d \tau \\&amp;= x(0) + F x(0) t +  \int_0^t \int_0^{\tau}F^2 x(\tau') d\tau' d \tau \\&amp;= x(0) + F x(0) t +  F^2 x(0) \frac{t^2}{2} + \int_0^t\int_0^{\tau} \int_0^{\tau'}F^3 x(\tau'')  d \tau''d \tau' d \tau \\&amp;= ...\\&amp;= x(0) + F x(0) t +  F^2 x(0) \frac{t^2}{2} + F^3 x(0)\frac{t^3}{6} + \cdots\\&amp;=  (1 + Ft + F^2 t^2 /2 + F^3 t^3 /3! + \cdots) x(0)\\&amp;=  e^{F t} x(0).  \qquad (\text{Taylor expansion})\end{aligned}\]</span></p><table style="width:26%;"><colgroup><col style="width: 26%"></colgroup><thead><tr><th style="text-align: left;">For the <strong>multidimensional</strong>linear <strong>homogeneous</strong> differential equation with fixedinitial condition at <span class="math inline">\(t = 0\)</span>: <span class="math display">\[\frac{d \vec{x}(t)}{dt} = \mathbf{F} \vec{x}(t),  \vec{x}(0) =\text{given}, \qquad (5)\]</span> where <span class="math inline">\(\mathbf{F}\)</span> is aconstant matrix.</th></tr></thead><tbody><tr><td style="text-align: left;">Next, let's move to the linear<strong>time-invariant</strong> <strong>inhomogeneous</strong>differential equations: <span class="math display">\[\frac{d \vec{x}(t)}{dt} = \mathbf{F} \vec{x}(t) + \mathbf{L}\vec{w}(t),  \vec{x}(t_0) = \text{given}, \qquad (7)\]</span> where <span class="math inline">\(\mathbf{F}\)</span> is aconstant matrix, <span class="math inline">\(\mathbf{L}\)</span> is aconstant matrix, and <span class="math inline">\(\vec{w}(t)\)</span> isa vector-valued function of time.</td></tr><tr><td style="text-align: left;">First, we can move the <span class="math inline">\(\mathbf{F} \vec{x}(t)\)</span> to the left-handside of the equation and multify both sides with a integrating factor<span class="math inline">\(\exp(- \mathbf{F}t)\)</span>, and get <span class="math display">\[\begin{aligned}\exp(- \mathbf{F}t) \frac{d \vec{x}(t)}{dt} - \exp(-\mathbf{F}t)\mathbf{F} \vec{x}(t) = \exp(- \mathbf{F}t) \mathbf{L}\vec{w}(t),\end{aligned}\]</span> Since <span class="math inline">\(\frac{d}{dt} \exp(-\mathbf{F}t) \vec{x}(t) = \exp(- \mathbf{F}t) \frac{d \vec{x}(t)}{dt} -\exp(- \mathbf{F}t)\mathbf{F} \vec{x}(t)\)</span>, we can rewrite theabove equation as <span class="math display">\[\frac{d}{dt} \exp(- \mathbf{F}t) \vec{x}(t) = \exp(- \mathbf{F}t)\mathbf{L} \vec{w}(t).\]</span> Integrating both sides from <span class="math inline">\(t_0\)</span> to <span class="math inline">\(t\)</span>, we get <span class="math display">\[\exp(- \mathbf{F}t) \vec{x}(t) - \exp(- \mathbf{F}t_0) \vec{x}(t_0) =\int_{t_0}^t \exp(- \mathbf{F}\tau) \mathbf{L} \vec{w}(\tau) d \tau.\]</span> Then, we can get the solution of equation (7): <span class="math display">\[\vec{x}(t) = \exp(\mathbf{F}(t - t_0)) \vec{x}(t_0) + \int_{t_0}^t\exp(\mathbf{F}(t - \tau)) \mathbf{L} \vec{w}(\tau) d \tau. \qquad (8)\]</span></td></tr><tr><td style="text-align: left;">### Solutions of General Linear ODEs Theprevious section only consider the linear time-invariant differentialequations(<span class="math inline">\(F\)</span> is a constant). In thissection, we will consider the general linear differential equations withtime-varying coefficients.</td></tr><tr><td style="text-align: left;">For the linear<strong>homogeneous</strong> <strong>time-varying</strong> differentialequation with fixed initial condition at <span class="math inline">\(t =t_0\)</span>: <span class="math display">\[\frac{d \vec{x}(t)}{dt} = \mathbf{F}(t) \vec{x}(t), \quad \vec{x}(t_0) =\text{given}, \qquad (9)\]</span> where <span class="math inline">\(\mathbf{F}(t)\)</span> is amatrix-valued function of time. We can not use the exponential matrix tosolve this equation, because the <span class="math inline">\(\mathbf{F}(t)\)</span> is a time-varying matrix.But the solution of this equation has a general form: <span class="math display">\[\vec{x}(t) = \mathbf{\Psi}(t, t_0) \vec{x}(t_0), \qquad (10)\]</span> where <span class="math inline">\(\mathbf{\Psi}(t,t_0)\)</span> is a matrix-valued function of time, and it is called the<strong>transition matrix</strong>. The transition matrix <span class="math inline">\(\mathbf{\Psi}(t, t_0)\)</span> satisfies thefollowing differential properties: <span class="math display">\[\begin{aligned}\frac{\partial \mathbf{\Psi}(\tau, t)}{\partial \tau} &amp;=\mathbf{F}(\tau) \mathbf{\Psi}(\tau, t), \\\frac{\partial \mathbf{\Psi}(\tau, t)}{\partial t} &amp;= -\mathbf{\Psi}(\tau, t) \mathbf{F}(t), \\\mathbf{\Psi}(\tau, t) &amp;= \mathbf{\Psi}(\tau, s) \mathbf{\Psi}(s, t)\qquad (\text{11}) \\\mathbf{\Psi}(t, \tau) &amp;= \mathbf{\Psi}^{-1}(\tau, t)  \\\mathbf{\Psi}(t, t) &amp;= \mathbf{I}.\end{aligned}\]</span> In most cases, the transition matrix <span class="math inline">\(\mathbf{\Psi}(t, t_0)\)</span> does not have aclosed-form solution.</td></tr></tbody></table><p>For the linear <strong>inhomogeneous</strong><strong>time-varying</strong> differential equation with fixed initialcondition at <span class="math inline">\(t = t_0\)</span>: <span class="math display">\[\frac{d \vec{x}(t)}{dt} = \mathbf{F}(t) \vec{x}(t) + \mathbf{L}(t)\vec{w}(t), \quad \vec{x}(t_0) = \text{given}, \qquad (12)\]</span> where <span class="math inline">\(\mathbf{F}(t)\)</span> is amatrix-valued function of time, <span class="math inline">\(\mathbf{L}(t)\)</span> is a matrix-valued functionof time, and <span class="math inline">\(\vec{w}(t)\)</span> is avector-valued function of time. The solution of this equation has ageneral form: <span class="math display">\[\vec{x}(t) = \mathbf{\Psi}(t, t_0) \vec{x}(t_0) + \int_{t_0}^t\mathbf{\Psi}(t, \tau) \mathbf{L}(\tau) \vec{w}(\tau) d \tau. \qquad(13)\]</span></p><p>When <span class="math inline">\(\mathbf{F}(t)\)</span> and <span class="math inline">\(\mathbf{L}(t)\)</span> is constant, the solutionof equation (7) is a special case of (13) and we can verfiy that the<span class="math inline">\(\Psi(t, t_0) = \exp(\mathbf{F}(t -t_0))\)</span> satisfies properties (10).</p><hr><p>Fourier tansforms and Laplace transforms are two useful methods tosolve inhomogeneous linear time-invariant ODE (Note that<strong>time-invariant</strong>).</p><h2 id="fourier-transforms">Fourier Transforms</h2><p>The Fourier transform of a function <span class="math inline">\(x(t)\)</span> is defined as: <span class="math display">\[X(i w) = \mathcal{F}[x(t)] = \int_{-\infty}^{\infty} x(t) \exp(-iwt)\text{d}t, \qquad (14)\]</span> where <span class="math inline">\(i\)</span> is the imaginaryunit.</p><p>The inverse Fourier transform of (14) is: <span class="math display">\[x(t) = \mathcal{F}^{-1}[X(i w)] = \frac{1}{2\pi} \int_{-\infty}^{\infty}X(iw) \exp(i w t) \text{d}t.\]</span></p><p>Some useful properties:</p><ul><li><p>differention: <span class="math inline">\(\mathcal{F}[\frac{\text{d}^n x(t)}{\text{d} t^n}]= (iw)^{n} \mathcal{F}[x(t)]\)</span>.</p></li><li><p>convolution: <span class="math inline">\(\mathcal{F}[x(t) \asty(t)] = \mathcal{F}[x(t)] \ast \mathcal{F}[y(t)]\)</span>, where theconvolution <span class="math inline">\(\ast\)</span> is defined as<span class="math display">\[x(t) \ast y(t) = \int_{-\infty}^{\infty} x(t - \tau)y(\tau) \text{d}\tau\]</span></p></li></ul><p><strong>If we want to use Fourier transform to solve ODEs, theinitial condition must be 0.</strong></p><p>Now, let's use Fourier transform to solve the linear<strong>time-invariant</strong> <strong>inhomogeneous</strong>differential equations: <span class="math display">\[\frac{d \vec{x}(t)}{dt} = \mathbf{F} \vec{x}(t) + \mathbf{L}\vec{w}(t),  \quad \vec{x}(t_0) = 0,\]</span> where <span class="math inline">\(\mathbf{F}\)</span> is aconstant matrix, <span class="math inline">\(\mathbf{L}\)</span> is aconstant matrix, and <span class="math inline">\(\vec{w}(t)\)</span> isa vector-valued function of time.</p><p>Take Fourier tranforms componentwise give <span class="math display">\[(iw)\vec{X}(iw) = \mathbf{F} \vec{X}(iw) + \mathbf{L} \vec{W}(iw),\]</span> thus, we can get <span class="math display">\[\vec{X}(iw) = [(iw)\mathbf{I} - \mathbf{F}]^{-1} \ast \mathbf{L}\vec{W}(iw),\]</span> The solution is the inverse Fourier transform <span class="math display">\[x(t) = \mathcal{F}^{-1}[[(iw)\mathbf{I} - \mathbf{F}]^{-1} \ast\mathbf{L} \vec{W}(iw)] = \mathcal{F}^{-1}[((iw)\mathbf{I} -\mathbf{F})^{-1}]\ast \mathbf{L} \vec{w}(t), \qquad (15)\]</span></p><p>Since <span class="math inline">\(\vec{x}(t_0) = 0\)</span>, comparethe solution (15) with solution (8), we can obtain <span class="math display">\[\mathcal{F}^{-1}[((iw)\mathbf{I} - \mathbf{F})^{-1}] = \exp(\mathbf{F}t)u(t),\]</span> where <span class="math inline">\(u(t)\)</span> is theHeaviside step function, which is 0 for <span class="math inline">\(t&lt;0\)</span> and 1 for <span class="math inline">\(t \geq 0\)</span>.</p><h2 id="laplace-transforms">Laplace Transforms</h2><p>The Laplace transform of a function <span class="math inline">\(f(t)\)</span> is defined on space <span class="math inline">\(\{t | t\geq 0\}\)</span>: <span class="math display">\[F(s) = \mathcal{L}[f(t)] = \int_{0}^{\infty} f(t)\exp(-st) \text{d}t,\qquad (16)\]</span> where <span class="math inline">\(s = \sigma +iw\)</span>.</p><p>The inverse transform is <span class="math inline">\(f(t) =\mathcal{L}^{-1}[F(s)]\)</span>.</p><p>Remember that the Fourier transform needs the initial condition <span class="math inline">\(x(0) = 0\)</span>. But Laplace transform can takethe initial conditions into account. If <span class="math inline">\(x(0)= \text{given}\)</span>, then <span class="math display">\[\mathcal{L}[\frac{\text{d} x(t)}{\text{d} t}] = s \mathcal{L}[x(t)] -x(0) = s X(s) - x(0).\]</span></p><p><span class="math display">\[\mathcal{L}[\frac{\text{d}^n x(t)}{\text{d} t^n}] = s^n X(s) - s^{n-1}x(0) - \cdots - \frac{\text{d} x^{n-1}}{\text{d} t^{n-1}}(0).\]</span></p><p>Now, let's use Laplace transform to solve the linear<strong>time-invariant</strong> <strong>inhomogeneous</strong>differential equations: <span class="math display">\[\frac{d \vec{x}(t)}{dt} = \mathbf{F} \vec{x}(t) + \mathbf{L}\vec{w}(t),  \quad \vec{x}(t_0) = \text{given} \ne 0,\]</span> where <span class="math inline">\(\mathbf{F}\)</span> is aconstant matrix, <span class="math inline">\(\mathbf{L}\)</span> is aconstant matrix, and <span class="math inline">\(\vec{w}(t)\)</span> isa vector-valued function of time.</p><p>Take Laplace tranforms componentwise give <span class="math display">\[s X(s) - x(0) = \mathbf{F} X(s) + \mathbf{L} W(s).\]</span> Then, <span class="math display">\[X(s) = [s\mathbf{I}-\mathbf{F}]^{-1} x(0) +[s\mathbf{I}-\mathbf{F}]^{-1} \ast  \mathbf{L} W(s).  \qquad (17)\]</span></p><p>Compare the solution (17) with solution (8), we can obtain <span class="math display">\[\mathcal{L}^{-1}[(s\mathbf{I}-\mathbf{F})^{-1}] = \exp(\mathbf{F}t)\]</span> for <span class="math inline">\(t \geq 0\)</span>.</p><h1 id="reference">Reference</h1><p><a href="https://users.aalto.fi/~asolin/sde-book/sde-book.pdf">SimoSärkkä and Arno Solin (2019). Applied Stochastic Differential Equations.Cambridge University Press.</a></p>]]></content>
      
      
      <categories>
          
          <category> Generative Model </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Diffusion </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Convergence of Markov Chain</title>
      <link href="/2023/07/15/Convergence%20of%20MC/"/>
      <url>/2023/07/15/Convergence%20of%20MC/</url>
      
        <content type="html"><![CDATA[<h1 id="total-variation-distance">Total Variation Distance</h1><p>Define Total Variation Distance: <span class="math display">\[d_{TV}(\mu(x), v(x)) = \frac{1}{2} \sum_{x \in \Omega} |\mu(x) - v(x)|\]</span> This is equivalent to the following: <span class="math display">\[d_{TV}(\mu(x), v(x)) = \sum_{x \in \Omega^{-}} (v(x) - \mu(x))= \sum_{x \in \Omega^{+}} (\mu(x) - v(x))\]</span> where <span class="math inline">\(\Omega^{+} = \{x \in \Omega:\mu(x) \geq v(x)\}\)</span>, <span class="math inline">\(\Omega^{-} =\{x \in \Omega: \mu(x) &lt; v(x)\}\)</span>, and <span class="math display">\[d_{TV}(\mu(x), v(x)) = \max_{S \subset \Omega} |\mu(S) - v(S)|\]</span> where <span class="math inline">\(\mu(S) = \sum_{x \in S}\mu(x)\)</span>, <span class="math inline">\(v(S) = \sum_{x \in S}v(x)\)</span>.</p><p>Proof: Define <span class="math inline">\(\Omega^{+} = \{x \in\Omega: \mu(x) \geq v(x)\}\)</span>, <span class="math inline">\(\Omega^{-} = \{x \in \Omega: \mu(x) &lt;v(x)\}\)</span>, then <span class="math display">\[\begin{aligned}d_{TV}(\mu(x), v(x)) &amp;= \frac{1}{2} \sum_{x \in \Omega} |\mu(x) -v(x)| \\&amp;= \frac{1}{2} \sum_{x \in \Omega^{+}} (\mu(x) - v(x)) + \frac{1}{2}\sum_{x \in \Omega^{-}} (v(x) - \mu(x))\end{aligned}\]</span> Since <span class="math inline">\(\sum_{x \in \Omega} \mu(x) =1 = \sum_{x \in \Omega^{+}} \mu(x) + \sum_{x \in \Omega^{-}}\mu(x)\)</span>, we have <span class="math display">\[\begin{aligned}d_{TV}(\mu(x), v(x)) &amp;= \frac{1}{2} \sum_{x \in \Omega^{+}} (\mu(x)- v(x)) + \frac{1}{2} \sum_{x \in \Omega^{-}} (v(x) - \mu(x)) \\&amp;= \frac{1}{2} \sum_{x \in \Omega^{-}} (v(x) - \mu(x)) + \frac{1}{2}\sum_{x \in \Omega^{-}} (v(x) - \mu(x)) \\&amp;= \sum_{x \in \Omega^{-}} (v(x) - \mu(x)) \\&amp;= \sum_{x \in \Omega^{+}} (\mu(x) - v(x))\end{aligned}\]</span> If <span class="math inline">\(S = \Omega^{+}\)</span> or<span class="math inline">\(\Omega^{-}\)</span>, then <span class="math display">\[\begin{aligned}d_{TV}(\mu(x), v(x)) &amp;= \frac{1}{2} \sum_{x \in \Omega} |\mu(x) -v(x)| \\&amp;=  \max_{S \in \Omega} \sum_{x \in S} |\mu(x) - v(x)| \\\end{aligned}\]</span></p><p>If <span class="math inline">\(S\)</span> contains any elements <span class="math inline">\(x \in \Omega^{-}\)</span>, then <span class="math inline">\(d_{TV}(\mu(x), v(x)) = |\sum_{x \in S} (\mu(x) -v(x))| \leq \sum_{x \in \Omega^{+}} (\mu(x) - v(x))\)</span> when <span class="math inline">\(S = \Omega^{+}\)</span>.</p><h1 id="convergence-of-markov-chain">Convergence of Markov Chain</h1><p>Assume we start from state <span class="math inline">\(x\)</span>,run Markov chain for <span class="math inline">\(t\)</span> steps, thenwe get the distribution <span class="math inline">\(P_{x}^{t}\)</span>.If we want to prove <span class="math inline">\(P_{x}^{t}\)</span>converges to stationary distribution <span class="math inline">\(\pi\)</span>, we need to prove <span class="math inline">\(d_{TV}(P_{x}^{t}, \pi) \rightarrow 0\)</span> as<span class="math inline">\(t \rightarrow \infty\)</span>. Thus, we needto bound <span class="math inline">\(d_{TV}(P_{x}^{t},\pi)\)</span>.</p><p>Define <span class="math inline">\(d_{x}(t) := d_{TV}(P_{x}^{t},\pi)\)</span>.</p><p>Consider all possible initial states <span class="math inline">\(x\in \Omega\)</span>, we define <span class="math inline">\(d(t) :=\max_{x \in \Omega} d_{TV} (P_{x}^{t}, \pi)\)</span>.</p><p>Assume there are two Markov chains <span class="math inline">\(x_{t}\sim P_{x}^{t}\)</span>, <span class="math inline">\(y_{t} \simP_{y}^{t}\)</span>. <span class="math inline">\(x_{t}\)</span> and <span class="math inline">\(y_{t}\)</span> share the same transitionprobability matrix <span class="math inline">\(P\)</span> but start fromdifferent states. Then we define <span class="math display">\[\bar{d}(t):= \max_{x, y \in \Omega} d_{TV}(P_{x}^{t}, P_{y}^{t})\]</span>.</p><p>Next, we will prove this Lemma: &gt; Lemma 1. &gt; <span class="math display">\[d(t) \leq \bar{d}(t) \leq 2 d(t).\]</span></p><p>Proof: Let's prove the second inequality <span class="math inline">\(\bar{d}(t) \leq 2 d(t)\)</span>. <span class="math display">\[\begin{aligned}\bar{d}(t) &amp;= \max_{x, y \in \Omega} d_{TV}(P_{x}^{t}, P_{y}^{t}) \\&amp;= \max_{x, y \in \Omega} \frac{1}{2} \sum_{z \in \Omega}|P_{x}^{t}(z) - P_{y}^{t}(z)| \\&amp; = \max_{x, y \in \Omega} \frac{1}{2} \sum_{z \in \Omega}|P_{x}^{t}(z) - \pi(z) + \pi(z) - P_{y}^{t}(z)| \\&amp;\leq \max_{x, y \in \Omega} [\frac{1}{2} \sum_{z \in \Omega}|P_{x}^{t}(z) - \pi(z)| + \frac{1}{2} \sum_{z \in \Omega} |\pi(z) -P_{y}^{t}(z)| ]\\&amp;= \max_{x \in \Omega} d_{TV}(P_{x}^{t}, \pi) + \max_{y \in \Omega}d_{TV}(P_{y}^{t}, \pi) \\&amp;= d(t) + d(t) \\&amp;= 2 d(t)\end{aligned}\]</span></p><p>For the first inequality <span class="math inline">\(\bar{d}(t) \leq2 d(t)\)</span>, we need to prove <span class="math inline">\(d(t) \leq\bar{d}(t)\)</span>.</p><p>Define <span class="math display">\[S_{x,y}^{*} = \arg\max_{S \subset \Omega} （P_{x}^{t}(S) -P_{y}^{t}(S)）\]</span></p><p><span class="math display">\[S_{x}^{**} = \arg\max_{S \subset \Omega, y \in \Omega} （P_{x}^{t}(S) -P_{y}^{t}(S)）\]</span></p><p><span class="math display">\[S_{x}^{*} = \arg\max_{S \subset \Omega} \sum_{y \in \Omega}\pi(y)（P_{x}^{t}(S) - P_{y}^{t}(S)）\]</span></p><p><span class="math display">\[\begin{aligned}d(t) &amp;= \max_{x \in \Omega} d_{TV}(P_{x}^{t}, \pi) \\&amp;= \max_{x \in \Omega} \max_{S \subset \Omega}|P_{x}^{t}(S) -\pi(S)| \\&amp;= \max_{x \in \Omega} \max_{S \subset \Omega^{+}}(P_{x}^{t}(S) -\pi(S))  \\&amp;= \max_{x \in \Omega} \max_{S \subset \Omega^{+}}(P_{x}^{t}(S) -\sum_{y \in \Omega} \pi(y) P_{y}^{t}(S))  \qquad  (\pi(y) = \sum_{x \in\Omega} \pi(x) P_{x,y}) \\&amp;= \max_{x \in \Omega} \max_{S \subset \Omega^{+}} \sum_{y \in\Omega} \pi(y) (P_{x}^{t}(S) - P_{y}^{t}(S)).\end{aligned}\]</span> Since for all <span class="math inline">\(S\)</span>: <span class="math inline">\(（P_{x}^{t}(S_{x,y}^{*}) -P_{y}^{t}(S_{x,y}^{*})）\geq （P_{x}^{t}(S) -P_{y}^{t}(S)）\)</span></p><p>we have, <span class="math display">\[\begin{aligned}d(t) &amp;= \max_{x \in \Omega} \max_{S \subset \Omega^{+}} \sum_{y \in\Omega} \pi(y) (P_{x}^{t}(S) - P_{y}^{t}(S)) \\&amp; \leq \max_{x \in \Omega} \sum_{y \in \Omega} \pi(y)(P_{x}^{t}(S_{x,y}^{*}) - P_{y}^{t}(S_{x,y}^{*})) \\&amp;= \max_{x \in \Omega} \sum_{y \in \Omega} \pi(y)\max_{S}  (P_{x}^{t}(S) - P_{y}^{t}(S)) \\&amp; \leq  \max_{x \in \Omega} \sum_{y \in \Omega}\pi(y)   (P_{x}^{t}(S_{x}^{**}) - P_{y}^{t}(S_{x}^{**})) \\&amp; = \max_{x \in \Omega} \sum_{y \in \Omega} \pi(y) \max_{y,S}(P_{x}^{t}(S) - P_{y}^{t}(S)) \\&amp;= \max_{x, y \in \Omega}\max_{ S \subset \Omega}(P_{x}^{t}(S) -P_{y}^{t}(S))   \qquad (\sum_{y \in \Omega} \pi(y) = 1) \\&amp;= \max_{x, y \in \Omega} d_{TV}(P_{x}^{t}, P_{y}^{t}) \\&amp;= \bar{d}(t).\end{aligned}\]</span> that is, <span class="math inline">\(d(t) \leq \bar{d}(t) \leq2d(t)\)</span>.</p><p><strong>What we have proved is that, <span class="math inline">\(d(t)\)</span> is bounded by <span class="math inline">\(\bar{d}(t)\)</span>, and <span class="math inline">\(\bar{d}(t)\)</span> is controlled by <span class="math inline">\(\max_{x,y \in \Omega} d_{TV}(P_{x}^{t},P_{y}^{t})\)</span>. Let's find a way to bound <span class="math inline">\(d_{TV}(P_{x}^{t},P_{y}^{t})\)</span>!</strong></p><blockquote><p>Define 1. (Coupling) Assume <span class="math inline">\(x , y \in\Omega\)</span>, <span class="math inline">\(x \sim \mu, y \simv\)</span>, <span class="math inline">\(\mu, v\)</span> are twodistributions. A joint distribution <span class="math inline">\(w(x,y)\)</span> on <span class="math inline">\(\Omega \times \Omega\)</span>is called <strong>couping</strong> if <span class="math inline">\(\forall x \in \Omega\)</span>, <span class="math inline">\(\sum_{y}w(x, y) = \mu(x)\)</span>, <span class="math inline">\(\forall y \in \Omega\)</span>, <span class="math inline">\(\sum_{x} w(x, y) = v(y)\)</span>.</p></blockquote><blockquote><p>Lemma 2. Consider <span class="math inline">\(\mu, v\)</span> definedon <span class="math inline">\(\Omega\)</span>,<br>(a) For any coupling <span class="math inline">\(w(x, y)\)</span> of<span class="math inline">\(\mu, v\)</span>, <span class="math inline">\(d_{TV}(\mu, v) \leq P(x \ne y)\)</span>.<br>(b) There always exists a coupling <span class="math inline">\(w(x,y)\)</span> of <span class="math inline">\(\mu, v\)</span> such that<span class="math inline">\(d_{TV}(\mu, v) = P(x \ne y)\)</span>.</p></blockquote><p>Proof: (a) <span class="math inline">\(\forall z\)</span>, <span class="math inline">\(w(z, z) \leq \sum_{y \in \Omega} w(z, y) =\mu(z)\)</span>. Similarly, <span class="math inline">\(w(z, z) \leqv(z)\)</span>. Thus, <span class="math inline">\(w(z, z) \leq\min(\mu(z), v(z))\)</span>.</p><p><span class="math display">\[\begin{aligned}P(x \ne y) &amp;= 1 - P(x = y) \\&amp;= 1 - \sum_{z \in \Omega} w(z, z) \\&amp;\geq 1 - \sum_{z \in \Omega} \min(\mu(z), v(z)) \\&amp;= \sum_{z \in \Omega} \mu(z) - \sum_{z \in \Omega} \min(\mu(z),v(z)) \\&amp;= \sum_{z \in \Omega^{+}} (\mu(z) - v(z)) \\&amp;= d_{TV}(\mu, v)\end{aligned}\]</span></p><ol start="2" type="a"><li>We can construct a coupling <span class="math inline">\(w(x,y)\)</span>: <span class="math display">\[w(x, y) = \left\{\begin{aligned}&amp; \min \{\mu(x), v(y)\}, \quad x = y \\&amp;\frac{(\mu(x) - w(x, x))(v(y) - w(y,y))}{1 - \sum_{z \in\Omega}w(z,z)}, \quad x \ne y\end{aligned}\right.\]</span> For this joint distribution, we have <span class="math display">\[\begin{aligned}P(x \ne y) &amp;= 1 - P(x = y) \\&amp;= 1 - \sum_{z \in \Omega} w(z, z) \\&amp;= 1 - \sum_{z \in \Omega} \min(\mu(z), v(z)) \\&amp;= \sum_{z \in \Omega} \mu(z) - \sum_{z \in \Omega} \min(\mu(z),v(z)) \\&amp;= \sum_{z \in \Omega^{+}} (\mu(z) - v(z)) \\&amp;= d_{TV}(\mu, v)\end{aligned}\]</span> Thus, this joint distribution <span class="math inline">\(w(x,y)\)</span> satisfies <span class="math inline">\(d_{TV}(\mu, v) = P(x\ne y)\)</span>. Now, we need to prove that this joint distribution<span class="math inline">\(w(x, y)\)</span> is a coupling of <span class="math inline">\(\mu, v\)</span>.</li></ol><p><span class="math inline">\(\forall x \in \Omega\)</span>, <span class="math display">\[\begin{aligned}\sum_{y \in \Omega} w(x, y) &amp;= \sum_{y=x} \min \{\mu(x), v(y)\} +\sum_{y \in \Omega, y \ne x} \frac{(\mu(x) - w(x, x))(v(y) - w(y,y))}{1- \sum_{z \in \Omega}w(z,z)} \\&amp;= w(x, x) + \frac{(\mu(x) - w(x, x))}{1 - \sum_{z \in\Omega}w(z,z)} \sum_{y \ne x} (v(y) - w(y,y)) \\&amp;= w(x,x) + \frac{(\mu(x) - w(x, x))}{1 - \sum_{z \in \Omega}w(z,z)}(1 - v(x) - (\sum_{z} w(z,z) - w(x,x)))  \qquad (\sum_{y \ne x} v(y) = 1- v(x))\\&amp;= w(x,x) + \frac{(\mu(x) - w(x, x))}{1 - \sum_{z \in \Omega}w(z,z)}(1 - \sum_{z} w(z,z) + w(x,x) - v(x) ) \qquad (\sum_{y \ne x} w(y,y) =\sum_{z} w(z,z) - w(x, x))\end{aligned}\]</span> If $ x ^{+} = {x | (x) v(x)}$, then <span class="math inline">\(w(x,x) = v(x)\)</span>, <span class="math inline">\(\sum_{y \in \Omega} w(x, y)  = v(x) + (\mu(x) -v(x)) = \mu(x)\)</span>.</p><p>If $ x ^{-} = {x | (x) &lt; v(x)}$, then <span class="math inline">\(w(x,x) = \mu(x)\)</span>, <span class="math inline">\(\sum_{y \in \Omega} w(x, y)  = \mu(x) + 0 =\mu(x)\)</span>. Thus, <span class="math inline">\(w(x, y)\)</span> is acoupling of <span class="math inline">\(\mu, v\)</span>.</p><p>Last but not least, let's begin to prove the nonincreasing propertyof <span class="math inline">\(d(t)\)</span>!! <strong>Almost close tothe end!! ^o/</strong></p><blockquote><p>Lemma 3. Consider two Markov chains <span class="math inline">\(x^{t}\sim P_{x}^{t}\)</span>, <span class="math inline">\(y^{t} \simP_{y}^{t}\)</span>, <span class="math inline">\(x^{t}\)</span> and <span class="math inline">\(y^{t}\)</span> share the same transitionprobability matrix <span class="math inline">\(P\)</span> but start fromdifferent states. If <span class="math inline">\(x^t = y^t\)</span>,then <span class="math inline">\(x^{t+1} = y^{t+1}\)</span>, elif <span class="math inline">\(x^t \ne y^t\)</span>, then <span class="math inline">\(x^{t+1} \ne y^{t+1}\)</span>, <span class="math inline">\(x^{t+1}\)</span> and <span class="math inline">\(y^{t+1}\)</span> are independent.</p></blockquote><blockquote><p>Define 2. (Coupling of Markov chains) Consider two Markov chains<span class="math inline">\(x^{t} \sim P_{x}^{t}\)</span>, <span class="math inline">\(y^{t} \sim P_{y}^{t}\)</span>, <span class="math inline">\(x^{t}\)</span> and <span class="math inline">\(y^{t}\)</span> share the same transitionprobability matrix <span class="math inline">\(P\)</span> but start fromdifferent states. If <span class="math display">\[P(y^{t+1} | x^{t}, y^{t}) = P(y^{t+1} | y^{t})\]</span> and <span class="math display">\[P(x^{t+1} | x^{t}, y^{t}) = P(x^{t+1} | x^{t})\]</span> then we say <span class="math inline">\(x^{t}\)</span> and<span class="math inline">\(y^{t}\)</span> are coupled.</p></blockquote><p>Define <span class="math inline">\(w^{t} := w^{t}(x^t, y^t)\)</span>is a coupling of <span class="math inline">\(P_{x}^{t},P_{y}^{t}\)</span>, <span class="math inline">\(x^t \sim P_x^t, y \simP_{y}^{t}\)</span>, and <span class="math inline">\(w^{t}\)</span>satisfies Lemma 2(b).</p><p><span class="math display">\[P_{w^{t}}(x^{t+1} \ne y^{t+1}) = P_{w^{t}}(x^{t+1} \ne y^{t+1} | x^t \ney^t) p(x^t \ne y^t) + P_{w^{t}}(x^{t+1} \ne y^{t+1} | x^t = y^t) p(x^t =y^t)\]</span> If <span class="math inline">\(x^t = y^t\)</span>, accordingto Lemma 3, <span class="math inline">\(P_{w^{t}}(x^{t+1} \ne y^{t+1}) =P_{w^{t}}(x^{t+1} = y^{t+1} | x^t = y^t) p(x^t = y^t)  \leqP_{w^{t}}(x^{t} \ne y^t)\)</span>;</p><p>If <span class="math inline">\(x^t \ne y^t\)</span>, <span class="math inline">\(P_{w^{t}}(x^{t+1} \ne y^{t+1}) = P_{w^{t}}(x^{t+1}\ne y^{t+1} | x^t \ne y^t) p(x^t \ne y^t) \leq P_{w^{t}}(x^{t} \ney^t)\)</span>.</p><p><span class="math inline">\(d_x(t+1) = d_{TV}(P_{x}^{t+1},P_y^{t+1}） \leq P_{w^{t}}(x^{t+1} \ne y^{t+1}) \leq P_{w^{t}}(x^{t} \ney^t) = d_{TV}(P_x^t, P_y^{t}) = d_x(t)\)</span></p><p><span class="math inline">\(d_{x}(t) := d_{TV}(P_{x}^{t},\pi)\)</span>.</p><p><span class="math inline">\(d(t) = \max_{x \in \Omega}d_{x}(t)\)</span>.</p><p><span class="math inline">\(d(t) \leq \bar{d}(t) \leq2d(t)\)</span>.</p><p><span class="math inline">\(\bar{d}(t) := \max_{x, y \in \Omega}d_{TV}(P_{x}^{t}, P_{y}^{t})\)</span></p><p>???????????? Q: if we consider <span class="math inline">\(d_{TV}(P_x^t, \pi)\)</span>, then the coupling<span class="math inline">\(w^{t} := w^{t}(x^t, y)\)</span> should be acoupling of <span class="math inline">\(P_{x}^{t}, \pi\)</span>, <span class="math inline">\(x^t \sim P_x^t, y \sim \pi\)</span>.</p><p><span class="math inline">\(\pi\)</span> is different from <span class="math inline">\(P_y^t\)</span> at first.</p><p><span class="math display">\[d_x(t) = d_{TV}(P_{x}^{t}, \pi) \leq d(t) \leq \bar{d}(t) = \max_{x, y\in \Omega} d_{TV}(P_{x}^{t}, P_{y}^{t}) = P_{w^t}(x^t \ne y^t)\]</span></p><p>Now, we have already proved that <span class="math inline">\(d(t)\)</span> is nonincreasing. Next, we willprove that <span class="math inline">\(d(t)\)</span> converges to 0.</p><h1 id="useful-lectures">Useful Lectures:</h1><ul><li><a href="https://people.eecs.berkeley.edu/~sinclair/cs294/n7.pdf">L1</a></li><li><a href="https://courses.cs.duke.edu/spring13/compsci590.2/slides/lec5.pdf">MarkovChains and Coupling</a></li><li><a href="https://faculty.cc.gatech.edu/~vigoda/MCMC_Course/MC-basics.pdf">MarkovChains, Coupling, Stationary Distribution</a></li><li><a href="https://mpaldridge.github.io/math2750/S10-stationary-distributions.html">Stationarydistributions</a></li><li><a href="https://mpaldridge.github.io/math2750/S11-long-term-chains.html">Long-termbehaviour of Markov chains</a></li></ul>]]></content>
      
      
      <categories>
          
          <category> Stochastic Process </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Markov Chains </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Metropolis-Hastings</title>
      <link href="/2023/07/15/Metropolis-Hastings/"/>
      <url>/2023/07/15/Metropolis-Hastings/</url>
      
        <content type="html"><![CDATA[<h1 id="introduction">Introduction</h1><p>The first MCMC algorithm is the Metropolis algorithm, published byMetropolis et al. (1953). It was generalized by Hastings (1970) andPeskun (1973, 1981) towards statistical applications. After a long time,it was rediscovered by Geman and Geman (1984), Tanner and Wong (1987),and Gelfand and Smith (1990).</p><p>Assume the probability density <span class="math inline">\(\pi\)</span> is the target distribution and <span class="math inline">\(q(\cdot | \cdot)\)</span> is the proposaltransition distribution. From an initial state <span class="math inline">\(X_0\)</span>, the Metropolis-Hastings algorithmaims to generate a Markov chain <span class="math inline">\(\{X_1, X_2,...\}\)</span>, such that <span class="math inline">\(X_t\)</span>converges to distribution <span class="math inline">\(\pi\)</span>.</p><h1 id="metropolis-hastings-algorithm">Metropolis-Hastingsalgorithm</h1><p>Input: initial value <span class="math inline">\(X_0\)</span>,transition <span class="math inline">\(q(\cdot | \cdot)\)</span>, numberof iterations <span class="math inline">\(T\)</span>.</p><p>Output: Markov chain <span class="math inline">\(\{X_1, X_2, ...,X_T\}\)</span>.</p><p>For <span class="math inline">\(t = 1, 2, ..., T\)</span>:</p><ol type="1"><li>Generate <span class="math inline">\(u\)</span> from uniformdistribution <span class="math inline">\(U(0, I)\)</span>.</li><li>Generate <span class="math inline">\(X\)</span> from <span class="math inline">\(q(X | X_{t-1})\)</span>.</li><li>Compute the acceptance probability <span class="math inline">\(A(X,X_{t-1}) = \min\{1, \frac{\pi(X)q(X_{t-1} | X)}{\pi(X_{t-1})q(X |X_{t-1})}\}\)</span>.</li><li>if <span class="math inline">\(u \leq A(X, X_{t-1})\)</span>, then<span class="math inline">\(X_t = X\)</span>; else <span class="math inline">\(X_t = X_{t-1}\)</span>.</li></ol><p>Now, we prove that <span class="math inline">\(\pi\)</span> is one ofthe stationary distribution of the generated Markov chain.</p><p>Proof: Recall the detailed balance equation, for any <span class="math inline">\(i, j \in \Omega\)</span>, we have <span class="math display">\[\pi_i P_{i,j} = \pi_j P_{j,i},\]</span> then <span class="math inline">\(\pi\)</span> is a stationarydistribution of the Markov chain.</p><p>For the Metropolis-Hastings algorithm, we have the transitionprobability <span class="math display">\[K(X_t | X_{t-1}) = q(X_t| X_{t-1}) A(X_t, X_{t-1}) + \delta(X_t =X_{t-1}) (1 - \sum_{X \in \Omega} q(X|X_{t-1}) A(X, X_{t-1})).\]</span></p><p>Now we need to prove <span class="math display">\[\pi(X_{t-1})K(X_t | X_{t-1}) = \pi(X_t)K(X_{t-1} | X_t).\]</span></p><p>If <span class="math inline">\(X_t = X_{t-1}\)</span>, then theequation holds. If <span class="math inline">\(X_t \neqX_{t-1}\)</span>, then <span class="math display">\[\pi(X_{t-1})K(X_t | X_{t-1}) = \pi(X_{t-1})q(X_t| X_{t-1}) A(X_t,X_{t-1}) = \min\{\pi(X_{t-1})q(X_t| X_{t-1}), \pi(X_t)q(X_{t-1} |X_t)\}.\]</span> and <span class="math display">\[\pi(X_{t})K(X_{t-1} | X_{t}) = \pi(X_{t})q(X_{t-1}| X_{t}) A(X_{t-1},X_{t}) = \min\{\pi(X_{t})q(X_{t-1}| X_{t}), \pi(X_{t-1})q(X_{t} |X_{t-1})\}.\]</span> So, <span class="math inline">\(\pi(X_{t-1})K(X_t | X_{t-1}) =\pi(X_{t})K(X_{t-1} | X_{t})\)</span>. <span class="math inline">\(\pi\)</span> is a stationary distribution of theMarkov chain.</p><hr><p>Remark 1. The detailed balance condition is a sufficient but notnecessary condition for <span class="math inline">\(\pi\)</span> to be astationary distribution of the Markov chain. If we want to prove <span class="math inline">\(\pi\)</span> is the unique stationary distributionof the Markov chain, we need to prove the Markov chain is<strong>irreducible and positive recurrent</strong>. 【？？？】</p><p>Remark 2. The first initial state <span class="math inline">\(X_0\)</span> is randomly generated and usuallyremoved from the sample as burn-in or warm-up.</p><p>Q: <strong>Since it is recurrent, it must return to the initialvalues. Will this initial be rejected with a highprobability?</strong></p><p>Remark 3. In practice, the performances of the algorithm areobviously highly dependent on the choice of the transition <span class="math inline">\(q(\cdot | \cdot)\)</span>, since some choices seethe chain unable to converge in a manageable time.</p><p>Remark 4. We need to able to evaluate a function <span class="math inline">\(p(x) \propto \pi(x)\)</span>. Since we only needto compute the ratio <span class="math inline">\(\pi(y)/\pi(x)\)</span>,the proportionality constant is irrelevant. Similarly, we only careabout <span class="math inline">\(q(\cdot | \cdot)\)</span> up to aconstant</p><h1 id="reference">Reference</h1><ul><li>C.P. Robert. (2016). The Metropolis-Hastings algorithm.</li></ul>]]></content>
      
      
      <categories>
          
          <category> Stochastic Process </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Markov Chains </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Discrete Markov Chains</title>
      <link href="/2023/07/12/Markov-Chains/"/>
      <url>/2023/07/12/Markov-Chains/</url>
      
        <content type="html"><![CDATA[<h1 id="markov-chains-definitions-and-representations">Markov Chains:Definitions and Representations</h1><p>A stochastic process <span class="math inline">\(X = \{ x(t): t\inT\}\)</span> is a collection of random variables.</p><p>There are two elements:</p><ul><li>Time <span class="math inline">\(t\)</span>:<ul><li>discrete time (<span class="math inline">\(T\)</span> is a countablyinfinite set; under this case, we call 'Markov chain')</li><li>continuous time (under this case, we call 'Markov process')</li></ul></li><li>Space <span class="math inline">\(\Omega\)</span>:<ul><li>discrete space (<span class="math inline">\(X_{t}\)</span> comesfrom a countably infinite set)</li><li>continuous space.</li></ul></li></ul><p>Markov chain is a <strong>discrete-time</strong> process for whichthe future behaviour, given the past and the present, only depends onthe present and not on the past.</p><p>Markov process is the <strong>continuous-time</strong> version of aMarkov chain.</p><blockquote><p>Definition 1.[Markov chain] A discrete time stochastic process $ X_0,X_1, X_2, <span class="math inline">\(. . . is a Markov chainif\)</span>$ P(X_{t} = a_t | X_{t-1} = a_{t-1}, X_{t-2} = a_{t-2}, ...,X_0 = a_0) = P(X_{t} = a_t | X_{t-1} = a_{t-1}) = P_{a_{t-1}, a_{t}}$$</p></blockquote><p>Remark 1: This is time-homogeneous markov chain, for <span class="math inline">\(\forall t\)</span>, for <span class="math inline">\(\forall a_{t-1}, a_{t} \in \Omega\)</span>, thetransition probability <span class="math inline">\(P_{a_{t-1}, a_{t}}\)</span> is the same.</p><p>Remark 2: In DDPM, it is not a time-homogeneous chain, as thetransition probability at t is obtained by a network(t).</p><p>The state <span class="math inline">\(X_{t}\)</span> depends on theprevious state <span class="math inline">\(X_{t-1}\)</span> but isindependent of the particular history <span class="math inline">\(X_{t-2}, X_{t-3},...\)</span>. This is called the<strong>Markov property</strong> or <strong>memorylessproperty</strong>.</p><p>The Markov property does not imply that <span class="math inline">\(X_{t}\)</span> is independent of the randomvariables <span class="math inline">\(X_{0}\)</span>, <span class="math inline">\(X_{1}\)</span>,..., <span class="math inline">\(X_{t-2}\)</span>; it just implies that <strong>anydependency of <span class="math inline">\(X_{t}\)</span> on the past iscaptured in the value of <span class="math inline">\(X_{t-1}\)</span></strong>.</p><p>The Markov chain is <strong>uniquely</strong> defined by the one-steptransition probability matrix P: <span class="math display">\[P =\begin{pmatrix}P_{0,0} &amp; P_{0, 1} &amp; \cdots &amp; P_{0, j} &amp; \cdots\\\vdots &amp; \vdots &amp; \ddots &amp; \vdots&amp; \vdots\\P_{i,0} &amp; P_{i, 1} &amp; \cdots &amp; P_{i, j} &amp; \cdots\\\vdots &amp; \vdots &amp; \ddots &amp; \vdots&amp; \vdots\\\end{pmatrix}\]</span> where <span class="math inline">\(P_{i,j}\)</span> is theprobability of transition from state <span class="math inline">\(i\)</span> to state <span class="math inline">\(j\)</span>. <span class="math inline">\(P_{i,j} =P(X_{t} = j| X_{t-1} = i), i,j \in \Omega\)</span>. For <span class="math inline">\(i\)</span>, <span class="math inline">\(\sum_{j\geq 0} P_{i,j} = 1\)</span>.</p><h1 id="classification-of-states">Classification of States</h1><p>For simplicity, we assume that the state space <span class="math inline">\(\Omega\)</span> is finite. ## Communicatingclass</p><blockquote><p>Definition 2. [Communicating class] A state <span class="math inline">\(j\)</span> is reachable from state <span class="math inline">\(i\)</span> if there exists a positive integer<span class="math inline">\(n\)</span> such that <span class="math inline">\(P_{i,j}^{(n)} &gt; 0\)</span>. We write <span class="math inline">\(i \rightarrow j\)</span>. If <span class="math inline">\(j\)</span> is reachable from <span class="math inline">\(i\)</span>, and <span class="math inline">\(i\)</span> is reachable from <span class="math inline">\(j\)</span>, then the states <span class="math inline">\(i\)</span> and <span class="math inline">\(j\)</span> are said to<strong>communicate</strong>, denoted by <span class="math inline">\(i\leftrightarrow j\)</span>. A communicating class <span class="math inline">\(C\)</span> is a <strong>maximal</strong> set ofstates that communicate with each other. <strong>No state in <span class="math inline">\(C\)</span> communicates with any state not in<span class="math inline">\(C\)</span>.</strong></p></blockquote><h2 id="irreducible">Irreducible</h2><blockquote><p>Definition 3: A Markov chain is <strong>irreducible</strong> if allstates belong to <strong>one</strong> communicating class.</p></blockquote><p>This means that <strong>any state can be reached from any otherstate</strong>. For <span class="math inline">\(\forall i, j \in\Omega\)</span>, <span class="math inline">\(P_{i,j} &gt;0\)</span>.</p><blockquote><p>Lemma 1. A finite Markov chain is irreducible if and only if itsgraph representation is a strongly connected graph.</p></blockquote><h3 id="transient-vs-recurrent-states">Transient vs Recurrentstates</h3><p>Let <span class="math inline">\(r_{i,j}^{t}\)</span> denote theprobability that the chain, starting at state <span class="math inline">\(i\)</span>, <strong>the first time</strong>transition to state <span class="math inline">\(j\)</span> occurs attime <span class="math inline">\(t\)</span>. That is, <span class="math display">\[r_{i,j}^{t} = P(X_{t} = j, X_{s} \neq j, \forall 1 \leq s \leq t-1 |X_{0} = i)\]</span></p><blockquote><p>Definition 4. A state is <strong>recurrent</strong> if <span class="math inline">\(\sum_{t \geq 1} r_{i,i}^{t} = 1\)</span> and it is<strong>transient</strong> if <span class="math inline">\(\sum_{t \geq1} r_{i,i}^{t} &lt; 1\)</span>. A Markov chain is recurrent if everystate in the chain is recurrent.</p></blockquote><ul><li><p>If state i is recurrent then, once the chain visits that state,it will (with probability 1) eventually return to that state. Hence thechain will visit state <span class="math inline">\(i\)</span> over andover again, <strong>infinitely</strong> often.</p></li><li><p>A transient state has the property that a Markov chain startingat this state returns to this state only <strong>finitelyoften</strong>, with probability 1.</p></li><li><p>If one state in a communicating class is transient (respectively,recurrent) then all states in that class are transient (respectively,recurrent).</p></li></ul><blockquote><p>Definition 5. An irreducible Markov chain is called recurrent if atleast one (equivalently, every) state in this chain is recurrent. Anirreducible Markov chain is called transient if at least one(equivalently, every) state in this chain is transient.</p></blockquote><p>Let <span class="math inline">\(\mu_{i} = \sum_{t \geq 1} t \cdotr_{i,i}^{t}\)</span> denote the expected time to return to state <span class="math inline">\(i\)</span> when starting at state <span class="math inline">\(i\)</span>.</p><blockquote><p>Definition 6. A state <span class="math inline">\(i\)</span> is<strong>positive recurrent</strong> if <span class="math inline">\(\mu_{i} &lt; \infty\)</span> and <strong>nullrecurrent</strong> if <span class="math inline">\(\mu_{i} =\infty\)</span>.</p></blockquote><p>Here we give an example of a Markov chain that has null recurrentstates. Consider the following markov chain whose states are thepositive integers.</p><figure><img src="/2023/07/12/Markov-Chains/image.png" alt="Fig. 1. An example of a Markov chain that has null recurrent states"><figcaption aria-hidden="true">Fig. 1. An example of a Markov chain thathas null recurrent states</figcaption></figure><p>Starting at state 1, the probability of not having returned to state1 within the first <span class="math inline">\(t\)</span> steps is <span class="math display">\[\prod_{j=1}^{t} \frac{j}{j+1} = \frac{1}{t+1}.\]</span> The probability of never returning to state 1 from state 1 is0, and state 1 is recurrent. Thus, the probability of the first timetransition to state <span class="math inline">\(1\)</span> occurs attime <span class="math inline">\(t\)</span> is <span class="math display">\[r_{1,1}^{t} = \frac{1}{t} \cdot \frac{1}{t+1} = \frac{1}{t(t+1)}.\]</span> The expected number of steps until the first return to state 1when starting at state 1 is <span class="math display">\[\mu_{1} = \sum_{t = 1}^{\infty} t \cdot r_{1,1}^{t} = \sum_{t =1}^{\infty} \frac{1}{t+1} = \infty.\]</span> State 1 is recurrent but null recurrent.</p><blockquote><p>Lemma 2. In a finite Markov chain: 1. at least one state isrecurrent; and 2. all recurrent states are positive recurrent.</p></blockquote><p>Thus, all states of a finite, irreducible Markov chain are positiverecurrent.</p><h3 id="periodic-vs-aperiodic-states">Periodic vs Aperiodic states</h3><blockquote><p>Definition 7. A state <span class="math inline">\(j\)</span> in adiscrete time Markov chain is <strong>periodic</strong> if there existsan integer <span class="math inline">\(k&gt;1\)</span> such that <span class="math inline">\(P(X_{t+s}= j | X_t = j) = 0\)</span> unless <span class="math inline">\(s\)</span> is divisible by <span class="math inline">\(k\)</span>. A discrete time Markov chain isperiodic if any state in the chain is periodic. A state or chain that isnot periodic is <strong>aperiodic</strong>.</p></blockquote><p>A state <span class="math inline">\(i\)</span> is periodic means thatfor <span class="math inline">\(s = k, 2k, 3k,...\)</span>, <span class="math inline">\(P(X_{t+s}= j | X_t = j) &gt; 0\)</span>.</p><p><strong>NB: k &gt; 1</strong></p><h3 id="ergodic">Ergodic</h3><blockquote><p>Definition 8. An <strong>aperiodic</strong>, <strong>positiverecurrent</strong> state is an <strong>ergodic</strong> state. A Markovchain is ergodic if all its states are ergodic.</p></blockquote><blockquote><p>Corollary 1. Any finite, irreducible, and aperiodic Markov chain isan ergodic chain.</p></blockquote><h3 id="stationary-distribution">Stationary distribution</h3><p>Consider the two-state “broken printer” Markov chain:</p><figure><img src="/2023/07/12/Markov-Chains/2023-07-22-11-00-52.png" alt="Transition diagram for the two-state broken printer chain"><figcaption aria-hidden="true">Transition diagram for the two-statebroken printer chain</figcaption></figure><p>There are two state (0 and 1) in this Markov chain, and assume thatthe initial distribution is <span class="math display">\[P(X_0 = 0) = \frac{\beta}{\alpha+\beta}, \qquad P(X_0 = 1) =\frac{\alpha}{\alpha+\beta}.\]</span> Then, according to the transition probability matrix <span class="math inline">\(P\)</span>, after one step, the distribution is<span class="math display">\[\begin{align*}P(X_1 = 0) &amp;= P(X_0 = 0)P(X_1 = 0 | X_0 = 0) + P(X_0 = 1)P(X_1 = 0 |X_0 = 1) \\&amp;= \frac{\beta}{\alpha+\beta} \cdot (1-\alpha) +\frac{\alpha}{\alpha+\beta} \cdot \beta = \frac{\beta}{\alpha+\beta}, \\P(X_1 = 1) &amp;= P(X_0 = 0)P(X_1 = 1 | X_0 = 0) + P(X_0 = 1)P(X_1 = 1 |X_0 = 1) \\&amp;= \frac{\beta}{\alpha+\beta} \cdot \alpha +\frac{\alpha}{\alpha+\beta} \cdot (1-\beta) =\frac{\alpha}{\alpha+\beta}.\end{align*}\]</span> Apparently, the distribution of <span class="math inline">\(X_1\)</span> is the same as the initialdistribution. Similarly, we can prove that the distribution of <span class="math inline">\(X_t\)</span> is the same as the initialdistribution for any <span class="math inline">\(t\)</span>. Here, <span class="math inline">\(\pi = (\frac{\beta}{\alpha+\beta},\frac{\alpha}{\alpha+\beta})\)</span> is called <strong>stationarydistribution</strong>.</p><blockquote><p>Definition 9. A probability distribution <span class="math inline">\(\pi = (\pi_i)\)</span>, <span class="math inline">\(\sum_{i \in \Omega} \pi_i = 1\)</span>(<strong>rowvector</strong>) on the state space <span class="math inline">\(\Omega\)</span> is called a <strong>stationarydistribution</strong> (or an equilibrium distribution) for the Markovchain with transition probability matrix <span class="math inline">\(P\)</span> if <span class="math inline">\(\pi =\pi P\)</span>, equivalently, <span class="math inline">\(\pi_j =\sum_{i \in \Omega}\pi_i P_{i,j}\)</span> for all <span class="math inline">\(j \in \Omega\)</span>.</p></blockquote><ul><li><p>One interpretation of the stationary distribution: if we startedoff a <strong>thousand</strong> Markov chains, choosing each startingposition to be state <span class="math inline">\(i\)</span> withprobability <span class="math inline">\(\pi_i\)</span>, then(roughly)<strong><span class="math inline">\(1000 \pi_j\)</span></strong> of themwould be in state <span class="math inline">\(j\)</span> at any time inthe future – but not necessarily the same ones each time.</p></li><li><p>If a chain ever reaches a stationary distribution then itmaintains that distribution for all future time, and thus a stationarydistribution represents a steady state or an equilibrium in the chain’sbehavior.</p></li></ul><h4 id="finding-a-stationary-distribution">Finding a stationarydistribution</h4><p>Consider the following no-claims discount Markov chain with statespace <span class="math inline">\(\Omega = \{1,2,3\}\)</span> andtransition matrix <span class="math display">\[P =\begin{pmatrix}\frac{1}{4} &amp; \frac{3}{4} &amp; 0\\\frac{1}{4} &amp; 0 &amp; \frac{3}{4}\\0 &amp; \frac{1}{4} &amp; \frac{3}{4}\end{pmatrix}\]</span></p><ul><li><p>Step 1: Assume $= {_1, _2, _3} $ is a stationary distribution.According to the definition 9 of stationary distribution, we need tosolve the following equations: <span class="math display">\[\begin{align*}\pi_1 &amp;= \frac{1}{4}\pi_1 + \frac{1}{4}\pi_2, \\\pi_2 &amp;= \frac{3}{4}\pi_1 + \frac{1}{4}\pi_3, \\\pi_3 &amp;= \frac{3}{4}\pi_2 + \frac{3}{4}\pi_3.\end{align*}\]</span> Adding the normalising condition <span class="math inline">\(\pi_1 + \pi_2 + \pi_3 = 1\)</span>, we get fourequations in three unknown parameters.</p></li><li><p>Step 2: Choose one of the parameters, say <span class="math inline">\(\pi_1\)</span>, and solve for the other twoparameters in terms of <span class="math inline">\(\pi_1\)</span>. Weget <span class="math display">\[\pi_1 = \frac{1}{4}\pi_1 + \frac{1}{4}\pi_2 \Rightarrow \pi_2 = 3\pi_1,\qquad \pi_3 = 3\pi_2 = 9\pi_1.\]</span></p></li><li><p>Step 3: Combining with the normalising condition, we get <span class="math display">\[\pi_1 + 3\pi_1 + 9\pi_1 = 1 \Rightarrow \pi_1 = \frac{1}{13}, \qquad\pi_2 = \frac{3}{13}, \qquad \pi_3 = \frac{9}{13}.\]</span> Finally, we get the stationary distribution <span class="math inline">\(\pi = (\frac{1}{13}, \frac{3}{13},\frac{9}{13})\)</span>.</p></li></ul><h4 id="existence-and-uniqueness">Existence and uniqueness</h4><p>Given a Markov chaine, how can we know whether it has a stationarydistribution? If it has, is it unique? At this part, we will answerthese questions.</p><p>Some notations: - Hitting time to hit the state <span class="math inline">\(j\)</span>: <span class="math inline">\(H_{j} =\min \{ t \in \{0, 1, 2,...\}: X_t = j\}\)</span>. Note that here weinclude time <span class="math inline">\(t = 0\)</span>.</p><ul><li>Hitting probability to hit the state <span class="math inline">\(j\)</span> staring from state <span class="math inline">\(i\)</span>: <span class="math inline">\(h_{i,j} =P(X_t = j, \text{for some} \ t \geq 0 | X_0 = i) = P(H_{j} &lt; \infty |X_0 = i) = \sum_{t \geq 0} r_{i,j}^{t}\)</span>.</li></ul><p>Note that this is different from <span class="math inline">\(r_{i,j}^{t}\)</span>, which denotes theprobability that the chain, starting at state <span class="math inline">\(i\)</span>, the <strong>first</strong> timetransition to state <span class="math inline">\(j\)</span><strong>occurs at time <span class="math inline">\(t\)</span></strong>.</p><p>We also have <span class="math display">\[h_{i,j} =\begin{cases}\sum_{k \in \Omega}P_{i,k}h_{k,j} &amp; , &amp; \text{if} \quad j \ne i,\\1 &amp; , &amp; \text{if} \quad  j = i.\end{cases}\]</span> - Expected hitting time: <span class="math inline">\(\eta_{i,j} = E(H_{j} | X_0 = i) = \sum_{t \geq 0}t \cdot r_{i,j}^{t}\)</span>. The expected time until we hit state <span class="math inline">\(j\)</span> starting from state <span class="math inline">\(i\)</span>. We also have <span class="math display">\[\eta_{i,j} =\begin{cases}1 + \sum_{k \in \Omega}P_{i,k}\eta_{k,j} &amp; , &amp; if j \ne i, \\0 &amp; , &amp; if j = i.\end{cases}\]</span> (For the first case, we add 1 because we need to consider thefirst step from state <span class="math inline">\(i\)</span> to state<span class="math inline">\(k\)</span>.)</p><ul><li>Return time: <span class="math inline">\(M_i = \min \{ t \in \{1,2,...\}: X_t = i\}\)</span>. It is different from <span class="math inline">\(H_{i}\)</span>, as we exclude time <span class="math inline">\(t = 0\)</span>. It is the first time that thechain returns to state <span class="math inline">\(i\)</span> after<span class="math inline">\(t = 0\)</span>.</li><li>Return probability: <span class="math inline">\(m_{i} = P(X_t = i  \\text{for some} \ n \geq 1 | X_0 = i) = P(M_i &lt; \infty | X_0 = i) =\sum_{t&gt;1}r_{i,i}^{t}.\)</span></li><li>Expected return time: <span class="math inline">\(\mu_{i} = E(M_i |X_0 = i) = \sum_{t \geq 1} t \cdot r_{i,i}^{t}\)</span>. The expectedtime until we return to state <span class="math inline">\(i\)</span>starting from state <span class="math inline">\(i\)</span>. <span class="math display">\[m_{i} = \sum_{j \in \Omega} P_{i,j}h_{j,i},  \qquad \mu_{i} = 1 +\sum_{j \in \Omega} P_{i,j}\eta_{j,i}.\]</span></li></ul><table style="width:24%;"><colgroup><col style="width: 23%"></colgroup><tbody><tr><td>&gt; Theorem 1. Consider an irreducible Markov chain (<strong>finiteor infinite</strong>), &gt; (1) if it is <strong>positiverecurrent</strong>, <span class="math inline">\(\exists\)</span> anunique stationary distribution <span class="math inline">\(\pi\)</span>,such that <span class="math inline">\(\pi_i =\frac{1}{\mu_{i}}\)</span>. &gt; (2) if it is <strong>nullrecurrent</strong> or <strong>transient</strong>, no stationarydistribution exists.</td></tr><tr><td>Remark: If the chain is <strong>finite</strong> irreducible, it mustbe positive recurrent, thus it has an unique stationarydistribution.</td></tr><tr><td>Remark: If the Markov chain is not irreducible, we can decompose thestate space into several communicating classes. Then, we can considereach communicating class separately. - If none of the classes arepositive recurrent, then no stationary distribution exists. - If exactlyone of the classes is positive recurrent (and therefore closed), thenthere exists a unique stationary distribution, supported only on thatclosed class. - If more the one of the classes are positive recurrent,then many stationary distributions will exist.</td></tr><tr><td>Now, we give the proof of Theorem 1. We first prove that if a Markovchain is irreducible and positive recurrent, then there<strong>exists</strong> a stationary distribution. Next, we will provethe stationary distribution is <strong>unique</strong>. Since the secondpart with the null recurrent or transitive Markov chains is lessimportant and more complicated, we will omit it. If you are interestedin it, you can refer to the book <a href="https://www.statslab.cam.ac.uk/~james/Markov/">Markov Chains</a>by James Norris.</td></tr><tr><td>Proof. (1) Suppose that <span class="math inline">\((X_0, X_1...)\)</span> a recurrent Markov chain, which can be positive recurrentor null recurrent. Then we can desigh a stationary distribution asfollows. (If we can desigh a stationary distribution, then it must beexisted.)</td></tr><tr><td>Let <span class="math inline">\(\nu_i\)</span> be the expectednumber of visits to <span class="math inline">\(i\)</span> before wereturn back to <span class="math inline">\(k\)</span>, <span class="math display">\[\begin{align*}\nu_i &amp;= \mathbb{E}(\# \text{visits to $i$ before returning to } k |X_0 = k) \\&amp;= \mathbb{E}\sum_{t=1}^{M_k} P(X_t = i | X_0 = k) \\&amp;= \mathbb{E}\sum_{t = 0}^{M_k - 1} P(X_t = i | X_0 = k)\end{align*}\]</span> The last equation holds because of $ P(X_0 = i | X_0 = k) = 0$and $ P(X_{M_k} = i | X_0 = k) = 0$.</td></tr><tr><td>If we want design a stationary distribution, it must statisfy <span class="math inline">\(\pi P = \pi\)</span> and <span class="math inline">\(\sum_{i \in \Omega}\pi_i = 1\)</span>.</td></tr><tr><td>(a) We first prove that <span class="math inline">\(\nu P =\nu\)</span>. <span class="math display">\[\begin{align*}\sum_{i \in \Omega} \nu_i P_{i,j} &amp;= \mathbb{E}\sum_{i \in \Omega}\sum_{t = 0}^{M_k - 1} P(X_t = i, X_{t+1} = j | X_0 = k) \\&amp;= \mathbb{E}\sum_{t = 0}^{M_k - 1}  \sum_{i \in \Omega}  P(X_t = i,X_{t+1} = j | X_0 = k) \\&amp;=  \mathbb{E} \sum_{t = 0}^{M_k - 1} P(X_{t+1} = j | X_0 = k) \\&amp;= \mathbb{E} \sum_{t = 1}^{M_k } P(X_{t} = j | X_0 = k) \\&amp;= \mathbb{E} \sum_{t = 0}^{M_k - 1} \nu_i \\&amp;= \nu_j.\end{align*}\]</span> (b) Next, what we need to do is to normalize <span class="math inline">\(\nu\)</span> to get a stationary distribution. Wehave <span class="math display">\[\sum_{i \in \Omega} \nu_i = \sum_{i \in \Omega} \mathbb{E} \sum_{t =0}^{M_k - 1} P(X_t = i | X_0 = k) =\mathbb{E} \sum_{t = 0}^{M_k -1}  \sum_{i \in \Omega}  P(X_t = i | X_0 = k) = E(M_k | X_0 = i) =\mu_k.\]</span> Thus, we can define <span class="math inline">\(\pi_i =\nu_i/\mu_k\)</span>, <span class="math inline">\(\pi = \{\pi_i, i \in\Omega\}\)</span> is one of the stationary distribution.</td></tr><tr><td>(2) Next, we prove that if a Markov chain is irreducible andpositive recurrent, then the stationary distribution is<strong>unique</strong> and is given by <span class="math inline">\(\pi_j = \frac{1}{\mu_j}\)</span>.</td></tr><tr><td>Given a stationary distribution <span class="math inline">\(\pi\)</span>, if we prove that for all <span class="math inline">\(i\)</span>, <span class="math inline">\(\pi_j ==\frac{1}{\mu_j}\)</span>, then we prove that the stationary distributionis unique.</td></tr><tr><td>Remember that the expected hitting time: <span class="math display">\[\eta_{i,j} = 1 + \sum_{k \in \Omega}P_{i,k}\eta_{k,j},  j \ne i  \qquad(eq:1)\]</span> We multiply both sides of (eq:1) by <span class="math inline">\(\pi_i\)</span> and sum over <span class="math inline">\(i (i \ne j)\)</span> to get <span class="math display">\[\sum_{i \ne j} \pi_i \eta_{i,j} = \sum_{i \ne j} \pi_i + \sum_{i \ne j}\sum_{k \in \Omega} \pi_i P_{i,k}\eta_{k,j}\]</span> Since <span class="math inline">\(\eta_{j,j} = 0\)</span>, wecan rewrite the above equation as <span class="math display">\[\sum_{i \in \Omega} \pi_i \eta_{i,j} = \sum_{i \ne j} \pi_i + \sum_{i\ne j} \sum_{k \in \Omega} \pi_i P_{i,k}\eta_{k,j}. \qquad (eq:2)\]</span></td></tr><tr><td>(The above equality lacks <span class="math inline">\(j\)</span>,and we also want to design <span class="math inline">\(\pi_j =1/\mu_j\)</span>.) Remember that the expected return time: <span class="math display">\[ \mu_{j} = 1 + \sum_{i \in \Omega}P_{j,i}\eta_{i,j}. \qquad (eq:3) \]</span> We multiply both sides of(eq:2) by <span class="math inline">\(\pi_j\)</span> to get <span class="math display">\[\pi_j \mu_{j} =\pi_j +  \sum_{k \in \Omega} \pi_j P_{j,k}\eta_{k,j}\qquad (eq:4)\]</span> Adding (eq:2) and (eq:4), we get <span class="math display">\[\begin{align*}\sum_{i \in \Omega} \pi_i \eta_{i,j} + \pi_j \mu_{j} &amp;= \sum_{i \in\Omega} \pi_i + \sum_{i \in \Omega} \sum_{k \in \Omega} \pi_iP_{i,k}\eta_{k,j} \\&amp;= 1 + \sum_{k \in \Omega} \sum_{i \in \Omega}  \pi_iP_{i,k}\eta_{k,j} \\&amp;= 1 +  \sum_{k \in \Omega} \pi_k \eta_{k,j}  \qquad (\text{since}\sum_{i \in \Omega} \pi_i P_{i,k} = \pi_k) \\\end{align*}\]</span> <strong>Since the Markov chain is irreducible and positiverecurrent, that means all states belong to a communication class and theexpected return time of each state is finite. Thus, the space <span class="math inline">\(\Omega\)</span> is a finite dimensionalspace.</strong> We can substract <span class="math inline">\(\sum_{k \in\Omega} \pi_k \eta_{k,j}\)</span> and $_{i } <em>i </em>{i,j} $ (equal)from both sides of the above equation to get <span class="math display">\[\pi_j \mu_{j}=1,\]</span> which means <span class="math inline">\(\pi_j =1/\mu_j\)</span>. Similarly, we can prove that <span class="math inline">\(\pi_i = 1/\mu_i\)</span> for all <span class="math inline">\(i \in \Omega\)</span>.</td></tr></tbody></table><blockquote><p>Theorem 2 (Limit theorem) Consider an irreducible, aperiodic Markovchain (maybe infinite), we have <span class="math inline">\(\lim\limits_{t \to \infty} P_{i,j}^{t} =\frac{1}{\mu_{j}}\)</span>. Spectially, (1) Suppose the Markov chain ispositive recurrent. Then <span class="math inline">\(\lim\limits_{t \to\infty} P_{i,j}^{t} = \pi_j = \frac{1}{\mu_{j}}\)</span>. (2) Supposethe Markov chain is null recurrent or transient. Then there is no limiteprobability.</p></blockquote><ul><li>Three conditions for convergence to an equilibrium probabilitydistribution: irreducibility, aperiodicity, and positive recurrence. Thelimit probability <span class="math display">\[P =\begin{pmatrix}\pi_1 &amp; \pi_2 &amp; \cdots &amp; \pi_N\\\pi_1 &amp; \pi_2 &amp; \cdots &amp; \pi_N\\\vdots &amp; \vdots &amp; \ddots &amp; \vdots\\\pi_1 &amp; \pi_2 &amp; \cdots &amp; \pi_N\\\end{pmatrix}\]</span> where each row is identical.</li></ul><table style="width:7%;"><colgroup><col style="width: 6%"></colgroup><tbody><tr><td>Define <span class="math inline">\(V_{i,j}^{t} = |\{ n &lt; t | X_n= j\}|\)</span>. <span class="math inline">\(V_{i,j}^{t}\)</span> is thenumber of visits to state <span class="math inline">\(j\)</span> beforetime <span class="math inline">\(t\)</span> starting from state <span class="math inline">\(i\)</span>. Then we can interpret <span class="math inline">\(V_{i,j}^{t}/t\)</span> as the proportion of timeup to time <span class="math inline">\(t\)</span> spent in state <span class="math inline">\(j\)</span>.</td></tr><tr><td>&gt; Theorem 3 [Ergodic theorem] Consider an irreducible Markovchain, we have <span class="math inline">\(\lim\limits_{t \to \infty}V_{i,j}^{t}/t = \frac{1}{\mu_{j}}\)</span> <strong>almostsurely</strong>. Spectially, &gt; (1) Suppose the Markov chain ispositive recurrent. Then <span class="math inline">\(\lim\limits_{t \to\infty}  V_{i,j}^{t}/t = \pi_j = \frac{1}{\mu_{j}}\)</span><strong>almost surely</strong>. &gt; (2) Suppose the Markov chain isnull recurrent or transient. Then $ V_{i,j}^{t}/t $ <strong>almostsurely</strong> for all <span class="math inline">\(j\)</span>.</td></tr><tr><td><strong>almost surely</strong> means that the convergenceprobability of the event is 1.</td></tr></tbody></table><blockquote><p>Theorem 4[Detailed balance condition]. Consider a finite,irreducible, and ergodic Markov chain with transition matrix <span class="math inline">\(P\)</span>. If there are nonnegative numbers <span class="math inline">\(\bar{\pi} = (\pi_0, \pi_1, ..., \pi_n)\)</span>such that <span class="math inline">\(\sum_{i=0}^{n} \pi_i = 1\)</span>and if, for any pair of states <span class="math inline">\(i,j\)</span>, <span class="math display">\[\pi_i P_{i,j} = \pi_{j} P_{j,i},\]</span> then <span class="math inline">\(\bar{\pi}\)</span> is thestationary distribution corresponding to <span class="math inline">\(P\)</span>.</p></blockquote><p>Proof. <span class="math display">\[\sum_{i} \pi_i P_{i,j} = \sum_{i}\pi_{j} P_{j,i} = \pi_{j}\]</span> Thus, <span class="math inline">\(\bar{\pi} =\bar{\pi}P\)</span>. Since this is a finite, irreducible, and ergodicMarkov chain, <span class="math inline">\(\bar{\pi}\)</span> must be theunique stationary distribution of the Markov chain.</p><p>Remark: Theorem 2 is a sufficient but not necessary condition.</p><h2 id="reference">Reference</h2><ul><li>Mitzenmacher, M., &amp; Upfal, E. (2005). Probability and Computing.Cambridge University Press.</li><li><a href="https://mpaldridge.github.io/math2750/S09-recurrence-transience.html">Recurrenceand transience</a></li><li><a href="https://mpaldridge.github.io/math2750/S07-classes.html">Classstructure</a></li><li><a href="https://mpaldridge.github.io/math2750/S10-stationary-distributions.html">Stationarydistributions</a></li><li>Stirzaker, David. <a href="https://www.ctanujit.org/uploads/2/5/3/9/25393293/_elementary_probability.pdf">ElementaryProbability</a></li></ul>]]></content>
      
      
      <categories>
          
          <category> Stochastic Process </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Markov Chains </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Gradient Descent, Stochastic Gradient Descent, Variance Reduction</title>
      <link href="/2021/02/27/GD/"/>
      <url>/2021/02/27/GD/</url>
      
        <content type="html"><![CDATA[<h1 id="svrg2013-accelerating-stochastic-gradient-descent-using-predictive-variance-reduction">[SVRG2013]Accelerating Stochastic Gradient Descent using Predictive VarianceReduction</h1><h2 id="introduction">Introduction</h2><p>考虑如下优化问题 <span class="math display">\[\min P(\omega) = \frac{1}{n}\psi_{i}(\omega)\]</span></p><ul><li>如果采用平方损失，最小二乘回归；</li><li>考虑正则项，令 <span class="math inline">\(\psi_{i}(\omega) = \ln(1+ \exp(-\omega^{T}x_{i}y_{i})) + 0.5\lambda\omega^{T}\omega, y_{i} \in\{-1,1\}\)</span>，regularized logistic regression。</li></ul><p>梯度下降算法更新过程： <span class="math display">\[\omega^{(t)} = \omega^{(t-1)} - \eta_{t}\nabla P(\omega^{(t-1)}) =\omega^{(t-1)} - \frac{\eta_{t}}{n}\sum_{i=1}^{n}\nabla\psi_{i}(\omega^{(t-1)})\]</span></p><p>但是，在每一步，GD都需要计算 <span class="math inline">\(n\)</span>个一阶偏导，计算量大。所以，一个改进就是随机梯度下降SGD：在每一步迭代时，随机从<span class="math inline">\(\{1,...,n\}\)</span> 中抽取 <span class="math inline">\(i_{t}\)</span>，然后 <span class="math display">\[\omega^{(t)} = \omega^{(t-1)} - \eta_{t}\nabla\psi_{i_{t}}(\omega^{(t-1)})\]</span></p><p>期望 <span class="math inline">\(E[\omega^{(t)}|\omega^{(t-1)}]\)</span>同梯度更新的结果一致。 SGD的更一般表达形式为 <span class="math display">\[\omega^{(t)} = \omega^{(t-1)} - \eta_{t}g_{t}(\omega^{(t-1)},  \xi_{t})\]</span></p><p><span class="math inline">\(\xi_{t}\)</span> 为一个依赖于 <span class="math inline">\(\omega^{(t-1)}\)</span> 的随机变量， 并且期望<span class="math inline">\(E[g_{t}(\omega^{(t-1)},  \xi_{t})|\omega^{(t-1)}]= \nabla P(\omega^{(t-1)})\)</span>。</p><p>SGD的优势就是每步迭代只需要计算一个梯度，因此计算成本是GD的 <span class="math inline">\(\frac{1}{n}\)</span>。但是，SGD的一个缺点就是随机性引入了方差：虽然<span class="math inline">\(g_{t}(\omega^{(t-1)},  \xi_{t})\)</span>的期望等于梯度 <span class="math inline">\(\nablaP(\omega^{(t-1)})\)</span>，但是每个<span class="math inline">\(g_{t}(\omega^{(t-1)},  \xi_{t})\)</span>是不同的。方差的出现导致收敛速度变慢。对于SGD，由于随机取样带来的方差，一般都会要求其步长 <span class="math inline">\(\eta_{t} = O(1/t)\)</span>，从而得到一个sub-linear 收敛率 <span class="math inline">\(O(1/t)\)</span>。</p><ul><li>GD: 每步迭代计算慢，收敛快。</li><li>SGD：每步迭代计算快，收敛慢。</li></ul><p>为了改进SGD，一些学者开始设计算法以减少方差，从而可以使用较大的步长<span class="math inline">\(\eta_{t}\)</span>。有一些算法被提出来，比如：SAG(stochasticaveragegradient)，SDCA。但是这两个算法需要存储所有的梯度，一些情况下不太实际。因此作者提出了一个新的算法，该算法不需要存储所有的梯度信息，并且有较快的收敛速度，可以应用于非凸优化问题。</p><h2 id="stochastic-variance-reduced-gradient-svrg">Stochastic VarianceReduced Gradient (SVRG)</h2><p>为了保证收敛，SGD的步长必须衰减到0，从而导致收敛率变慢。需要较小步长的原因就是SGD的方差。作者提出一个解决方案。每进行<span class="math inline">\(m\)</span> 次SGD迭代后，记录当前参数 <span class="math inline">\(\tilde{\omega}\)</span> 以及平均梯度： <span class="math display">\[\tilde{\mu} = \nabla P(\tilde{\omega}) = \frac{1}{n}\sum_{i=1}^{n}\nabla \psi_{i}(\tilde{\omega}).\]</span></p><p>然后接下来的更新为：</p><p><span class="math display">\[\omega^{(t)} = \omega^{(t-1)} - \eta_{t}(\nabla\psi_{i_{t}}(\omega^{(t-1)}) - \nabla \psi_{i_{t}}(\tilde{\omega}) +\tilde{\mu})\]</span></p><p>注意到： <span class="math display">\[E[\omega^{(t)}|\omega^{(t-1)}] = \omega^{(t-1)} - \eta_{t}\nablaP(\omega^{(t-1)})\]</span></p><p>算法如下：</p><p><img src="/2021/02/27/GD/SVRG.png"></p><p>更新步骤中梯度的方差是减小的。当 <span class="math inline">\(\tilde{\omega}\)</span> 和 <span class="math inline">\(\omega^{(t)}\)</span> 收敛到最优参数 <span class="math inline">\(\omega_{*}\)</span>，<span class="math inline">\(\tilde{\mu} \to 0\)</span>， <span class="math inline">\(\nabla\psi_{i}(\tilde{\omega}) \to \nabla\psi_{i}(\omega_{*})\)</span>，有 <span class="math display">\[\nabla\psi_{i}(\omega^{(t-1)}) - \nabla\psi_{i}(\tilde{\omega}) + \tilde{\mu} \to \nabla\psi_{i}(\omega^{(t-1)}) -\nabla\psi_{i}(\omega_{*}) \to 0\]</span></p><p>SVRG的学习率不需要衰减，因此能有较快的收敛速度。作者提到，参数 <span class="math inline">\(m\)</span> 应该 <span class="math inline">\(O(n)\)</span>， 比如对于凸问题：<span class="math inline">\(m = 2n\)</span>，非凸问题：<span class="math inline">\(m = 2n\)</span>。</p><h1 id="saga2015-saga-a-fast-incremental-gradient-method-with-support-for-non-strongly-convex-composite-objectives">[SAGA2015]SAGA: A Fast Incremental Gradient Method With Support for Non-StronglyConvex Composite Objectives</h1><p>考虑最小化函数： <span class="math display">\[f(x) = \frac{1}{n}\sum_{i=1}^{n}f_{i}(x)\]</span></p><p>作者提出了一个叫做SAGA的算法，在目标函数为强凸函数时，SAGA的收敛速度优于SAG和SVRG。算法如下：</p><p><img src="/2021/02/27/GD/SAGA.png"></p><p>本文给出了variance reduction算法的一个解释： <img src="/2021/02/27/GD/SAGA1.png"></p><p>几种算法比较： <img src="/2021/02/27/GD/SAGA2.png"></p><h1 id="variance-reduced-stochastic-gradient-descent-with-neighbors-2015">[VarianceReduced Stochastic Gradient Descent with Neighbors 2015]</h1><h1 id="katyusha-2017-katyusha-the-first-direct-acceleration-of-stochastic-gradient-methods">[Katyusha2017] Katyusha: The First Direct Acceleration of Stochastic GradientMethods</h1><p>Nesterov's momentum通常用于加速梯度下降算法，但是，对于随机梯度下降，Nesterov's momentum可能无法对算法进行加速，即使优化目标为凸函数。因此，针对SGD，作者提出Katyusha 算法，借助于动量Katyusha momentum实现加速SGD。</p><p>考虑如下优化问题：</p><p><span class="math display">\[\min_{x \in \mathbb{R}^{d}} \{F(x) = f(x) + \psi(x) =\frac{1}{n}\sum_{i=1}^{n} f_{i}(x) + \psi(x)\}\]</span> 其中 <span class="math inline">\(f(x) =\frac{1}{n}\sum_{i=1}^{n} f_{i}(x)\)</span> 为凸函数，并且是 <span class="math inline">\(n\)</span> 个凸函数的有限平均。<span class="math inline">\(\psi(x)\)</span>为凸函数，可为近端函数。大多数假设 <span class="math inline">\(\psi(x)\)</span> 为 <span class="math inline">\(\sigma\)</span>-strongly，并且 <span class="math inline">\(f_{i}(x)\)</span> L-smooth。</p><p>作者提出一个可以求解上述优化问题的加速随机梯度下降算法-Katyusha：</p><p><img src="/2021/02/27/GD/ka.png"></p><p>其中，<span class="math inline">\(\tilde{x}\)</span> 为snapshotpoint，每经过 <span class="math inline">\(n\)</span>次迭代更新一次。<span class="math inline">\(\tilde{\nabla}_{k+1}\)</span> 为variance reduction中的梯度形式。<span class="math inline">\(\tau_{1}\)</span>，<span class="math inline">\(\tau_{2} \in[0,1]\)</span> 为两个动量参数，<span class="math inline">\(\alpha = \frac{1}{3\tau_{1}L}\)</span>。</p><h2 id="our-new-technique-katyusha-momentum">Our New Technique –Katyusha Momentum</h2><p>最novel的部分是 <span class="math inline">\(x_{k+1}\)</span>的更新步骤，是 <span class="math inline">\(y_{k}\)</span>, <span class="math inline">\(z_{k}\)</span> 以及 <span class="math inline">\(\tilde{x}\)</span> 的凸组合。理论建议参数 <span class="math inline">\(\tau_{2} = 0.5\)</span>，<span class="math inline">\(\tau_{1} = \min\{\sqrt{n\sigma/L},0.5\}\)</span>。</p><p>对于传统的加速梯度下降算法，<span class="math inline">\(x_{k+1}\)</span> 仅仅是 <span class="math inline">\(y_{k}\)</span> 和 <span class="math inline">\(z_{k}\)</span> 的凸组合，<span class="math inline">\(z_{k}\)</span>起到了一个“动量”的作用，即将历史加权的梯度信息添加到 <span class="math inline">\(y_{k+1}\)</span> 上。比如假设 <span class="math inline">\(\tau_{2} = 0, \tau_{1} = \tau\)</span>, <span class="math inline">\(x_{0} = y_{0} = z_{0}\)</span>，我们可以得到：</p><p><img src="/2021/02/27/GD/ka1.png"></p><p>由于 <span class="math inline">\(\alpha\)</span> 通常大于 <span class="math inline">\(1/3L\)</span>，上述递推过程意味着随着迭代进行，梯度<span class="math inline">\(\tilde{\nabla}_{t}\)</span>的贡献越高。比如，<span class="math inline">\(\tilde{\nabla}_{1}\)</span> 的权重不断增大 (<span class="math inline">\(\frac{1}{3L} &lt; ((1-\tau)\frac{1}{3L} +\tau\alpha) &lt; ((1 - \tau)^{2}\frac{1}{3L} + (1 - (1 -\tau)^{2})\alpha)\)</span>)。这就是一阶加速方法的核心思想。</p><p>在Katyusha算法中，作者认为 <span class="math inline">\(\tilde{x}\)</span> 同等重要，它能保证 <span class="math inline">\(x_{k+1}\)</span> 不要太远离 <span class="math inline">\(\tilde{x}\)</span>。<span class="math inline">\(\tilde{x}\)</span> 的添加可以看作是一个 “negativemomentum”，使 <span class="math inline">\(x_{k+1}\)</span> back to <span class="math inline">\(\tilde{x}\)</span>，抵消一部分前面迭代时的positive momentum”。</p><p>当 <span class="math inline">\(\tau_{1} = \tau_{2} = 0.5\)</span>时，Katyusha同SVRG几乎一致。</p><h1 id="l-svrg-l-katyusha-2019-dont-jump-through-hoops-and-remove-those-loops-svrg-and-katyusha-are-betterwithout-the-outer-loop">[L-SVRG,L-Katyusha 2019] Don’t Jump Through Hoops and Remove Those Loops: SVRGand Katyusha are BetterWithout the Outer Loop</h1><p>SVRG和Katyusha算法的共同关键结构就是两者都包含一个外层循环 (outerloop)。最初先在outerloop上使用所有样本计算梯度，然后计算出来的结果再用于内层循环 (innerloop)，结合新的随机梯度信息，构造variance-reduced梯度估计量。作者指出，由于SVRG和Katyusha算法都包括一个outerloop，所以存在一些问题，比如：算法很难分析；人们需要决定内部循环的次数。对于SVRG，理论上内部循环的最优次数取决于<span class="math inline">\(L\)</span>和 <span class="math inline">\(\mu\)</span>，但是 <span class="math inline">\(\mu\)</span>通常未知。由于这些问题存在，人们只能选择次优的inner loopsize，通常设置内部循环次数为 <span class="math inline">\(O(n)\)</span>或者 <span class="math inline">\(n\)</span>。</p><p>在这篇论文中，作者将外层循环 (outer loop)丢弃，在每次迭代时采用掷硬币技巧决定是否计算梯度，从而解决了上述问题。作者证明，新提出的算法和原始两个算法具有同样的理论性质。</p><p><img src="/2021/02/27/GD/loopless1.jpg"> <img src="/2021/02/27/GD/loopless1.jpg"></p><h1 id="l-svrg-l-katyusha-2019-l-svrg-and-l-katyusha-with-arbitrary-sampling">[L-SVRG,L-Katyusha 2019] L-SVRG and L-Katyusha with Arbitrary Sampling</h1><h1 id="参考文献">参考文献</h1><ul><li>Johnson, R. and Zhang, T. Accelerating stochastic gradient descentusing predictive variance reduction. In Advances in Neural InformationProcessing Systems 26, pp. 315–323, 2013a.<br></li><li>Defazio, A., Bach, F., and Lacoste-Julien, S. SAGA: a fastincremental gradient method with support for non-strongly convexcomposite objectives. In Advances in Neural Information ProcessingSystems, pp. 1646–1654, 2014.<br></li><li>Hofmann, T., Lucchi, A., Lacoste-Julien, S., and McWilliams, B.Variance reduced stochastic gradient descent with neighbors. In Advancesin Neural Information Processing Systems, pp.2305–2313, 2015.<br></li><li>Allen-Zhu, Z. Katyusha: The first direct acceleration of stochasticgradient methods. In Proceedings of the 49th Annual ACM SIGACT Symposiumon Theory of Computing, pp.1200–1205. ACM,2017.</li><li>Kovalev, D., Horváth, S., and Richtárik, P. Don’t jump through hoopsand remove those loops: SVRG and Katyusha are better without the outerloop. In Proceedings of the 31st International Conference on AlgorithmicLearning Theory, 2020.</li><li>Qian, X., Qu, Z., and Richtárik, P. L-SVRG and L-Katyusha witharbitrary sampling. arXiv preprint arXiv:1906.01481, 2019a.</li></ul>]]></content>
      
      
      <categories>
          
          <category> Machine Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> GD </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>ADMMdecentralized</title>
      <link href="/2021/02/03/ADMMdecentralized/"/>
      <url>/2021/02/03/ADMMdecentralized/</url>
      
        <content type="html"><![CDATA[<ul><li><a href="#abstract">Abstract</a></li><li><a href="#introduction">Introduction</a></li><li><a href="#preliminaries">Preliminaries</a><ul><li><a href="#notations-and-problem-setting">Notations and ProblemSetting</a></li></ul></li><li><a href="#model-propagation">Model Propagation</a><ul><li><a href="#asynchronous-gossip-algorithm">Asynchronous GossipAlgorithm</a></li></ul></li><li><a href="#collaborative-learning">Collaborative Learning</a><ul><li><a href="#problem-formulation">Problem Formulation</a></li><li><a href="#asynchronous-gossip-algorithm-1">Asynchronous GossipAlgorithm</a></li></ul></li><li><a href="#experiments">Experiments</a><ul><li><a href="#collaborative-linear-classification">Collaborative LinearClassification</a></li></ul></li><li><a href="#参考文献">参考文献</a></li></ul><!-- /TOC --><h1 id="abstract">Abstract</h1><p>考虑点对点的协作网络。本文解决的问题是：每个节点如何与具有相似目标的其他节点进行通信来改善本地模型？作者介绍了两种完全去中心化的算法，一种是受标签传播的启发，旨在平滑预先训练好的局部模型；第二种方法，节点基于本地数据核相邻节点进行迭代更新来共同学习和传播。</p><h1 id="introduction">Introduction</h1><p>数据不断产生，当前从数据中提取信息的主要方式是收集所有用户的个人数据于一个服务器上，然后进行数据挖掘。但是，中心化的方式存在一些问题，比如说一些用户拒绝提供个人数据，带宽和设备花费问题。即使一些算法允许数据分布在用户设备上，通常需要中心端来进行聚合和协调。</p><p>在本文中，作者考虑完全去中心化的点对点网络。不同于那些求解全局模型的算法，本文关注于每个节点可以根据自身目标函数学习一个个性化模型。作者假设网络结构已知，该网络结构能够反映出不同节点的相似度（如果两个节点具有相似的目标函数，那么这两个节点在网络中是邻居），每个节点只知道与其直接相邻的节点。一个节点不仅可以根据自身数据学习模型，还可以结合它的邻居。假设每个节点只知道相邻节点的信息，不知道整个网络结构。</p><p>作者提出两个算法。第一个是 modelpropagation：首先，每个节点先基于自己的局部数据学习到模型参数，然后，结合整个网络结构，平滑这些参数。第二个是collaborativelearning，这个算法更加灵活，它通过优化一个模型参数正则化（平滑）和局部模型准确性上的折中问题。作者基于分布式的ADMM算法提出一个异步gossip算法。</p><h1 id="preliminaries">Preliminaries</h1><h2 id="notations-and-problem-setting">Notations and ProblemSetting</h2><p>考虑 <span class="math inline">\(n\)</span> 个节点 <span class="math inline">\(V = [n] = \{1,...,n\}\)</span>。凸的损失函数 <span class="math inline">\(l: \mathbb{R}^{p} \times \mathcal{X} \times\mathcal{Y}\)</span>，节点 <span class="math inline">\(i\)</span>的目标是学习模型参数 <span class="math inline">\(\theta_{i} \in\mathbb{R}^{p}\)</span>，使得关于未知分布 <span class="math inline">\(\mu_{i}\)</span> 的期望损失 <span class="math inline">\(E_{(x_{i}, y_{i})\sim \mu_{i}}l(\theta_{i}; x_{i},y_{i})\)</span> 很小。节点 <span class="math inline">\(i\)</span> 具有<span class="math inline">\(m_{i}\)</span> 个来自分布 <span class="math inline">\(\mu_{i}\)</span> 的 i.i.d 的训练样本 <span class="math inline">\(S_{i} = \{(x_{i}^{j},y_{i}^{j})\}_{j=1}^{m_{i}}\)</span>。允许不同节点的样本量相差很大。每个节点可以最小化局部损失函数得到<span class="math inline">\(\theta_{i}^{sol}\)</span>:</p><p><span class="math display">\[\theta_{i}^{sol} \in \argmin_{\theta \in \mathbb{R}^{p}} L_{i}(\theta) =\sum_{j=1}^{m_{i}} l(\theta;x_{i}^{j}, y_{i}^{j}).\]</span></p><p>我们目标是通过结合其他节点信息，进一步改善上述模型。考虑一个加权网络结构<span class="math inline">\(G = (V, E)\)</span>，具有 <span class="math inline">\(V\)</span> 个节点，<span class="math inline">\(E\subseteq V \times V\)</span> 为无向边。定义 <span class="math inline">\(W \in \mathbb{R}^{n \times n}\)</span> 为由 <span class="math inline">\(G\)</span> 得到的对称非负加权矩阵，如果 <span class="math inline">\((i,j) \ne E\)</span> or <span class="math inline">\(i = j\)</span>， <span class="math inline">\(W_{ij} =0\)</span>。本文假设权重矩阵已知。定义对角阵 <span class="math inline">\(D\in \mathbb{R}^{n \times n}\)</span>，<span class="math inline">\(D_{ii} = \sum_{j=1}^{n} W_{ij}\)</span>。节点<span class="math inline">\(i\)</span> 的邻域 ：<span class="math inline">\(\mathcal{N}_{i} = \{j \ne i: W_{ij} &gt;0\}\)</span>。</p><h1 id="model-propagation">Model Propagation</h1><p>假设每个节点通过最小化局部损失函数得到各自的模型 <span class="math inline">\(\theta_{i}^{sol}\)</span>。由于每个节点上的模型都是在不同大小数据集上考虑得到，作者使用<span class="math inline">\(c_{i} \in (0,1]\)</span>定义每个节点模型的可信度。 <span class="math inline">\(c_{i}\)</span>的值应该和节点 <span class="math inline">\(i\)</span>的样本量大小呈正相关，可以设置为 <span class="math inline">\(c_{i} =\frac{m_{i}}{\max_{j} m_{j}}\)</span>。如果 <span class="math inline">\(m_{i}=0\)</span>，可以设置为一个小量。</p><p>定义 <span class="math inline">\(\Theta = [\theta_{1};\theta_{2};...;\theta_{n}] \in \mathbb{R}^{n \timesp}\)</span>，我们要优化的目标函数为：</p><figure><img src="https://cdn.jsdelivr.net/gh/sugar-xue/BlogImages@main/Picbed1612018569.jpg" alt="1612018569"><figcaption aria-hidden="true">1612018569</figcaption></figure><p>第一项二次函数用来平滑相邻节点的参数，当两个节点间权重越大时，节点间参数越相近；第二项的目的是使具有较高置信度的模型的参数不要太远离各自模型上的参数。具有较低置信度的模型的参数被允许具有较大的偏差，容易被相邻节点影响。<span class="math inline">\(D_{ii}\)</span> 的目的是为了normalization。</p><p><img src="https://cdn.jsdelivr.net/gh/sugar-xue/BlogImages@main/Picbed1612018994(1).png" alt="1612018994(1)"><br><img src="https://cdn.jsdelivr.net/gh/sugar-xue/BlogImages@main/Picbed1612019022(1).png" alt="1612019022(1)"></p><p>计算 (4)需要知道整个网络的信息以及所有节点的独立模型信息，这对于节点而言是未知的，因为每个节点只知道相邻节点的信息。因此，作者提出下面的迭代形式：</p><figure><img src="https://cdn.jsdelivr.net/gh/sugar-xue/BlogImages@main/Picbed1612019342(1).png" alt="1612019342(1)"><figcaption aria-hidden="true">1612019342(1)</figcaption></figure><p>作者证明，无论初始值 <span class="math inline">\(\Theta(0)\)</span>取何值，上述迭代序列收敛到 (4)。(5) 式可以进一步分解为</p><figure><img src="https://cdn.jsdelivr.net/gh/sugar-xue/BlogImages@main/Picbed1612019480(1).png" alt="1612019480(1)"><figcaption aria-hidden="true">1612019480(1)</figcaption></figure><p>考虑一个同步计算：在每一步，每个节点都和其所有相邻节点进行通信，收集它们当前参数，然后使用它们的参数更新上式。同步更新会导致很大的延迟，因为任何节点都必须等剩余节点更新完后才能进行下一步更新。并且，每一步，所有节点都需要和其邻居节点进行通信，降低了算法的效率。所以作者提出一个异步算法。</p><h2 id="asynchronous-gossip-algorithm">Asynchronous GossipAlgorithm</h2><p>在异步设置中，每个节点都有一个局部clock ticking at times of rate 1Poisson process.由于节点都是独立同分布的，所以相当于在每一步时等概率激活每个节点。</p><p>在时间 <span class="math inline">\(t\)</span>时，每个节点都存有相邻节点的信息。以数学形式表示，考虑矩阵 <span class="math inline">\(\tilde{\Theta}_{i}(t) \in \mathbb{R}^{n \timesp}\)</span>，第 <span class="math inline">\(i\)</span> 行 <span class="math inline">\(\tilde{\Theta}_{i}^{i}(t) \in\mathbb{R}^{p}\)</span> 为节点 <span class="math inline">\(i\)</span>在时刻 <span class="math inline">\(t\)</span> 的模型参数，<span class="math inline">\(\tilde{\Theta}_{i}^{j}(t) \in \mathbb{R}^{p} (j\ne i)\)</span> 为节点 <span class="math inline">\(i\)</span>储存的关于邻居节点 <span class="math inline">\(j\)</span> 的lastknowledge. 对于 <span class="math inline">\(j \notin \mathcal{N}_{i}\bigcup \{i\}\)</span>，<span class="math inline">\(\forall t &gt;0\)</span>，<span class="math inline">\(\tilde{\Theta}_{i}^{j}(t) =0\)</span>。令<span class="math inline">\(\tilde{\Theta} =[\tilde{\Theta}_{1}^{T}, ...,\tilde{\Theta}_{n}^{T}] \in\mathbb{R}^{n^{2} \times p}\)</span>。</p><p>如果在时间 <span class="math inline">\(t\)</span> 时，节点 <span class="math inline">\(i\)</span> wakes up，执行如下步骤：</p><ul><li><p>communication: 节点 <span class="math inline">\(i\)</span>随机选择一个邻居节点 <span class="math inline">\(j \in\mathcal{N}_{i}\)</span>，(先验概率 <span class="math inline">\(\pi_{i}^{j}\)</span>)，节点 <span class="math inline">\(i\)</span> 和节点 <span class="math inline">\(j\)</span> 同时更新它们的参数： <span class="math display">\[\tilde{\Theta}_{i}^{j}(t+1) = \tilde{\Theta}_{j}^{j}(t) \qquad\tilde{\Theta}_{j}^{i}(t+1) = \tilde{\Theta}_{i}^{i}(t),\]</span></p></li><li><p>update: 基于当前信息，节点 <span class="math inline">\(i\)</span>和节点 <span class="math inline">\(j\)</span> 更新自己的模型参数： <span class="math display">\[\tilde{\Theta}_{l}^{l}(t+1) = (\alpha +\bar{\alpha}c_{l})^{-1}(\alpha\sum_{k \in \mathcal{N}_{l}}\frac{W_{lk}}{D_{ll}}\tilde{\Theta}_{l}^{k}(t+1) +\bar{\alpha}c_{l}\theta^{sol}_{l}) \quad(l \in \{i,j\}).\]</span></p><p>网络中的其他变量保持不变。作者提出的算法属于 gossipalgorithms，每个节点每次最多只和一个邻居节点通信。</p><p>作者证明，上述算法可以收敛到使每个节点具有最优参数。</p><figure><img src="https://cdn.jsdelivr.net/gh/sugar-xue/BlogImages@main/Picbed1612340701.jpg" alt="1612340701"><figcaption aria-hidden="true">1612340701</figcaption></figure></li></ul><h1 id="collaborative-learning">Collaborative Learning</h1><p>上述算法先在局部节点上进行学习，然后在进行网络通信。在这部分，作者提出了一个使节点可以同时进行基于局部数据和邻居节点信息更新模型参数的算法。相较于前面的算法，该算法通信成本较高，但是估计精度高于前者。</p><h2 id="problem-formulation">Problem Formulation</h2><p>优化目标：</p><figure><img src="https://cdn.jsdelivr.net/gh/sugar-xue/BlogImages@main/Picbed1612341031(1).png" alt="1612341031(1)"><figcaption aria-hidden="true">1612341031(1)</figcaption></figure><p>注意到，这里的置信度通过 <span class="math inline">\(\mathcal{L}_{i}\)</span> 体现，因为 <span class="math inline">\(\mathcal{L}_{i}\)</span> 为局部节点 <span class="math inline">\(i\)</span> 上所有观测的损失函数和。</p><p>一般情况下，上述问题没有解析解，作者提出一个分散式迭代算法进行求解。</p><h2 id="asynchronous-gossip-algorithm-1">Asynchronous GossipAlgorithm</h2><p>作者基于ADMM提出了一个异步分散式算法。本文的目的不是寻找一个consensus解，因为我们的目标是为了学习到每个节点的personalized model.作者通过将问题 (7)进行变换为一个部分consensus问题，使用ADMMD进行求解。</p><p>令 <span class="math inline">\(\Theta_{i} \in\mathbb{R}^{(|\mathcal{N_{i}}|+1)\times p}\)</span> 为变量 <span class="math inline">\(\theta_{j} \in \mathbb{R}^{p}(j \in\mathcal{N_{i}} \bigcup \{i\})\)</span> 的集合。定义 <span class="math inline">\(\theta_{j}\)</span> 为 <span class="math inline">\(\Theta_{i}^{j}\)</span>。优化问题(7)重新写为：</p><figure><img src="https://cdn.jsdelivr.net/gh/sugar-xue/BlogImages@main/Picbed1612341938(1).png" alt="1612341938(1)"><figcaption aria-hidden="true">1612341938(1)</figcaption></figure><p>在这个目标函数中，所有的节点相互依赖，因为它们共享一个优化变量 <span class="math inline">\(\in\Theta\)</span>。为了使用ADMM，需要将各个节点的优化变量独立，对于每个节点<span class="math inline">\(i\)</span>，定义一个local copy <span class="math inline">\(\tilde{\Theta}_{i} \in\mathbb{R}^{(|\mathcal{N_{i}}|+1)\times p}\)</span>，添加等式约束：<span class="math inline">\(\tilde{\Theta}_{i}^{i} =\tilde{\Theta}_{j}^{i}\)</span>，对于所有的 <span class="math inline">\(i \in [n], j \in \mathcal{N}_{i}\)</span>。</p><figure><img src="https://cdn.jsdelivr.net/gh/sugar-xue/BlogImages@main/Picbed1612342502(1).png" alt="1612342502(1)"><figcaption aria-hidden="true">1612342502(1)</figcaption></figure><p>增广拉格朗日乘子：</p><figure><img src="https://cdn.jsdelivr.net/gh/sugar-xue/BlogImages@main/Picbed1612342580(1).png" alt="1612342580(1)"><figcaption aria-hidden="true">1612342580(1)</figcaption></figure><p>算法如下，假设时刻 <span class="math inline">\(t\)</span> 时节点<span class="math inline">\(i\)</span> wakes up，选取邻居节点 <span class="math inline">\(j \in \mathcal{N}_{i}\)</span>，定义 <span class="math inline">\(e = (i,j)\)</span>，</p><figure><img src="https://cdn.jsdelivr.net/gh/sugar-xue/BlogImages@main/Picbed1612342728(1).png" alt="1612342728(1)"><figcaption aria-hidden="true">1612342728(1)</figcaption></figure><figure><img src="https://cdn.jsdelivr.net/gh/sugar-xue/BlogImages@main/Picbed1612342765(1).png" alt="1612342765(1)"><figcaption aria-hidden="true">1612342765(1)</figcaption></figure><h1 id="experiments">Experiments</h1><h2 id="collaborative-linear-classification">Collaborative LinearClassification</h2><p>考虑100个节点，每个节点的目标是建立一个线性分类模型 in <span class="math inline">\(\mathbb{R}^{p}\)</span>。为了方便可视化，每个节点的真实参数位于2维子空间：将其参数看作是<span class="math inline">\(\mathbb{R}^{p}\)</span>空间中的向量，前两项从正态分布中随机产生，剩余项为0。两个节点 <span class="math inline">\(i\)</span> 和节点 <span class="math inline">\(j\)</span> 的相似度通过参数距离的高斯核定义，定义<span class="math inline">\(\phi_{ij}\)</span>为两个真实参数在单位圆上投影的夹角，<span class="math inline">\(W_{ij} =\exp(\cos \phi_{ij} - 1)/\sigma\)</span>，<span class="math inline">\(\sigma =0.1\)</span>。权重为负值的将被忽略。每个节点具有随机的训练样本，样本的标签为二元标签，由线性分类模型产生。以概率0.05随机使标签反转，以产生噪音数据。每个节点的损失函数为hinge损失：<span class="math inline">\(l(\theta;(x_{i}, y_{i})) = \max(0,1-y_{i}\theta^{T}x_{i})\)</span>。作者评估了模型在100个测试样本上的预测精度。</p><figure><img src="https://cdn.jsdelivr.net/gh/sugar-xue/BlogImages@main/Picbed1612343843(1).png" alt="1612343843(1)"><figcaption aria-hidden="true">1612343843(1)</figcaption></figure><h1 id="参考文献">参考文献</h1><ul><li>Vanhaesebrouck, P., Bellet, A. &amp; Tommasi, M.. (2017).Decentralized Collaborative Learning of Personalized Models overNetworks. Proceedings of the 20th International Conference on ArtificialIntelligence and Statistics, in PMLR 54:509-517</li></ul>]]></content>
      
      
      <categories>
          
          <category> Distributed Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> FL </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Lower Bounds and Optimal Algorithms for Personalized Federated Learning</title>
      <link href="/2021/01/26/AL2SGD/"/>
      <url>/2021/01/26/AL2SGD/</url>
      
        <content type="html"><![CDATA[<ul><li><a href="#introduction">Introduction</a></li><li><a href="#contributions">Contributions</a></li><li><a href="#lower-complexity-bounds">Lower complexity bounds</a><ul><li><a href="#lower-complexity-bounds-on-the-communication">Lowercomplexity bounds on the communication</a></li><li><a href="#lower-complexity-bounds-on-the-local-computation">Lowercomplexity bounds on the local computation</a></li></ul></li><li><a href="#优化算法">优化算法</a><ul><li><a href="#accelerated-proximal-gradient-descent-apgd-for-federated-learning">AcceleratedProximal Gradient Descent (APGD) for Federated Learning</a></li><li><a href="#beyond-proximal-oracle-inexact-apgd-iapgd">Beyond proximaloracle: Inexact APGD (IAPGD)</a></li><li><a href="#accelerated-l2sgd">Accelerated L2SGD+</a></li></ul></li><li><a href="#experiments">Experiments</a></li><li><a href="#参考文献">参考文献</a></li></ul><!-- /TOC --><h1 id="introduction">Introduction</h1><p>作者在前文考虑了一个新的优化问题： <span class="math display">\[\min_{\mathbf{x_{1},...,x_{n}} \in \mathbb{R}^{d}} \{ F(\mathbf{x}) :=f(\mathbf{x}) + \lambda \psi(\mathbf{x})\}\]</span> <span class="math display">\[f(\mathbf{x}):= \frac{1}{n}\sum_{i=1}^{n}f_{i}(\mathbf{x_{i}}), \quad\psi(\mathbf{x}):= \frac{1}{2n}\sum_{i=1}^{n}\|\mathbf{x_{i}} -\mathbf{\bar{x}}\|^{2}\]</span></p><p>Remark：该问题的最优解 <span class="math inline">\(\mathbf{x}^{*} =[\mathbf{x}_{1}^{*},..., \mathbf{x}_{n}^{*}] \in\mathbb{R}^{nd}\)</span> 可以被表示为 <span class="math inline">\(\mathbf{x}^{*}_{i} = \mathbf{\bar{x}}^{*} -\frac{1}{\lambda}\nabla f_{i}(\mathbf{x}_{i}^{*})\)</span>，其中 <span class="math inline">\(\mathbf{\bar{x}}^{*} = \frac{1}{n}\sum_{i=1}^{n}\mathbf{x}_{i}^{*}\)</span>，该形式与MAML相似。</p><h1 id="contributions">Contributions</h1><p>在这篇论文中，作者给出了求解上述优化问题的通信和局部计算复杂度（迭代次数）的最低界限，并且给出了几种能够达到最低界限的算法。</p><ul><li><p>lower bound on the communication complexity.作者证明对于任意一个满足一定假设条件的算法，会有一个L-smooth, <span class="math inline">\(\mu\)</span>-strongly convex 局部目标函数 <span class="math inline">\(f_{i}\)</span> 至少需要通信 <span class="math inline">\(O(\sqrt{\frac{\min\{L,  \lambda\}}{\mu}}\log\frac{1}{\epsilon})\)</span>轮才能得到最优解 <span class="math inline">\(\epsilon\)</span>邻域内的解。</p></li><li><p>lower complexity bound on the number of local oracle calls.作者证明对于局部近端梯度下降，至少需要迭代<span class="math inline">\(O(\sqrt{\frac{\min\{L,  \lambda\}}{\mu}}\log\frac{1}{\epsilon})\)</span>次；对于局部梯度下降，至少需要进行 <span class="math inline">\(O(\sqrt{\frac{L}{\mu}}\log\frac{1}{\epsilon})\)</span>次迭代；若每个目标函数为 <span class="math inline">\(m\)</span>个有限和形式（<span class="math inline">\(\tilde{L}\)</span>-smooth)，至少需要 <span class="math inline">\(O((m +\sqrt{\frac{m\tilde{L}}{\mu}})\log\frac{1}{\epsilon})\)</span>次。</p></li><li><p>作者讨论了不同的用于求解上述优化问题的算法，这些算法在不同设定下可以达到最优通信复杂度和最优局部梯度复杂度。。首先是加速近端梯度下降算法(APGD)，作者考虑两种不同的应用方式，第一种是：函数<span class="math inline">\(f\)</span> 采用梯度下降，<span class="math inline">\(\lambda \psi\)</span>采用近端梯度下降，第二种是反过来。对于第一种情况，当 <span class="math inline">\(L \leq \lambda\)</span>时，我们可以实现最优通信复杂度和局部梯度复杂度 <span class="math inline">\(O(\sqrt{\frac{L}{\mu}}\log\frac{1}{\epsilon})\)</span>；对于第二种情况，当<span class="math inline">\(L \geq \lambda\)</span>时，我们可以得到最优通信复杂度和局部近端复杂度 <span class="math inline">\(O(\sqrt{\frac{\lambda}{\mu}}\log\frac{1}{\epsilon})\)</span>。受一篇论文启发，作者提到局部近端可以由局部加速梯度下降 (Local AGD) 近似(inexactly) 得到，当目标函数为有限和形式，还可以采用Katyusha算法近似得到。Local AGD 可以得到 <span class="math inline">\(O(\sqrt{\frac{\lambda}{\mu}}\log\frac{1}{\epsilon})\)</span>的通信复杂度，以及 <span class="math inline">\(\tilde{O}(\sqrt{\frac{L+\lambda}{\mu}})\)</span>的局部梯度复杂度，当 <span class="math inline">\(L \geq\lambda\)</span>（取决于对数因子）时，两者都能达到最优。同样，当局部采用 Katyusha时，我们可以得到通信复杂度 <span class="math inline">\(O(\sqrt{\frac{\lambda}{\mu}}\log\frac{1}{\epsilon})\)</span>和局部梯度复杂度 <span class="math inline">\(\tilde{O}(m\sqrt{\frac{\lambda}{\mu}} + \sqrt{m\frac{\tilde{L}}{\mu}})\)</span>，前者当 <span class="math inline">\(L\geq \Lambda\)</span> 时能达到最优，后者当 <span class="math inline">\(m\lambda \leq \tilde{L}\)</span>（取决于对数因子）时达到最优。</p></li><li><p>作者提出了加速的L2SGD+算法-AL2SGD+，该算法可以实现最优通信复杂 度<span class="math inline">\(O(\sqrt{\frac{\min\{\tilde{L},  \lambda\}}{\mu}}\log\frac{1}{\epsilon})\)</span>，以及局部梯度复杂度<span class="math inline">\(O((m + \sqrt{\frac{m(\tilde{L} +\lambda)}{\mu}})\log\frac{1}{\epsilon})\)</span>，当 <span class="math inline">\(\lambda \leq \tilde{L}\)</span>时最优。但是，两者无法同时实现最优。</p><p><img src="/2021/01/26/AL2SGD/t1.jpg"></p></li></ul><h1 id="lower-complexity-bounds">Lower complexity bounds</h1><h2 id="lower-complexity-bounds-on-the-communication">Lower complexitybounds on the communication</h2><p><img src="/2021/01/26/AL2SGD/3.1.png"> <img src="/2021/01/26/AL2SGD/3.11.png"></p><h2 id="lower-complexity-bounds-on-the-local-computation">Lowercomplexity bounds on the local computation</h2><p><img src="/2021/01/26/AL2SGD/3.2.png"></p><h1 id="优化算法">优化算法</h1><h2 id="accelerated-proximal-gradient-descent-apgd-for-federated-learning">AcceleratedProximal Gradient Descent (APGD) for Federated Learning</h2><p>首先介绍非加速版本的近端梯度下降算法(PGD): <img src="/2021/01/26/AL2SGD/1.png"></p><p>根据另一篇论文，有两种不同的方式可以将梯度下降算法应用到上述优化问题上。最直接的方式是令<span class="math inline">\(h = f\)</span>，<span class="math inline">\(\phi =\lambda\psi\)</span>，那么可以得到如下更新步骤： <img src="/2021/01/26/AL2SGD/2.png"></p><p>另一种方式是令 <span class="math inline">\(h(\mathbf{x}) = \lambda\phi(\mathbf{x}) + \frac{\mu}{2n}\|\mathbf{x}\|^{2}\)</span>， <span class="math inline">\(\phi(\mathbf{x}) = f(\mathbf{x}) -\frac{\mu}{2n}\|\mathbf{x}\|^{2}\)</span>。由此得到的更新过程如下： <img src="/2021/01/26/AL2SGD/3.png"></p><p>同FedProx算法一致。</p><p>由于上述两种情况下，每次迭代都需要进行一轮通信，相应的通信复杂度次优。但是可以结合动量算法，程序(6)可以结合Nesterov'smomentum，能够得到最优通信复杂度，以及最优局部近端复杂度（当 <span class="math inline">\(\lambda \leqL)\)</span>，该算法定义为APGD1，具体如下：</p><p><img src="/2021/01/26/AL2SGD/a2.png"></p><p>将更新过程(5)和动量结合，可以得到最优通信复杂度以及最优局部近端复杂度（当<span class="math inline">\(\lambda \geqL）\)</span>。将该算法定义为APGD2，具体如下：</p><p><img src="/2021/01/26/AL2SGD/a3.png"></p><h2 id="beyond-proximal-oracle-inexact-apgd-iapgd">Beyond proximaloracle: Inexact APGD (IAPGD)</h2><p>在多数情况下，如果采用局部近端操作，每一步迭代时都需要得到子问题的精确解，这是不实际的。因此，作者提出了一个针对(6)的加速非精确的算法，每个节点只需要进行局部梯度运算（AGD, Katyusha)：</p><p><img src="/2021/01/26/AL2SGD/a1.png"></p><h2 id="accelerated-l2sgd">Accelerated L2SGD+</h2><p>作者给出L2SGD+算法的一个加速版本-AL2SGD+。作者指出AL2SGD+算法不过是L-Katyusha算法与非均匀抽样的结合。</p><p><img src="/2021/01/26/AL2SGD/a4.png"></p><h1 id="experiments">Experiments</h1><p>在第一个实验中，作者比较了当局部损失为有限和形式时，算法IAPGD+Katyusha、AL2SGD+以及L2SGD+的收敛速度。结果如下图：</p><p><img src="/2021/01/26/AL2SGD/5.jpg"></p><p>对于通信轮数，IAPGD+Katyusha和AL2SGD+都显著优于L2SGD+；对于局部计算次数，AL2SGD+表现最优，IAPGD+Katyusha不如L2SGD+。</p><p>第二个实验中，作者研究了数据异质性对算法的影响，结果如下图所示。可以看出，数据异质性不影响算法的收敛速度，各个算法的表现同第一个实验相似。</p><p><img src="/2021/01/26/AL2SGD/6.png"></p><p>在第三个实验中，作者比较了APGD算法的两种变形：APGD1和APGD2。作者不断改变参数<span class="math inline">\(\lambda\)</span>的取值，其余参数保持不变。在理论上，APGD2算法应该不受参数 <span class="math inline">\(\lambda\)</span> 影响，而APGD1 算法的收敛率会随着<span class="math inline">\(\lambda\)</span> 而增加 (<span class="math inline">\(\sqrt{\lambda}\)</span>)。 当 <span class="math inline">\(\lambda \leq L = 1\)</span>时，APGD1是最优选择；当<span class="math inline">\(\lambda &gt; L = 1\)</span> 时，APGD2应该是最优选择。实验结果如下图所示，结果与理论一致。</p><p><img src="/2021/01/26/AL2SGD/7.png"></p><h1 id="参考文献">参考文献</h1><ul><li>Filip Hanzely (KAUST) · Slavomír Hanzely (KAUST) · Samuel Horváth(King Abdullah University of Science and Technology)· Peter Richtarik(KAUST). Lower Bounds and Optimal Algorithms for Personalized FederatedLearning.arXiv e-prints.https://ui.adsabs.harvard.edu/abs/2020arXiv201002372H</li></ul>]]></content>
      
      
      <categories>
          
          <category> Distributed Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> FL </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Federated Learning of a Mixture of Global and Local Models</title>
      <link href="/2021/01/26/L2SGD/"/>
      <url>/2021/01/26/L2SGD/</url>
      
        <content type="html"><![CDATA[<ul><li><a href="#introduction">Introduction</a><ul><li><a href="#11-federated-learning">1.1 Federated learning</a></li></ul></li><li><a href="#contributions">Contributions</a></li><li><a href="#新的优化问题">新的优化问题</a></li><li><a href="#l2gd-loopless-local-gd">L2GD: Loopless Local GD</a><ul><li><a href="#收敛理论">收敛理论</a></li><li><a href="#收敛率优化">收敛率优化</a></li></ul></li><li><a href="#loopless-local-sgd-with-variance-reduction">Loopless LocalSGD with Variance Reduction</a><ul><li><a href="#问题设置">问题设置</a></li><li><a href="#理论">理论</a></li></ul></li><li><a href="#experiments">Experiments</a></li><li><a href="#附录">附录</a><ul><li><a href="#experimental-setup-and-further-experiments">ExperimentalSetup and further experiments</a></li><li><a href="#其余算法">其余算法</a></li></ul></li><li><a href="#参考文献">参考文献</a></li></ul><!-- /TOC --><h1 id="introduction">Introduction</h1><h2 id="federated-learning">1.1 Federated learning</h2><p>联邦学习的目标函数： <span class="math display">\[  \min_{\mathbf{x} \in \mathbb{R}^{d}}\frac{1}{n}\sum_{i=1}^{n}f_{i}(\mathbf{x})  \]</span> 其中 <span class="math inline">\(n\)</span>表示参与训练的节点个数，<span class="math inline">\(\mathbf{x} \in\mathbb{R}^{d}\)</span>为全模型优化变量。 <span class="math inline">\(f_{i}(\mathbf{x})\)</span>为节点<span class="math inline">\(i\)</span>上的损失函数。</p><h1 id="contributions">Contributions</h1><ul><li>提出了新的FL优化形式，尝试学习全局模型和局部模型的混合。</li><li>给出了新的优化形式的理论性质。作者证明了最优局部模型以<span class="math inline">\(O(1/\lambda)\)</span>收敛到传统的全局模型；作者证明了在局部模型上得到的损失不高于全局模型上的损失(定理3.1)；作者指出局部模型的最优解等于所有局部模型最优解的平均值减去对应局部模型上损失函数的一阶梯度，这一点和MAML一致。</li><li>Loopless LGD：作者提出了一个随机梯度算法 — Loopless Local GradientDescent(L2GD)（算法1）来解决提出的优化问题。该算法不是一个标准的SGD，它可以看作是一个关于损失函数和惩罚项的不均匀抽样。当抽到损失函数部分时，每个节点执行一次随机梯度下降；当抽到惩罚项时，进行信息聚合。</li><li>收敛理论。假设函数 <span class="math inline">\(f_{i}\)</span> 为<span class="math inline">\(L-smooth\)</span>，并且为 <span class="math inline">\(\mu-strong \, convex\)</span>，可以得到抽样概率<span class="math inline">\(p^{*} = \frac{\lambda}{\lambda +L}\)</span>，固定期望局部更新次数为 <span class="math inline">\(1 +\frac{L}{\lambda}\)</span>，作者证明通信 (communication)复杂度为（通信次数上界）为 <span class="math inline">\(\frac{2\lambda}{\lambda +L}\frac{L}{\mu}\log\frac{1}{\epsilon}\)</span>。当 <span class="math inline">\(\lambda \to 0\)</span>时，通信次数非常小；当$$时，根据新优化问题得到的解收敛到全局模型最优解，并且L2GD算法的通信上界为 <span class="math inline">\(O(\frac{L}{\mu}\log\frac{1}{\epsilon})\)</span>。</li><li>推广。部分连接，局部SGD，variancereduction（variance来自三部分：非均匀抽样，部分连接，从节点样本随机抽样）。</li><li>可用于异质数据。</li><li>经验表现不错。</li></ul><h1 id="新的优化问题">新的优化问题</h1><p><span class="math display">\[\min_{\mathbf{x_{1},...,x_{n}} \in \mathbb{R}^{d}} \{ F(\mathbf{x}) :=f(\mathbf{x}) + \lambda \psi(\mathbf{x})\}\]</span> <span class="math display">\[f(\mathbf{x}):= \frac{1}{n}\sum_{i=1}^{n}f_{i}(\mathbf{x_{i}}), \quad\psi(\mathbf{x}):= \frac{1}{2n}\sum_{i=1}^{n}\|\mathbf{x_{i}} -\mathbf{\bar{x}}\|^{2}\]</span></p><ul><li>Local model (<span class="math inline">\(\lambda = 0\)</span>)</li><li>Mixed model (<span class="math inline">\(\lambda \in (0,\infty)\)</span>)</li><li>Global model (<span class="math inline">\(\lambda =\infty\)</span>)</li></ul><h1 id="l2gd-loopless-local-gd">L2GD: Loopless Local GD</h1><p>在这一部分中，作者给出一个算法求解上述优化问题，该算法可以看作是一个非均匀SGD，要么抽取<span class="math inline">\(\nabla f\)</span>，要么抽取 <span class="math inline">\(\nabla \psi\)</span> 估计 <span class="math inline">\(\nabla F\)</span>。令 <span class="math inline">\(0 &lt; p &lt; 1\)</span>，定义一个随机梯度如下：<span class="math display">\[G(\mathbf{x}):= \begin{cases} \frac{\nabla f(\mathbf{x})}{1-p}, &amp;\text {概率 $1-p$} \\ \frac{\lambda \nabla \psi(\mathbf{x})}{p}, &amp;\text{概率 $p$ } \end{cases}\]</span> 显然，<span class="math inline">\(G(\mathbf{x})\)</span>为<span class="math inline">\(\nabla F(\mathbf{x})\)</span>的无偏估计量。每步的更新为: <span class="math display">\[\mathbf{x}^{k+1} = \mathbf{x}^{k} - \alpha G(\mathbf{x}).\]</span></p><figure><img src="/2021/01/26/L2SGD/l2gd.png" alt="Algorithm 1"><figcaption aria-hidden="true">Algorithm 1</figcaption></figure><p><span class="math inline">\(\textbf{Lemma 4.2}\)</span> 经过 <span class="math inline">\(k\)</span> 步迭代后，期望的通信次数为 <span class="math inline">\(p(1-p)k\)</span>。</p><h2 id="收敛理论">收敛理论</h2><p>作者首先证明梯度估计量<span class="math inline">\(G(\mathbf{x})\)</span>的期望具有光滑性质，然后证明了算法L2GD的收敛性质。（<span class="math inline">\(\mathbf{x(\lambda)}\)</span>为最优解，定理4.4表明，L2GD算法只能收敛到最优解邻域。） <img src="/2021/01/26/L2SGD/4.3.png"></p><h2 id="收敛率优化">收敛率优化</h2><p>作者给出最优抽样概率 <span class="math inline">\(p^{*} =\frac{\lambda}{L + \lambda}\)</span>，步长 <span class="math inline">\(\alpha\)</span> 要满足 <span class="math inline">\(\frac{\alpha\lambda}{np} \leq\frac{1}{2}\)</span>. <img src="/2021/01/26/L2SGD/4.4.png"></p><h1 id="loopless-local-sgd-with-variance-reduction">Loopless Local SGDwith Variance Reduction</h1><p>L2GD算法仅线性收敛到最优解的邻域，无法收敛到最优解。假设每个子目标函数具有有限和形式，作者提出了一个算法L2SGD+，在每个节点上进行随机梯度下降，并且具有线性收敛速度。L2SGD是一个具有variancereduction 的局部SGD算法，关于SGD的variance reduction，见另一篇博客：SGDwith variance reduction.</p><h2 id="问题设置">问题设置</h2><p>假设 <span class="math inline">\(f_{i}\)</span> 具有有限和结构：<span class="math display">\[f_{i} = \frac{1}{m}\sum_{j=1}^{m}f_{i,j}(\mathbf{x}_{i})\]</span></p><p>那么目标函数变为： <span class="math display">\[F(\mathbf{x}) =\frac{1}{n}\sum_{i=1}^{n}(\frac{1}{m}\sum_{i=1}^{m}f_{i,j}(\mathbf{x}_{i}))+ \lambda\frac{1}{2n}\sum_{i=1}^{n}\|\mathbf{x}_{i} -\mathbf{\bar{x}}\|^{2}\]</span></p><p><img src="/2021/01/26/L2SGD/l2sgd+.png"></p><p>L2SGD算法仅在两次抽样不同时才会发生通信，经过 <span class="math inline">\(k\)</span> 次迭代后，需要进行 <span class="math inline">\(p(1-p)k\)</span>次聚合平均。但是，L2SGD算法还需要通信控制变量 <span class="math inline">\(\mathbf{J_{i}I,  \Psi_{i}}\)</span>，因此通信次数变为原来的3倍。在附录中，作者给出了一个高效的L2SGD+，不需要通信控制变量。</p><h2 id="理论">理论</h2><p>作者给出了L2SGD算法的理论性质，并且给出最优抽样概率 <span class="math inline">\(p^{*}\)</span>。</p><p><img src="/2021/01/26/L2SGD/5.1.jpg"> <img src="/2021/01/26/L2SGD/5.2.png"> <img src="/2021/01/26/L2SGD/5.3.png"></p><h1 id="experiments">Experiments</h1><p>作者考虑Logistic回归问题，数据为LibSVM data(Chank &amp; Lin,2011)。数据首先进行normalized，以使得 <span class="math inline">\(f_{ij}\)</span>为1-smooth。步长根据定理5.2确定。每个数据集被划分为不同个数的节点，具体参数设置如下：<img src="/2021/01/26/L2SGD/table1.png"></p><p>作者考虑三种算法：L2SGD+, L2SGD(L2GD with local SGD), L2SGD2(L2GDwith local subsampling and control variates constructed for <span class="math inline">\(\Psi\)</span>)。根据理论分析，L2SGD+线性收敛到最优解，而L2SGD和L2SGD2收敛到最优解邻域。</p><p>作者考虑了两种数据分割方式。对于homogeneous data,首先将观测样本随机打乱，然后按照打乱后的数据划分到不同节点上；对于heterogeneousdata,首先根据观测样本的标签将样本排序，然后将排序后的数据依次划分到不同节点上(the worst-case heterogeneity)。</p><p><img src="/2021/01/26/L2SGD/figure3.png"></p><p>结果表明 - L2SGD+ (Full variance reduction)可以收敛到最优解，而L2SGD(without variance reduction)和 L2SGD2(with partial variancereduction) 只收敛到最优解邻域。 - 进行variancereduction是非常有必要的。它可以保证较快的全局收敛。 -数据异质性对算法收敛性没有影响。</p><h1 id="附录">附录</h1><h2 id="experimental-setup-and-further-experiments">Experimental Setupand further experiments</h2><ul><li>参数 <span class="math inline">\(p\)</span>如何影响算法L2SGD+的收敛速度</li><li>参数 <span class="math inline">\(\lambda\)</span>如何影响算法L2SGD+的收敛速度</li></ul><h2 id="其余算法">其余算法</h2><ul><li><p>Local GD with variance reduction</p><p>当每个节点采用梯度下降算法，且考虑variance reduction时，</p><p><img src="/2021/01/26/L2SGD/b1.png"> <img src="/2021/01/26/L2SGD/a3.png"></p></li><li><p>Efficient implementation of L2SGD+考虑到L2SGD+需要通信控制变量，增加了通信次数。作者给出了一个高效的版本，不需要通信控制变量，<span class="math inline">\(k\)</span>次迭代只需要通信 <span class="math inline">\(p(1-p)k\)</span>次。</p><p><img src="/2021/01/26/L2SGD/a4.png"></p></li><li><p>Local SGD with variance reduction – general method在这部分中，作者给出了一个使用性更广的版本。每个节点上目标函数可以包含一个非光滑正则项：</p><p><img src="/2021/01/26/L2SGD/b3.png"></p><p>另外，该版本算法允许从所有节点中任意抽样，允许节点结构任意（比如节点数据集大小，目标函数光滑程度，每个节点抽样方式任意）。</p><p><img src="/2021/01/26/L2SGD/a5.png"></p></li><li><p>Local stochastic algorithms</p><p>在这部分中，作者给出两个简单算法，不考虑variance reduction的LocalSGD(算法6)以及只考虑部分variance reduction的Local SGD (算法7)。</p><p><img src="/2021/01/26/L2SGD/a6.png"></p><p><img src="/2021/01/26/L2SGD/a7.png"></p></li></ul><h1 id="参考文献">参考文献</h1><ul><li>Hanzely, F. , &amp; Richtárik, Peter. (2020). Federated learning ofa mixture of global and local models.</li></ul>]]></content>
      
      
      <categories>
          
          <category> Distributed Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> FL </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>FLreview</title>
      <link href="/2021/01/21/FLreview/"/>
      <url>/2021/01/21/FLreview/</url>
      
        <content type="html"><![CDATA[<h1 id="aaai21-personalized-cross-silo-federated-learning-on-non-iid-data">[AAAI21]Personalized Cross-Silo Federated Learning on Non-IID Data</h1><p>该算法的目标函数为： <img src="https://cdn.jsdelivr.net/gh/sugar-xue/BlogImages@main/Picbed1613893223.png" alt="1613893223"></p><p>第二项<span class="math inline">\(A(\|\omega_{i} -\omega_{j}\|^{2})\)</span>的作用是使不同节点进行信息交流。该函数的定义如下：</p><figure><img src="https://cdn.jsdelivr.net/gh/sugar-xue/BlogImages@main/Picbed1614603445(1).png" alt="1614603445(1)"><figcaption aria-hidden="true">1614603445(1)</figcaption></figure><p>作者提出了一个求解上述目标函数的算法-FedAMP，具体如下：</p><figure><img src="https://cdn.jsdelivr.net/gh/sugar-xue/BlogImages@main/Picbed1613893256(1).png" alt="1613893256(1)"><figcaption aria-hidden="true">1613893256(1)</figcaption></figure><p>注意到，函数<span class="math inline">\(A(\cdot)\)</span>中的变量为<span class="math inline">\(\|\omega_{i} -\omega_{j}\|^{2}\)</span>，由于是子模型参数距离二范数的平方，在式(3)进行求导时，会出现<span class="math inline">\((\omega_{i} -\omega_{j})\)</span>项，进而式(3)可以表示为模型参数 <span class="math inline">\(\omega_{1}^{k-1},...,\omega_{m}^{k-1}\)</span>的线性组合：<img src="https://cdn.jsdelivr.net/gh/sugar-xue/BlogImages@main/Picbed1613893464(1).png" alt="1613893464(1)"></p><p>我们可以将 <span class="math inline">\(u_{i}\)</span> 看作是节点<span class="math inline">\(i\)</span>在云端子模型的参数，可以聚合各个节点的参数<span class="math inline">\(\omega_{1}^{k-1},...,\omega_{m}^{k-1}\)</span>信息。计算得到 <span class="math inline">\(u_{i}^{k}\)</span> 后，我们可以根据公式（4）在节点<span class="math inline">\(i\)</span> 上更新 <span class="math inline">\(\omega_{i}^{k}\)</span>:</p><figure><img src="https://cdn.jsdelivr.net/gh/sugar-xue/BlogImages@main/Picbed1613893769(1).png" alt="1613893769(1)"><figcaption aria-hidden="true">1613893769(1)</figcaption></figure><p>借助于<span class="math inline">\(u_{i}\)</span>聚合其他节点的参数，节点<span class="math inline">\(i\)</span>可以获取其他节点的信息。在云端优化完<span class="math inline">\(A(W)\)</span>后，对于每个节点，再利用式(6)优化损失函数<span class="math inline">\(F_{i}(w)\)</span>。</p><figure><img src="https://cdn.jsdelivr.net/gh/sugar-xue/BlogImages@main/Picbed1613893796(1).png" alt="1613893796(1)"><figcaption aria-hidden="true">1613893796(1)</figcaption></figure><figure><img src="https://cdn.jsdelivr.net/gh/sugar-xue/BlogImages@main/Picbed1613893828(1).png" alt="1613893828(1)"><figcaption aria-hidden="true">1613893828(1)</figcaption></figure><p>在聚合其他节点参数时，式(5)中不同节点参数的权重为 <span class="math display">\[\xi_{i,j} = \alpha_{k}A'(\|\omega_{i}^{k-1} -\omega_{j}^{k-1}\|^{2}), \quad (i \ne j)\]</span> 根据定义1，在<span class="math inline">\([0,\infty)\)</span>上，<span class="math inline">\(A\)</span>是一个increasing and concave函数，函数A的导数<span class="math inline">\(A'\)</span>在<span class="math inline">\((0, \infty)\)</span>上为non-negative andnon-increasing 函数，所以<span class="math inline">\(A'(\|\omega_{i}^{k-1} -\omega_{j}^{k-1}\|^{2})\)</span>相当于一个相似度函数，如果两个节点的参数<span class="math inline">\(w_{i}^{k-1}\)</span>和<span class="math inline">\(w_{j}^{k-1}\)</span>的欧氏距离小，那么这两个节点的相似度要高，对应到<span class="math inline">\(u_{i}^{k}\)</span>和<span class="math inline">\(u_{j}^{k}\)</span>中，它们的权重更高，因而<span class="math inline">\(u_{i}^{k}\)</span>和<span class="math inline">\(u_{j}^{k}\)</span>更接近，进一步，<span class="math inline">\(w_{i}^{k}\)</span>和<span class="math inline">\(w_{j}^{k}\)</span>更接近。</p><h1 id="aaai21-tornadoaggregate-accurate-and-scalable-federated-learning-via-the-ring-based-architecture">[AAAI21]TornadoAggregate: Accurate and Scalable Federated Learning via theRing-Based Architecture</h1><p>在这篇文章中，作者提出一种可以提高精度和稳定性的聚合方式，并且讨论了当前已有的各种聚合方式。</p><figure><img src="https://cdn.jsdelivr.net/gh/sugar-xue/BlogImages@main/Picbed1613894143.png" alt="1613894143"><figcaption aria-hidden="true">1613894143</figcaption></figure><p>作者指出STAR 这种全局聚合结构的稳定性差，相较而言，RING结构通过移除全局聚合，解决了STAR稳定性差的问题。但是，RING结构在FL中不切实际，假设共 <span class="math inline">\(|N|\)</span>个节点，RING需要进行的通信轮数是 STAR结构的 <span class="math inline">\(|N|\)</span> 倍数。除此之外，作者也总结讨论了其他已有聚合结构：STAR-stars, STAR-rings,RING-stars, RING-rings。</p><p>作者基于RING结构提出了两种新的聚合结构，通过减少RING结构带来的方差，提高了稳定性和精度。</p><h1 id="icml20-fedboost-communication-efficient-algorithms-for-federated-learning">[ICML20]FedBoost: Communication-Efficient Algorithms for Federated Learning</h1><p>作者借助集成的思想以减少FL中的通信成本。一些预先训练好的弱模型可以通过可获得的公共数据集训练。假设我们有<span class="math inline">\(q\)</span> 个已经训练好的弱模型 <span class="math inline">\(H =(h_{1},...,h_{q})\)</span>，本文的目标是学习组合权重 <span class="math inline">\(\alpha = \{ \alpha_{1}, ...,\alpha_{q}\}\)</span>，从而得到 <span class="math inline">\(\sum_{k=1}^{q} \alpha_{k}h_{k}\)</span>使得损失最小化。</p><figure><img src="https://cdn.jsdelivr.net/gh/sugar-xue/BlogImages@main/Picbed1613895221(1).png" alt="1613895221(1)"><figcaption aria-hidden="true">1613895221(1)</figcaption></figure><h1 id="icml20-fetchsgd-communication-efficient-federated-learning-with-sketching">[ICML20]FetchSGD: Communication-Efficient Federated Learning with Sketching</h1><p>作者提出一个新的算法，算法思想为：在每一轮，每个节点基于自己的局部信息计算得到一个梯度，然后在进行聚合前，作者使用一种叫做CountSketch的数据结构对梯度进行压缩。中心端保留momentum和error accumulationCount Sketches，每轮更新的权重参数根据error accumulationsketch得到。</p><h1 id="icml20-federated-learning-with-only-positive-labels">[ICML20]Federated Learning with Only Positive Labels</h1><h1 id="icml20-from-local-sgd-to-local-fixed-point-methods-for-federated-learning">[ICML20]From Local SGD to Local Fixed-Point Methods for Federated Learning</h1><h1 id="nips20-lower-bounds-and-optimal-algorithms-for-personalized-federated-learning">[NIPS20]Lower Bounds and Optimal Algorithms for Personalized Federatedlearning</h1><p>L2SGD # [NIPS20] Federated Bayesian Optimization # [NIPS20] FederatedMulti-Task Learning MOCHA # [NIPS20] FedSplit: An algorithmic frameworkfor fast federated optimization作者首先讨论了两种已有算法FedSGD和FedProx算法，作者证明这两种算法都不具有可行的收敛理论保证，因为它们得到的稳定点都不是它们预先要求解的目标函数的解。因此，作者提出FedSplit算法，该算法得到的稳定点是优化问题的最优解。</p><figure><img src="https://cdn.jsdelivr.net/gh/sugar-xue/BlogImages@main/Picbed1613897029.png" alt="1613897029"><figcaption aria-hidden="true">1613897029</figcaption></figure><h1 id="nips20-an-efficient-framework-for-clustered-federated-learning">[NIPS20]An Efficient Framework for Clustered Federated Learning</h1><p>作者提出一个迭代的聚类算法，论文假设所有的节点都能够被划分为若干类。由于每个节点所属类别未知，该算法可以交替估计每个节点所属的类别，并且通过梯度下降优化模型参数。论文中的算法可以解决数据分布的异质性问题。但是需要预先给定聚类个数<span class="math inline">\(k\)</span>。 <img src="https://cdn.jsdelivr.net/gh/sugar-xue/BlogImages@main/Picbed1613897504(1).png" alt="1613897504(1)"></p><h1 id="nips20-group-knowledge-transfer-federated-learning-of-large-cnns-at-the-edge">[NIPS20]Group Knowledge Transfer: Federated Learning of Large CNNs at theEdge</h1><p>作者提出一种新的交替最小化算法，该算法在每个节点上先训练较小的CNN网络，然后通过信息迁移训练一个较大的中心端CNN网络。<img src="https://cdn.jsdelivr.net/gh/sugar-xue/BlogImages@main/Picbed1613898317(1).jpg" alt="1613898317(1)"></p><p>上图展示了每个节点有一个特征提取器和分类器，可以在单个节点上进行模型训练。进行局部训练后，每个节点生成同样的张量，将其特征输出到中心端进行训练，然后借助于最小化预测标签和真实标签的KD损失函数训练参数。为了提升节点模型的表现，中心端会将其预测的标签发送给每个节点，然后每个节点可以基于其预测标签和中心端预测结果的损失函数训练子模型。</p><p><img src="https://cdn.jsdelivr.net/gh/sugar-xue/BlogImages@main/Picbed1613898899(1).png" alt="1613898899(1)"> # [NIPS20] Personalized Federated Learning withMoreau Envelopes为了解决异质性问题，作者考虑给每个节点的损失函数添加正则项： <span class="math display">\[f_{i}(\theta_{i}) + \frac{\lambda}{2}\|\theta_{i} - w\|^{2}，\]</span></p><p>优化问题表示为： <img src="https://cdn.jsdelivr.net/gh/sugar-xue/BlogImages@main/Picbed1613899301(1).jpg" alt="1613899301(1)"></p><h1 id="nips20-tackling-the-objective-inconsistency-problem-in-heterogeneous-federated-optimization">[NIPS20]Tackling the Objective Inconsistency Problem in Heterogeneous FederatedOptimization</h1><p>大多数论文在分析算法的收敛性时，往往会假设每个节点上进行局部更新的次数相同，它们的工作表明算法能够达到全局目标函数的稳定点。事实上，论文指出当不同节点局部更新次数不一致时，算法收敛到的稳定点不是原始目标函数的最优解，而是另一个目标函数。</p><p>解决这个问题的最简单想法就是固定每个节点的局部更新次数，在进行新的一轮迭代前，要等所有节点进行迭代完才能开始。这种方法能够保证目标函数的一致性，但是会带来训练成本。一些算法比如FedProx,VRLSGD以及SCAFFOLD用于处理non-IID问题，可以减少目标函数的不一致问题，但是要么有较慢的收敛速度，要么需要额外的通信成本和内存。</p><p>本文作者提出FedNova算法，可以保证目标函数的一致性问题。</p><h1 id="nips20-throughput-optimal-topology-design-for-cross-silo-federated-learning">[NIPS20]Throughput-Optimal Topology Design for Cross-Silo FederatedLearning</h1><h1 id="nips20-federated-principal-component-analysis">[NIPS20]Federated Principal Component Analysis</h1><h1 id="nips20-ensemble-distillation-for-robust-model-fusion-in-federated-learning">[NIPS20]Ensemble Distillation for Robust Model Fusion in Federated Learning</h1><h1 id="nips20-differentially-private-federated-linear-bandits">[NIPS20]Differentially-Private Federated Linear Bandits</h1><h1 id="nips20-inverting-gradients---how-easy-is-it-to-break-privacy-in-federated-learning">[NIPS20]Inverting Gradients - How easy is it to break privacy in federatedlearning?</h1><h1 id="nips20-distributionally-robust-federated-averaging">[NIPS20]Distributionally Robust Federated Averaging</h1><h1 id="iclr20-fair-resource-allocation-in-federated-learning">[ICLR20]FAIR RESOURCE ALLOCATION IN FEDERATED LEARNING</h1><p>作者提出 q-FFL算法，目的是解决FL中的公平问题：不同节点上的精度均匀。通过最小化一个加权的损失函数，具有较高损失的节点具有较高的权重。</p><figure><img src="https://cdn.jsdelivr.net/gh/sugar-xue/BlogImages@main/Picbed1613901146(1).png" alt="1613901146(1)"><figcaption aria-hidden="true">1613901146(1)</figcaption></figure><p>目标函数：</p><figure><img src="https://cdn.jsdelivr.net/gh/sugar-xue/BlogImages@main/Picbed1613901254(1).png" alt="1613901254(1)"><figcaption aria-hidden="true">1613901254(1)</figcaption></figure><p>具体算法略。 # [ICLR20] DIFFERENTIALLY PRIVATE META-LEARNING</p><h1 id="iclr20-dba-distributed-backdoor-attacks-against-federated-learning">[ICLR20]DBA: DISTRIBUTED BACKDOOR ATTACKS AGAINST FEDERATED LEARNING</h1><h1 id="iclr20-generative-models-for-effective-ml-on-private-decentralized-datasets">[ICLR20]GENERATIVE MODELS FOR EFFECTIVE ML ON PRIVATE, DECENTRALIZEDDATASETS</h1><h1 id="iclr20-attack-resistant-federated-learning-with-residual-based-reweighting">[ICLR20]ATTACK-RESISTANT FEDERATED LEARNING WITH RESIDUAL-BASED REWEIGHTING</h1>]]></content>
      
      
      <categories>
          
          <category> Distributed Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> FL </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>

<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>Model Acceleration</title>
      <link href="/2024/11/08/Model_Acceleration/"/>
      <url>/2024/11/08/Model_Acceleration/</url>
      
        <content type="html"><![CDATA[<h1 id="training-optimizations">Training Optimizations</h1><h2 id="flashattention">FlashAttention</h2><p><a href="https://arxiv.org/abs/2205.14135">FlashAttention: Fast andMemory-Efficient Exact Attention with IO-Awareness</a></p><p>由于 self-attention 的时间和内存复杂度是序列长度的平方，因此Transformers 在长序列上运行缓慢且占用内存。存在一些近似注意力方法试图通过牺牲模型质量来降低计算复杂度，从而解决这一问题。</p><p>FlashAttention 的核心创新是通过<strong>IO-aware（输入/输出感知）</strong> 和 <strong>精确计算</strong>的方式，利用<strong>tiling</strong>（块处理）技术来重组注意力计算，从而减少 GPU高带宽内存（HBM）和 GPU 片上 SRAM之间的数据交换量。通过这些改进，FlashAttention能够显著提升速度，并将内存使用从通常的 <strong>O(N^2)</strong> 降低到<strong>O(N)</strong>，其中 <span class="math inline">\(N\)</span>是序列长度。</p><p>传统的 attention 层计算过程： <img src="/2024/11/08/Model_Acceleration/flash0.jpg"></p><p>由于 HBM 的读写速度很慢，FlashAttention 使用分块技术避免在 HBM上传输整个 <span class="math inline">\(N \times N\)</span>注意力矩阵。</p><p><img src="/2024/11/08/Model_Acceleration/FlashAttention.jpg"></p><p>分块处理的具体步骤包括：</p><ol type="1"><li><p><strong>外层循环（红色箭头）</strong>：FlashAttention 在计算时对<span class="math inline">\(K\)</span> 和 <span class="math inline">\(V\)</span> 矩阵进行分块处理。首先将 $K$ 和 <span class="math inline">\(V\)</span>矩阵分成小块，每块包含的数据量足够放入片上缓存（SRAM）中。每次处理一块时，将其从HBM 加载到 SRAM。</p></li><li><p><strong>内层循环（蓝色箭头）</strong>：对于每个 <span class="math inline">\(K\)</span> 和 <span class="math inline">\(V\)</span> 块，FlashAttention 对 <span class="math inline">\(Q\)</span> 矩阵也进行分块处理。</p><ul><li><p>每次从 <span class="math inline">\(Q\)</span>矩阵加载一个小块到片上缓存，并与当前的 <span class="math inline">\(K\)</span> 和 <span class="math inline">\(V\)</span> 块进行计算。</p></li><li><p>在块内完成的计算包括：对 <span class="math inline">\(Q\)</span>和 <span class="math inline">\(K\)</span> 的局部块执行矩阵乘法 <span class="math inline">\(Q K^T\)</span>，得到局部注意力得分矩阵。</p></li><li><p>接着，对得分矩阵进行 softmax操作，得到归一化后的注意力权重。</p></li></ul></li><li><p><strong>结果写回 HBM</strong>：对当前块计算完成后，将结果写回 HBM中，存储为最终的注意力输出。</p></li></ol><p>通过上述分块方法，FlashAttention <strong>不需要存储完整的 <span class="math inline">\(N \times N\)</span>注意力矩阵</strong>，而是逐块处理并写回计算结果。这避免了大规模的 HBM读写操作，从而显著提升了计算效率。与传统注意力计算方法不同，FlashAttention不会在 HBM 中生成和存储整个注意力矩阵，而是逐步在 SRAM中完成小块计算。</p><p>右图展示了 FlashAttention 的加速效果。FlashAttention 实现了对 PyTorch中 GPT-2 注意力计算的 <strong>7.6 倍加速</strong>。性能提升的核心在于：- 减少了大规模的 HBM 数据传输。 - 利用了快速的 SRAM进行局部计算，避免了反复从 HBM 中调取大块数据。</p><p>这种方法有效地优化了长序列情况下的注意力计算，极大减少了对内存带宽的需求，并在大型语言模型（例如GPT-2）上展现出显著的加速效果。</p><p>下面给出了算法的实现过程，具体推导细节可见论文。</p><p><img src="/2024/11/08/Model_Acceleration/flash1.jpg"></p><h2 id="flashattention-2">FlashAttention-2</h2><p><a href="https://arxiv.org/abs/2307.08691">FlashAttention-2: FasterAttention with Better Parallelism and Work Partitioning</a></p><p>尽管 FlashAttention在内存和速度上实现了显著优化，但其性能仍然未能达到 GPU的理论计算速度上限，尤其是与 GEMM 操作相比。具体来说， - FlashAttention中虽然也涉及矩阵计算，但由于其复杂的分块、循环机制和动态的内存管理操作，它的计算模式比GEMM更复杂，导致执行效率偏低。尤其是在大规模计算的场景中，FlashAttention的结构性处理操作（如加载和处理不同的矩阵块）会产生一定的开销，这些开销限制了其最高计算性能。</p><ul><li>由于 FlashAttention的设计需要进行大量的内存加载、分块和小规模计算，因此每秒执行的浮点运算数（FLOPs/s）通常只有GPU 理论上限的 25-40% 左右。这意味着 FlashAttention虽然比传统的注意力计算更高效，但在计算密集型任务上仍远逊于最优化的GEMM。</li></ul><p><strong>FlashAttention-2</strong> 是在 FlashAttention基础上的优化版本，旨在通过更优的工作分配来解决效率问题。原始的FlashAttention 在 GPU上线程块和线程组（warps）之间的工作划分较差，导致了低占用率或不必要的共享内存读/写。为了解决这些问题，Dao及其团队对算法进行了以下改进：</p><ol type="1"><li><p><strong>减少非矩阵乘法的FLOPs</strong>：通过优化算法结构，减少了与矩阵乘法无关的浮点运算次数，从而使计算更高效。</p></li><li><p><strong>并行化注意力计算</strong>：针对单个 attention head的计算，FlashAttention-2 通过不同的线程块并行化，以增加 GPU的占用率（occupancy），充分利用 GPU 的计算资源。</p></li><li><p><strong>减少共享内存通信</strong>：在每个线程块内，将工作分配到不同的线程组（warps）中，以尽量减少线程间通过共享内存进行的通信，从而降低延迟。</p></li></ol><p>这些改进使得 FlashAttention-2 的速度相比于 FlashAttention提升了约两倍，在 NVIDIA A100 GPU 上达到了 50-73% 的理论最高浮点运算速度(FLOPs/s)，接近于高度优化的矩阵乘法（GEMM）操作的效率。</p><p><img src="/2024/11/08/Model_Acceleration/flash2.jpg"></p><h2 id="multi-query-attention-mqa">Multi-Query Attention (MQA)</h2><p><a href="https://arxiv.org/pdf/1911.02150">Fast Transformer Decoding:One Write-Head is All You Need</a></p><p>在标准的多头注意力中，每个注意力头（head）都会分别拥有自己的“键”（keys）和“值”（values）向量，这在训练中不会带来太大问题，因为可以并行处理整个序列。然而，在增量推理阶段（如在语言模型中逐步生成文本的过程中），这种机制导致了性能瓶颈。由于每个新的生成步骤都需要反复加载这些巨大的“键”和“值”张量，频繁的数据传输会消耗大量的内存带宽，从而导致较低的推理速度。</p><p>在 MQA中，所有注意力头共享同一个“键”和“值”张量，这意味着不再为每个注意力头生成独立的“键”和“值”，从而显著减少了张量的大小和存储需求。这一共享机制降低了增量推理中的内存带宽消耗，使模型推理更为高效。</p><p>MQA的做法其实很简单。在MHA中，input embedding 分别经过 <span class="math inline">\(W_Q\)</span>, <span class="math inline">\(W_K\)</span>, <span class="math inline">\(W_V\)</span> 变换后，会被划分为 <span class="math inline">\(n\)</span> 份 (n = headnumber)，相应的，维度降低为 <span class="math inline">\(d_{head} =d_{model}/n\)</span>。然后分别进行不同的 attention head计算，最后再拼接为维度为 <span class="math inline">\(d_{model}\)</span>的 embedding。而对于MQA，在线性变换之后，只对 query 进行切分，对于 key 和value，直接在线性变换时把维度降到了 $d_{head} $。然后这 <span class="math inline">\(n\)</span> 个 query 分别和同一个key和value进行计算，最后把结果拼接为维度为 <span class="math inline">\(d_{model}\)</span>的 embedding。</p><p><img src="/2024/11/08/Model_Acceleration/GQA.jpg"></p><h2 id="grouped-query-attention-gqa">Grouped-Query Attention (GQA)</h2><p><a href="https://arxiv.org/pdf/2305.13245">GQA: Training GeneralizedMulti-Query Transformer Models from Multi-Head Checkpoints</a></p><p>MQA 仅使用单一的 key-value 头，大幅提升了解码器的推理速度。然而，MQA可能会导致模型质量下降。GQA提出了一个折中的办法，既能减少MQA效果的损失，又相比MHA需要更少的缓存。</p><p>在 GQA 中，query 的划分不变。由于使用一套 key 和 value的效果不好，因此可以划分多套 key-value，但是其数量仍然少于query。然后，将 query 的多个头划分到不同的 group, 同一个 group 内的query 共享同一套 key-value。</p><p>从实验结果上看，GQA 的速度相比 MHA 有明显提升，而效果上比 MQA也好一些，能做到和 MHA 基本没差距。</p><h2 id="longformer">Longformer</h2><p><a href="https://arxiv.org/abs/2004.05150">Longformer: theLong-Document Transformer</a></p><h1 id="inference-optimizations">Inference Optimizations</h1><h2 id="kv-cache-key-value-cache">KV Cache: Key-Value Cache</h2><p>在解码过程中，随着每个新 token 的生成，模型需要不断地计算新的 Key 和Value 来更新注意力机制。这些新的 Key 和 Value 将被缓存，以便下次使用，不需要每次都重新计算。通过缓存键（Key）和值（Value）张量，模型可以在后续步骤中重用这些张量，而无需重新计算它们。这显著减少了计算开销，特别是在处理长序列时，能够有效提高计算效率。</p><p>推荐博客：<a href="https://www.linsight.cn/3dc22f96.html#%E8%A7%A3%E7%A0%81%E4%B8%AD%E7%9A%84kv-cache">解码中的KVCache</a></p><ul><li>KV 缓存只存储 Key 和 Value 的表示，因为这些表示不随时间变化，每个token 的 Key 和 Value 可以在计算时重用。而 Query是动态变化的，每个时间步的 Query 都是与输入的当前 token 和历史 tokens的组合相关，因此它不需要缓存。</li></ul><h1 id="references">References:</h1><ul><li><p><a href="https://aman.ai/primers/ai/model-acceleration/">DistilledA</a></p></li><li><p><a href="https://www.linsight.cn/3dc22f96.html">理解Attention:从起源到MHA,MQA和GQA</a></p></li></ul>]]></content>
      
      
      <categories>
          
          <category> Foundation Model </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Transformer </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>VLM</title>
      <link href="/2024/11/08/VLM/"/>
      <url>/2024/11/08/VLM/</url>
      
        <content type="html"><![CDATA[<h1 id="reference">Reference</h1><p><a href="https://aman.ai/primers/ai/">Distilled AI</a></p>]]></content>
      
      
      <categories>
          
          <category> Foundation Model </category>
          
      </categories>
      
      
        <tags>
            
            <tag> multi-modal </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Agents</title>
      <link href="/2024/11/08/agents/"/>
      <url>/2024/11/08/agents/</url>
      
        <content type="html"><![CDATA[<h2 id="agent-工作流程">1. Agent 工作流程</h2><p>当前大语言模型（LLM）的使用大多以“zero-shot”模式进行，模型通过逐词生成最终输出而不进行重新审查或完善。这一过程类似于让某人从头到尾一气呵成地写完一篇文章，而不进行任何修正。尽管在这种限制下，LLM仍表现出高度的有效性，但一种agent 式、迭代的工作方式通常能带来更为稳健的结果。</p><p>在这种迭代框架中，LLM可以通过一系列有条理的步骤来完成任务，包括：</p><ol type="1"><li>为任务制定大纲；</li><li>判断是否需要进行额外的网络搜索或研究；</li><li>撰写初稿；</li><li>审阅并识别潜在的薄弱环节或无关内容；</li><li>基于发现的改进区域进行修订。</li></ol><p>这种结构化的多步骤流程类似于人类作者在完善文本内容时采取的系统性方法。通过多次迭代，AIagent能够比单次生成的方式更有效地提高结果质量。这一方法在推动更高精度和更高质量的生成任务中具有重要作用。</p><h2 id="the-agent-framework">2. The Agent Framework</h2><p><img src="/2024/11/08/agents/coreAI.jpg"> [Image credits to <a href="https://developer.nvidia.com/blog/introduction-to-llm-agents/">source</a>]</p><h3 id="agent-core-llm">Agent core (LLM)</h3><p>Agent Core 是 agent 的核心部分，充当主要的决策引擎，比如利用 OpenAI的 GPT-4来处理高级推理和动态任务管理。这个组件包括以下几个关键部分：</p><ol type="1"><li><p><strong>决策引擎</strong>：负责分析输入数据、记忆和目标，以生成合适的响应。</p></li><li><p><strong>目标管理系统</strong>：根据任务进展不断更新 agent的目标。</p></li><li><p><strong>集成总线</strong>：管理记忆、工具和规划模块之间的信息流动，确保数据交换的连贯性。</p></li></ol><p>Agent Core 利用 LLM的能力完成任务，必要时生成新任务，并根据任务上下文的变化动态调整优先级。这种结构使得agent 能够更灵活地适应任务环境的变化，从而有效地推进任务的完成。</p><h3 id="memory-modules">Memory Modules</h3><p>记忆模块是 agent 框架中的基础部分，通过使用向量数据库（如Pinecone、Weaviate、Chroma等）提供任务相关数据的强大存储和检索机制。记忆模块通过以下方式提升 agent的上下文感知能力和任务相关性：</p><ol type="1"><li><p><strong>短期记忆（STM）</strong>：管理临时数据，用于满足当前任务需求。短期记忆通过易于清理的堆栈或队列等易失性结构存储，以支持快速访问和频繁清除。</p></li><li><p><strong>长期记忆（LTM）</strong>：使用向量数据库来持久化存储历史交互数据，使agent能够在较长时间内参考过去的对话或数据。长期记忆采用基于语义相似性的检索方式，以提高相关性，并结合数据的时间性和重要性，实现高效访问。</p></li></ol><p>这种记忆架构使得 agent能够在动态任务中保持对历史和当前上下文的理解，提升了任务的执行效果和智能性。</p><h3 id="tools">Tools</h3><p>工具模块为 agent 提供了执行任务的专业能力，通常使用 LangChain框架来构建结构化的工作流程。工具模块包括以下几个主要部分：</p><ol type="1"><li><p><strong>可执行工作流程</strong>：基于 LangChain定义，提供结构化和数据感知的任务处理方式，使 agent能够有条理地完成任务。</p></li><li><p><strong>API 接口</strong>：使 agent能够安全地访问内部和外部数据源，扩展其功能范围，丰富了任务处理的资源和数据支持。</p></li><li><p><strong>中间件</strong>：支持核心模块与工具之间的数据交换，负责数据格式转换、错误检查，并确保数据安全性。</p></li></ol><p>LangChain 的集成使 agents能够动态地与其环境交互，从而在处理不同任务时提供灵活性和适应性。这种结构化的工具模块极大地增强了agents 的功能覆盖范围和操作精度。</p><h3 id="planning-module">Planning Module</h3><p>在复杂问题求解中，规划模块提供结构化方法，例如任务分解和反思，以帮助agent优化解决方案。在此模块中，任务管理系统使用双端队列（deque）数据结构，能够自主生成、管理和优先排序任务。该系统会根据任务完成情况和新生成的任务实时调整优先级，确保任务的执行始终与目标保持一致，推动任务进展更高效、更有条理。</p><h2 id="design-patterns">3. Design Patterns</h2><p>常见agentic设计模式分类框架：</p><h3 id="reflection反思">Reflection（反思）</h3><p>为了提升大语言模型的效果，一个关键方法是在其工作流程中引入反思机制。反思是一种自我评估和迭代改进的方式，使LLM能够自主识别输出中的不足，并基于反馈进行调整，最终提供更加精确、高效且符合用户需求的响应。通过这种结构化的迭代过程，LLM从典型的问答式互动转变为动态的持续改进循环。以下是反思工作流程的主要步骤和具体操作：</p><p><strong>1. 初始输出生成：</strong>在典型任务中（如代码编写），LLM首先会被提示生成一个初始响应，以完成特定目标（例如完成“任务X”）。该响应可以作为草稿，后续将接受进一步的审查。</p><p><strong>2. 自我评估和建设性反馈：</strong>生成初始输出后，可以引导LLM对其输出进行评估。例如，在代码生成的场景下，可提示它：&gt;“以下是用于任务X的代码：[之前生成的代码]。仔细检查代码的正确性、风格和效率，并提供建设性的改进意见。”</p><p>这个自我批评阶段使LLM能够识别自身输出中的缺陷，包括正确性、效率和风格方面的问题，从而发现需要改进的领域。</p><p><strong>3. 基于反馈的修订：</strong>在LLM生成了对其自身输出的反馈后， agent工作流程将提示模型根据反馈进行修订。在此阶段，模型会结合初始输出和其生成的批评意见，生成反映出改进的修订版本。批评和重写的循环可重复多次，通过迭代提升最终输出的质量。</p><p><strong>4. 整合额外工具：</strong>为了进一步增强反思效果，可以为LLM配备辅助工具，帮助其定量评估自己的输出。例如：-<strong>代码评估</strong>：模型可以通过单元测试运行代码，验证其准确性。-<strong>文本验证</strong>：LLM可以通过互联网搜索或外部数据库进行事实核查，确保文本内容的准确性。</p><p>当这些工具发现错误或不准确之处时，LLM能够根据差异生成额外反馈并提出改进建议，这种基于工具的反思使LLM的优化更加有效，将自我批评与外部验证相结合。</p><p><strong>5. 多 agent 框架增强反思流程：</strong>为优化反思过程，可以采用 multi-agent 框架。该配置中包括两个不同的agents： - <strong>输出生成agent</strong>：主要负责有效生成初始任务响应。 - <strong>批评agent</strong>：专门评估第一个 agent的输出，提供建设性反馈以提升质量。</p><p>通过两个 agent的互动，LLM能够更好地识别并修正输出中的缺陷。这种协作引入了二级反思，使LLM能够获得单一agent 设置中可能遗漏的见解。</p><h3 id="functiontoolapi-calling-工具使用">Function/Tool/API Calling(工具使用)</h3><p>工具使用指的是LLM在响应过程中调用特定功能的能力，如执行代码、进行网页搜索或与生产力工具互动，从而使其功能扩展至传统语言生成之外。这种方法使得LLM能够通过选择性调用各种外部工具来处理更复杂的查询和多方面的任务。通过工具使用设计模式，LLM逐步从单一的语言生成系统演变为可以自主完成多种复杂任务的智能助手。</p><p>大型多模态模型的发展，如LLaVa、GPT-4V和Gemini，标志着工具使用的又一重要里程碑。在这些模型之前，LLM无法直接处理或操作图像，任何与图像相关的任务都必须委托给特定的计算机视觉功能，如物体识别或场景分析。而GPT-4在2023年引入的函数调用能力进一步推动了工具使用的发展，它建立了一个更为通用的功能接口，为一个多功能、多模态的AI生态系统奠定了基础，使得模型能够无缝集成文本、图像和其他数据类型。这一新功能的出现，促使了越来越多的LLM被设计来利用工具使用，扩展了其应用范围，并提升了其整体适应性。</p><h4 id="评估">评估</h4><p>为了确保工具使用功能能够满足各种现实场景的需求，必须对LLM的函数调用性能进行严格评估。此评估包括在Python和非Python编程环境中对模型性能的评估，重点是评估模型执行函数的能力、选择合适工具的能力，以及在对话上下文中判断何时需要调用特定函数的能力。评估的一个关键方面是测试模型根据用户提示准确调用函数的能力，以及判断某些函数是否适用或需要。</p><p>Berkeley Function-Calling Leaderboard (BFCL) 是由 UC Berkeley创建的一个排行榜，用于评估和比较不同大语言模型在“函数调用”任务中的表现。它包含2,000个问题-函数-答案对，涵盖了多种编程语言（如Python、Java、JavaScript、RESTAPI、SQL等）。这个排行榜专注于衡量LLM在执行外部工具或功能（例如编程、数据检索、计算等）时的能力。其目标是促进更高效、精准的模型开发，特别是在实际应用中，模型能够通过调用合适的外部工具来增强其功能，超越仅依赖语言生成的传统局限。</p><p>BFCL的评估内容涉及多个方面，主要包括：</p><ol type="1"><li><p>函数调用的复杂。简单调用：单个函数的调用；多个函数调用：多个函数调用的组合；并行函数调用：要求模型能够同时调用多个函数，并且能够处理这些函数的并行执行。</p></li><li><p>函数相关性检测。该评估重点检查模型是否能够识别并排除与任务无关的函数，避免“幻觉”（即产生不相关或错误的输出）。如果模型使用了不相关的函数，应该能够返回错误信息。</p></li><li><p>编程语言分类。Python类评估：包括简单函数调用、多个函数调用、并行函数调用等不同场景的评估。非Python类评估：包括普通聊天功能、函数相关性检测、以及API调用和其他编程语言（如RESTAPI、SQL、Java、JavaScript）的评估。</p></li></ol><p>在评估模型性能时，使用了两种主要方法：</p><ol type="1"><li>抽象语法树（AST）评估。AST评估涉及解析模型生成的函数调用，并检查其结构是否符合预期输出。它验证函数名称、参数是否存在以及类型是否正确。AST评估适用于以下情况：</li></ol><ul><li><strong>执行不可行</strong>：当由于语言限制或其他原因无法执行代码时，AST评估仍然可以进行。</li><li><strong>结果无法轻易执行</strong>：当函数执行结果无法直接验证时，AST评估仍然有用。</li></ul><ol start="2" type="1"><li>可执行函数评估。可执行函数评估通过执行模型生成的函数调用并将其输出与预期结果进行比较，来测试模型的实际应用能力。这种评估方法着重于以下几个方面：</li></ol><ul><li><strong>函数调用能否成功运行</strong>：验证函数是否可以被正确执行。</li><li><strong>输出类型正确性</strong>：确保函数返回的数据类型符合预期。</li><li><strong>结构一致性</strong>：函数输出的结构是否符合预期。</li></ul><p>AST评估和可执行评估的结合确保了全面的评估，提供了模型输出的<strong>语法正确性</strong>和<strong>功能正确性</strong>的深入洞察。这两种方法互为补充，共同帮助测试模型在生成函数时的表现，确保其输出不仅符合预定的结构要求，也能够在实际运行中产生正确的结果。</p><h4 id="gorilla-openfunctions-v2-llm">Gorilla OpenFunctions-v2 LLM</h4><p>Gorilla OpenFunctions-v2是一个开源的大型语言模型（LLM），提供先进的功能调用能力，其性能可与GPT-4 相媲美。该模型扩展了 LLM的聊天能力，可以从自然语言指令生成可执行的 API 调用，并根据相关的 API上下文执行任务。它支持多种编程语言和复杂的功能调用。通过它的多功能支持、并行调用、功能相关性检测等特性，用户可以轻松处理各种任务，显著提高工作效率和精度。</p><h3 id="planning规划">Planning（规划）</h3><p>Planning 是一种基础性的设计模式，赋予 LLM自主制定和执行计划或策略的能力，用以完成任务。通过这种动态决策过程，AI将广泛的目标分解为更小、更易管理的步骤，并按照结构化的顺序执行这些步骤，以产生连贯且通常复杂的输出。本文探讨了规划在agent 型 AI设计中的重要性，通过示例展示其功能，并分析其当前能力和局限性。</p><h4 id="planning-vs.-deterministic-approaches">Planning vs.Deterministic Approaches</h4><p>Planning 并非在每个 agent工作流中都是必需的。对于一些较简单的任务或那些遵循预定顺序的任务，采用确定性的逐步方法就足够了。例如，如果一个agent被编程为反思并固定次数地修订其输出，它可以在无需适应性规划的情况下执行这一系列步骤。</p><p>然而，对于复杂或开放性任务，在任务执行过程中很难预定义所需的步骤顺序时，Planning允许 AI动态地决定合适的步骤。这种适应性的方法在任务中可能出现意外挑战或需要agent从多个工具和方法中选择最佳方案时尤为宝贵。通过这种方式，规划能够帮助 AI在不断变化的环境中保持灵活性，并在复杂的任务中做出有效决策。</p><h3 id="multi-agent-collaboration多-agent-协作">Multi-agentCollaboration（多 agent 协作）</h3><p>Multi-agent Collaboration通过将复杂任务分解为易于管理的子任务来执行这些任务。通过将这些子任务分配给专门的agent。每个 agent都是软件工程师、产品经理、设计师、质量保证工程师等。多个 agent协作，每个 agent 都执行特定的指定角色。这些 agent，无论是通过以各种方式提示单个 LLM，还是通过使用多个LLM，都能以量身定制的能力执行指定任务。例如，通过指示一个 LLM"编写清晰、高效的代码"，让它扮演 "软件工程师"的角色，这样它就能只专注于这一方面，从而使其产出符合软件工程子任务的要求。</p><h2 id="benchmarks">4. Benchmarks</h2><p>尽管基于大型语言模型的智能体在多个领域中表现出色，但量化和客观评估这些智能体的性能仍然具有挑战性。为此，多个基准测试框架被设计用来评估LLM 智能体的表现，常见的基准包括：</p><ul><li><p><strong>AgentBench</strong>：一个开源框架，用于评估和比较多种基于agent 的 AI 系统。</p></li><li><p><strong>IGLU</strong>：专注于评估 LLM智能体在生成语言、执行任务等方面的表现。</p></li><li><p><strong>ClemBench</strong>：用于评估多功能任务和复杂环境中的agent 表现。</p></li><li><p><strong>ToolBench</strong>：侧重于评估智能体在使用工具（如 API调用、代码执行等）时的能力。</p></li><li><p><strong>GentBench</strong>：旨在评估智能体在生成和理解复杂指令的表现。</p></li><li><p><strong>MLAgentBench</strong>：专注于机器学习 agent的评估框架，尤其是在自学习和环境适应性方面。</p></li></ul><p>这些基准测试框架通常从多个维度对 LLM 智能体的表现进行评估，包括：</p><ol type="1"><li><p><strong>效用（Utility）</strong>：指任务完成的效果和效率，通常通过成功率和任务结果来衡量。</p></li><li><p><strong>社交能力（Sociability）</strong>：语言沟通能力、合作与谈判能力、以及角色扮演的能力等。</p></li><li><p><strong>价值观（Values）</strong>：包括智能体遵守道德和伦理标准、诚实性、无害性以及在特定情境下的适当性。</p></li><li><p><strong>持续进化能力（Ability to EvolveContinually）</strong>：指智能体的持续学习、自我驱动学习能力，以及适应新环境的能力。</p></li><li><p><strong>对抗性鲁棒性（AdversarialRobustness）</strong>：智能体对对抗性攻击的敏感度，通常通过对抗训练和人工监督等方法来增强鲁棒性。</p></li><li><p><strong>可信度（Trustworthiness）</strong>：包括校准问题和训练数据的偏差对智能体可信度的影响，努力引导模型展示其思维过程或解释，以增强其可信度。</p></li></ol><h2 id="构建和开发智能体的常用框架">5. 构建和开发智能体的常用框架</h2><p>以下是一些框架和库的介绍，适用于构建和开发智能体系统：</p><ol type="1"><li><p><strong>AutoGen Studio</strong>：AutoGen Studio是微软研究院提供的一个低代码界面，用于快速原型设计 AI 智能体。它建立在AutoGen框架之上，除了原型设计外，还可以用于调试和评估多智能体工作流。</p></li><li><p><strong>AutoGen</strong>：AutoGen 是微软开源的框架，用于构建 AI智能体系统。它简化了事件驱动、分布式、可扩展且具有弹性的智能体应用的创建。用户可以利用该框架来快速开发和部署智能体系统。</p></li><li><p><strong>Swarm</strong>：Swarm 是 OpenAI提供的一个框架，旨在探索轻量级的多智能体编排。它强调在协作中使用少量资源和简单的设计，适用于构建多个智能体之间的协作系统。</p></li><li><p><strong>CrewAI</strong>：CrewAI 是一个用于编排角色扮演和自主 AI智能体的前沿框架。它通过促进协作智能，使得多个智能体能够无缝协作，共同应对复杂任务。</p></li><li><p><strong>Letta</strong>：Letta 是一个开源框架，用于构建有状态的LLM 应用程序。使用Letta，用户可以构建具有高级推理能力和透明长期记忆的智能体。</p></li><li><p><strong>Llama Stack</strong>：Llama Stack 是 Meta提供的框架，它定义并标准化了生成性 AI应用程序所需的构建块。该框架涵盖整个开发生命周期：从模型训练和微调，到产品评估，再到构建和运行生产环境中的AI 智能体。</p></li><li><p><strong>AutoRAG</strong>：AutoRAG是一个用于寻找“你自己的数据”最优 RAG（Retrieval-AugmentedGeneration）管道的工具。它允许用户自动评估各种 RAG模块，并使用自己的评估数据找到最适合自己用例的 RAG 管道。</p></li><li><p><strong>Beam</strong>：Beam是一个领先的智能体过程自动化平台，致力于通过自动化智能体的流程和操作，提高生产效率。</p></li><li><p><strong>AutoAgents</strong>：AutoAgents是一个新型框架，旨在动态生成和协调多智能体，使语言模型能够为各种任务构建适应性的AI 团队。与传统系统依赖于静态预定义智能体不同，AutoAgents可以自主生成任务特定的智能体，允许在多个领域灵活协作。该框架引入了草稿和执行阶段，用于处理复杂的任务环境并促进有效的角色分配和解决方案规划。</p></li><li><p><strong>BabyAGI</strong>：BabyAGI是一个广泛使用的、面向任务的自主智能体，用于处理多个领域的各种任务。它利用了包括OpenAI 的 GPT-4 语言模型、Pinecone 向量搜索平台和 LangChain框架等先进技术。BabyAGI 的核心组件如下：</p><ul><li><strong>任务完成</strong>：系统首先处理任务列表中的任务，结合 GPT-4和 LangChain 的链式和智能体功能生成结果，必要时对结果进行优化，并存储在Pinecone 中供未来参考。</li><li><strong>任务生成</strong>：完成一个任务后，系统利用 GPT-4创建新的任务，确保新任务不会与现有任务重复。</li><li><strong>任务优先级排序</strong>：系统根据新生成任务的重要性重新排序任务列表，GPT-4帮助系统进行优先级评估。</li></ul></li></ol><h1 id="reference">Reference</h1><p><a href="https://aman.ai/primers/ai/agents/#overview">DistilledAI</a></p>]]></content>
      
      
      <categories>
          
          <category> Foundation Model </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Agents </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>LLM</title>
      <link href="/2024/10/28/LLM/"/>
      <url>/2024/10/28/LLM/</url>
      
        <content type="html"><![CDATA[<h2 id="大模型常用微调方法lora和ptuning的原理">1.大模型常用微调方法LORA和Ptuning的原理</h2><ul><li><p>LORA: Low-Rank Adaptation.核心是在大型语言模型上对指定参数增加额外的低秩矩阵，也就是在原始pre-trainedLM旁边增加一个旁路，做一个降维再升维的操作。假设模型中有一个需要更新的权重矩阵<span class="math inline">\(W \in \mathbb{R}^{d \timesk}\)</span>，LORA的思想是修改为： <span class="math inline">\(W' = W+ \delta W\)</span>，其中 <span class="math inline">\(\Delta W = A\times B\)</span>, <span class="math inline">\(A \in \mathbb{R}^{d\times r}\)</span>, <span class="math inline">\(B \in \mathbb{R}^{r\times k}\)</span>, 且 <span class="math inline">\(r &lt;&lt; \min\{d,k\}\)</span>。在模型训练过程中，固定PLM的参数，只训练降维矩阵 <span class="math inline">\(A\)</span> 与升维矩阵 <span class="math inline">\(B\)</span>。</p></li><li><p>Ptuning: Prompt Tuning.</p></li></ul><h2 id="diffusion-models-and-stable-diffusion">2. Diffusion models andStable diffusion</h2><ul><li><a href="https://lilianweng.github.io/posts/2021-07-11-diffusion-models/#prog-distll">Whatare Diffusion Models?</a></li><li><h2 id="diffusion-models-for-video-generation"><a href="https://lilianweng.github.io/posts/2024-04-12-diffusion-video/">DiffusionModels for Video Generation</a></h2></li></ul><h2 id="llm的幻觉的问题">3. LLM的幻觉的问题</h2><p>Great thanks to this blog: <a href="https://aman.ai/primers/ai/hallucination/">NLP • HallucinationMitigation</a></p><p>AI文本生成中的<strong>幻觉</strong>现象指的是模型生成的文本虽然在语法上可能是正确的，并且看起来合理，但与输入内容并不一致，甚至可能是事实错误的。这种问题在像GPT-3这样的系统中尤为常见，生成的细节可能会偏离甚至与输入内容相矛盾。</p><h3 id="幻觉产生的原因">幻觉产生的原因</h3><p>造成幻觉的原因可以归结为以下几个方面：</p><p><strong>1.训练数据不足</strong>：如果模型在训练中没有接触到多样化的数据，它可能无法准确地建立输入与合适输出之间的关联，从而导致幻觉内容的产生。</p><p><strong>2.模型过拟合</strong>：过拟合于训练数据会导致模型生成的输出过于依赖训练集，但在面对新的或不同的输入时与实际不符。</p><p><strong>3.监督不足</strong>：如果没有充分的指导，模型可能会过度依赖其内部逻辑，导致生成的内容出现“幻觉”。</p><p><strong>4.知识截止</strong>：像ChatGPT这样的语言模型有知识截止日期，因此对于截止日期之后的信息一无所知。在这种情况下，它可能在不知情的情况下提供过时或不再相关的回答。</p><h3 id="如何解决幻觉">如何解决幻觉</h3><h4 id="训练阶段">训练阶段</h4><p><strong>Reinforcement Learning from Human Feedback(RLHF)</strong>。使用RLHF来减少幻觉的核心思想是让人类提供有关模型响应准确性和相关性的反馈。通过将这些反馈融入训练过程，模型可以逐步学习区分准确信息和不准确信息，从而降低产生幻觉的可能性。此外，RLHF还能帮助模型理解其输出带来的影响，进而提高生成相关且符合事实的回应能力。</p><h4 id="训练之后">训练之后</h4><p>在对 LLM 进行训练之后，可以使用 <strong>Prompting</strong>减轻幻觉。</p><p><strong>1. Retrieval Augmented Generation（RAG）</strong>。通过在生成过程中提供额外的上下文信息，有助于消除大语言模型中的幻觉问题。幻觉现象通常发生在LLM基于训练数据中的模式生成响应，而不是依赖真实知识时，尤其当模型缺乏特定领域的信息或难以识别其知识边界时更容易出现。RAG通过将外部知识源整合到生成过程中来解决这一问题。它使LLM能够在生成响应时访问来自外部数据库的最新或特定上下文的数据。这种方法为模型注入了更多的上下文信息，帮助其更好地理解主题，降低幻觉出现的概率。例如，在设计用于提供汽车信息的聊天机器人中，RAG可以从外部数据库中检索产品的具体细节和上下文信息，以补充用户的输入。这样，LLM可以接收到更全面和详细的提示，从而生成更准确和相关的响应。</p><p><strong>2. ContextualPrompting</strong>。旨在通过为模型提供明确的上下文或背景信息来改善其生成的输出。这种方法通过在提示（prompt）中包含相关的上下文信息，帮助模型更好地理解任务，并生成更准确、相关性更高的回答或内容。在给大语言模型（LLM）提供问题和上下文时，附加的上下文段落通常是来自维基百科文章、书籍章节等的摘要。这些上下文片段通过在句末插入唯一标识符进行标记，例如“(source1234)”或“(source 4567)”。例如：</p><ul><li><p>“巴黎是法国的首都。(source 1234)”</p></li><li><p>“法国位于西欧。(source 4567)”</p><p>这些来源标签是与原始上下文片段中的特定句子相对应的唯一编号。具体来说，ContextualPrompting涉及将一段上下文或背景知识与问题或任务一起输入模型。这段上下文可以是来自外部知识库的文本、前面对话中的信息、或任何与当前任务相关的数据。上下文为模型提供了额外的信息，使其能够更好地理解用户的意图，并在生成内容时参考这些背景知识。在使用这些带标签的上下文提示LLM时，研究方法还会在问题后附加指令，例如“提供细节并在答案中包含来源。”通过这种方式，LLM在生成响应时被引导引用这些标记的来源。这些标签为验证LLM的响应是否基于提供的上下文信息提供了参考。如果响应中包含匹配的来源标签，就表明LLM依赖于提供的上下文，而不是凭空生成（幻觉）的内容。</p></li></ul><p><strong>3. Chain of Verification（CoVe）</strong>。CoVe方法让大语言模型（LLM）在生成初始回答后，经过多个步骤来提升准确性：(1).生成初始回答，可能包含不准确或幻觉。(2).规划验证问题——模型生成一系列验证问题以自我查证。(3).执行验证——模型独立回答这些验证问题 (Verification questions are oftenanswered more accurately than facts stated in long passages)。(4).基于验证结果修正初始回答，生成最终答案。 <img src="/2024/10/28/LLM/cove.png"></p><h2 id="llm-alignment">4. LLM Alignment</h2><p>Great thanks to this blog: <a href="https://aman.ai/primers/ai/llm-alignment/">LLM Alignment</a></p><h3 id="overview">Overview</h3><ul><li>2017 年，OpenAI 在其论文 <a href="https://arxiv.org/abs/1706.03741">Deep reinforcement learning fromhuman preferences</a> 中提出了一种开创性的机器学习方法，称为"从人类反馈出发的强化学习"(RLHF)，特别关注人类偏好。这一创新概念自此激发了该领域的进一步研究和发展。</li><li>RLHF 概念:使用一个预先训练好的语言模型，由人类评估员对其输出进行排序。然后，这种排序会让模型对某些类型的回答产生偏好，从而产生更可靠、更安全的输出。</li><li>RLHF可以有效利用人类反馈来提高语言模型的性能。它将强化学习算法的优势与对人类输入的细微理解相结合，促进了模型的持续学习和改进。结合人类反馈，RLHF不仅能提高模型的自然语言理解和生成能力，还能提高其在文本分类或翻译等特定任务中的效率。此外，RLHF在解决语言模型中的偏差方面也发挥着至关重要的作用。通过允许人工输入来指导和纠正模型的语言使用，它可以促进更加公平和包容的交流。不过，在这一过程中，必须注意人为因素可能导致的偏差。</li></ul><h3 id="reinforcement-learning-强化学习基础概念">Reinforcement Learning强化学习基础概念</h3><p><img src="/2024/10/28/LLM/rl.png"></p><p>如图所示，agent 采取一定的 action，对于当前的action，环境会反馈其状态 state 以及 给出reward。其中，reward是要优化的目标，state是环境当前的状态，policy用于根据state选择action.</p><h3 id="reinforcement-learning-from-human-feedback-rlhf">ReinforcementLearning from Human Feedback (RLHF)</h3><p>LLM的最初目标是准确地预测下一个token。但是，这种方式无法保证输出的结果是有用、无害且诚实的，有可能产生不符合人类道德或安全标准的内容。为解决这一问题，需要有一种方式来引导模型输出符合人类价值观的结果。</p><p><img src="/2024/10/28/LLM/rlhf.png"></p><p>图中给出了使用RLHF训练LM的三个步骤，具体来说，</p><ol type="1"><li><p>Collect Demonstration Data, and Train a Supervised Policy.首先，从 prompts 中选择一个prompt；然后人类标注者给出希望得到的输出；最后这些经过标注后的数据用于对LM进行supervised fine-tune.</p></li><li><p>Collect Comparison Data, and Train a Reward Model.首先，选取一个prompt，模型给出几个可能的输出结果；标注者根据有用性、准确性等准则对结果进行从好到差的排序；这些排序后的数据用来训练一个reward model. Reward model用来评估模型输出结果的质量。</p></li><li><p>Optimize a Policy Against the Reward Model Using ReinforcementLearning. 产生新的prompt, 基于当前的policy, model 得到新的输出 response;Reward model 评估 response，然后得到 reward；基于得到的 reward以及一些强化学习算法，比如PPO，对 policy 进行更新。调整 policy是为了增加未来产生 higher-reward outputs 的可能性。</p></li></ol><p>Chip Huyen provides a zoomed out view of how the overall processworks in her flowchart below:</p><p><img src="/2024/10/28/LLM/rlhf1.jpeg"></p><h4 id="reward-model">REWARD MODEL</h4><p>Reward model 的主要功能是评估给定的输入（如文本序列）并产生scalarreward。这种reward 量化了输出与人类偏好或期望行为的一致程度。</p><p><img src="/2024/10/28/LLM/rlhf2.png"></p><p>Reward 模型的结构包括 - LM 分类器：一个二元分类器微调的LLM，可对哪种反应更符合人类偏好进行评分。 - valuenetworks：一个回归模型，根据输入预测人类偏好评分。 -评论生成器：经过训练的LM，可生成评价性评论，解释哪种回答更好以及原因。该评论可用于指令调整。</p><h4 id="optimizing-the-policy">Optimizing the Policy</h4><p><strong>策略（policy）</strong>：在强化学习中，策略是一组规则或决策机制，指导智能体（agent）根据它所处的环境状态或观察结果来选择行动。也就是说，策略定义了智能体如何在不同的情境下采取什么样的行为。</p><p><strong>PPO（Proximal PolicyOptimization，邻近策略优化）</strong>：是一种常用的强化学习算法。在PPO中，策略是通过反复迭代来优化的。其目标是最大化奖励，即让智能体的行为逐步改善，获得更高的回报。但是，PPO会确保策略的更新不会发生剧烈变化。这是通过引入一种约束，使更新后的策略保持与之前的策略相似性，以避免不稳定性或训练失败的情况。</p><p><strong>DPO（Direct PreferenceOptimization，直接偏好优化）</strong>：是一种不同的策略优化方法。在DPO中，策略直接基于人类偏好进行优化。具体来说，它通过二元交叉熵损失函数（binarycross entropyloss），增加模型生成的优选输出的相对对数概率，而减少非优选输出的概率。这种方法直接根据人类的反馈进行优化，旨在使模型生成更符合人类期望的输出。与此同时，DPO也通过KL散度约束来保持平衡，防止策略发生过大的偏离。</p><h4 id="training-llama-2">Training Llama 2</h4><p><img src="/2024/10/28/LLM/llama.jpeg"></p><p>以下是Llama 2 的主要训练阶段的介绍：</p><ol type="1"><li><strong>预训练阶段</strong>（Pretraining）：<ul><li>在最初的预训练阶段，Llama 2使用大量数据通过<strong>自监督学习</strong>进行训练。这一阶段让模型学习语言模式和上下文的基本结构，使其能够理解语言的基本规则和含义。</li><li>自监督学习的方式通常是通过预测文本中隐藏的部分（如下一句话或遮盖的单词）来训练模型，帮助它积累广泛的语言知识。</li></ul></li><li><strong>有监督微调阶段</strong>（Supervised Fine-Tuning）：<ul><li>在此阶段，模型进一步通过<strong>指令数据</strong>进行有监督微调。具体来说，模型会根据特定的指令进行训练，学习如何对不同的提示做出合适的响应。</li><li>这个过程使模型能够在实际应用中根据明确的要求或任务生成准确、相关的回答。</li></ul></li><li><strong>奖励模型创建（RLHF步骤1）</strong>（Reward Models Creation -RLHF Step 1）：<ul><li>为了进一步优化模型输出的质量，Llama 2创建了两个<strong>奖励模型</strong>，一个针对<strong>帮助性（helpfulness）</strong>，另一个针对<strong>安全性（safety）</strong>。</li><li>这些奖励模型通过<strong>人类偏好数据</strong>训练，预测在两种不同的输出中哪一个更符合人类的判断。此阶段基于二元比较，模型通过评估每对输出的优劣来学习。</li></ul></li><li><strong>边际损失与排名</strong>（Margin Loss and Ranking）：<ul><li>Llama 2使用二元比较数据集来优化排名。在每次比较中，标注者只需要选择两种响应中的一个，并通过<strong>边际标签</strong>来表示偏好的强度。这种边际标签可以用于进一步计算<strong>排名损失</strong>，提高模型对不同偏好的敏感性。</li></ul></li><li><strong>拒绝采样与PPO对齐（RLHF步骤2）</strong>（Rejection Samplingand PPO - RLHF Step 2）：<ul><li>在最后一步，Llama 2使用<strong>拒绝采样</strong>和<strong>邻近策略优化（PPO）</strong>来进一步优化模型。</li><li>拒绝采样是指从模型生成的多个输出中，选择<strong>奖励最高</strong>的输出用于更新梯度，从而增强模型生成高质量输出的能力。</li><li>之后通过PPO算法对模型进行进一步对齐，使其生成的回答更加安全且有帮助，同时确保优化过程中策略更新的稳定性。</li></ul></li></ol><p>总的来说，Llama 2的训练流程结合了大规模的自监督学习、基于指令的有监督微调，以及基于人类偏好的强化学习，通过一系列精细的步骤来提升模型的语言理解、输出的帮助性和安全性。</p><h4 id="proximal-policy-optimization-ppo">Proximal Policy Optimization(PPO)</h4><p>建议先阅读以下两篇优秀博客： - <a href="https://www.cnblogs.com/xingzheai/p/15826847.html">详解策略梯度算法</a>- <a href="https://www.cnblogs.com/xingzheai/p/15931681.html">详解近端策略优化</a></p><p><strong>PPO-clip</strong>:在PPO（邻近策略优化）中，代理损失函数（surrogate loss）是通过当前策略和参考策略下执行同一动作的概率比率来定义的。这一比率用于引导策略向那些能够获得更高奖励的动作倾斜，同时确保策略更新的幅度不会过大，从而保持训练的稳定性。为防止策略的更新幅度过大，PPO引入了剪裁，限制比率在一定范围内。通过在一定阈值外“剪裁”比率的变化，模型可以避免发生过大的更新，从而保证训练过程的稳定性。</p><p>定义 <span class="math inline">\(\pi_{\theta}\)</span>为当前策略（参数为 <span class="math inline">\(\theta\)</span> 的一个网络），<span class="math inline">\(\pi_{ref}\)</span>是实际的、可参考的策略空间。<span class="math inline">\(A(s_t,a_t)\)</span>为在状态 <span class="math inline">\(s_t\)</span>下采取行为 <span class="math inline">\(a_t\)</span>时得到的奖励。近端策略优化裁剪函数为： <span class="math display">\[L(\theta) = E_{(s_t, a_t) \sim \pi_{ref}} \min{(\frac{p_{\theta}(a_t |s_t)}{p_{\pi_{ref}}(a_t|s_t)} A(s_t, a_t), clip(\frac{p_{\theta}(a_t |s_t)}{p_{\pi_{ref}}(a_t|s_t)}, 1-\epsilon, 1+\epsilon)A(s_t, a_t))},\]</span> <span class="math inline">\(\epsilon\)</span>是一个超参数，要需要我们调整的，一般设置为0.1或0.2。</p><p><strong>PPO-penalty</strong>: 在PPO中，除了使用剪裁目标函数（clippedobjective）外，另一种常见的方法是直接在目标函数中加入KL散度惩罚项。这意味着算法会根据新策略与参考策略的偏离程度对目标函数进行惩罚。具体损失函数为：<span class="math display">\[L(\theta) = E_{(s_t, a_t) \sim \pi_{ref}} \frac{p_{\theta}(a_t |s_t)}{p_{\pi_{ref}}(a_t|s_t)} A(s_t, a_t) - \betaKL(\pi_{ref}||\pi_{\theta}),\]</span></p><p>通过<strong>最大化目标函数</strong>得到最优策略。对于大规模语言模型（LLM）来说，这个目标函数反映了模型对齐的目标，比如生成<strong>有帮助</strong>、<strong>真实</strong>、<strong>无害</strong>的回答。</p><p><strong>参考策略 (ReferencePolicy)</strong>：参考策略是训练过程中用作<strong>基准</strong>或<strong>对照</strong>的一套策略。它通常是一个<strong>稳定的策略</strong>，模型可以从这个基准出发，或者在训练过程中参考该策略来指导学习。它确保最优策略的更新不会偏离初始策略太远，防止训练过程中产生剧烈变化或不稳定的行为。</p><h3 id="reinforcement-learning-with-ai-feedback-rlaif">ReinforcementLearning with AI Feedback (RLAIF)</h3><p>RLAIF使用AI生成的偏好（而不是人工标注的偏好）来训练大规模语言模型（LLMs）。这种方法通过利用强大的预训练模型（如GPT-4）生成反馈，为训练其他LLM提供高效、成本更低的替代方案。在RLAIF中，反馈生成的语言模型相当于充当了“虚拟人工标注者”的角色。它评估训练中的模型生成的多个输出，选择优选响应或提供改进建议。</p><h4 id="direct-preference-optimization-dpo">Direct PreferenceOptimization (DPO)</h4><p>本文前面讨论的 RLHF主要包括两个阶段：根据人类偏好标签训练奖励模型，然后使用强化学习（RL）对LM 进行微调，使其与这些偏好保持一致。然而，RLHF存在复杂性和不稳定性问题，它需要拟合一个奖励模型，然后训练一个策略来优化该奖励，这就容易产生稳定性问题。</p><p>DPO算法摆脱了传统RL方法中的两个阶段。通过定义新的损失函数来训练LLM，以避免不稳定性问题。DPO使用一种特殊格式的数据集，形式为：&lt;prompt, worse completion,bettercompletion&gt;（即“提示，较差的完成，较好的完成”）。在训练过程中，DPO的损失函数鼓励模型增加较好完成的概率，同时降低较差完成的概率。这个过程是通过加权实现的，权重基于隐含的奖励模型。这里的关键在于，LLM本身充当了奖励模型，因此不再需要一个显式的奖励模型。下图给出了DPO和RLHF的区别。</p><p><img src="/2024/10/28/LLM/DPO.jpg"></p><p><strong>Binary Cross-Entropy Loss</strong> DPO通过使用二元交叉熵（Binary Cross-Entropy,BCE）损失函数来优化语言模型以更好地与人类偏好对齐的训练方法。对于每个输入，模型会生成两个响应，并由人类标注者指明他们的偏好（哪个响应更好）。DPO通过比较模型生成的响应对（即优选响应和不优选响应）与人类偏好进行训练。</p><p>损失定义如下： <span class="math display">\[L_{DPO}(\theta) = -E_{(x, y_w, y_l) \sim D} [\log \sigma (\beta\log\frac{\pi_{\theta}(y_w| x)}{\pi_{ref}(y_w|x)} - \beta \beta\log\frac{\pi_{\theta}(y_l| x)}{\pi_{ref}(y_l|x)})],\]</span> 其中，<span class="math inline">\(\pi_{\theta}\)</span>为要训练的策略模型， <span class="math inline">\(\pi_{ref}\)</span>是参考的策略模型；<span class="math inline">\(y_w\)</span> 和 <span class="math inline">\(y_l\)</span> 分别表示优选response 和 不优选的response. <span class="math inline">\(\beta\)</span>控制待训练模型与参考策略模型的接近程度。<span class="math inline">\(\sigma\)</span> 为 logistic 函数。</p><ul><li>DPO标志着语言模型训练方法的转变，通过将强化学习与人类反馈（RLHF）过程整合为<strong>单个的端到端</strong>优化步骤，简化了模型的训练。</li></ul><p><strong>DPO 的训练过程</strong> -选择一个已经经过基础指令调优的语言模型作为参考模型，这个模型提供了良好的基础。-使用不同的采样/解码方法（例如不同的温度设置）对同一提示生成成对输出，并让人类选择他们喜欢的哪一个。这一过程将产生一个人类偏好/反馈的数据集。-在LLM上添加一个线性层，使得模型能够输出一个标量值。这一层将帮助模型在训练过程中产生更具体的数值输出。-使用DPO损失，该损失函数基于二元交叉熵损失。计算参考模型和正在调优模型的标量输出的对数比率，并乘以一个散度参数，以调整模型的输出。-在训练完成后，去掉最后的线性层，这样就得到了一个基于人类反馈微调的LLM。</p><p>通过以上步骤，DPO方法通过简化RLHF过程，去掉了复杂的强化学习步骤和专门的奖励模型，使得模型训练更为高效和直接。这样，最终得到的模型能够更好地反映人类的偏好，提供更优质的输出</p><h4 id="kahneman-tversky-optimization-kto">Kahneman-Tversky Optimization(KTO)</h4><p>人类在面对不确定事件时，由于‘厌恶损失’，往往会做出无法最大化期望值的决策。直接以人的偏好指导大模型的训练，其训练的数据中包含了大量的人类偏好，往往无法做出期望最大的决策。KTO是一种对齐手段，将重点从传统训练目标（如下一个标记预测或拟合配对偏好数据）转向直接优化被<strong>认为有价值或可取</strong>的输出。</p><p>KTO消除了对配对偏好排名或比较数据的需求，显著简化了数据要求。它只需要二元标签，指示某个LLM输出是可取的还是不可取的。这种二元偏好数据的需求使KTO在现实场景中更为实用，因为收集详细的偏好数据往往比较困难。</p><p><strong>前景理论 (prospect theory)</strong></p><p>KTO 的灵感来自 Daniel Kahneman 和 Amos Tversky提出的决策行为模型，特别是他们的前景理论 (prospect theory)。KTO将这些概念调整为损失函数，通过捕捉人类的偏差（如损失规避和风险敏感性），使LLM 与人类反馈保持一致。</p><p>在前景理论中，人类在不确定性下的决策行为偏离了预期效用最大化的原则，主要是因为一些心理偏差，如损失厌恶（lossaversion）和非线性概率加权（nonlinear probabilityweighting）。这些概念是KTO损失函数的基础。</p><p><strong>1. 价值函数 (ValueFunction)</strong>：前景理论中的价值函数用于描述人们如何看待收益和损失的差异。它具有以下特征：</p><ul><li><p><strong>对收益的凹性</strong>：当收益增加时，价值函数是凹的，这意味着人们在获得相同金额的收益时，所感受到的价值增加会逐渐减小。这反映了人们在面对收益时的风险厌恶（riskaversion）。</p></li><li><p><strong>对损失的凸性</strong>：当面临损失时，价值函数是凸的，这意味着在损失相同金额时，所感受到的损失会逐渐增大，反映了人们在面对损失时的风险寻求（risk-seeking）行为。</p></li><li><p><strong>损失的影响大于收益</strong>：损失对人们的情感影响通常大于收益，这一点通过损失厌恶参数<span class="math inline">\(\lambda\)</span>来建模。该参数通常大于1，意味着人们在面对损失时的感受强于获得相同金额收益时的感受。</p></li></ul><p><strong>2. 数学表达式</strong>. 价值函数 <span class="math inline">\(v(x)\)</span> 可以用以下公式表示： <span class="math display">\[v(x) = \begin{cases}x^\alpha &amp; \text{if } x \geq 0 \\-\lambda (-x)^\beta &amp; \text{if } x &lt; 0\end{cases}\]</span> 其中：</p><ul><li>$ (0,1)$ 和 <span class="math inline">\(\beta \in (0,1)\)</span>控制对收益和损失的减敏感性（diminishingsensitivity）。这意味着随着收益或损失的增加，人们的感知效应会逐渐减弱。</li><li>$ $是损失厌恶因子，通常大于1，这表示人们对损失的反应比对收益更为强烈。</li></ul><p><strong>3. 概率加权函数 (Probability Weighting Function)</strong>:人们在判断概率时，往往会倾向于高估小概率事件和低估大概率事件。尽管这一元素并非KTO的核心部分，但它强调了主观不确定性感知如何影响决策。这种加权使得人们在面对不确定性时的决策并不是完全理性的，而是受到了心理因素的影响。</p><p>Kahneman-Tversky Optimization (KTO)的损失函数是基于前景理论构建的，其设计目标是直接最大化语言模型生成输出的效用。以下是KTO 损失函数的关键要素及其解释：</p><p><strong>KTO‘s loss function</strong></p><ul><li><p>KTO 使用了一个 <strong>逻辑函数 <span class="math inline">\(\sigma\)</span></strong>，而不是经典前景理论中的分段价值函数。这种逻辑函数保持了对收益的<strong>凹性</strong>和对损失的<strong>凸性</strong>，反映了人类对风险的感知。</p></li><li><p><strong>风险厌恶参数 <span class="math inline">\(\beta\)</span></strong>被纳入模型中，用于控制风险厌恶程度。这一参数影响价值函数饱和的陡峭程度，进而影响模型如何感知收益和损失。</p></li><li><p>在 KTO 中，传统的损失厌恶参数 <span class="math inline">\(\lambda\)</span>被替换为两个独立的超参数：<strong><span class="math inline">\(\lambda_D\)</span></strong>（用于积极反馈的输出）和<strong><span class="math inline">\(\lambda_U\)</span></strong>（用于消极反馈的输出）。允许模型根据输出类型的不同（积极或消极），以更细致的控制方式来处理反馈，从而更好地反映人类的风险厌恶特性。</p></li><li><p>模型的参考点通过 <strong>KL 散度</strong>来定义，表示当前模型策略 <span class="math inline">\(\pi_\theta\)</span>与参考策略 <span class="math inline">\(\pi_{\text{ref}}\)</span>之间的差异。KL散度项控制当前模型输出与预训练参考模型的偏离程度，并作为优化中评估收益和损失的参考点<span class="math inline">\(z_0\)</span>。</p></li></ul><p>KTO（Kahneman-Tversky Optimization）损失函数的数学公式如下： <span class="math display">\[L_{KTO}(\pi_\theta, \pi_{\text{ref}}) = \mathbb{E}_{x,y \simD}[\lambda_y - v(x,y)], \\\quad \\v(x,y) =   \begin{cases}   \lambda_D \sigma(\beta(r_\theta(x,y) - z_0)), &amp; \text{if } y \sim\text{desirable} \\   \lambda_U \sigma(\beta(z_0 - r_\theta(x,y))), &amp; \text{if } y \sim\text{undesirable}   \end{cases}\]</span></p><p>其中：</p><ul><li><p><strong><span class="math inline">\(\mathbb{E}_{x,y \simD}\)</span></strong>：表示对数据集 <span class="math inline">\(D\)</span> 中的样本进行期望计算，其中 <span class="math inline">\(x\)</span> 是输入，<span class="math inline">\(y\)</span> 是模型生成的输出。</p></li><li><p><strong><span class="math inline">\(\lambda_y\)</span></strong>：代表与输出 <span class="math inline">\(y\)</span> 相关的损失厌恶参数，可以是<strong><span class="math inline">\(\lambda_D\)</span></strong>（用于积极输出）或<strong><span class="math inline">\(\lambda_U\)</span></strong>（用于消极输出），用于表示人类对损失的厌恶程度。</p></li><li><p><strong><span class="math inline">\(r_\theta(x,y)\)</span></strong>： $ r_(x,y) = . $该函数表示在当前策略 <span class="math inline">\(\pi_\theta\)</span>下生成输出 <span class="math inline">\(y\)</span> 的对数概率与参考策略<span class="math inline">\(\pi_{\text{ref}}\)</span>下生成同一输出的对数概率之比。它衡量了当前模型与参考模型在生成特定输出时的相对表现。</p></li><li><p><strong><span class="math inline">\(z_0\)</span></strong>： <span class="math inline">\(z_0 = KL(\pi_\theta(y'|x) \|\pi_{\text{ref}}(y'|x))\)</span>. 这里量化当前策略 <span class="math inline">\(\pi_\theta\)</span> 和参考策略 <span class="math inline">\(\pi_{\text{ref}}\)</span>之间的差异。它作为评估当前策略与参考策略偏离程度的参考点。</p></li><li><p><strong><span class="math inline">\(v(x,y)\)</span></strong>：价值函数，依赖于输出<span class="math inline">\(y\)</span> 的性质. <strong><span class="math inline">\(\sigma\)</span></strong>：逻辑函数，用于对价值函数进行调整，使其保持凹性（对于收益）和凸性（对于损失），模型就会在收益时更加规避风险，在损失时更加追求风险。<strong><span class="math inline">\(\beta\)</span></strong>：风险厌恶参数，控制风险厌恶的程度。增加<span class="math inline">\(\beta\)</span>会增加收益时的风险规避行为和损失时的风险追求行为。</p></li></ul><h3 id="ppo-dpo-以及-kto-的对比">PPO, DPO 以及 KTO 的对比</h3><table><colgroup><col style="width: 11%"><col style="width: 29%"><col style="width: 29%"><col style="width: 29%"></colgroup><thead><tr><th>Aspect</th><th>PPO</th><th>DPO</th><th>KTO</th></tr></thead><tbody><tr><td>目标</td><td>最大化预期奖励，同时防止策略更新过大（目标函数clip）。</td><td>根据人类偏好直接优化策略，使用二元分类目标（使用 KL散度约束）。</td><td>通过最大化 LLM生成的效用对齐模型，基于前景理论，不需要详细的偏好对。</td></tr><tr><td>输入</td><td>来自环境的状态和奖励。</td><td>来自环境的状态和人类偏好反馈。</td><td>带有二元标签（可取或不可取结果）的 LLM 输出。</td></tr><tr><td>输出</td><td>在环境中采取的行动。</td><td>在环境中采取的行动，与人类偏好对齐。</td><td>与简化人类效用函数对齐的 LLM 生成结果。</td></tr><tr><td>学习机制</td><td>使用clip替代目标的策略梯度来更新策略和价值网络。</td><td>在人类偏好数据上进行二元交叉熵优化，更新单个策略网络。</td><td>基于 LLM 输出与二元反馈的对齐进行优化，无需复杂的偏好模型。</td></tr><tr><td>网络结构</td><td>独立的策略网络和价值网络。</td><td>单个策略网络。</td><td>针对 KTO 方法学调整的 LLM 框架。</td></tr><tr><td>反馈机制</td><td>使用来自环境的奖励作为学习的反馈。</td><td>使用人类偏好数据作为直接反馈进行学习。</td><td>利用对 LLM 输出的二元反馈来指导对齐，无需复杂的偏好数据。</td></tr><tr><td>稳定性</td><td>目标函数中的剪辑机制保持策略更新的稳定性。</td><td>通过直接优化偏好，利用动态逐例重要性加权实现内在稳定性。</td><td>通过简化反馈机制和聚焦于效用最大化来实现稳定的对齐。</td></tr><tr><td>复杂性</td><td>由于双网络结构和奖励最大化与策略更新稳定性之间的平衡，较复杂。</td><td>更简单，因为它绕过显式的奖励建模，直接从人类偏好优化政策。</td><td>通过消除对详细偏好建模的需求，专注于二元效用优化，降低复杂性。</td></tr><tr><td>适用性</td><td>适用于各种 RL 环境，其中奖励信号可用。</td><td>在与人类偏好对齐至关重要的场景中特别有效。</td><td>在快速和简化对齐人类反馈的场景中尤为有用。</td></tr></tbody></table><h3 id="对齐可能引入的偏差以及解决策略">对齐可能引入的偏差以及解决策略</h3><p>在讨论 <strong>强化学习人类反馈（RLHF）</strong> 和<strong>强化学习人工反馈（RLAIF）</strong>时，一个重要的问题是：这些方法是否会给模型引入偏见？答案是肯定的，正如任何依赖人类输入的机器学习方法，RLHF也有引入偏见的潜力。</p><p><strong>可能引入的不同形式的偏见</strong></p><ol type="1"><li><p><strong>选择偏见</strong>：RLHF依赖于人类评估者的反馈，这些评估者可能会有自己的偏见和偏好，因此他们的反馈可能局限于他们能够关联的主题或情境。这可能导致模型没有接触到其在现实世界中将遇到的行为和结果的真实范围。</p></li><li><p><strong>确认偏见</strong>：人类评估者可能更倾向于提供确认他们已有信念或预期的反馈，而不是根据代理的表现提供客观反馈。这可能导致模型在某些行为或结果上受到强化，而这些行为或结果在长远来看可能并不理想或可取。</p></li><li><p><strong>评分者间的差异</strong>：不同的人类评估者可能对代理表现的质量有不同的看法或判断，导致agent收到的反馈不一致。这使得有效训练agent变得困难，并可能导致次优表现。</p></li><li><p><strong>反馈有限</strong>：人类评估者可能无法对agent表现的所有方面提供反馈，导致agent 学习的缺口，可能在某些情况下表现不佳。</p></li></ol><p><strong>缓解策略</strong></p><ol type="1"><li><p><strong>多样化评估者选择</strong>：选择具有不同背景和视角的评估者可以帮助减少反馈中的偏见，就像在工作场所中一样。这可以通过从不同的人口群体、地区或行业招募评估者来实现。</p></li><li><p><strong>共识评估</strong>：使用共识评估，即多个评估者对同一任务提供反馈，可以减少个体偏见的影响，提高反馈的可靠性。这几乎就像是对评估进行“归一化”。</p></li><li><p><strong>评估者的校准</strong>：通过提供培训和指导来校准评估者，帮助提高反馈的质量和一致性。</p></li><li><p><strong>反馈过程的评估</strong>：定期评估反馈过程，包括反馈质量和培训过程的有效性，可以帮助识别和解决可能存在的偏见。</p></li><li><p><strong>agent 表现的评估</strong>：定期评估 agent在各种任务和不同环境中的表现，可以确保其没有过拟合于特定示例，并且能够推广到新的情境。</p></li><li><p><strong>平衡反馈</strong>：将人类评估者的反馈与其他反馈来源（如自我对话或专家演示）进行平衡，有助于减少反馈中的偏见影响，提高训练数据的整体质量。</p></li></ol><h3 id="trl---transformer-reinforcement-learning">TRL - TransformerReinforcement Learning</h3><p><strong>TRL</strong>（Transformer ReinforcementLearning）库可用于通过<strong>监督微调（SFT）</strong>、<strong>奖励建模（RM）</strong>、<strong>近端策略优化（PPO）</strong>以及 <strong>直接偏好优化（DPO）</strong>等方法，对转换器语言模型和扩散模型进行微调和对齐。</p><h2 id="mixture-of-experts">5. Mixture of Experts</h2><p><a href="https://aman.ai/primers/ai/mixture-of-experts/">Mixture ofExperts</a></p><h2 id="encoder-vs.-decoder-vs.-encoder-decoder">6. Encoder vs. Decodervs. Encoder-Decoder？</h2><p><a href="https://discuss.huggingface.co/t/suggestions-and-guidance-finetuning-bert-models-for-next-word-prediction/14043">模型总结</a><img src="./statistic/enc_dec.jpeg"></p><h3 id="编码器模型">编码器模型</h3><p>基于编码器的模型专注于理解输入文本。它们通过在包含重建损坏输入任务的预训练（如掩码部门tokens）中，捕获丰富的上下文信息。<strong>BERT</strong>（双向编码器表示的变换器）是编码器模型的一个重要例子。在BERT 中，部分输入标记会被特殊的 [MASK]标记替代，模型通过周围的上下文预测这些被掩蔽的标记。这使得 BERT能够学习双向表示，从而捕捉标记左侧和右侧的上下文。编码器模型在自然语言理解（NLU）任务中尤其有效，例如文本分类、情感分析和抽取式问答。这些任务受益于模型深刻理解和表示输入文本的能力。</p><p>BERT 可以利用双向语境进行预测 maskedtokens，相较于自回归模型，使用双向信息可以提高性能。然而，BERT在预训练时使用的 [MASK]等人工符号在微调时并不存在于真实数据中，这就造成了预训练数据与微调数据分布之间的异质性。此外，由于预测的tokens在输入中被mask，BERT无法像自回归那样使用乘积规则建立联合概率模型。换句话说，<strong>BERTassumes the predicted tokens are independent of each other given theunmasked tokens, which is oversimplified as high-order, long-rangedependency is prevalent in natural language.（在给定其他未被掩码的tokens时，BERT对每个被掩盖的单词独立地进行预测，所有需要预测的token之间被假设是相互独立的）</strong></p><p>BERT（及其所有变体，如 RoBERTa、DistilBERT、ALBERT 等）和 XLM就是编码器模型的例子。</p><p><img src="/2024/10/28/LLM/bert.jpg"></p><p><strong>优点：</strong></p><ol type="1"><li><strong>上下文依赖性：</strong>在BERT等Encoder模型中，模型可以同时获取左右两侧的上下文信息（双向上下文）。相比之下，像GPT这样的自回归模型只能访问当前位置之前的内容（单向上下文）。双向上下文对于理解文本中的复杂依赖关系和语义结构至关重要，因此Encoder模型在许多自然语言处理任务上表现优异。</li><li><strong>丰富的上下文理解：</strong>Encoder模型擅长捕捉输入数据中的复杂模式和关系，尤其适合需要深入理解和分析的任务，比如命名实体识别、文本分类、阅读理解等。这使得模型能够更好地理解句子的语义和结构，从而在相关任务中表现出色。</li></ol><p><strong>缺点：</strong></p><ol type="1"><li><strong>输入噪声：</strong>BERT在预训练阶段使用了人工符号（如[MASK]）来掩盖部分输入单词，但这些符号在实际应用和微调时并不存在。这种差异导致了预训练和微调之间数据分布的异质性，影响模型的迁移能力。</li><li><strong>独立性假设：</strong>BERT在预测被掩盖的单词时，假设每个掩盖的单词是独立于其他掩盖词的，只与未被掩盖的单词相关。例如，在句子“itshows that the housing crisis was turned into a bankingcrisis”中，如果掩盖了“banking”和“crisis”两个词，模型会分别预测它们，而不考虑二者之间的隐含关系。这种独立性假设限制了模型对掩盖词之间复杂依赖关系的捕捉，影响了在需要更精细理解的任务中的表现。</li></ol><h3 id="解码器模型">解码器模型</h3><p>基于解码器的模型（自回归模型）旨在执行文本生成任务。这些模型一次生成一个token，并使用之前生成的token作为上下文来预测下一个token。<strong>GPT</strong>（生成式预训练变换器）、GPT-2和 GPT-3是解码器模型的典型例子。这些模型在大规模语料库上进行预训练，以预测句子中的下一个词，从而能够生成连贯且语境相关的文本。解码器模型在自然语言生成（NLG）任务中表现出色，例如语言翻译、文本摘要和对话生成。它们生成流利且上下文适当的文本的能力使其成为从头生成内容任务的理想选择。</p><p>自回归语言模型利用上下文中的词来预测下一个词，通过估计文本语料库的概率分布来实现。具体来说，给定一个文本序列$ x = (x_1, , x_T) <span class="math inline">\(，自回归语言建模将似然分解为前向积：\)</span>$p(x) = <em>{t=1}^{T} p(x_t | x</em>{&lt;t}) $$</p><p>或者反向的形式： <span class="math display">\[p(x) = \prod_{t=1}^{T} p(x_t | x_{&gt;t})\]</span></p><p>通常使用<strong>多项式分布</strong>（MultinomialDistribution）来建模下一个词的生成过程。这是因为语言模型的目标是根据先前的词预测当前词的概率，而语言中词汇的生成通常是一个离散事件。因此，给定上下文$x_{&lt;t} $，下一个词 $ x_t $ 的条件概率可以表示为： <span class="math display">\[p(x_t \mid x_{&lt;t}) = \text{softmax}(z_t),\]</span> 其中，$ z_t $是通过神经网络计算得到的未归一化的得分，通常代表每个词在词汇表中的相对可能性。通过使用softmax函数，这些得分被转换为一个有效的概率分布，保证所有可能词的概率和为1。</p><p>在这种情况下，模型会学习每个条件分布的参数模型（例如神经网络）。由于自回归语言模型只训练编码单向上下文（要么是前向的，要么是后向的），因此它在建模深层双向上下文方面并不有效。下面的图示展示了前向和后向的方向性。</p><p><img src="/2024/10/28/LLM/auto1.jpg"></p><p><img src="/2024/10/28/LLM/auto2.jpg"></p><p><strong>优点：</strong></p><ol type="1"><li><strong>生成能力强</strong>：自回归语言模型非常适合生成式自然语言处理（NLP）任务。由于它们采用因果注意力机制来预测下一个标记，因此在内容生成方面自然适用。它们能够生成流畅且与上下文相关的文本，这使得它们在需要自然语言生成的任务中表现出色。</li><li><strong>训练数据生成简单</strong>：训练这些模型的数据生成相对简单，因为目标只是预测给定序列中的下一个标记。这利用了语言数据的固有结构，使得数据准备过程更加高效。</li></ol><p><strong>缺点：</strong></p><ol type="1"><li><strong>上下文限制</strong>：自回归语言模型只能使用前向上下文或后向上下文，这意味着它们不能同时利用双向上下文。这种限制可能会影响它们在需要深刻理解双向上下文的任务中的表现。例如，在处理复杂句子结构或含义依赖时，缺乏双向上下文可能导致理解不够准确。</li></ol><h4 id="编码器-解码器模型">编码器-解码器模型</h4><p>编码器-解码器模型（也称为seq2seq模型）结合了编码器和解码器架构的优势。这些模型使用编码器处理和理解输入序列，使用解码器生成输出序列。这种架构在输入和输出都是序列的任务中特别有效，这些序列可能具有不同的长度或格式，甚至是不同的语言。编码器将输入序列转换为固定长度的上下文向量或中间表示，捕捉输入的含义和上下文。解码器然后接收这个上下文向量，逐个标记地生成输出序列，通常采用类似于解码器模型中的自回归技术。</p><ul><li><strong>T5 (Text-To-Text TransferTransformer)</strong>（文本到文本的迁移变换器）是编码器-解码器模型的一个显著例子。T5将每个自然语言处理问题都视为一个文本到文本的问题，其中输入和输出都是文本序列。这种方法使T5 能够应用于广泛的任务，包括翻译、摘要和问答。<br></li><li><strong>BART (Bidirectional and Auto-RegressiveTransformers)</strong>（双向自回归变换器）是另一个强大的编码器-解码器模型。BART通过使用任意噪声函数破坏文本并学习重建原始文本进行预训练。这使得它在需要基于对输入的理解生成文本的任务（如摘要和对话生成）中非常有效。<br></li><li><strong>BigBird</strong>使用稀疏注意力机制来处理较长的序列。这使得它适合处理长文档的任务，如文档分类和长篇问答。</li></ul><p><strong>优点：</strong></p><p>编码器-解码器模型能够同时处理输入的理解和输出的生成，使它们在以下任务中特别有效：</p><ul><li><p>机器翻译：将一种语言翻译成另一种语言。</p></li><li><p>文本摘要：生成文本的简要概述。</p></li><li><p>对话生成：在对话系统中生成合适的回应。</p></li></ul><p><strong>缺点：</strong></p><ol type="1"><li><p><strong>计算资源需求高</strong>：编码器-解码器模型通常参数量庞大，尤其是在处理复杂任务时。它们需要大量的计算资源和内存，训练和推理速度可能较慢。</p></li><li><p><strong>长序列处理能力有限</strong>：虽然一些模型（如BigBird）专门针对长序列进行了优化，但传统的编码器-解码器模型在处理非常长的输入时仍然面临挑战，因为它们的输入长度受限。</p></li><li><p><strong>依赖于大量标注数据</strong>：这些模型通常需要大量的高质量标注数据进行训练，这在某些领域可能难以获得。此外，训练过程中数据的多样性和质量直接影响模型的性能。</p></li><li><p><strong>对输入输出长度不匹配的敏感性</strong>：编码器-解码器模型在处理输入和输出长度差异较大的任务时，可能会表现不佳。例如，当输入很长而输出很短时，模型可能难以有效地提取和生成信息。</p></li></ol><h3 id="总结">总结</h3><ul><li>编码器模型在需要理解和解释文本的任务中表现出色。由于其能够捕捉双向上下文，这使得它们适用于理解整个句子或文档上下文至关重要的任务。例如，命名实体识别、情感分析和文本分类等任务都依赖于模型对输入文本的深刻理解。</li><li>解码器模型则非常擅长生成文本，因此非常适合创意任务，如故事生成、聊天机器人回复和文本补全等。它们通过利用先前生成的内容作为上下文来预测下一个词，从而能够生成流畅且与上下文相关的文本。</li><li>编码器-解码器模型提供了一种灵活的架构，可以处理广泛的任务，从机器翻译、文本摘要到复杂的问题回答和文档生成。这种模型能够同时理解和生成文本，使其在需要深刻理解和流利文本生成的任务中非常有效。</li></ul><p>例如，在机器翻译中，编码器处理源语言的输入句子，生成一个上下文向量，而解码器则利用这个上下文向量生成目标语言的翻译。类似地，在文本摘要中，编码器阅读并理解原始文本，而解码器则生成一个简洁的摘要。这种架构的优势在于它能够结合理解和生成的能力，适用于多种自然语言处理任务。</p><h2 id="目标检测的常用评估指标">7. 目标检测的常用评估指标</h2><h3 id="intersection-over-union-iou">Intersection Over Union (IoU)</h3><p><img src="/2024/10/28/LLM/iou.png"> Image credits to <a href="https://jonathan-hui.medium.com/map-mean-average-precision-for-object-detection-45c121a31173">source</a></p><p>IoU 越高，拟合效果越好。IoU对任何大小和形状的物体都很有效。这一针对每个物体的指标与精确度和召回率共同构成了完整的物体检测指标--平均精确度(mAP) 的基础。</p><h3 id="average-precision-ap">Average Precision (AP)</h3><p>目标检测器会生成多个预测结果：每张图片可能包含多个被预测的目标，而需要进行推理的图片也很多。每个预测目标都会被赋予一个置信度，表示检测器对该预测的信心程度。我们可以选择不同的<strong>置信度阈值</strong>，以决定接受哪些预测结果。例如，若将阈值设置为0.7，那么只接受置信度大于 0.7的预测，低置信度的预测将被舍弃。由于可以选择的阈值很多，使用精确率-召回率曲线进行评估。</p><p>模型越好，其的精确度和召回率就越高：这会将曲线的边界推向顶部和右侧。可以用曲线下的面积来概括模型的性能。这样就得到了一个介于0 和 1 之间的数值，数值越大越好。这个指标通常称为平均精度 (AP)。</p><h3 id="mean-average-precision-map">Mean Average Precision (mAP)</h3><p>mAP用来衡量模型的整体检测性能。它结合了精确率（precision）和召回率（recall），并对不同类别和不同阈值下的检测结果进行综合评价。这里的类别指的是目标检测任务中模型需要识别的不同目标种类或类型。例如，在物体检测任务中，每个目标物体都属于某个特定类别（如“人”、“车”、“猫”、“狗”等），这些类别就是检测任务中模型需要识别和区分的对象。</p><h4 id="计算过程">计算过程</h4><p>首先，我们只考虑单张图片和单个类别。假设网络在一张图片中预测了 10个属于某一类别的目标：每个预测包含一个boundingboxes、预测的类别以及预测的置信度（即网络对其预测的信心）。</p><ol type="1"><li><p>使用IoU来决定每个预测是否正确。对于每个真实目标和相应的预测，如果同时满足：（1）预测的类别与真实类别匹配；（2）IoU大于某个阈值。那么认为该预测是正确的（TruePositive）。否则，该预测被认为是错误的（False Positive）。</p></li><li><p>将所有的预测按置信度从高到低排序。在表格中，按置信度从高到低排列预测结果，右侧显示累积的精确率和召回率。</p></li></ol><p><img src="/2024/10/28/LLM/table1.png"></p><ol start="3" type="1"><li><p>根据这张表格，对于每个置信度（从最大到最小），计算出截至该点的精确度和召回率。将其绘制成图，然后得到该图像和类别的原始精确度-召回率曲线：<img src="/2024/10/28/LLM/graph1.png"></p></li><li><p>将该曲线锯齿平滑化，从而绘制出网络针对该图像和类别的最终精确度-召回曲线。根据平滑后的精确度-召回率曲线计算平均精确度（曲线下的面积）：<img src="/2024/10/28/LLM/graph2.png"> (Images credits to <a href="https://cs230.stanford.edu/section/8/">source</a>)</p></li><li><p>对每张图像和每个类别的 AP进行平均，从而得出模型在整个数据集的平均精度 mAP.</p></li></ol><h2 id="文本生成模型的常用评估指标">8. 文本生成模型的常用评估指标</h2><h3 id="困惑度-perplexity">困惑度 Perplexity</h3><p>Perplexity衡量语言模型生成文本流畅性和质量的一个常见指标，反映模型对词序预测的准确性。它本质上是模型对给定文本的“不确定性”的度量。困惑度得分越低，说明语言模型在计算给定序列中可能出现的下一个单词时越自信，而困惑度得分越高，说明模型对词的预测较为不确定，生成的文本可能不流畅，或者不符合语言结构。</p><p>熵是随关于机变量的不可预测性或随机性的度量。对于一个离散随机变量<span class="math inline">\(x\)</span>，概率分布为 <span class="math inline">\(p(x)\)</span>，其熵定义为 <span class="math display">\[H(x) = -\sum_x p(x) \log_2 p(x).\]</span>基于此，困惑度定义为一个序列的负对数似然的指数平均。具体来说，假设模型为一段文本$ (w_1, w_2, ..., w_N)$ 生成概率，困惑度定义为： <span class="math display">\[\begin{aligned}\text{Perplexity} &amp;= 2^{-\frac{1}{N} \sum_{i=1}^{N} \log_2 P( w_1,w_2, ..., w_{i-1}, w_i)} \\&amp;= P(w_1, w_2, ..., w_{i-1}, w_i)^{-\frac{1}{N}} = \prod_{i=1}^{N}P(w_i | w_1, w_2, ..., w_{i-1})^{-\frac{1}{N}}.\end{aligned}\]</span> 其中，$ P(w_i | w_1, w_2, ..., w_{i-1}) $ 是模型在前面 <span class="math inline">\(i-1\)</span> 个词的条件下预测第 $ i $个词的概率。</p><h3 id="突发性-burstiness">突发性 Burstiness</h3><p>突发性意味着如果一个词在文档中使用了一次，那么它很可能会再次出现。这种现象称为突发性，它表明第二次及之后出现的词的重要性低于第一次出现的词。值得注意的是，词语的突发性与其语义内容呈正相关性：信息量更大的词也往往更加突发。突发性基本上衡量了一段内容的可预测性，体现在句子长度和结构的均一性上。</p><p>通过收集句子长度，将文本中的每个句子按照其包含的单词数计算长度。计算均值（句子长度的平均值）和方差（句子长度的波动程度）。突发性<strong>b</strong> 的数学计算公式为：<br><span class="math display">\[b = \left( \frac{\sigma_{\tau}}{m_{\tau}} - 1 \right) \left(\frac{\sigma_{\tau}}{m_{\tau}} + 1 \right)^{-1}\]</span> 其中，<strong>b</strong> 的取值范围在 <span class="math inline">\([-1, 1]\)</span> 之间。</p><p>一般来说，人工智能具有统一有规律的特点。因此可以假设人类作者的突发性高于AI 的突发性: <span class="math display">\[b_H - b_{AI} \geq 0\]</span> 其中，<strong>b_H</strong>表示人类作者的平均突发性，<strong>b_{AI}</strong> 表示AI（如某个特定的大型语言模型）的平均突发性。</p><p>总之：</p><ul><li><p>Perplexity主要用于评估模型的语言流畅性，适合衡量语言模型在生成或预测下一个词时的能力。它衡量的是整个文本的语言建模性能。</p></li><li><p>Burstiness则侧重于文本中词语分布的不均匀性，尤其关注某些词或话题在文本中突然集中出现的现象。它更多地用于分析文本结构中的局部特征。</p></li></ul><h3 id="bleu">BLEU</h3><p>BLEU 是双语评估研究（Bilingual EvaluationUnderstudy）的缩写，<strong>主要用于机器翻译</strong>。它通过与一组参考译文进行比较来量化机器翻译文本的质量。BLEU分数计算的关键是机器翻译文本中 n-grams （给定文本样本中 n个单词的连续序列）的精确度。不过，为了防止因句子较短而高估精确度，BLEU包含一个简短度惩罚因子。尽管 BLEU被广泛使用，但值得注意的是，<strong>BLEU主要关注的是精确度，缺少召回率部分。</strong></p><p>对于 unigram (single word)，精度计算公式为 <span class="math display">\[\text{precision} = \frac{Number of correct words in machinetranslation}{Total words in machine translation}.\]</span> <strong>Unigram matches tend to measure adequacy while longern-grams matches account for fluency.</strong> 为避免夸大精度，BLEU采用修正的精度计算方法。计算方式如下：</p><ol type="1"><li><p>对于 n-gram，计算预测序列中所有匹配的 n-gram与该序列中所有n-gram的商。 <span class="math display">\[p_n = \frac{\sum_{n-gram \in \text{hypothesis}}\text{count}_{\text{match}}(n-gram)}{\sum_{n-gram \in \text{hypothesis}}\text{count}(n-gram)}.\]</span> 示例： <img src="/2024/10/28/LLM/bleu-unigrams.png"> Imagecredits to <a href="https://aman.ai/primers/ai/evaluation-metrics/#example-1">source</a>.如图所示，对于unigram，计算得到的 <span class="math inline">\(p_n =7/9\)</span>.</p></li><li><p>计算得到不同 n-grams 的精度，然后对其对数进行加权平均： <span class="math display">\[\text{BLUE}_N = \text{BP} \exp(\sum_{n=1}^{N}w_n \log p_n).\]</span></p></li></ol><ul><li>为了防止机器翻译生成的翻译过于简短，增加了简洁性惩罚 BP，BP是参考序列和预测训练长度的函数。 <span class="math display">\[\text{BP} = \begin{cases}1 &amp; \text{if } l_{hyp} &gt; l_{ref} \\e^{(1 - \frac{l_{ref}}{l_{hyp}})} &amp; \text{if } l_{hyp} \geq l_{ref}\end{cases}\]</span> BLEU 分数是一个介于 0 和 1 之间的标量值，0.6 或 0.7分是当前可以达到的较好的分数。</li></ul><h3 id="rouge">ROUGE</h3><p>ROUGE 分数代表 <strong>Recall-Oriented Understudy for GistingEvaluation</strong>，最早在《ROUGE: A Package for Automatic EvaluationofSummaries》中提出，主要用于自动总结的评估，有时也用于机器翻译的评估。</p><p>ROUGE 的关键特征是其对召回率的重视，测量系统生成的摘要中有多少参考n-gram 被找出。这使得 ROUGE 尤其适用于需要覆盖关键点的任务。ROUGE的变体中，<strong>ROUGE-N</strong> 计算 n-gram的重叠情况，<strong>ROUGE-L</strong>使用最长公共子序列来衡量句子层级的结构相似性，<strong>ROUGE-S</strong>则包含跳跃双字组的统计信息。</p><p><strong>ROUGE-N</strong>: <span class="math display">\[\text{ROUGE-N} = \frac{\text{Number of N-grams in both system andreference summary}}{\text{Total number of N-grams in reference summary}}\]</span></p><p><strong>ROUGE-L</strong>: 或称 <strong>ROUGE-LCS</strong>。基于最长公共子序列（LCS）的长度进行评估。为了弥补纯召回率指标（如ROUGE-N）的不足，ROUGE-L 通过计算 <span class="math inline">\(F_{\beta}\)</span>分数，结合了精确率和召回率。</p><p>ROUGE-LCS的优势在于，它不仅仅关注 n-gram的连续词汇重叠，还考虑了序列匹配（即重叠的词不一定需要按照相同顺序出现）。另一个更大的优势是，它自动包含最长的序列内的公共n-gram，因此不需要预先定义 n-gram 的长度。 <span class="math display">\[\text{ROUGE-LCS} = \frac{(1 + \beta^2) R_{LCS} P_{LCS}}{R_{LCS} +\beta^2 P_{LCS}}\]</span> 其中： - <strong>LCS(reference,hypothesis)</strong>：表示参考文本和生成文本的最长公共子序列。 - <span class="math inline">\(R_{LCS}\)</span>：表示基于参考文本的 LCS召回率，公式为 $ $。 - <span class="math inline">\(P_{LCS}\)</span>：表示基于生成文本的 LCS精确率，公式为 $ $。</p><p>示例： <img src="/2024/10/28/LLM/rouge-l.png"> Image credits to <a href="https://aman.ai/primers/ai/evaluation-metrics/#example-1">source</a>。在该例子中，可以计算得到 <span class="math inline">\(R_{LCS} = 7/10\)</span>, <span class="math inline">\(P_{LCS} = 7/9\)</span>, <span class="math inline">\(\text{ROUGE-LCS} = \frac{(1 + \beta^2) 49}{70 +\beta^2 63}\)</span></p><h3 id="bertscore">BERTScore</h3><p><strong>BERTScore</strong> 和 <strong>MoverScore</strong>是两个用于评估文本生成任务（如机器翻译、文本摘要等）质量的指标，它们基于语义相似性，弥补了传统n-gram 匹配方法（如BLEU、ROUGE）的不足。这两者通过引入深度学习模型（特别是BERT）来捕捉文本的语义信息，更加注重内容的语义一致性。</p><p>BERTScore是一种基于 BERT预训练模型的文本评估方法。它通过计算生成文本和参考文本在语义上的相似性，避免了传统n-gram 方法忽略语义匹配的不足。BERTScore主要是通过比较句子的词嵌入（word embeddings）来衡量语义相似度。</p><h4 id="关键步骤">关键步骤</h4><ol type="1"><li><strong>词嵌入表示</strong>：使用 BERT模型将生成文本和参考文本中的单词转换为高维的向量表示（嵌入表示）。</li><li><strong>相似性计算</strong>：计算生成文本和参考文本中每个词的嵌入表示的余弦相似度（cosinesimilarity）。</li><li><strong>匹配</strong>：为每个生成文本中的词找到参考文本中最相似的词，并基于余弦相似度进行匹配。对于候选句子中的每个token，选择与参考句子中任何token的余弦相似度得分最高的token，反之亦然。这些得分用于计算精确度、召回率和F1 score。</li><li><strong>平均相似度</strong>：基于这些匹配，对整个数据集的精确度、召回率和F1 分数进行汇总，计算生成文本和参考文本之间的整体相似度分数。</li></ol><h4 id="bertscore-的优势">BERTScore 的优势</h4><ul><li>能够捕捉到词汇和语义之间的细微差别，特别适合于语义相似但词汇不完全相同的场景。</li><li>相比于 BLEU 和 ROUGE 等基于 n-gram的方法，它更加关注语义层面的匹配，而不仅仅是词汇层面的匹配。</li></ul><h3 id="moverscore">MoverScore</h3><p><strong>MoverScore</strong>是一种改进的文本相似度评估指标，它将<strong>词移动距离（Word Mover'sDistance, WMD）</strong>与<strong>BERT嵌入</strong>相结合，用来衡量生成文本和参考文本之间的语义差异。MoverScore不仅考虑了词汇的相似性，还考虑了将生成文本中的词映射到参考文本中的最小代价。</p><h4 id="关键步骤-1">关键步骤</h4><ol type="1"><li><strong>词嵌入表示</strong>：与 BERTScore 类似，使用 BERT模型将文本中的每个词转换为向量表示。</li><li><strong>计算词移动距离</strong>：计算生成文本和参考文本之间的词嵌入的最小移动代价，即参考文本中的词映射到生成文本中的词需要“移动”多少距离。</li><li><strong>匹配得分</strong>：基于词移动距离计算生成文本和参考文本的相似度分数，得分越高表示两者的语义越接近。</li></ol><h4 id="moverscore-的优势">MoverScore 的优势</h4><ul><li>结合了词嵌入的语义相似度和词移动距离，能有效捕捉句子层次的结构信息和语义变化。</li><li>对词序、句子结构、语义信息具有更高的鲁棒性，特别适用于更复杂的语言生成任务评估。</li></ul><h3 id="对比总结">对比总结</h3><ul><li>语义匹配与标记相似性：MoverScore 使用单词移动距离（Word Mover'sDistance）和预训练嵌入来衡量整体句子级语义相似性。相比之下，BERTScore则侧重于使用上下文嵌入的标记级相似性。</li><li>评估目标：MoverScore通过测量词嵌入之间的距离来提供精细的语义评估，而 BERTScore则评估标记嵌入的精确度、召回率和 F1 分数。</li><li>对同义词的鲁棒性：MoverScore依赖于嵌入距离，因此对同义词和意译具有固有的鲁棒性，而 BERTScore通过上下文嵌入也能很好地处理同义词和意译。</li></ul><p>总之，MoverScore 提供了一种sentence-level语义相似性测量方法，可以捕捉句子的整体含义和结构，而 BERTScore则提供了一种详细的token-level相似性评估方法，重点关注精确度、召回率和 F1分数。</p><h2 id="国内外基座大模型">9. 国内外基座大模型</h2><h3 id="国内">国内</h3><ul><li><p>百度文心一言，2023年3月。具有强大的自然语言理解和生成能力，在搜索、信息流推荐、广告投放、智能写作、对话系统等场景中实现智能化升级。<img src="/2024/10/28/LLM/wenxin.jpg"> (image credits to <a href="https://wenxin.baidu.com">source</a>)</p></li><li><p>阿里通义千文，2023年4月。通义千问-Max（通义千问系列效果最好的模型，适合复杂、多步骤的任务），通义千问-Plus（能力均衡，推理效果、成本和速度介于通义千问-Max和通义千问-Turbo之间，适合中等复杂任务），通义千问-Turbo（通义千问系列速度最快、成本很低的模型，适合简单任务）。2024年8月推出通义千问-72B（Qwen-72B）。通义千问-72B（Qwen-72B）是阿里云研发的通义千问大模型系列的720亿参数规模模型，它的预训练数据类型多样、覆盖广泛，包括大量网络文本、专业书籍、代码等。Qwen-72B-Chat是在Qwen-72B的基础上，使用对齐机制打造的基于大语言模型的AI助手。</p></li></ul><p><img src="/2024/10/28/LLM/ali.jpg"> (image credits to <a href="https://help.aliyun.com/zh/model-studio/getting-started/models?spm=a2c4g.11186623.0.0.41f9253aof0b1F">source</a>)</p><ul><li><p>上海人工智能实验室等联合推出的书生·视觉大模型（InternVL）。书生浦语，书生万象，书生风乌，书生翼飞，书生天际，书生济世。InternVL2-Llama3-76B.[github]（https://github.com/OpenGVLab/InternVL）</p></li><li><p>科大讯飞星火大模型，2023年5月。星火大模型在教育、医疗、政务、司法等行业应用场景中广泛使用，尤其是在智能语音合成、语音识别、语义理解和知识图谱构建等方面表现突出。</p></li><li><p>百川智能-百川大模型，2023年6月。</p></li><li><p>华为盘古大模型，2023年7月。</p></li><li><p>腾讯混元大模型，2023年9月。</p></li><li><p>商汤日日新，2024年4月。<a href="https://platform.sensenova.cn/doc?path=/model/llm/GeneralLLM.md">doc</a></p></li><li><p>字节豆包，2024年9月。</p></li><li><p>清华-ChatGLM</p></li><li><p>智谱AI，GLM-4。</p></li><li><p>Minimax系列模型</p></li><li><p>京东言犀大模型。专为其电商平台定制的人工智能模型，尤其擅长在智能客服、智能营销和智能供应链管理等方面发挥作用</p></li><li><p>360GPT大模型。</p></li></ul><h3 id="国外">国外</h3><ul><li>Meta：LLaMA系列。2023年2月LLaMA1（7B、13B、33B、65B（650亿）），2023年7月LLaMA2（7B、13B、34B和70B），2024年4月LLaMA3（8B和70B）。</li></ul><p>相比于Llama 1，Llama2将预训练的语料token数量扩充到了2T（万亿），同时将模型的上下文长度从2048翻倍到了4096，并引入了分组查询注意力机制（grouped-queryattention, GQA）技术，GQA可以在最佳性能(multi-queryattention，MQA)和最佳模型质量(multi-headattention，MHA)之间做到很好的权衡。</p><p>相比Llama 2，Llama3支持8K长文本，并采用了编码效率更高的tokenizer，词表的大小为128K。在预训练数据方面，Llama3使用了超过15T token的语料，这比Llama 2的7倍还多。Llama3在性能上取得了巨大飞跃，并在相同规模的大模型中取得了最优异的性能。</p><ul><li><p>ref:<a href="https://zhuanlan.zhihu.com/p/694072728">万字长文详解LlaMA3的前世今生</a></p></li><li><p>ref:<a href="http://arthurchiao.art/blog/llama-paper-zh/">LLaMA：开放和高效的基础语言模型集</a></p></li><li><p>谷歌：Gemini系列。</p></li><li><p>微软：Bing.</p></li><li><p>亚马逊：拥有自己的大模型技术，并在电商、云服务、智能音响等领域有所应用。</p></li><li><p>苹果：在Siri、Face ID等方面已有所应用。</p></li></ul><h2 id="智能体和多模态模型的区别">10. 智能体和多模态模型的区别</h2><h3 id="智能体agent"><strong>智能体（Agent）</strong></h3><p>智能体（Agent）是一种能够自主感知环境、做出决策并执行行动的系统，广泛应用于不同领域，如机器人、自动化系统、游戏角色、虚拟助理等。</p><h4 id="主要特点">主要特点：</h4><ul><li><strong>自主性</strong>：智能体能够根据感知到的环境信息，独立进行决策和行动，而不需要外部持续的控制。</li><li><strong>感知-决策-行动循环</strong>：智能体能够感知外部环境（通过传感器或输入），根据某种规则或策略进行决策，并在环境中执行相应的行为。这是智能体的核心特性。</li><li><strong>持续性</strong>：智能体通常在持续的时间框架中工作，不断与环境互动。</li><li><strong>适应性与学习</strong>：有些智能体可以通过学习（如强化学习）在复杂的环境中不断优化其行为。</li></ul><h4 id="举例">举例：</h4><ul><li>机器人智能体通过传感器感知周围环境，规划路径并自主导航。</li><li>自动驾驶汽车智能体根据道路情况实时调整驾驶策略。</li><li>游戏中的 AI 角色根据玩家行为做出回应并采取行动。</li></ul><h3 id="多模态-gpt"><strong>多模态 GPT</strong></h3><p>多模态 GPT 是基于 <strong>Transformer</strong>架构的预训练语言模型（GPT），它能够处理和生成多种模态的数据，如文本、图像、音频等。传统GPT 模型专注于自然语言处理，而多模态 GPT可以跨越多种模态，将它们结合在一起进行任务处理，如从文本生成图像、理解图文组合等。</p><h4 id="主要特点-1">主要特点：</h4><ul><li><strong>多模态输入与输出</strong>：多模态 GPT可以处理多种类型的数据。例如，它可以接收图像和文本作为输入，然后生成文本描述，或根据文本输入生成相关图像。</li><li><strong>基于 Transformer 架构</strong>：多模态 GPT 继承了 GPT 的Transformer架构，通过大规模的预训练进行自监督学习，从而具备强大的生成和理解能力。</li><li><strong>生成能力</strong>：多模态 GPT强调生成能力，尤其在需要跨模态任务时表现出色，如生成图像、音频或视频，或通过对话生成文本内容。</li><li><strong>推理与回答</strong>：它可以通过整合不同模态的数据进行复杂的推理和回答，适用于许多生成和理解任务，如图文理解、文本生成等。</li></ul><h4 id="举例-1">举例：</h4><ul><li><strong>DALL·E</strong>：OpenAI 的 DALL·E 是多模态 GPT的一个典型例子，能够根据文字描述生成高质量的图像。</li><li><strong>CLIP</strong>：CLIP是一个多模态模型，可以理解图像和文本之间的关系，通过文本找到相关图像，或通过图像生成对应的文本描述。</li></ul><h3 id="智能体-vs-多模态-gpt区别与联系"><strong>智能体 vs 多模态GPT：区别与联系</strong></h3><h4 id="区别"><strong>区别</strong></h4><ul><li><strong>核心功能</strong>：<ul><li><strong>智能体</strong>：侧重于感知环境、决策与行动的闭环循环。智能体可以是物理的（如机器人），也可以是虚拟的（如自动化软件），并且通常需要与动态的环境进行交互。</li><li><strong>多模态GPT</strong>：主要用于处理和生成多种模态的数据（如图像、文本），侧重于模态间的数据理解和生成。它并不具备自主的决策和行动能力。</li></ul></li><li><strong>任务性质</strong>：<ul><li><strong>智能体</strong>：通常任务是交互性的，智能体在动态环境中持续工作，例如自动驾驶、游戏角色AI、机器人执行任务等。智能体不仅需要感知，还需要执行行动。</li><li><strong>多模态GPT</strong>：主要任务是生成式或理解式的。例如，生成图像、生成文本回答问题、或理解图文关系。它在一个静态输入的任务上更为强大，但并不在环境中主动采取行动。</li></ul></li><li><strong>学习机制</strong>：<ul><li><strong>智能体</strong>：可能采用强化学习、进化算法等方法来在与环境的互动中学习最优策略。</li><li><strong>多模态GPT</strong>：使用大规模预训练进行自监督学习，主要依赖于大量的跨模态数据进行学习。</li></ul></li></ul><h4 id="联系"><strong>联系</strong></h4><ul><li><p><strong>感知能力</strong>：虽然智能体和多模态 GPT的主要目标不同，但两者都涉及感知能力。智能体可以使用多模态感知（如视觉、听觉），而多模态GPT 直接处理多种模态的数据输入。未来的智能体可能会集成多模态 GPT模型，使其在处理复杂多模态数据（如图像、文本）时更加智能。</p></li><li><p><strong>跨模态理解</strong>：多模态 GPT可以为智能体提供更强大的理解和生成能力。例如，一个多模态 GPT模型可以嵌入到智能体中，使其能够通过文本描述生成视觉信息（如在机器人视觉系统中辅助感知）或根据视觉信息生成文本描述（如在自动驾驶中生成自然语言报告）。</p></li><li><p><strong>语言生成</strong>：某些智能体，例如聊天机器人，可以使用多模态GPT 的生成能力来与用户进行自然语言交互，提供图像或文本回答。</p></li></ul><h3 id="总结-1">总结</h3><ul><li><strong>智能体</strong>是一个自主的实体，能够感知环境、决策和执行行动，强调的是行动循环和与环境的持续交互。</li><li><strong>多模态 GPT</strong>是一个生成和理解多模态数据的语言模型，强调的是跨模态数据的处理和生成能力。</li></ul><p>设计一个 AI智能体（Agent）是一个系统化的过程，涉及多个阶段，包括任务定义、感知环境、决策机制、行动执行和学习改进等。以下是详细的步骤来帮助设计一个AI 智能体：</p><ul><li><strong>明确任务与目标</strong>：清楚智能体的作用和目标。</li><li><strong>感知模块</strong>：设计感知环境的方式，获取数据。</li><li><strong>决策模块</strong>：设计如何根据感知的数据做出决策，可以基于规则、规划、机器学习或强化学习。</li><li><strong>行为执行模块</strong>：设计如何执行智能体的决策。</li><li><strong>学习与优化</strong>：引入学习机制，让智能体能够根据经验或新数据不断改进。</li><li><strong>反馈与评估</strong>：持续评估智能体的表现，优化其任务执行效果。</li></ul><h1 id="reference">Reference</h1><p><a href="https://aman.ai/primers/ai/">Distilled AI</a></p>]]></content>
      
      
      <categories>
          
          <category> Foundation Model </category>
          
      </categories>
      
      
        <tags>
            
            <tag> multi-modal </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Machine Learning</title>
      <link href="/2024/10/08/statistic/"/>
      <url>/2024/10/08/statistic/</url>
      
        <content type="html"><![CDATA[<h1 id="一机器学习相关">一、机器学习相关</h1><h2 id="基本概念">1. 基本概念</h2><h3 id="排序">1.1. 排序</h3><h4 id="why-does-sorting-a-collection-have-at-least-on-log-n-complexity-n-is-the-length-of-the-collection.">1.Why does sorting a collection have (at least) <span class="math inline">\(O(n log n)\)</span> complexity? n is the length ofthe collection.</h4><ul><li><span class="math inline">\(O(n log n)\)</span> is the optimal valuefor a comparison sort.[https://theartofmachinery.com/2019/01/05/sorting_is_nlogn.html].</li></ul><h4 id="几种排序算法介绍以及复杂度分析">2.几种排序算法介绍以及复杂度分析。</h4><ul><li><p>排序有内部排序和外部排序，内部排序是数据记录在内存中进行排序，而外部排序是因排序的数据很大，一次不能容纳全部的排序记录，在排序过程中需要访问外存。</p></li><li><p>排序的稳定性是指：若待排序的序列中，存在多个具有相同元素，经过排序，这些元素的相对次序保持不变，则称该算法是稳定的；若经排序后，元素的相对次序发生了改变，则称该算法是不稳定的。</p></li><li><p>常见的八种排序方法都属于内部排序，交换排序（冒泡排序、快速排序）、选择排序（简单选择排序、堆排序）、插入排序（直接插入排序、希尔排序）、归并排序、基数排序。<img src="/2024/10/08/statistic/sort.jpg"></p></li><li><p>冒泡排序：两两比较待排序数据元素的大小，如果两个数据元素的次序相反时则进行交换，直到没有反序的数据元素未知。<strong>平均时间复杂度为<span class="math inline">\(O(n^2)\)</span>，空间复杂度为 <span class="math inline">\(O(1)\)</span>，属于稳定排序。</strong></p></li><li><p>快速排序：在当前无序区任取一个元素作为待比较的基准，用此基准将当前无序区划分为左右两个较小的无序区，其中左边无序子区的数据元素均小于等于基准元素，右边无序子区中的元素均大于等于基准元素，而基准则始终位于最终排序位置上。依次再对左右两个无序子区进行上述划分过程，直到所有无序子区中的元素都已排序为止。<strong>平均时间复杂度为<span class="math inline">\(O(n \log_2 n)\)</span>，空间复杂度为 <span class="math inline">\(O(\log_2 n) - O(n)\)</span>（可能退化为冒泡排序），属于不稳定算法。</strong></p></li><li><p>简单选择排序：每一趟从待排序的数据元素中选出最小（或最大）的一个元素，顺序放在已排好序的数列最后，直到全部待排序的数据元素排完。<strong>平均时间复杂度为<span class="math inline">\(O(n^2)\)</span>，空间复杂度为<span class="math inline">\(O(1)\)</span>，属于稳定排序。</strong></p></li><li><p>堆排序：将数组看成是一棵完全二叉树的顺序存储结构，利用完全二叉树中的双亲结点和孩子结点之间的内在关系来选择最小的元素。小根堆：树中任一非叶子结点的值均小于等于其孩子结点的值（堆顶最小）；大根堆：树中任一非叶子结点的值均大于等于其孩子结点的值（堆顶最大）。<strong>平均时间复杂度为<span class="math inline">\(O(n \log_2 n)\)</span>，空间复杂度为 <span class="math inline">\(O(1)\)</span>，属于不稳定排序。</strong></p></li><li><p>直接插入排序：每次将一个待排序的数据元素，插入到前面已经排好序的数列中的适当位置，使数列依然有序；直到待排序数据元素全部插入完为止。<strong>平均时间复杂度为<span class="math inline">\(O(n^2)\)</span>，空间复杂度为 <span class="math inline">\(O(1)\)</span>，属于稳定排序。</strong></p></li><li><p>希尔排序：先取一个小于 n 的整数 <span class="math inline">\(d_1\)</span>作为第一个增量，把所有元素分为若干组，所有下标距离为 <span class="math inline">\(d_1\)</span>的倍数的元素放在同一组中。先在各组内进行直接插入排序。然后取第二个增量<span class="math inline">\(d_2 &lt;d_1\)</span>，按照原始位置下标，重复上述分组和排序，直到所取的增量 <span class="math inline">\(d_t =1\)</span>，即所有元素都放在同一组中进行直接插入排序为止。<strong>平均时间复杂度为<span class="math inline">\(O(n^{1.3})\)</span>，空间复杂度为O(1)，不稳排序。</strong></p></li><li><p>归并排序：归并排序最核心的部分是合并（merge）过程，将两个有序的数组a 和 b 合并为一个有序数组 c。从左往右枚举 a[i] 和b[j]，找出最小的值并放入数组 c[k]；重复上述过程直到 a 和 b有一个为空时，将另一个数组剩下的元素放入c。为保证排序的稳定性，前段首元素小于或等于后段首元素时（a[i] &lt;=b[j]）而非小于时（a[i] &lt; b[j]）就要作为最小值放入c[k]。<strong>平均时间复杂度为 <span class="math inline">\(O(n \logn)\)</span>，空间复杂度为 <span class="math inline">\(O(1)\)</span>，稳定排序。</strong></p></li><li><p>基数排序：将待排序的元素拆分为 <span class="math inline">\(k\)</span> 个关键字，先对第 <span class="math inline">\(1\)</span>关键字进行稳定排序，然后对于每组具有相同关键字的元素 再对第 <span class="math inline">\(2\)</span>关键字进行稳定排序（递归执行）。最后对于每组 具有相同关键字的元素 再对第<span class="math inline">\(k\)</span>关键字进行稳定排序。如果对自然数进行比较，将自然数按个位对齐后往高位补齐<span class="math inline">\(0\)</span>，则一个数字从左往右数第 <span class="math inline">\(i\)</span> 位数就可以作为第 <span class="math inline">\(i\)</span> 关键字。<strong>设数据量为<span class="math inline">\(n\)</span>, 数据为<span class="math inline">\(d\)</span>进制, 最大位数为 <span class="math inline">\(k\)</span>, 则对某一位执行计数排序使用<span class="math inline">\(O(n+d)\)</span>时间，排序所有 <span class="math inline">\(k\)</span> 位使用<span class="math inline">\(O((n+ d) \times k)\)</span> 时间。空间复杂度为 <span class="math inline">\(O(n + d)\)</span>,非原地排序。如果对内层关键字的排序是稳定的，则基数排序是稳定的排序算法。</strong></p></li></ul><h3 id="二分查找">1.2. 二分查找</h3><p>以在一个升序数组中查找一个数为例。它每次考察数组当前部分的中间元素，如果中间元素刚好是要找的，就结束搜索过程；如果中间元素小于所查找的值，那么左侧的只会更小，不会有所查找的元素，只需到右侧查找；如果中间元素大于所查找的值同理，只需到左侧查找。</p><p>二分查找的最优时间复杂度为 <span class="math inline">\(O(1)\)</span>。二分查找的平均时间复杂度和最坏时间复杂度均为<span class="math inline">\(O(\logn)\)</span>。因为在二分搜索过程中，算法每次都把查询的区间减半，所以对于一个长度为<span class="math inline">\(n\)</span> 的数组，至多会进行 <span class="math inline">\(O(\log n)\)</span> 次查找。</p><p>迭代版本的二分查找的空间复杂度为 <span class="math inline">\(O(1)\)</span>。</p><h3 id="回归模型和分类模型常用损失函数有哪些">1.3.回归模型和分类模型常用损失函数有哪些？</h3><h4 id="回归模型常用的损失函数">回归模型常用的损失函数</h4><ol type="1"><li><p>0-1损失函数： <span class="math display">\[L(\hat{y},y) =\begin{cases}  1,&amp;y \neq \hat{y} \\0,&amp;y = \hat{y}\end{cases}  \]</span></p></li><li><p>平均绝对误差MAE：异常点多的情况下鲁棒性更强，不像MSE那样过度惩罚大误差；但不方便求导<span class="math display">\[L(\hat{y}, y) = \frac{1}{n} \sum_{i=1}^{n}|y_i - \hat{y}_i|.\]</span></p></li><li><p>均方误差MSE：求导方便，能够用梯度下降法优化；对异常值敏感，异常值的存在会导致MSE值急剧增大，从而影响模型效果。<span class="math display">\[L(\hat{y},y) = \frac{1}{n} \sum_{i=1}^{n}(y_i - \hat{y}_i)^2.\]</span></p></li><li><p>对数损失函数/对数似然损失函数： <span class="math display">\[L(P(Y|X),Y) = -{\rm log} P(Y|X)\]</span></p></li><li><p>Huber损失函数：结合了MAE和MSE的优点，对异常值更加鲁棒，比MSE对大的误差的惩罚更加温和；在误差较小时仍然能够提供与MSE类似的梯度更新；缺点是需要调整超参数<span class="math inline">\(\delta\)</span> <span class="math display">\[L_{Huber}(\hat{y}, y) =\begin{cases}(\hat{y}-y)^2&amp;|\hat{y}-y| \leq \delta\\2 \delta |\hat{y}-y| - \delta^2&amp;|\hat{y}-y| &gt; \delta\end{cases}\]</span></p></li><li><p>对数余弦 Log-Cosh损失函数：近似于MSE，但对大误差的惩罚更温和，兼顾了MSE和MAE的优点。同时二阶处处可微（牛顿法要求二阶可微），更加平滑且容易优化。但是，不如MSE那样简单直观，在某些应用中表现不如MSE。<span class="math display">\[L(\hat{y},y) = \log \cosh(\hat{y}-y)\]</span></p></li></ol><h4 id="分类模型中常用的损失函数">分类模型中常用的损失函数</h4><ol type="1"><li>交叉熵损失。对于二元分类，公式为如下。其中，<span class="math inline">\(y\)</span>是真实标签，<span class="math inline">\(p\)</span>是模型预测的概率。 <span class="math display">\[  L(p,y) = -[y \log p + (1-y) \log (1-p)].  \]</span></li></ol><ul><li>优点：直接衡量模型预测的概率与真实分类标签之间的差异，适合分类任务。对概率差异敏感，能够较好地区分高概率和低概率的预测。</li><li>缺点：当预测的概率非常接近0或1时，交叉熵的梯度可能变得极端，导致训练不稳定。对于不平衡数据，模型倾向于偏向多数类，需要配合其他技术如加权损失函数或欠采样来处理。</li></ul><ol start="2" type="1"><li>Hinge loss。通常用于支持向量机，公式如下。<span class="math inline">\(y \in {-1,1}\)</span>是真实标签，<span class="math inline">\(f(x)\)</span>是模型输出。 <span class="math display">\[  L(f(x),y) = \max(0, 1 - y \times f(x))  \]</span></li></ol><ul><li>优点：强调分类边界的最大化，适用于SVM；对小的误差不敏感，能够防止过拟合。</li><li>缺点：不适用于概率输出的分类模型。仅适用于二分类问题，多分类任务中扩展性较差。</li></ul><ol start="3" type="1"><li>Kullback-Leibler 散度。假设<span class="math inline">\(p(x)\)</span>是真实分布，<span class="math inline">\(q\)</span>是模型预测的概率分布，公式如下： <span class="math display">\[  D_{\text{KL}}(p||q) = \sum p(x) \log(\frac{p(x)}{q(x)}),  \]</span></li></ol><ul><li>优点：可以量化两个分布之间的差异，适合概率模型；</li><li>缺点：对极端概率值（接近于1或0）的预测非常敏感，可能导致数值不稳定。</li></ul><h3 id="什么是结构误差和经验误差">1.4. 什么是结构误差和经验误差？</h3><p>经验风险（经验损失）：模型 <span class="math inline">\(f(X)\)</span>关于训练数据集的平均损失 <span class="math display">\[R_{\rm emp}(f) = \frac{1}{N} \sum_{i=1}^N L(y_i,f(x_i)).\]</span></p><p>结构风险：是在经验风险上加上表示模型复杂度的正则化项 <span class="math display">\[R_{\rm srm}(f) = \frac{1}{N} \sum_{i=1}^{N}L(y_i,f(x_i))+\lambda J(f).\]</span></p><p>经验风险最小化的策略认为，经验风险最小的模型是最优的模型。</p><p>结构风险最小化是为了防止过拟合而提出的，结构风险最小化等价于正则化。结构风险最小化的策略认为结构风险最小的模型是最优的模型。</p><h3 id="如何选择合适的模型评估指标rocauc精准度召回率f1值">1.5.如何选择合适的模型评估指标？ROC、AUC、精准度、召回率、F1值</h3><p>模型评估指标用于衡量机器学习模型在测试集或验证集上的表现，帮助评估其性能。</p><p>混淆矩阵，又称误差矩阵，就是分别统计分类模型归错类，归对类的观测值个数，然后把结果放在一个表里展示出来。这个表就是混淆矩阵。</p><p>混淆矩阵是ROC曲线绘制的基础，同时它也是衡量分类型模型准确度中最基本，最直观，计算最简单的方法。</p><table><thead><tr><th style="text-align: center;">混淆矩阵</th><th style="text-align: center;">预测结果</th><th style="text-align: center;">预测结果</th></tr></thead><tbody><tr><td style="text-align: center;">真实情况</td><td style="text-align: center;">反例</td><td style="text-align: center;">正例</td></tr><tr><td style="text-align: center;">反例</td><td style="text-align: center;">TN（真反例）</td><td style="text-align: center;">FP（假正例）</td></tr><tr><td style="text-align: center;">正例</td><td style="text-align: center;">FN（假反例）</td><td style="text-align: center;">TP（真正例）</td></tr></tbody></table><ul><li>TN：True Negative，将样本中的负类样本预测为负类的数量</li><li>FP：False Positive，将样本中的负类样本预测为正类的数量</li><li>FN：False Negative，将样本中的正类样本预测为负类的数量</li><li>TP：True Positive，将样本中的正类样本预测为正类的数量</li></ul><h4 id="分类任务指标">分类任务指标</h4><h4 id="accuracy准确率分类正确的样本占总样本个数的比例">Accuracy（准确率）：分类正确的样本占总样本个数的比例</h4><p><span class="math display">\[\text{Accuracy} = \frac{n_{correct}}{n_{total}}\]</span> -缺点：不同类别的样本比例非常不均衡时，占比大的类别往往成为影响准确率的最主要因素。比如，当负样本占99%时，分类器把所有样本都预测为负样本也可以获得99%的准确率。-解决：可以使用每个类别下的样本准确率的算术平均（平均准确率）作为模型评估的指标。</p><p>在数据集不平衡的情况下（阴性数据多，阳性数据少），精确度和召回率是合适的性能指标。精确度和召回率都只关注阳性类（少数类），而不关心真正的阴性类（多数类）。简单地说，在数据类别不平衡的情况下，即有大量反类和少量正类时，精确度和召回率是首选指标。换句话说，精确度和召回率可以评估分类器在少数类别上的性能。</p><h4 id="precision精确率分类正确的正样本个数占分类器判定为正样本的样本个数的比例">Precision（精确率）：分类正确的正样本个数占分类器判定为正样本的样本个数的比例</h4><p>取值范围：0 到 1，其中 1 表示完全精确（没有误报），0表示没有正确的正预测。 <span class="math display">\[\text{Precision} = \frac{TP}{TP+FP}\]</span></p><h4 id="recall召回率-又称灵敏度-sensitivity分类正确的正样本数占真正的正样本个数的比例">Recall（召回率，又称灵敏度sensitivity）：分类正确的正样本数占真正的正样本个数的比例</h4><p>取值范围：0 至 1，其中 1 表示完全召回（无假阴性），0表示未识别出真阳性。 <span class="math display">\[\text{Recall} = \frac{TP}{TP+FN}\]</span></p><h4 id="f1-scoreprecision和recall的调和平均值当精确率和召回率都高时f1值也会高特别适用于类别不平衡的数据集">F1-score：precision和recall的调和平均值；当精确率和召回率都高时，F1值也会高；特别适用于类别不平衡的数据集</h4><p><span class="math display">\[F1 = \frac{2 \times \text{Precision} \times \text{Recall}}{\text{Precision} + \text{Recall} }\]</span> 取值范围：0 到 1，其中 1 表示精确度和召回率之间的最佳平衡，0表示精确度、召回率或两者均为 0。</p><p>如果我们认为精确率或召回率中的某一个比另一个更重要，那么可以使用<span class="math inline">\(F_{\beta}\)</span>分数，这是精确率和召回率的加权调和平均数。这个分数特别适用于一些特定的应用场景，例如在医学检测中，假阴性可能比假阳性代价更高的情况。<span class="math inline">\(F_{\beta}\)</span>能够根据实际需求调整精确率和召回率的相对权重。</p><p><span class="math inline">\(F_{\beta}\)</span> 分数公式： <span class="math display">\[F_{\beta} = (1 + \beta^2) \cdot \frac{ {\text{Precision} \cdot\text{Recall}}}{ {\beta^2 \cdot \text{Precision} + \text{Recall}} }.\]</span> - <span class="math inline">\(\beta\)</span>:用于控制精确率和召回率之间的权衡。 - 如果 <span class="math inline">\(\beta&gt;1\)</span>，则更重视召回率；适用于假阴性代价较高的场景。- 如果 <span class="math inline">\(\beta&lt;1\)</span>，则更重视精确率；适用于假阳性代价较高的场景。- 当 <span class="math inline">\(\beta = 1\)</span>时，<span class="math inline">\(F_{\beta}\)</span> 分数即为常见的 F1分数，它将精确率和召回率同等看待。</p><ul><li>在医学检测中，<strong>假阴性</strong>可能意味着病人未能及时得到治疗，因此我们可能更关注召回率，选择较高的<span class="math inline">\(\beta\)</span> 值以降低漏诊的概率。</li><li>在反垃圾邮件系统中，<strong>假阳性</strong>（将正常邮件误判为垃圾邮件）可能带来更大影响，此时可以使用较小的<span class="math inline">\(\beta\)</span> 值，更加注重精确率。</li></ul><h4 id="pr指标应用场景">PR指标应用场景</h4><ul><li>一个完美的分类器的精确度和召回率都等于 1。</li><li>精确度和召回率应一并报告，不应单独报告。因为很容易通过改变模型的召回率（灵敏度）来提高精确度，反之亦然。</li></ul><p>在正类（也称为少数类）稀少的情况下，PR能够处理类不平衡问题。但是，如果数据集不平衡，负类是罕见的一类，那么 PR曲线就不是最佳曲线，可能会产生误导。在这种情况下，ROC曲线可能更合适。</p><p>具体应用场景：</p><ul><li>当两个类别同样重要时：如果模型的目标是在两个类别上都有同样好的表现，那么PR是可使用的指标。猫和狗的图像分类就是一个很好的例子，因为在猫上的表现与在狗上的表现同样重要。</li><li>当少数类别更重要时：如果模型的重点是正确识别出尽可能多的正面样本，那么PR就是要使用的指标。以垃圾邮件检测器为例，其目标是找出所有可能的垃圾邮件。普通邮件根本不值得关注--它们会影响阳性样本的数量。</li></ul><h4 id="p-r-曲线">P-R 曲线</h4><p>在排序问题中，通常没有一个确定的阈值把得到的结果直接判定为正样本或负样本，而是采用TopN返回结果的Precision和Recall值来衡量排序模型的性能。即认为模型返回的TopN结果就是模型判定的正样本，计算前N个位置的Precision@N和Recall<span class="citation" data-cites="N">@N</span>。为了综合评估一个排序模型的好坏，不仅要看模型在不同TopN下的Precision@N和Recall<span class="citation" data-cites="N">@N</span>，而且最好画出模型的P-R曲线。P-R曲线的横轴是Recall，纵轴是Precision。</p><p>当数据集的类别不平衡时，精度和召回率是比准确率更好的指标。同样，对于不平衡的类别，精度-召回曲线比ROC 曲线更合适。精确度-召回率曲线是不同阈值下精确度（y 轴）和召回率（x轴）的曲线图，与 ROC曲线类似。需要注意的是，在计算精确度和召回率时，绝对不能使用真实负值，这些指标只考虑正确预测。</p><p><strong>Area Under the PR Curve (AUPRC)</strong>：AUPRC将一系列阈值的曲线汇总为一个分数。该分数可作为二元分类问题中不同模型之间的比较点，其中1.0 分代表最完美的分类器。</p><p><strong>ROC曲线适用于每个类别之间的样本平衡的情况，而精度-召回曲线则适用于不平衡的数据集。</strong></p><h4 id="特异性specificity">特异性（specificity）</h4><p><strong>Specificity</strong>（特异性）用于衡量模型在识别负类时的准确性。特异性反映了模型避免将真实的负类样本误判为正类样本的能力，通俗地说就是“没有病的人被正确地诊断为没有病的比例”。<span class="math display">\[\text{Specificity} = \frac{\text{TN}}{\text{TN} + \text{FP}}\]</span></p><ul><li>如果特异性高，说明模型能够很好地避免误报（将实际的负类错判为正类）。</li><li>特异性常与敏感性(sensitivity)一起使用，后者用于衡量模型识别正类（“有病”的人）能力的表现。</li></ul><p>Image credits to <a href="https://medium.com/swlh/how-to-remember-all-these-classification-concepts-forever-761c065be33">source</a></p><p><img src="/2024/10/08/statistic/pr_ss.jpeg"></p><h4 id="roc-曲线">ROC 曲线</h4><p>横坐标为假阳性率（False Positive Rate，FPR）；纵坐标为真阳性率（TruePositive Rate，TPR） <span class="math display">\[FPR = \frac{FP}{N}, \quadTPR = \frac{TP}{P},\]</span>其中，P是真实的正样本的数量，N是真实的负样本的数量，TP是P个正样本中被分类器预测为正样本的个数，FP是N个负样本中被预测为正样本的个数。</p><p><strong>如何绘制ROC曲线</strong></p><p>通过不断移动分类器的“截断点”来生成曲线上的一组关键点。在二分类问题中，模型输出一般是预测样本为正例的概率，在输出最终的正例负例之前，我们需要制定一个阈值。大于该阈值的样本判定为正例，小于该阈值的样本判定为负例。通过动态调整截断点，绘制每个截断点对应位置，再连接所有点得到最终的ROC曲线。比如，阈值为0时，此时所有样本被预测为负例，TPR = 0, FPR =0；当阈值增加为1时，此时所有样本被预测为正例，TPR = 1, FPR = 1.</p><p><strong>一般情况下，PR曲线易受样本数量的影响，样本数量不均衡情况下PR曲线会有明显变化，故一般使用ROC曲线。</strong></p><p><strong>AUC</strong>：ROC曲线下的面积大小。计算AUC值只要沿着ROC横轴做积分就可以。AUC取值在0.0~1之间。AUC越大，分类性能越好。AUC表示预测的正例排在负例前面的概率。</p><p>指标想表达的含义，简单来说其实就是随机抽出一对样本（一个正样本，一个负样本），然后用训练得到的分类器来对这两个样本进行预测，预测得到正样本的概率大于负样本概率的概率。AUC为0.5表明对正例和负例没有区分能力，对于不论真实类别是1还是0，分类器预测为1的概率是相等的。</p><p>我们希望分类器达到的效果：对于真实类别为1的样本，分类器预测为1的概率（TPR）要大于真实类别为0而预测类别为1的概率（FPR），即y&gt;x</p><p>AUC的计算方法同时考虑了分类器对于正例和负例的分类能力，在样本不平衡的情况下，依然能够对分类器作出合理的评价。</p><h4 id="回归任务指标">回归任务指标</h4><h4 id="均方根误差rmse计算预测值和实际值的平均误差">均方根误差RMSE：计算预测值和实际值的平均误差</h4><p><span class="math display">\[{\rm RMSE} = \sqrt{\frac{\sum_{i=1}^n (y_i-\hat{y}_i)^2}{n}}\]</span></p><h4 id="均方误差mse">均方误差MSE</h4><h4 id="平均绝对误差mae">平均绝对误差MAE</h4><h4 id="决定系数-r2">决定系数 <span class="math inline">\(R^2\)</span></h4><p>表示模型解释变量总方差的比例，反映了模型拟合的好坏。</p><p>SST: sum of squares total，总的偏差平方和，表示变量<span class="math inline">\(y\)</span>相对于中心<span class="math inline">\(\bar{y}\)</span>的异动。 <span class="math display">\[SST = \sum_{i=1}^n (y_i - \bar{y}_i)^2.\]</span></p><p>SSR: sum of squares regression, 回归平方和，表示估计值 <span class="math inline">\(\hat{y}\)</span>相对于中心 <span class="math inline">\(\bar{y}\)</span>的异动。</p><p><span class="math display">\[SSR = \sum_{i=1}^{n} (\hat{y}_i - \bar{y}_i)^2.\]</span></p><p>SSE: sum of squares error,残差平方和，表示拟合数据和原始数据之间的误差的平方和。 <span class="math display">\[SSE = \sum_{i=1}^n(y_i - \hat{y}_i)^2.\]</span></p><p><span class="math display">\[R^2 = 1 - \frac{\sum_{i=1}^n(y_i - \hat{y}_i)^2}{\sum_{i=1}^n(y_i -\bar{y})^2} = 1 - \frac{SSE}{SSR}.\]</span></p><p>决定系数<span class="math inline">\(R^2\)</span>的取值范围为<span class="math inline">\([0,1]\)</span>，0表示没有线性关系，1表示拟合模型可以解释所有变异y。</p><h4 id="调整的决定系数-barr2">调整的决定系数 <span class="math inline">\(\bar{R}^2\)</span></h4><p>对于决定系数 <span class="math inline">\(R^2\)</span>，当解释变量个数增加（即模型复杂度提高，偏差降低）时，<span class="math inline">\(R^2\)</span>会不断增加。但是，随着模型复杂度的提高，方差可能会越来越大。因此，引入了调整的决定系数<span class="math inline">\(\bar{R}^2\)</span>： <span class="math display">\[\bar{R}^2 = 1 - \frac{SSE/df_{err}}{SSR/df_{tot}} = 1 - （1 - R^2)\times \frac{n-1}{n-p-1},\]</span> 其中，df_{err} 表示残差平方和的自由度，为<span class="math inline">\(n-p-1\)</span>，df_{tot}表示关于总平方和的自由度，为 <span class="math inline">\(n-1\)</span>。</p><p>调整后的 <span class="math inline">\(\bar{R}^2\)</span>可以是负值，其值总是小于或等于 <span class="math inline">\({R}^2\)</span>。当引入更多解释变量时，决定系数<span class="math inline">\({R}^2\)</span>会增加，导致<span class="math inline">\(\bar{R}^2\)</span>的增加。但是后面的分数项会降低<span class="math inline">\(\bar{R}^2\)</span>。只有当减少的偏差大于引入的方差时，<span class="math inline">\(\bar{R}^2\)</span>才会增加。因此，<span class="math inline">\(\bar{R}^2\)</span> 可以看作是对bias-variance间的tradeoff.</p><h4 id="平均绝对百分比误差mean-absolute-percentage-error-mape">平均绝对百分比误差：（MeanAbsolute Percentage Error, MAPE）</h4><p>MAPE表示预测误差相对于真实值的百分比。 <span class="math display">\[MAPE = \frac{1}{n}\sum_{1}^{n} |\frac{y_i - \hat{y}_i}{y_i}| \times100\%.\]</span>优点：易于解释，特别适用于需要对误差进行相对度量的场景。缺点：当真实值接近0时，MAPE会变得不稳定。</p><h3 id="bias-variance-trade-off模型过拟合欠拟合">1.6. Bias-variancetrade-off，模型过拟合、欠拟合</h3><p><img src="/2024/10/08/statistic/bias_variance.jpg"></p><p><strong>误差分析</strong>：通过训练误差和测试误差来分析模型是否存在高方差、高偏差。</p><ul><li>如果训练误差较高：说明模型的偏差较大，模型出现了欠拟合。</li><li>如果训练误差较低，而测试误差较高：说明模型的方差较大，出现了过拟合。</li><li>如果训练误差较低，测试误差也较低：说明模型的方差和偏差都适中，是一个比较理想的模型。</li><li>如果训练误差较高，且测试误差更高：说明模型的方差和偏差都较大。</li></ul><p>上述分析的前提是：训练集、测试集的数据来自于同一个分布，且最优误差较小。否则讨论更复杂。</p><p><strong>欠拟合</strong>：模型过于简单，没有很好地学习到数据间的关系，训练集效果差。<strong>模型复杂度低，此时模型预测的方差较小，表示预测较稳定。但是模型预测的偏差会较大，表示预测不准确。。</strong></p><p><strong>过拟合</strong>：指学习时选择的模型所包含的参数过多，以至出现这一模型对已知数据预测得很好，但对未知数据预测得很差的现象。训练集效果好，测试集效果差。<strong>模型复杂度高，此时模型预测的方差大，偏差小。</strong></p><h4 id="欠拟合解决方法">欠拟合解决方法</h4><ol type="1"><li>增加特征</li><li>提高模型复杂度：神经网络提高神经元数、增加层数；SVM使用核函数；</li><li>减小正则项的系数</li></ol><h4 id="过拟合解决方法">过拟合解决方法</h4><ol type="1"><li>提高样本数量。神经网络：Data Augmentation（数据增强）</li><li>简化模型。神经网络使用 Dropout、EarlyStopping；决策树剪枝、限制树的深度。</li><li>加入正则化项（L1或L2）或提高惩罚系数</li><li>使用集成学习</li><li>神经网络中使用dropout机制</li><li>early stopping</li></ol><h3 id="奥卡姆剃刀定律是什么对机器学习模型优化有何启发">1.7.奥卡姆剃刀定律是什么？对机器学习模型优化有何启发？</h3><p>奥卡姆剃刀定律：若有多个假设与观察一致，则选最简单的那个。</p><p>奥卡姆剃刀原理应用于模型选择时变为以下想法：在所有可能选择的模型中，能够很好地解释已知数据并且十分简单才是最好的，也就是应该选择的模型。</p><p>从贝叶斯估计的角度来看，正则化项对应于模型的先验概率。可以假设复杂的模型有较小的先验概率，简单的模型有较大的先验概率。</p><h3 id="线性模型-vs.-非线性模型-生成式模型-vs.-判别式模型-概率模型-vs.-非概率模型-参数化模型-vs.-非参数化模型">1.8.线性模型 vs. 非线性模型， 生成式模型 vs. 判别式模型， 概率模型 vs.非概率模型， 参数化模型 vs. 非参数化模型</h3><p><strong>线性模型 vs. 非线性模型</strong></p><p>非概率模型可以分为线性模型和非线性模型。如果函数 <span class="math inline">\(y=f(x)\)</span> 或 <span class="math inline">\(z =g(x)\)</span>是线性函数，则称模型是线性模型，否则成模型为非线性模型。</p><p>线性模型：感知机、线性支持向量机、k近邻、k均值、潜在语义分析</p><p>非线性模型：核函数支持向量机、AdaBoost，神经网络</p><p><strong>生成式模型 vs. 判别式模型</strong></p><p>监督学习方法分为生成方法（generativeapproach）和判别方法（discriminativeapproach）。所学习到的模型分别称为生成模型和判别模型。监督学习的模型一般形式为决策函数：<span class="math inline">\(Y = f(X)\)</span> 或者条件概率分布 <span class="math inline">\(P(Y|X)\)</span>。</p><p>生成方法：由数据学习联合概率分布 <span class="math inline">\(P(X,Y)\)</span>，然后求出条件概率分布 <span class="math inline">\(P(Y|X)\)</span>作为预测模型： <span class="math display">\[P(Y|X) = \frac{P(X,Y)}{P(X)}\]</span> 之所以叫做生成方法，是因为模型表示了给定输入 <span class="math inline">\(X\)</span> 产生输出 <span class="math inline">\(Y\)</span>的生成关系。典型的生成模型：朴素贝叶斯法、隐马尔可夫模型。</p><p>判别方法：由数据直接学习决策函数 <span class="math inline">\(f(X)\)</span> 或者条件概率分布 <span class="math inline">\(P(X,Y)\)</span>作为预测的模型，关心的是对给定的输入<span class="math inline">\(X\)</span>，应该预测什么样的输出 <span class="math inline">\(Y\)</span>。典型的判别模型：k近邻、感知机、决策树、逻辑斯蒂回归、最大熵模型、支持向量机、提升方法、条件随机场。</p><p><strong>概率模型 vs. 非概率模型</strong></p><p>概率模型与非概率模型的区别在于模型的内在结构。<strong>概率模型一定可以表示为联合概率分布的形式</strong>，其中的变量表示输入、输出、因变量甚至参数。而针对非概率模型则不一定存在这样的联合概率分布。</p><p>统计学习的模型可以分为概率模型（probabilisticmodel）和非概率模型（non-probabilisticmodel）或者确定性模型（deterministicmodel）。在监督学习中，概率模型取条件概率分布形式 <span class="math inline">\(p(y|x)\)</span>，非概率模型取函数形式 <span class="math inline">\(y=f(x)\)</span>，其中<span class="math inline">\(x\)</span>是输入，<span class="math inline">\(y\)</span>是输出。在无监督学习中，概率模型取条件概率分布形式<span class="math inline">\(p(z|x)\)</span>或 <span class="math inline">\(p(x|z)\)</span>，其中<span class="math inline">\(x\)</span>是输入，<span class="math inline">\(z\)</span>是输出。在监督学习中，概率模型是生成模型，非概率模型是判别模型。</p><p>概率模型：决策树、朴素贝叶斯、隐马尔可夫模型、条件随机场、概率潜在语义分析、潜在狄利克雷分布、高斯混合模型</p><p>非概率模型：感知机、支持向量机、k近邻、AdaBoost、k均值、潜在语义分析、神经网络</p><p>逻辑斯蒂回归即可看做概率模型，又可看做非概率模型。</p><p><strong>参数化模型 vs. 非参数化模型</strong></p><p>统计学习模型又可以分为参数化模型（parametricmodel）和非参数化模型（non-parametricmodel）。参数化模型假设模型参数的维度固定，模型可以由有限维参数完全刻画；非参数模型假设模型参数的维度不固定或者说无穷大，随着训练数据量的增加而不断增大。</p><p>参数化模型：感知机、朴素贝叶斯、逻辑斯蒂回归、k均值、高斯混合模型</p><p>非参数化模型：决策树、支持向量机、AdaBoost、k近邻、潜在语义分析、概率潜在语义分析、潜在狄利克雷分配</p><h3 id="缺失值如何处理">1.9. 缺失值如何处理？</h3><p><strong>1.</strong>缺失值较多.直接将该特征舍弃掉，否则可能反倒会带入较大的噪声，对结果造成不良影响。</p><p><strong>2.</strong>缺失值较少,其余的特征缺失值都在10%以内，我们可以采取很多的方式来处理:（1）把NaN直接作为一个特征，假设用0表示；（离散特征取值k维扩充到k+1维）（2）用均值填充；（连续特征-均值，离散特征-特征取值的众数）（3）用随机森林等算法预测填充。</p><h3 id="标准化归一化介绍">1.10. 标准化、归一化介绍</h3><p>为了消除数据特征之间的量纲影响，我们需要对特征进行归一化/标准化处理，使得不同指标之间具有可比性。以梯度下降过程为例，如果不做归一化/标准化处理，在学习速率相同的情况下，大量纲变量的更新速度会大于小量纲，需要较多迭代才能找到最优解。如果将其变换到相同的数值区间后，更新速度变得更为一致，容易更快地通过梯度下降找到最优解。</p><p><strong>1. 归一化（Min-MaxScaling）</strong>：将数据缩放到一个特定的范围（通常是 [0, 1] 或 [-1,1]）。它的核心思想是通过线性变换，将数据映射到指定区间中。 <span class="math display">\[  X_{norm} = \frac{X-X_{min}}{X_{max}-X_{min}}\]</span>用于输入范围已知的模型（如神经网络），或者需要对距离敏感的算法（如KNN）。不适用于有异常值的数据，例如有异常大的数值，其他数据会被压缩到很小的数值。</p><p><strong>2. 标准化（Z-scoreNormalization）</strong>：是将数据进行中心化和缩放处理，使得数据的均值为0，标准差为1。这是通过减去数据的均值再除以数据的标准差来实现的，也叫Z-score标准化。假设原始特征的均值为 <span class="math inline">\(\mu\)</span>，方差为 <span class="math inline">\(\sigma\)</span> ，那么标准化公式定义为 <span class="math display">\[  z = \frac{x-\mu}{\sigma}.  \]</span>适用于数据服从高斯分布（正态分布）或在模型中需要假设数据是标准正态分布的情况，尤其在一些线性模型（如线性回归、逻辑回归、支持向量机）和PCA等算法中较为常用。</p><p><strong>3.对比分析</strong>：归一化可以保持原数据的形状和分布，仅改变其取值范围，因此不会改变数据的分布类型（例如正态分布仍是正态分布）。标准化会改变了数据的中心和尺度，将数据转化为标准正态分布，因此数据的分布会被改变。</p><h3 id="l1和l2正则为什么l1比l2更容易产生稀疏解">1.11.L1和L2正则，为什么L1比L2更容易产生稀疏解?</h3><p>L1和L2正则，都可以防止过拟合，增强模型的泛化能力；区别在于L1使参数更稀疏，达到特征选取的作用；L2使参数更接近于0.</p><p><img src="/2024/10/08/statistic/l1_l2.jpeg"></p><p><strong>从解空间的形状来看：</strong>L1正则项约束后的解空间是多边形，而L2正则项约束后的解空间是圆形。而多边形的解空间更容易在尖角处与等高线碰撞出稀疏解。图中红色等高线表示不同正则参数下的残差平方和，椭圆中心点为最小二乘估计。绿色区域分别表示使用<span class="math inline">\(l_1\)</span> 正则函数和<span class="math inline">\(l_2\)</span>正则函数对应的约束域。等高线与约束域的切点表示目标函数的最优解。从图中可以看出，当使用<span class="math inline">\(l_1\)</span>正则函数时，最优解有可能为稀疏解。</p><p><strong>从函数叠加的观点：</strong>L2正则化使得权重衰减，降低模型复杂度，避免模型过拟合问题。（1）通过限制权重的大小，L2正则化可以让模型更为“平滑”，即更关注输入特征的整体趋势而不是单个特征的微小变化。这降低了模型过度拟合训练数据中的噪声。（2）权重较小的模型通常更具泛化性，因为它们对数据中偶然的扰动（噪声）不太敏感。权重减小后，模型在验证集或测试集上也会有更好的表现。</p><h2 id="经典机器学习算法">2. 经典机器学习算法</h2><h3 id="线性回归和逻辑回归">2.1 线性回归和逻辑回归</h3><h4 id="线性回归">线性回归</h4><p>线性回归的五个基本假设条件： 1.自变量（解释变量）与因变量（响应变量）之间为线性关系。 2.自变量之间相互独立，无多重共线性。如果存在多重共线性，就很难确定每个预测因子的单独影响。可以通过计算皮尔逊相关系数，或者方差膨胀因子系数（VIF)进行检验。3.误差项之间相互独立，不存在自相关性。尤其是对时间序列数据尤为重要。可以使用DW检验。如果存在自相关性，可以采取自回归模型。4.误差项与自变量之间相互独立，无内生性。这一假设可确保自变量真正独立于误差项，且不会产生遗漏变量偏差。可以使用工具变量法等进行检验和处理。5.误差项应该呈正态分布，期望为0，方差为定值。这两个假设是为了保证回归模型在小样本下能够顺利进行假设检验，在进行假设检验（如计算p值或置信区间）时尤为重要。可以使用Q-Q图可视化来检验是否满足正态分布，Q-Q图趋近于落在一条直线上，说明残差满足正态分布。如果误差项的方差不是恒值（可以通过可视化残差观察到），那么存在异方差性，可以使用加权回归、稳健回归等方法解决。</p><p><strong>关于内生变量和外生变量</strong>：与干扰项（误差项）相关的变量称为内生变量(endogenousvariable)；与干扰项不相关的变量称为外生变量(exogenousvariable)。对于线性回归模型，自变量会对因变量产生影响，干扰项也会对因变量产生影响，且干扰项与自变量假设无关。那么此时，自变量就是外生变量，因变量是内生变量。但是有时，可能由于某种原因，干扰项也会对因变量产生一定影响，此时干扰项和因变量相关，出现内生性。主要原因有遗漏变量、双向因果和测量误差等导致无法满足第四条假设。</p><p>线性回归模型： <span class="math display">\[y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + ...+ \beta_p x_p + \epsilon.\]</span> 通过最小化残差平方和SSE来进行求解。最小二乘解为： <span class="math display">\[\hat{\beta} = (X^T X)^{-1}X^T y.\]</span></p><p><strong>模型评估</strong>： MSE 和 决定系数 <span class="math inline">\(R^2\)</span>。二者的具体定义可见section 1.5。</p><ul><li>线性假设指的是模型参数的线性，而不一定是原始数据的线性。可以引入变换或非线性特征（如交叉项<span class="math inline">\(x_1 \times x_2\)</span>、多项式项 <span class="math inline">\(x_1^2\)</span>），只要因变量与参数之间的关系保持线性即可。此时加入非线性特征（如交叉特征或多项式项）并不违反线性回归的假设，因为模型的参数仍然是"线性的"。</li></ul><p><strong>多重共线性</strong></p><p>当线性回归模型中的两个或多个自变量高度相关，导致信息重叠或冗余时，就会产生多重共线性。在这种情况下，模型很难分离出每个自变量对因变量的单独影响，从而导致不可靠的系数估计、标准误差膨胀以及解释上的挑战。</p><p>如何检测和解决多重共线性问题？ 1. 方差膨胀因子 (VIF)：VIF是一种常见的诊断工具，用于衡量回归系数的方差因多重共线性而膨胀的程度。VIF超过 5 或 10 表示多重共线性很高。 2.相关矩阵：检查自变量的相关矩阵可以发现高度相关的变量对（相关性接近 1或-1）。这表明存在潜在的多重共线性。 3.放弃其中一个相关变量：如果两个或多个变量高度相关，可考虑放弃其中一个变量。这可以简化模型并减少多重共线性。4. 主成分分析（PCA）：PCA可以将相关变量转化为一组不相关的成分，用于回归分析。这可以降低数据维度，避免多重共线性。5.岭回归或lasso回归：这些正则化技术有助于减轻多重共线性的影响。岭回归会对系数的大小进行惩罚，从而降低系数对多重共线性的敏感性（不具备变量筛选的能力，无法完全解决）。lasso回归则更进一步，可以将某些系数缩减为零，从而有效地选择预测因子的子集。</p><h4 id="逻辑回归">逻辑回归</h4><p>逻辑回归是一种广泛用于二元分类任务的监督学习算法。在机器学习中，监督学习包括对输入输出对进行模型训练，以学习能够预测未见数据的模式。逻辑回归专门预测基于输入特征的分类结果的概率，其中结果属于两个类别之一。虽然逻辑回归可以扩展到多个类别，但其最常见的应用还是二元分类。</p><p>逻辑回归模型： <span class="math display">\[p(y = 1 | x) = sigmoid(z) = \frac{1}{1 + e^{-z}}, z = {\beta}_0 +{\beta}_1 x_1 + ...+ {\beta}_k x_k.\]</span></p><p><strong>模型评估</strong></p><p>逻辑回归模型使用损失函数进行评估，该函数用于衡量模型预测真实结果的能力。对于二元分类，合适的损失函数是Log-Loss（也称为二元交叉熵）。对于有 <span class="math inline">\(n\)</span> 个样本的给定数据集，Log-Loss 的定义为:<span class="math display">\[Log-loss = - \sum_{i=1}^{n} [y_i \log p_i + (1-y_i)\log (1-p_i)],\]</span> 其中，<span class="math inline">\(y_i\)</span> 为第<span class="math inline">\(i\)</span>个样本的真实标签，<span class="math inline">\(p_i\)</span> 为第<span class="math inline">\(i\)</span>个样本的输出概率。</p><p><strong>系数估计算法</strong></p><p>有两种主流的方式去估计模型的参数 <span class="math inline">\({\beta}\)</span>，（1）梯度下降；（2）最大似然估计（MLE).</p><p>最大似然估计：对于逻辑回归，定义函数 <span class="math inline">\(h_{\beta}(x) = \frac{1}{1 +e^{-z}}\)</span>，其似然函数为所有样本观测值出现概率的乘积，可以表示为：<span class="math display">\[L(\beta) = \prod_{i=1}^{n} [h_{\beta}(x_i)]^{y_i}[1 -h_{\beta}(x_i)]^{(1-y_i)}.\]</span>最大化该似然函数，通常会转化为最大化其对数似然，然后用梯度下降法求解。对数似然函数为：<span class="math display">\[\log L(\beta) = \sum_{i=1}^{n} [y_i \log h_{\beta}(x_i)+(1-y_i)\log(1 -h_{\beta}(x_i))].\]</span> 显然，最大化对数似然等价于最小化log-loss。</p><p><strong>多标签分类</strong>：假设每个样本属于不同标签的概率服从几何分布，可以使用softmaxregression进行分类： $$ h_= = </p><p></p><p> $$ 其中 <span class="math inline">\(\theta_1,\theta_2 \dots,\theta_k\in \mathbb{R}^n\)</span></p><p>如果存在样本可能属于多个标签的情况时，可以训练k个二分类的逻辑回归分类器。第i个分类器用以区分每个样本是否可以归为第i类。</p><h4 id="二者之间的联系">二者之间的联系</h4><p>如果把一个事件的几率（odds）定义为该事件发生的概率与不发生概率的比值<span class="math inline">\(\frac{p}{1-p}\)</span>，那么逻辑回归可以看做是对于"y=1|x"这一事件的对数几率的线性回归 <span class="math display">\[{\rm log} \frac{p}{1-p} = \theta^{T}x ，其中\ p  = P(y=1|x).\]</span></p><h3 id="k-近邻算法k-nearest-neighborsknn">2.2. K-近邻算法（K-NearestNeighbors，KNN）</h3><p>K-近邻算法（KNN）是一种监督学习算法，主要用于分类和回归问题。KNN是一种基于实例的学习方法，其核心思想是：给定一个未标记的样本，找到与该样本最相似的<span class="math inline">\(k\)</span>个已知标签的样本（最近邻居），然后通过这些邻居的类别信息进行预测。 KNN适合小数据集的分类和回归任务。然而，随着数据规模的增长和维度的增加，KNN的计算成本和性能都会变得较差，因此在实际应用中常结合其他优化技术使用，比如KD 树、Ball 树等。</p><h4 id="计算流程">计算流程</h4><ol type="1"><li><p><strong>数据预处理</strong>：对于每个输入样本，<strong>首先需要进行标准化或归一化操作，确保不同特征值处于相同的数值范围，否则距离计算时可能会被某些特征主导。</strong></p></li><li><p><strong>计算距离</strong>：对新的测试样本，基于某一距离标准，计算它与所有训练样本之间的距离。</p></li><li><p><strong>选择K值</strong>：选择一个合适的 <span class="math inline">\(k\)</span>值（即考虑的邻居数量），通常是一个正整数。较小的 <span class="math inline">\(k\)</span> 可能导致模型过拟合，较大的 <span class="math inline">\(k\)</span> 则可能导致模型过于平滑。</p></li><li><p><strong>选择最近的K个邻居</strong>：根据距离计算的结果，选择离测试样本最近的<span class="math inline">\(k\)</span> 个训练样本。</p></li><li><p><strong>进行投票或加权</strong>：</p><ul><li><strong>分类问题</strong>：通过这 <span class="math inline">\(k\)</span>个最近邻居的类别进行投票，选择出现最多的类别作为预测结果。</li><li><strong>回归问题</strong>：通过这 <span class="math inline">\(k\)</span>个最近邻居的数值标签，通常取它们的均值或加权平均值作为预测结果。</li></ul></li><li><p><strong>输出预测结果</strong>：得到最终预测值，完成预测。</p></li></ol><p>在实际应用中，<span class="math inline">\(k\)</span>值一般取一个比较小的数值，例如采用交叉验证法来选择最优的 <span class="math inline">\(k\)</span> 值。</p><h4 id="优点">优点</h4><ol type="1"><li><p><strong>简单易懂</strong>：KNN不需要进行复杂的模型训练，只需要存储所有训练数据，直观易理解。</p></li><li><p><strong>无参数学习</strong>：KNN不假设数据的分布情况，它是一种非参数学习方法，因此对数据的分布形式没有要求。</p></li><li><p><strong>灵活性强</strong>：可以用于分类和回归问题，距离度量方法也可以根据实际需要灵活更改。</p></li><li><p><strong>增量学习</strong>：KNN可以适应动态变化的数据集，因为新样本只需要加入到训练集中即可，不需要重新训练模型。</p></li></ol><h4 id="缺点">缺点</h4><ol type="1"><li><p><strong>计算量大</strong>：每次预测都需要计算新样本与所有训练样本之间的距离，因此计算开销大，特别是在样本数量多时，效率较低。</p></li><li><p><strong>高维数据效果较差</strong>：在高维空间中，数据变得稀疏，"距离"的直观意义减弱，KNN在高维数据下表现通常不佳，这就是所谓的"维度灾难"。</p></li><li><p><strong>对数据量敏感</strong>：KNN对噪声和异常值敏感，噪声点可能严重影响最终的分类结果，尤其在 $ k $值较小的情况下。</p></li><li><p><strong>对特征缩放敏感</strong>：不同特征的量纲差异可能会导致某些特征主导距离计算，因此需要对数据进行标准化或归一化处理。</p></li></ol><h4 id="常用的距离衡量公式">常用的距离衡量公式</h4><p><strong>1. 闵可夫斯基距离</strong></p><p>假设特征空间 <span class="math inline">\(\mathcal X\)</span>是n维实数向量空间 <span class="math inline">\(\mathbf{R}^n\)</span>，<span class="math inline">\(x_i, x_j \in \mathcal{X}, x_i =(x_i^{(1)}, x_i^{(2)},\cdots x_i^{(n)}  ),  x_j = (x_j^{(1)}, x_j^{(2)},\cdots, x_j^{(n)})\)</span> 。则 <span class="math inline">\(x_i,x_j\)</span> 的 <span class="math inline">\(L_p\)</span>距离（闵可夫斯基距离）定义为 <span class="math display">\[  L_p(x_i, x_j) = (\sum_{l=1}^n |x_i^{(l)}-x_j^{(l)}|^p)^{\frac{1}{p}},\quad p \geq 1.\]</span></p><p><strong>2. 欧式距离</strong></p><p>当闵可夫斯基距离公式中 <span class="math inline">\(p=2\)</span>时，称为欧氏距离，用来衡量两点在多维空间中的直线距离，是严格定义的距离，满足正定性、对称性、三角不等式。<span class="math display">\[  L_2(x_i, x_j) = (\sum_{l=1}^n |x_i^{(l)}-x_j^{(l)}|^2)^{\frac{1}{p}}.\]</span>欧式距离对较大的差异很敏感，如果某个特征值差异大，欧氏距离会受到很大影响，因此数据常需要进行归一化或标准化。</p><p><strong>3. 曼哈顿距离</strong></p><p>当闵可夫斯基距离公式中 <span class="math inline">\(p=1\)</span>时，称为曼哈顿距离，用于衡量点与点之间的坐标差异之和，即从一个点到另一个点走直角路径的总距离。<span class="math display">\[  L_1(x_i, x_j) = \sum_{l=1}^n |x_i^{(l)}-x_j^{(l)}|.\]</span>曼哈顿距离通常用于街区格子的路径计算，模拟只能沿着水平和垂直方向移动的场景。在高维空间中，由于它对特征值间差异的处理方式相对温和，因此它可能比欧氏距离对噪声更加鲁棒。</p><p><strong>4. 切比雪夫距离</strong></p><p>当 <span class="math inline">\(p = \infty\)</span>时，称作切比雪夫距离。两个向量各个坐标距离数值差的绝对值的最大值。 <span class="math display">\[  L_{\infty}(x_i, x_j) = \mathop{\max}_{l} \  |x_i^{(l)}-x_j^{(l)}|.\]</span> 切比雪夫距离对数据的最大变化特别敏感。</p><p><strong>5. 马氏距离</strong></p><p>考虑各个分量（特征）之间的相关性并与各个分量的尺度无关。给定一个样本集合<span class="math inline">\(X\)</span>，<span class="math inline">\(X=(x_{ij})_{m\times n}\)</span>，其协方差矩阵记为<span class="math inline">\(S\)</span>。样本 <span class="math inline">\(x_i\)</span> 与样本 <span class="math inline">\(x_j\)</span> 之间的马氏距离 <span class="math inline">\(d_{ij}\)</span> 定义为 <span class="math display">\[d_{ij} = [(x_i - x_j)^TS^{-1}(x_i - x_j)]^{\frac{1}{2}}.\]</span> 当 <span class="math inline">\(S\)</span>为单位矩阵时，即样本数据的各个分量互相独立且各个分量的方差为1时，马氏距离就是欧氏距离。</p><p>马氏距离不仅考虑了点与点之间的距离，还考虑了特征之间的相关性（通过协方差矩阵），适合特征相关性较强的场景，在这些情况下比欧氏距离更加准确。</p><p><strong>6. 汉明距离</strong></p><p>汉明距离用于衡量<strong>两个等长的二进制向量或字符串</strong>之间有多少位不相同。换句话说，它表示两个字符串之间的不同字符个数。通常用于编码、错误检测与纠正等领域。</p><p>对于两个长度相同的二进制序列 $ p $ 和 $ q <span class="math inline">\(，汉明距离的定义为：\)</span>$ d(p, q) =_{i=1}^{n} (p_i, q_i). $$ 其中，$ (p_i, q_i) $ 为指示函数，当 $ p_i q_i$ 时，$ (p_i, q_i) = 1 $，否则 $ (p_i, q_i) = 0 $。</p><p>1011101 与 1001001 之间的汉明距离是 2。</p><p>2143896 与 2233796 之间的汉明距离是 3。</p><p>"toned" 与 "roses" 之间的汉明距离是 3。</p><p><strong>7. 相关系数</strong>（correlation coefficient）</p><p>相关系数用于衡量两个向量或变量之间的<strong>线性相关性</strong>。它的值范围在<span class="math inline">\([-1, 1]\)</span> 之间： - $ 1 $表示完全正相关； - $ -1 $ 表示完全负相关； - $ 0 $表示没有线性关系。</p><p>最常用的相关系数是<strong>皮尔逊相关系数</strong>（PearsonCorrelation Coefficient）。 <span class="math inline">\(x_i\)</span> 与<span class="math inline">\(x_j\)</span> 之间的相关系数定义为 <span class="math display">\[r_{ij} =\frac{\sum_{k=1}^{m}\left(x_{k i}-\overline{x}_{i}\right)\left(x_{kj}-\overline{x}_{j}\right)}{\left[\sum_{k=1}^{m}\left(x_{ki}-\overline{x}_{i}\right)^{2} \sum_{k=1}^{m}\left(x_{kj}-\overline{x}_{j}\right)^{2}\right]^{\frac{1}{2}}}.\]</span></p><p><span class="math display">\[\overline{x}_{i}=\frac{1}{m} \sum_{k=1}^{m} x_{k i}, \quad\overline{x}_{j}=\frac{1}{m} \sum_{k=1}^{m} x_{k j}\]</span></p><p><strong>8. 余弦相似度</strong></p><p>余弦相似度用于衡量两个向量之间的夹角，它用于计算两个向量在<strong>方向上</strong>的相似性，而不是在大小上的相似性。不是严格定义的距离，满足正定性、对称性，不满足三角不等式。余弦相似度的取值范围在 <span class="math inline">\([-1, 1]\)</span>之间: - 1 表示两个向量完全相同（方向一致）； - 0表示两个向量正交（没有相似性）； - -1 表示两个向量方向完全相反。</p><p>公式定义为： <span class="math display">\[  cos(A,B) = \frac{A \cdot B}{||A||_2 ||B||_2}.\]</span></p><p><strong>使用场景</strong></p><ol type="1"><li><p><strong>欧氏距离</strong>：适合维度较低、特征彼此独立且已归一化的数据。计算点与点之间的"直线距离"。</p></li><li><p><strong>曼哈顿距离</strong>：当特征值的分布差异较大或者关注的是各维度变化总和时效果较好。用于计算只能沿着水平和垂直方向的移动（如城市网格或棋盘）。</p></li><li><p><strong>闵可夫斯基距离</strong>：通用的度量方式，适合需要灵活调整距离公式的场景。</p></li><li><p><strong>切比雪夫距离</strong>：适合在某些场景下只关心各坐标轴的最大差异，如国际象棋中的"国王路径"问题。</p></li><li><p><strong>马氏距离</strong>：适用于多维数据且特征之间存在相关性，考虑数据的分布情况。</p></li><li><p><strong>汉明距离</strong>：适用于二进制向量或字符串的比较，主要衡量离散值之间的差异。</p></li><li><p><strong>相关系数</strong>：用于衡量两个数值变量的线性相关性，适合线性关系的分析。</p></li><li><p><strong>余弦相似度</strong>：用于评估向量之间的方向相似性，广泛用于文本处理和推荐系统中。</p></li></ol><h3 id="支持向量机svmsupport-vector-machine">2.3.支持向量机（SVM，Support Vector Machine）</h3><p>支持向量机（SVM）是一种<strong>监督学习</strong>算法，主要用于<strong>分类</strong>和<strong>回归</strong>问题，尤其擅长处理<strong>二分类问题</strong>。它的基本模型是定义在特征空间的<strong>间隔最大的线性分类器</strong>。SVM尝试在多维空间中找到一个<strong>超平面</strong>，将不同类别的数据点分开。在许多情况下，数据是线性不可分的，因此SVM通过将数据映射到<strong>高维特征空间</strong>，使得在这个新空间中可以找到一个线性可分的超平面。</p><ul><li><p>线性可分支持向量机：当训练数据线性可分，通过硬间隔最大化，学习一个线性的分类器</p></li><li><p>线性支持向量机：当训练数据近似线性可分，通过软间隔最大化，学习一个线性的分类器</p></li><li><p>非线性支持向量机：当训练数据线性不可分，通过使用核技巧及软间隔最大化，学习非线性分类器</p></li></ul><h4 id="超平面与支持向量">超平面与支持向量</h4><ul><li><p><strong>超平面</strong>：在二分类问题中，超平面是将数据点划分为两类的决策边界。在二维空间中，超平面是一个直线；在三维空间中，超平面是一个平面；在更高维的情况下，超平面是一个<span class="math inline">\(n-1\)</span> 维的结构。</p></li><li><p><strong>支持向量</strong>：支持向量是距离超平面<strong>最近的</strong>数据点。SVM通过这些支持向量来定义和构建超平面，因此它们对决策边界的确定具有重要作用。</p></li><li><p><strong>间隔（Margin）</strong>：SVM的核心思想是找到一个能<strong>最大化间隔</strong>的超平面，即使得支持向量到超平面的距离最大化。间隔越大，模型的泛化能力越强。</p></li></ul><h4 id="线性可分-svm">线性可分 SVM</h4><p>在<strong>线性可分</strong>的情况下，SVM寻找的是能够完全分离两类数据的超平面。给定一组训练样本 ${(x_1, y_1),(x_2, y_2), , (x_n, y_n)} $，其中 $ x_i ^n $ 是样本，$ y_i {-1, 1} $是类别标签，SVM 要找到的超平面可以表示为： <span class="math display">\[w \cdot x + b = 0,\]</span> 其中，$ w $ 是超平面的法向量，$ b $是偏移量。为了使超平面将两类数据完全分开，我们希望满足以下约束条件： -对于 $ y_i = 1 $ 类别的点，要求 $ w x_i + b $。 - 对于 $ y_i = -1 $类别的点，要求 $ w x_i + b $。</p><p>对于一个点 $ x $，它到超平面 $ w x + b = 0 $ 的几何距离可以表示为：<span class="math display">\[d(x, \text{hyperplane}) = \frac{|w \cdot x + b|}{\|w\|}.\]</span> 对于支持向量 $ x_i $，它们距离超平面最近，因此满足 $ w x_i + b= $ 的约束。 两个类别的支持向量分别位于 $ w x + b = 1 $ 和 $ w x + b =-1 $ 的平面上。</p><p><strong>间隔</strong>（即两个支持向量之间的距离）可以表示为两个支持向量平面之间的距离：<span class="math display">\[\text{Margin} = \frac{|1 - (-1)|}{\|w\|} = \frac{2}{\|w\|}\]</span> SVM 的目标就是<strong>最大化间隔</strong>，等价于**最小化 $|w| <span class="math inline">\(**。最终的优化问题可以写作：\)</span>$|w|^2 y_i (w x_i + b) , , i $$</p><h4 id="线性不可分-svm">线性不可分 SVM</h4><p>在很多实际场景中，数据并非线性可分。此时，我们可以引入<strong>软间隔（SoftMargin）</strong>，允许一些数据点出现在错误的侧面，从而使模型更具弹性。为了处理这种情况，SVM 引入了<strong>松弛变量</strong> $ <em>i <span class="math inline">\(，允许某些数据点不满足分类约束条件。优化问题变为：\)</span>$|w|^2 + C </em>{i=1}^{n} _i y_i (w x_i + b) - _i, , _i , $$ 其中，<span class="math inline">\(C\)</span>是一个正的常数，用于控制<strong>间隔大小和分类错误惩罚</strong>之间的权衡。较大的$ C $ 值会导致模型更加关注分类准确性，忽略间隔；较小的 $ C $值则更关注间隔大小，允许更多的分类错误。</p><p>通过拉格朗日乘子法，将 SVM原始问题中的约束优化问题转化为对偶形式的二次规划问题。使用二次规划求解器或SMO 算法求解对偶问题。 <span class="math display">\[\max_{\alpha} \sum_{i=1}^{n} \alpha_i - \frac{1}{2} \sum_{i=1}^{n}\sum_{j=1}^{n} \alpha_i \alpha_j y_i y_j (x_i \cdot x_j).\]</span> <span class="math display">\[\text{s.t.} \quad \sum_{i=1}^{n} \alpha_i y_i = 0, \quad 0 \leq \alpha_i\leq C.\]</span></p><p>对于训练一个不加入松弛变量的SVM模型时，训练误差为0的SVM分类器一定存在。对于加入松弛变量的SVM的训练误差不一定能达到0。</p><h4 id="核方法kernel-trick">核方法（Kernel Trick）</h4><p>在处理<strong>非线性分类问题</strong>时，直接在原始特征空间中很难找到线性可分的超平面。SVM通过<strong>核方法</strong>（KernelTrick）将原始数据映射到一个<strong>高维空间</strong>，在这个空间中数据可以线性分离。</p><p><strong>核函数定义</strong>：设 <span class="math inline">\(\mathcal{X}\)</span> 是输入空间，又设 <span class="math inline">\(\mathcal{H}\)</span> 为特征空间，如果存在一个从<span class="math inline">\(\mathcal{X}\)</span>到 <span class="math inline">\(\mathcal{H}\)</span> 的映射 <span class="math display">\[\phi(x) : \mathcal{X} \rightarrow \mathcal{H}\]</span> 使得对所有<span class="math inline">\(x, z \in\mathcal{X}\)</span>，函数<span class="math inline">\(K(x,z)\)</span>满足条件 <span class="math display">\[K(x, z)=\phi(x) \cdot \phi(z)\]</span> 则称 <span class="math inline">\(K(x,z)\)</span>为核函数，<span class="math inline">\(\phi(x)\)</span>为映射函数，式中 <span class="math inline">\(\phi(x) \cdot\phi(z)\)</span> 为 <span class="math inline">\(\phi(x)\)</span> 和<span class="math inline">\(\phi(z)\)</span>的内积。</p><p>在对偶问题中，可以将 <span class="math inline">\(x \cdot z\)</span>中的数据分别进行变换，即 <span class="math inline">\(\phi(x) \cdot\phi(z)\)</span>。为方便计算，可以直接使用核函数 <span class="math inline">\(K(x, z)\)</span> 代替。</p><p><strong>线性核函数</strong> <span class="math display">\[K(x,z) = x \cdot z\]</span>主要用于线性可分的情况。可以看到特征空间到输入空间的维度是一样的，其参数少速度快，对于线性可分数据，其分类效果很理想，因此我们通常首先尝试用线性核函数来做分类，看看效果如何，如果不行再换别的。</p><p><strong>多项式核函数</strong>（polynomial kernel function） <span class="math display">\[K(x, z)=(x \cdot z+1)^{p}\]</span> 对应的支持向量机是一个p次多项式分类器。分类决策函数为 <span class="math display">\[f(x)=\operatorname{sign}\left(\sum_{i=1}^{N_{s}} a_{i}^{*}y_{i}\left(x_{i} \cdot x+1\right)^{p}+b^{*}\right)\]</span> 其中 <span class="math inline">\(x\)</span> 是待分类样本。多项式核函数可以实现将低维的输入空间映射到高维的特征空间，但是多项式核函数的参数多，当多项式的阶数比较高的时候，核矩阵的元素值将趋于无穷大或者无穷小，计算复杂度会大到无法计算。</p><p><strong>高斯核函数</strong>（Gaussian kernel function） <span class="math display">\[K(x,z) = exp(-\frac{1}{2} \ ||x - z ||_2 ) = \phi(x) \cdot \phi(z)\]</span> 对应的支持向量机是高斯径向基函数（radial basisfunction）分类器，分类决策函数为 <span class="math display">\[f(x)=\operatorname{sign}\left(\sum_{i=1}^{N_{s}} a_{i}^{*} y_{i} \exp\left(-\frac{\|x-x_i\|^{2}}{2 \sigma^{2}}\right)+b^{*}\right)\]</span>高斯径向基函数是一种局部性强的核函数，其可以将一个样本映射到一个更高维的空间内，该核函数是应用最广的一个，无论大样本还是小样本都有比较好的性能，而且其相对于多项式核函数参数要少。</p><p><strong>Sigmod核函数</strong> <span class="math display">\[K\left(x, z\right)=\tanh \left(\eta \ x \cdot z +\theta\right)\]</span></p><p>核方法的关键在于，我们无需显式地将数据映射到高维空间，而是通过<strong>核函数</strong>直接在原始空间中计算高维空间的内积，从而实现非线性分类。</p><h4 id="优缺点">优缺点</h4><h5 id="优点-1">优点：</h5><ul><li><strong>高效处理高维数据</strong>：SVM在高维空间中仍能表现良好，尤其是使用核方法时可以处理复杂的非线性分类问题。</li><li><strong>强大的泛化能力</strong>：通过最大化间隔，SVM通常能够很好地避免过拟合问题，具有良好的泛化性能。</li><li><strong>对少量样本较为有效</strong>：即使在样本量较小的情况下，SVM也能表现出色，因为它只关注支持向量而非所有样本点。</li></ul><h5 id="缺点-1">缺点：</h5><ul><li><strong>对大规模数据集的效率较低</strong>：由于 SVM的复杂度较高，在处理非常大规模的数据集时，训练时间可能较长。</li><li><strong>对核函数的选择敏感</strong>：选择不恰当的核函数可能导致模型性能不佳，且参数（如$ C $ 和 $ $）需要精心调优。</li><li><strong>不适合噪声数据</strong>：SVM对噪声敏感，尤其是在数据中有重叠或混合时，模型可能表现不稳定。</li><li><strong>仅支持二分类问题</strong>：原生的 SVM是二分类模型，虽然可以通过某些策略扩展到多分类（如一对一、多对多方法），但相对其他多分类算法显得复杂。</li></ul><h4 id="svm-和-感知机的区别">SVM 和 感知机的区别</h4><ul><li><p>感知机的目标是<strong>找到一个可以将数据点分开的线性超平面</strong>，即将两类数据点尽可能正确地划分。感知机的训练过程是通过逐步调整权重来消除分类错误，直到找到一个可以完全分类的数据超平面。它不考虑<strong>分类间隔</strong>。SVM不仅要求找到一个分离超平面，还要找到能<strong>最大化分类间隔（margin）</strong>的超平面。最大化间隔可以使模型具有更好的泛化能力，因此SVM 的目标是找到分类超平面并且使到支持向量的间隔最大化。</p></li><li><p>感知机使用的是<strong>误分类驱动的梯度下降法</strong>，即每次在出现误分类时更新权重，直到所有数据点都被正确分类。SVM通过<strong>凸优化</strong>技术，直接求解带约束的优化问题。其目标函数是最大化分类间隔，通常通过<strong>拉格朗日对偶优化</strong>或<strong>二次规划</strong>求解，或者通过梯度下降法结合核方法来求解非线性问题。</p></li><li><p>感知机的泛化能力较弱，它只要找到一个可以完全分离数据的超平面即可，但在实际应用中可能会<strong>过拟合</strong>。SVM最大化间隔的策略使得它对<strong>噪声和过拟合</strong>有更强的抗性，泛化能力更强。尤其是在小样本或高维数据下，SVM的表现优于感知机。</p></li><li><p>感知机不考虑数据的线性不可分性，如果数据是线性不可分的，它将无法收敛。SVM可以通过引入<strong>软间隔</strong>处理线性不可分的情况，允许一些数据点处于错误的一侧。此外，SVM可以通过<strong>核技巧（KernelTrick）</strong>将数据映射到高维空间，在高维空间中实现线性分离。</p></li></ul><h3 id="朴素贝叶斯模型">2.4. 朴素贝叶斯模型</h3><p>朴素贝叶斯（NaiveBayes）模型是一种基于贝叶斯定理的简单而高效的分类算法，广泛应用于文本分类、垃圾邮件检测、情感分析等任务。它在假设特征之间相互独立的前提下进行分类，尽管这一假设在现实中通常不成立，但朴素贝叶斯在许多实际应用中表现仍然相当好。</p><h4 id="贝叶斯定理">贝叶斯定理</h4><p>朴素贝叶斯模型的基础是<strong>贝叶斯定理</strong>，该定理描述了在给定某些证据的情况下，如何计算某个事件发生的概率。贝叶斯定理的公式如下：<span class="math display">\[P(C|X) = \frac{P(X|C) \cdot P(C)}{P(X)},\]</span> 其中： - $ P(C|X) $：给定特征 $ X $ 时类别 $ C $发生的<strong>后验概率</strong>； - $ P(C) $：类别 $ C $的<strong>先验概率</strong>，即在没有观察到特征 $ X $ 时类别 $ C $发生的概率； - $ P(X|C) $：给定类别 $ C $ 时，特征 $ X $发生的概率，即<strong>似然函数</strong>； - $ P(X) $：特征 $ X $的<strong>边缘概率</strong>，即所有类别中特征 $ X $ 发生的概率。</p><h4 id="朴素贝叶斯分类器">朴素贝叶斯分类器</h4><p>朴素贝叶斯模型被称为“朴素”的原因是它基于一个<strong>朴素的独立假设</strong>，即假设所有特征$ X = [X_1, X_2, , X_n] $ 之间是相互独立的，给定类别 $ C $时，特征之间没有相关性。根据这一假设，贝叶斯定理中的 $ P(X|C) $可以简化为： <span class="math display">\[P(X|C) = P(X_1|C) \cdot P(X_2|C) \cdot \ldots \cdot P(X_n|C)\]</span> 即特征的联合概率可以表示为每个特征条件概率的乘积。</p><p>基于上述假设，朴素贝叶斯分类器的目标是对于每个类别 $ C$，计算后验概率 $ P(C|X) <span class="math inline">\(，并选择概率最大的类别作为预测结果：\)</span>$ =_C P(C|X) = <em>C P(C) </em>{i=1}^{n} P(X_i|C) $$</p><p><strong>朴素贝叶斯模型之所以被认为是线性分类器，主要是因为它在对数空间下，分类决策边界呈现为一个线性函数。</strong></p><h4 id="后验概率最大化的含义是什么">后验概率最大化的含义是什么？</h4><p>朴素贝叶斯法将实例分到后验概率最大的类中。后验概率最大化这等价于期望风险最小化。</p><p>假设选择0-1损失函数： <span class="math display">\[  L(Y, f(X))=\left\{    \begin{array}    {ll}{1,} &amp; {Y \neq f(X)} \\ {0,} &amp; {Y=f(X)}    \end{array}    \right.  \]</span> 其中 <span class="math inline">\(f(X)\)</span>是分类决策函数。这是期望风险函数为<span class="math display">\[  R_{\operatorname{cap}}(f)=E[L(Y, f(X))]  \]</span> 期望是对联合分布 <span class="math inline">\(P(X,Y)\)</span>取的。由此取条件期望 <span class="math display">\[  R_{\mathrm{exp}}(f)=E_{X} \sum_{k=1}^{K}\left[L\left(c_{k},f(X)\right)\right] P\left(c_{k} | X\right)  \]</span> 为了使期望奉献最小化，只需对 <span class="math inline">\(X=x\)</span> 逐个最小化，由此得到 <span class="math display">\[  \begin{aligned} f(x) &amp;=\arg \min _{y \in \mathcal{Y}}\sum_{k=1}^{K} L\left(c_{k}, y\right) P\left(c_{k} | X=x\right) \\&amp;=\arg \min _{y \in \mathcal{Y}} \sum_{k=1}^{K} P\left(y \neq c_{k}| X=x\right) \\ &amp;=\arg \min _{y \in\mathcal{Y}}\left(1-P\left(y=c_{k} | X=x\right)\right) \\ &amp;=\arg\max _{y \in \mathcal{Y}} P\left(y=c_{k} | X=x\right) \end{aligned}  \]</span> 这样一来，根据期望风险最小化准则就得到了后延概率最大化准则：<span class="math display">\[  f(x)=\arg \max _{c_{k}} P\left(c_{k} | X=x\right)  \]</span> 即朴素贝叶斯法所采用原理</p><h4 id="朴素贝叶斯的三种常见类型">朴素贝叶斯的三种常见类型</h4><p>根据特征的不同类型，朴素贝叶斯模型可以分为几种常见的变体：</p><p><strong>1. 高斯朴素贝叶斯（Gaussian Naive Bayes）</strong></p><p>当特征是<strong>连续值</strong>时，假设特征符合正态分布（高斯分布）。对于每个类别$ C $，特征的条件概率 $ P(X_i|C) $可以用以下正态分布的概率密度函数表示： <span class="math display">\[P(X_i|C) = \frac{1}{\sqrt{2\pi\sigma_C^2}} \exp\left( -\frac{(X_i -\mu_C)^2}{2\sigma_C^2} \right),\]</span> 其中，$ _C $ 和 $ _C $ 是特征 $ X_i $ 在类别 $ C $下的均值和标准差。</p><p><strong>1. 多项式朴素贝叶斯（Multinomial Naive Bayes）</strong></p><p>用于<strong>离散值特征</strong>，特别适合处理<strong>文本分类</strong>任务。假设每个特征是某个离散事件发生的次数或频率。此时，条件概率$ P(X_i|C) $ 被建模为多项式分布： <span class="math display">\[P(X_i|C) = \frac{N_{i,C} + 1}{N_C + |V|},\]</span> 其中： - $ N_{i,C} $ 是类别 $ C $ 中特征 $ X_i $ 出现的次数；- $ N_C $ 是类别 $ C $ 中所有特征的总和； - $ |V| $是特征词汇表的大小（加 1 处理为拉普拉斯平滑）。</p><p><strong>3. 伯努利朴素贝叶斯（Bernoulli Naive Bayes）</strong></p><p>适用于<strong>二元离散特征</strong>，即特征值仅为 0 或1（表示特征是否存在）。这在文本分类中常用于二元词袋模型（Binary Bag ofWords），其中每个词的出现与否作为特征： <span class="math display">\[P(X_i|C) = p_C^{X_i} (1 - p_C)^{(1 - X_i)},\]</span> 其中，$ p_C $ 是特征 $ X_i $ 在类别 $ C $ 中的概率。</p><h4 id="训练过程">训练过程</h4><ol type="1"><li><p><strong>计算先验概率</strong>：先验概率 $ P(C) $ 可以通过类别 $ C$ 在训练集中出现的频率估计： <span class="math display">\[P(C) = \frac{\text{样本中类别 } C \text{ 的数量}}{\text{样本总数}}\]</span></p></li><li><p><strong>计算条件概率</strong>：对于每个特征 $ X_i $，计算它在类别$ C $ 下的条件概率 $ P(X_i|C)$。根据具体任务的不同，这个概率可能通过频率估计或假设的分布（如高斯分布）来计算。</p></li><li><p><strong>构建分类器</strong>：利用贝叶斯定理，将计算得到的条件概率和先验概率结合在一起，构建分类模型。</p></li></ol><h4 id="优缺点-1">优缺点</h4><p><strong>优点：</strong></p><ul><li><strong>简单高效</strong>：模型非常简单，易于实现，尤其适用于高维数据。</li><li><strong>计算速度快</strong>：训练和预测的计算开销很小，适合大规模数据集。</li><li><strong>对小数据集有效</strong>：朴素贝叶斯模型对小数据集通常表现良好。</li><li><strong>处理缺失数据</strong>：可以轻松处理部分特征缺失的样本。</li><li><strong>适合文本分类</strong>：在文本分类任务中，朴素贝叶斯模型表现良好，常用于垃圾邮件分类、情感分析等任务。</li></ul><p><strong>缺点：</strong></p><ul><li><strong>特征独立性假设不现实</strong>：朴素贝叶斯的独立性假设通常在现实数据中不成立，导致模型性能可能受到影响。</li><li><strong>对稀有类别敏感</strong>：如果某一类别中某个特征从未出现过（即其条件概率为0），模型可能会错误地认为该类别不可能发生。这可以通过拉普拉斯平滑来缓解。</li><li><strong>无法处理复杂关系</strong>：由于假设特征之间相互独立，朴素贝叶斯无法处理特征之间复杂的依赖关系。</li></ul><h4 id="贝叶斯网络">贝叶斯网络</h4><p>朴素贝叶斯法假设输入变量都是条件独立的，如果假设它们之间<strong>存在概率依存关系</strong>，模型就被成了贝叶斯网络。贝叶斯网络也称为“信念网”，借助<strong>有向无环图</strong>来刻画属性之间的依赖关系，并使用<strong>条件概率表</strong>来描述属性的联合概率分布。贝叶斯网结构有效地表达了属性的条件独立性。</p><p>具体来说，一个贝叶斯网B由结构 <span class="math inline">\(G\)</span>和参数 <span class="math inline">\(\theta\)</span> 表示，即 <span class="math inline">\(B = &lt;G,\theta&gt;\)</span>。网络结构G是一个有向无环图，其每个节点对应于一个属性，若两个属性有直接依赖关系，则它们由一条边连接起来；参数<span class="math inline">\(\theta\)</span>定量描述这种依赖关系，假设属性 <span class="math inline">\(x_i\)</span>在G中的父节点集为 <span class="math inline">\(\pi_i\)</span>，则 <span class="math inline">\(\theta\)</span>包含了每个属性的条件概率 <span class="math inline">\(\theta_{x_i|\pi_i} = P_B(x_i|\pi_i)\)</span>。</p><p>给定父节点集，贝叶斯网假设每个属性与它的非后裔属性独立，于是将属性的联合概率分布定义为<span class="math display">\[P_{B}\left(x_{1}, x_{2}, \ldots, x_{d}\right)=\prod_{i=1}^{d}P_{B}\left(x_{i} | \pi_{i}\right)=\prod_{i=1}^{d} \theta_{x_{i} |\pi_{i}}\]</span></p><p>贝叶斯网的一个核心概念是条件独立性。通过有向无环图，可以直接可视化出哪些变量在条件下是独立的。具体来说，贝叶斯网通过图的结构来表示变量之间的条件独立性，简化联合概率的计算。- 父子节点之间的依赖性：每个节点依赖于它的父节点。 -马尔可夫性：给定某个节点的父节点，节点与其非后代节点条件独立。</p><p>贝叶斯网的推断可以通过多种算法实现，常见的推断算法包括：</p><ul><li>精确推断：如变量消去算法（VariableElimination）、信念传播算法（Belief Propagation）。</li><li>近似推断：如马尔可夫链蒙特卡罗方法（Markov Chain Monte Carlo,MCMC）和粒子过滤（Particle Filtering）。</li></ul><h3 id="决策树decision-tree">2.5. 决策树（Decision Tree）</h3><p>决策树是一种监督学习算法，广泛用于分类和回归任务。决策树通过一系列的特征划分逐步将数据集分成不同的子集，从而生成树状的结构，最终可以对输入样本进行分类或预测。其结构类似于树形图，由节点和分支组成：</p><ul><li><strong>根节点（RootNode）</strong>：包含整个数据集，代表模型开始进行决策的地方。</li><li><strong>内部节点（InternalNodes）</strong>：表示根据某个特征进行的决策（如按年龄划分，收入划分等）。</li><li><strong>叶节点（LeafNodes）</strong>：代表最终的输出（分类标签或回归值）。</li></ul><h4 id="建树过程">建树过程</h4><ol type="1"><li><p><strong>选择划分特征。</strong>在每一步，决策树会选择一个最优特征将数据集划分成若干子集。这个选择标准可以是<strong>信息增益</strong>、<strong>基尼系数</strong>等。</p></li><li><p><strong>划分数据集。</strong>选择最优的特征后，数据集被划分为若干子集。决策树的每一个子节点对应一个划分后的子集。</p></li><li><p><strong>递归地构建子树。</strong>对于每个子节点，重复第1步和第2步，直到满足停止条件。停止条件一般有以下几种：</p></li></ol><ul><li>节点的所有样本属于同一类别（即纯节点）。</li><li>达到树的最大深度。</li><li>节点中的样本数量少于某个阈值。</li></ul><ol start="4" type="1"><li><strong>生成叶节点。</strong>当递归停止时，生成叶节点，叶节点代表最终的决策结果（分类标签或回归值）。</li></ol><h4 id="决策树生成的算法">决策树生成的算法</h4><ol type="1"><li><p><strong>ID3算法（Iterative Dichotomiser 3）。</strong>该算法使用<strong>信息增益</strong>作为选择特征的标准，选择信息增益最大的特征进行划分。</p></li><li><p><strong>C4.5算法。</strong>这是ID3算法的改进版本，使用<strong>信息增益比</strong>作为划分标准，以避免ID3算法偏向多值特征的问题。</p></li><li><p><strong>CART算法（Classification and Regression Tree）。</strong>适用于分类和回归问题。对于分类任务，CART使用<strong>基尼指数</strong>选择特征；对于回归任务，使用<strong>最小方差</strong>来进行划分。</p></li></ol><h4 id="决策树的剪枝">决策树的剪枝</h4><p>通过<strong>剪枝</strong>防止过拟合。</p><p><strong>预剪枝</strong>是指在决策树生成的过程中，对每个节点在划分前先进行估计，若当前节点的划分不能带来决策树泛化性能提升，则停止划分，并将当前节点标记为叶子节点；此时可能存在不同类别的样本同时存于同个节点中，按照多数投票的原则判断节点所属类别</p><p>预剪枝对于何时停止决策树的生长：</p><ol type="1"><li><p>当树达到一定深度</p></li><li><p>当到达当前节点的样本数量小于某个阈值</p></li><li><p>计算每次分裂对测试集的准确度提升，小于某个阈值时停止</p></li></ol><p><strong>后剪枝</strong>则是先从训练集生成一棵完整的决策树，然后自底向上地对<strong>非叶子节点</strong>进行考察，若该节点对应的<strong>子树替换成叶子结点</strong>能带来泛化性能提升，则将该子树替换为叶子节点。</p><h4 id="熵联合熵条件熵kl散度信息增益信息增益比gini系数">熵、联合熵、条件熵、KL散度、信息增益、信息增益比、gini系数</h4><p><strong>熵</strong></p><p>熵（entropy）是表示随机变量不确定性的度量， <span class="math inline">\(X\)</span>是一个取有限个值的离散随机变量，其概率分布为 <span class="math display">\[P(X = x_i) = p_i, \ i=1,2,\cdots,n\]</span> 则随机变量 <span class="math inline">\(X\)</span> 的熵定义为<span class="math display">\[H(X) = -\sum_{i=1}^{n} p_i {\rm log } \ p_i\]</span> 熵越大，随机变量的不确定性就越大。</p><p>而熵其实表示的是一个系统的平均信息量。<strong>自信息量</strong>是用来描述某一事件带来的信息量大小<span class="math display">\[I = - {\rm log} \ p_i\]</span>通常以2为底，单位是bit；事件的概率越低，那么该事件发生时带来的信息量越大。而通常我们衡量整个系统的信息量，系统存在多个事件<span class="math inline">\(X=\{x_1,\cdots,x_n\}\)</span>，每个事件的概率分布<span class="math inline">\(P=\{p_1,\cdots,p_n\}\)</span>，<strong>熵是整个系统的平均信息量</strong> 。</p><p><strong>联合熵</strong>：将一维随机变量分布推广到多维随机变量分布<span class="math display">\[H(X,Y) = -\sum\limits_{x,y} p(x,y)\ {\rm log}\ p(x,y)\]</span> <strong>条件熵</strong>：某个特征A对于数据集D的经验条件熵<span class="math inline">\(H(D|A)\)</span> 为 <span class="math display">\[H(D|A) = - \sum_{i=1}^{n} \frac{|D_i|}{|D|} H(D_i) \\ = - \sum_{i=1}^{n}\frac{|D_i|}{|D|} \lgroup \sum_{k=1}^{K} \frac{|D_{ik}|}{|D_i|} {\rm log} \frac{|D_{ik}|}{|D_i|} \rgroup\]</span> <strong>信息增益</strong>： <span class="math inline">\(g(D,A)\)</span> 定义为数据集D的经验熵 <span class="math inline">\(H(D)\)</span> 与特征A给定条件下D的经验条件熵 <span class="math inline">\(H(D|A)\)</span> 的差 <span class="math display">\[g(D,A) = H(D) - H(D|A)\]</span></p><p><strong>信息增益比</strong>：特征A对于数据集D 的信息增益比定义为<span class="math display">\[g_R(D|A) = \frac{g(D|A)}{H_A(D)}\]</span> 其中 <span class="math display">\[H_A{(D)} = - \sum_{i=1}^{n} \frac{|D_i|}{|D|} {\rm log }\frac{|D_i|}{|D|}\]</span> 为数据集D关于A的取值熵；n为特征A在D上的取值数目；</p><p><strong>Gini系数</strong>：描述数据的不确定性。数据集D的Gini系数为<span class="math display">\[{\rm Gini}(D) = 1 - \sum_{k=1}^{K}(\frac{|C_k|}{|D|})^2\]</span> 其中 <span class="math inline">\(C_k\)</span>是D中第k类的样本子集，K是类的个数。例如二分类问题，K=2。基尼系数越大，样本集合的不确定性也就越大，这一点与熵相似。基尼系数Gini(D,A)表示经A=a分割后集合D的不确定性。</p><p><strong>交叉熵</strong>：刻画两个概率分布之间的距离，通过q来表示p的交叉熵为；一般<strong>p(x)为真实分布</strong>，<strong>q(x)为预测分布</strong></p><p>交叉熵不对称。交叉熵越小，概率分布越接近 <span class="math display">\[H(p,q) = - \sum\limits_{x} p(x) {\rm log } \ q(x)\]</span></p><p><strong>KL散度/相对熵</strong></p><p><span class="math display">\[D_{K L}(p \| q)=\sum_{i=1}^{n} p\left(x_{i}\right) \log\left(\frac{p\left(x_{i}\right)}{q\left(x_{i}\right)}\right)\]</span>n表示事件可能发生的情况总数，KL散度的值越小表示两个分布越接近。 <span class="math display">\[D_{KL}(p||q) = H(p,q) - H(p)\]</span></p><p>机器学习中，我们常常使用KL散度来评估predict和label之间的差别，但是由于KL散度的后半部分是一个常量，所以我们常常将前半部分的交叉熵作为损失函数，其实二者是一样的。</p><p><strong>ID3 最大信息增益</strong></p><p>信息增益 <span class="math inline">\(g(D,A)\)</span>定义为数据集D的经验熵 <span class="math inline">\(H(D)\)</span>与特征A给定条件下D的经验条件熵 <span class="math inline">\(H(D|A)\)</span> 的差 <span class="math display">\[g(D,A) = H(D) - H(D|A)\]</span> 选择 <span class="math inline">\(g(D,A)\)</span>最大的特征，所有样本根据此特征，划分到不同的节点上。在经验熵不为0的节点中继续生长。ID3算法只有树的生成，容易产生过拟合。</p><p><strong>C4.5 最大信息增益比</strong></p><p>因为信息增益对取值数目多的属性有所偏好，为了减少这种偏好带来的影响，使用信息增益比来选择最优划分属性。</p><p><strong>CART 基尼指数</strong></p><p>基尼系数Gini（D）用来表示集合D的不确定性。CART在每一次迭代中选择划分后<strong>基尼指数最小</strong>的特征及其对应的切分点进行分类。CART是一颗二叉树，每次将数据按特征A的区分分成两份，分别进入左右子树。</p><h4 id="优缺点-2">优缺点</h4><p><strong>优点</strong></p><ul><li>简单易懂，直观可解释。</li><li>适合处理类别和数值型数据。</li><li>可以处理不完整的数据。</li><li>无需进行特征缩放。</li></ul><p><strong>缺点</strong></p><ul><li>容易过拟合，特别是当树很深时。</li><li>对数据的微小变化敏感，可能生成完全不同的树。</li></ul><h3 id="随机森林random-forest">2.6. 随机森林（Random Forest）</h3><p><strong>随机森林</strong>是一种基于<strong>集成学习（EnsembleLearning）</strong>思想的算法，它通过构建多个决策树进行训练，并结合这些树的结果进行预测。随机森林可以用于<strong>分类</strong>和<strong>回归</strong>任务。由于它结合了多棵树的优点，具有较强的鲁棒性、抗噪能力，且能够有效防止<strong>过拟合</strong>。</p><p>随机森林通过组合多棵决策树的输出结果来提高预测的准确性。其基本原理主要包括两个关键部分：</p><ol type="1"><li><p><strong>Bootstrap抽样</strong>：从原始数据集中随机选择多个有放回的子集，用于训练每一棵树。这意味着每棵树看到的数据并不完全相同，有些样本会被多次选择，而有些样本可能不会被选中。</p></li><li><p><strong>特征随机性</strong>：在每次节点分裂时，随机选择特征的子集进行划分，而不是使用所有特征。这进一步增加了树的多样性，减少了单棵树的过拟合风险。</p></li></ol><p>随机森林使用了Bagging的思想，通过对数据再抽样，然后在每组样本上训练出来的模型取平均。Bagging是降低方差，防止过拟合。对n个独立不相关的模型的预测结果取平均，方差近似为原来单个模型的<span class="math inline">\(1/n\)</span> 。</p><h4 id="工作流程">工作流程</h4><p>随机森林的工作可以分为以下步骤：</p><ol type="1"><li><strong>构造随机森林</strong><ul><li><strong>数据集随机抽样</strong>：从训练数据集中进行Bootstrap抽样，生成若干个随机的子集，每个子集用于训练一棵决策树。</li><li><strong>构建决策树</strong>：对于每个样本子集，训练一棵决策树。不同于传统决策树，每次分裂时会随机选择特征子集来决定最优的划分。</li></ul></li><li><strong>训练过程</strong><ul><li>对于每个决策树，通过数据子集和随机选择的特征构造完全树，通常不对树进行剪枝。</li><li>每棵树会独立学习数据集中的模式。</li></ul></li><li><strong>预测过程</strong><ul><li><strong>分类问题</strong>：随机森林中的每棵树都会对输入样本进行预测，然后通过<strong>投票</strong>机制决定最终的分类结果。即选择预测次数最多的类别作为最终分类。</li><li><strong>回归问题</strong>：随机森林中的每棵树都会给出一个预测值，最终的预测结果通过所有树的<strong>平均值</strong>来决定。</li></ul></li></ol><h4 id="可否将rf的基分类模型由决策树改成线性模型或者knn">可否将RF的基分类模型由决策树改成线性模型或者knn？</h4><p>随机森林属于bagging类的集成学习方法，主要好处是减小集成后分类器的方差，比基分类器的方差小。所以Bagging所采用的的基分类器最好是本身对样本分布较为敏感（不稳定分类器），这样bagging才能体现效果。而线性分类器和KNN属于较为稳定的分类器，本身方差不大，所以将他们作为基分类器使用bagging不能再原基分类器的基础上获得更好的表现。相反地，可能因为bagging的采样而使得训练中难以收敛从而增大集成分类器的偏差。</p><h4 id="优缺点-3">优缺点</h4><p><strong>优点</strong></p><ol type="1"><li><strong>高精度</strong>：在多数情况下，随机森林的准确性高于单个决策树，特别是在大数据集或复杂任务上。</li><li><strong>抗过拟合能力强</strong>：由于集成了多棵树，随机森林的模型复杂度较低，避免了单棵决策树容易过拟合的问题。</li><li><strong>处理高维特征能力强</strong>：随机森林可以处理包含大量特征的数据集，且不需要对特征进行筛选。</li><li><strong>处理缺失值和不平衡数据</strong>：随机森林能够处理部分缺失的数据和类别不平衡的问题。</li><li><strong>重要特征选择</strong>：通过计算特征的重要性，随机森林可以帮助确定哪些特征对预测最为重要。</li></ol><p><strong>缺点</strong></p><ol type="1"><li><strong>计算开销大</strong>：构造大量的决策树需要较多的计算资源，尤其是在数据集较大时，训练时间可能较长。</li><li><strong>模型解释性较弱</strong>：相比于单棵决策树，随机森林的结构较为复杂，不容易解释单个决策路径。</li><li><strong>预测时间较慢</strong>：虽然训练时间较长，但随机森林的预测时间也会因为多棵树的组合而增加，尤其在实时预测系统中表现较为明显。</li></ol><h3 id="gradient-boosting梯度提升">2.7. GradientBoosting（梯度提升）</h3><p>Gradient Boosting是一种<strong>集成学习方法</strong>，通过将多个弱学习器（通常是决策树）串联起来，形成一个强大的预测模型。它逐步改进模型的预测能力，通过每一步的模型修正先前模型中的错误。常见的变种包括XGBoost，LightGBM，CatBoost等。</p><h4 id="工作原理">工作原理</h4><p>Gradient Boosting的核心思想是：每个新的模型都尝试减少前一组模型的<strong>残差（误差）</strong>。具体流程如下：1.<strong>初始模型</strong>：从简单的模型（如决策树）开始，预测目标变量。2.<strong>计算残差</strong>：计算模型的预测值和真实值之间的误差（残差）。3.<strong>构建新模型</strong>：新模型拟合前一个模型的残差（目标是修正错误）。新模型用来减少误差而不是直接预测目标变量。4.<strong>迭代过程</strong>：重复第2步和第3步，不断加入新的模型来修正前一轮模型的误差。5.<strong>加权求和</strong>：最终的预测结果是所有模型加权后的和，通常最后使用的是步长（learningrate）来控制加权的比例。</p><h4 id="算法步骤">算法步骤</h4><p>以回归为例，假设我们有训练数据 $ (x_1, y_1), (x_2, y_2), , (x_n, y_n)$，目标是拟合一个模型 $ F(x) $ 来最小化损失函数 $ L(y, F(x)) $：</p><ul><li><p><strong>步骤1</strong>：初始化模型 $ F_0(x)$，使其最小化损失函数（通常为目标值的均值）。</p><p><span class="math display">\[F_0(x) = \arg \min \sum_{i=1}^{n} L(y_i, F(x_i))\]</span></p></li><li><p><strong>步骤2</strong>：对每个迭代步骤 $ m = 1, 2, ..., M$，执行以下步骤：</p><ol type="1"><li>计算残差 $ r_{im} = - $，即目标函数的梯度。</li><li>训练一个新的模型 $ h_m(x) $ 来拟合残差 $ r_{im} $。</li><li>更新模型 $ F_{m}(x) = F_{m-1}(x) + h_m(x) $，其中 $ $是学习率，控制每一步的模型更新步长。</li></ol></li><li><p><strong>步骤3</strong>：最终模型为 $ F_M(x) = F_0(x) + _{m=1}^{M}h_m(x) $。</p></li></ul><p>负梯度方向就是残差拟合的方向，拟合负梯度就是在最小化损失函数，这使得每一轮的弱学习器都朝着减少预测误差的方向优化。</p><h4 id="优缺点-4">优缺点</h4><p><strong>优点：</strong> -<strong>强大的预测能力</strong>：通过修正之前模型的残差，最终模型性能通常非常优秀。-<strong>处理偏差-方差权衡</strong>：通过调整基学习器的复杂度（如决策树的深度）和学习率，GradientBoosting 可以很好地在偏差与方差之间取得平衡。</p><p><strong>缺点：</strong> -<strong>训练时间长</strong>：由于是逐步构建模型，训练时间相对较长。 -<strong>参数敏感</strong>：模型对超参数（如学习率、树的深度等）比较敏感，通常需要仔细调参。-<strong>容易过拟合</strong>：如果基学习器数量过多或者树太深，模型可能会过拟合，需要通过正则化手段（如早停、学习率调整等）来避免。</p><h4 id="梯度提升和梯度下降有什么区别和联系">梯度提升和梯度下降有什么区别和联系？</h4><p>两者都是在每一轮迭代中，利用损失函数相对于模型的负梯度方向的信息来对当前模型进行更新。梯度下降是一种优化算法，用于找到函数的最小值或最大值。通常应用在单一模型的训练过程中，用于优化模型的参数（如线性回归、神经网络等）。它通过计算损失函数的梯度，逐步调整模型参数，直到损失函数达到局部或全局最小值。梯度提升是一种集成学习方法，通过多个弱学习器（如决策树）的组合来提升预测性能。它逐步构建多个模型，每个模型通过拟合前一个模型的残差（即负梯度），从而改进预测结果。梯度提升算法使用梯度下降的思想来优化整个模型的性能，但其目标不是直接优化模型参数，而是优化模型集成的效果。</p><table><colgroup><col style="width: 14%"><col style="width: 43%"><col style="width: 42%"></colgroup><thead><tr><th>特性</th><th>梯度下降（Gradient Descent）-优化算法</th><th>梯度提升（Gradient Boosting） - 集成学习方法</th></tr></thead><tbody><tr><td><strong>作用对象</strong></td><td>单一模型的参数</td><td>集成模型中的多个弱学习器（如决策树）</td></tr><tr><td><strong>目标</strong></td><td>优化模型参数，使损失函数最小化</td><td>通过弱学习器的逐步组合，减少整体预测误差</td></tr><tr><td><strong>更新方式</strong></td><td>逐步更新模型的参数，沿着负梯度方向调整</td><td>逐步添加弱学习器，拟合前一轮的残差（负梯度）</td></tr><tr><td><strong>算法流程</strong></td><td>计算梯度 -&gt; 更新参数 -&gt; 重复迭代</td><td>计算残差（负梯度）-&gt; 训练新模型拟合残差 -&gt; 组合模型</td></tr><tr><td><strong>使用场景</strong></td><td>线性回归、神经网络等优化问题</td><td>回归、分类等场景下的集成学习方法，如 XGBoost、LightGBM</td></tr></tbody></table><h4 id="boosting和bagging的异同">Boosting和Bagging的异同？</h4><p>Bagging通过模型集成降低方差，提高弱分类器的性能。</p><p>Boosting通过模型集成降低偏差，提高弱分类器的性能。</p><table><colgroup><col style="width: 15%"><col style="width: 42%"><col style="width: 42%"></colgroup><thead><tr><th>特性</th><th><strong>Bagging</strong></th><th><strong>Boosting</strong></th></tr></thead><tbody><tr><td><strong>工作机制</strong></td><td><strong>并行训练</strong>：多个弱学习器<strong>独立</strong>训练，并行执行。</td><td><strong>串行训练</strong>：多个弱学习器<strong>顺序</strong>训练，每个模型依赖上一个模型的结果。</td></tr><tr><td><strong>样本处理</strong></td><td><strong>有放回随机抽样</strong>：每个弱学习器在一个随机采样的子集上训练。</td><td><strong>加权训练</strong>：每个弱学习器在原始训练集上训练，但每个样本有不同的权重。</td></tr><tr><td><strong>目标</strong></td><td>通过减少模型的<strong>方差</strong>，提升模型的稳定性和泛化能力。</td><td>通过逐步减少<strong>偏差和方差</strong>，提升模型的整体准确性。</td></tr><tr><td><strong>模型关注点</strong></td><td>各模型<strong>独立</strong>，无关其他模型的错误。</td><td>每个模型<strong>依赖</strong>前一个模型，集中在纠正之前模型的错误。</td></tr><tr><td><strong>弱学习器权重</strong></td><td>各个弱学习器权重相同，预测结果通过简单投票（分类）或平均（回归）来合并。</td><td>每个弱学习器根据其性能赋予不同权重，表现好的学习器权重大。</td></tr><tr><td><strong>处理偏差/方差</strong></td><td><strong>主要减少方差</strong>，通过减少模型对数据噪声的敏感性来提高泛化能力。</td><td><strong>同时减少偏差和方差</strong>，通过纠正错误逐步改进模型。</td></tr><tr><td><strong>算法代表</strong></td><td><strong>Random Forest</strong> 是 Bagging 的典型算法。</td><td><strong>AdaBoost</strong>、<strong>Gradient Boosting</strong> 是Boosting 的典型算法。</td></tr><tr><td><strong>并行性</strong></td><td>支持并行训练，因各模型独立，可在多核或分布式系统上加速。</td><td>顺序训练，难以并行化，因为每个学习器依赖于前一个学习器的结果。</td></tr></tbody></table><h3 id="k-means">2.8. K-means</h3><p><strong>K-means</strong>是一种常用的<strong>无监督学习算法</strong>，用于<strong>聚类分析</strong>。它试图将数据集划分为$ K $个互不重叠的簇（Cluster），每个簇由具有相似特征的数据点组成。K-means的目标是最小化簇内数据点与簇中心（质心，Centroid）之间的距离，从而使簇内的数据点更加紧密，簇间的数据点差异更大。</p><h4 id="k-means-算法的步骤">K-means 算法的步骤</h4><ol type="1"><li><p><strong>确定簇的数量 $ K $</strong>：事先指定 $ K$，即要将数据集划分成 $ K $ 个簇。</p></li><li><p><strong>随机初始化质心</strong>：随机选择 $ K $个数据点作为初始的质心（Centroid），质心是用于表示每个簇中心的点。</p></li><li><p><strong>分配数据点到簇</strong>：将每个数据点分配给距离其最近的质心，形成$ K $ 个簇。距离通常通过<strong>欧氏距离</strong>计算： <span class="math display">\[  \text{距离} = \sqrt{(x_1 - \mu_1)^2 + (x_2 - \mu_2)^2 + \cdots + (x_n- \mu_n)^2}  \]</span> 其中，$ x_i $ 表示数据点的坐标，$ _i $表示质心的坐标。</p></li><li><p><strong>更新质心</strong>：对每个簇，重新计算该簇所有数据点的平均值，作为该簇的新质心：<span class="math display">\[  \mu_j = \frac{1}{|C_j|} \sum_{x \in C_j} x  \]</span> 其中，$ _j $ 是簇 $ C_j $ 的新质心，$ |C_j| $是簇中数据点的数量。</p></li><li><p><strong>重复分配和更新</strong>：反复执行<strong>分配数据点到簇</strong>和<strong>更新质心</strong>这两个步骤，直到质心不再变化或变化很小，或者达到指定的迭代次数。</p></li><li><p><strong>算法结束</strong>：当质心收敛时，K-means算法结束，最终形成 $ K $ 个簇。</p></li></ol><p>K-means 的目标是<strong>最小化簇内平方误差和（Sum of Squared Errors,SSE）</strong>，即每个簇内数据点与其质心的距离平方和。数学形式为： <span class="math display">\[SSE = \sum_{j=1}^{K} \sum_{x \in C_j} ||x - \mu_j||^2\]</span> 其中，$ K $ 是簇的数量，$ C_j $ 是第 $ j $ 个簇，$ x $是簇内的数据点，$ _j $ 是第 $ j $ 个簇的质心。</p><h4 id="优缺点-5">优缺点</h4><p><strong>优点：</strong></p><ol type="1"><li><p><strong>简单易实现</strong>：K-means算法的流程清晰明了，容易理解和实现。</p></li><li><p><strong>计算效率高</strong>：K-means 的时间复杂度为 $ O(n K d t)$，其中 $ n $ 是数据点数，$ K $ 是簇的数量，$ d $ 是特征维度，$ t $是迭代次数。适合大规模数据集的处理。</p></li><li><p><strong>适用于凸形簇</strong>：对于形状规则、分布较为均匀的数据，K-means能很好地分离不同的簇。</p></li></ol><p><strong>缺点：</strong> 1. <strong>需要预先指定 K值</strong>：K-means 需要事先确定聚类数 $ K$，但是在实际应用中，往往无法确定数据集的簇数量。</p><ol start="2" type="1"><li><p><strong>对初始质心敏感</strong>：K-means的结果依赖于初始质心的选择，可能会陷入局部最优解。为了解决这个问题，可以使用<strong>K-means++</strong>，通过一种巧妙的质心初始化方法来改进初始质心的选择。kmeans聚类属于启发式方法，不能保证收敛到全局最优，初始中心的选择会直接影响聚类结果。</p></li><li><p><strong>只能找到线性可分的簇</strong>：K-means只能找到形状为圆形或球形的簇，不能很好地处理非凸形的簇。</p></li><li><p><strong>对噪声和离群点敏感</strong>：K-means使用欧氏距离来衡量相似度，容易受到极端数据点（离群点）的影响，导致结果偏差。</p></li><li><p><strong>簇大小不平衡问题</strong>： 对于大小相差较大的簇，K-means可能无法正确聚类，较大的簇可能会掩盖较小的簇。</p></li></ol><h4 id="k-means-1">K-means++</h4><p><strong>K-means++</strong> 是 K-means算法的一种改进版本，主要针对<strong>初始质心选择</strong>问题。K-means++的初始化步骤如下： 1. 随机选择一个数据点作为第一个质心。 2.对于每一个剩下的数据点，计算它与最近质心的距离平方。 3.按照距离平方的概率，随机选择下一个质心。距离较大的数据点有更大的概率被选为质心。4. 重复第 2 和第 3 步，直到选出 $ K $ 个质心。</p><p>通过这种初始化方法，K-means++ 可以有效避免 K-means对初始质心的敏感性，减少迭代次数，并提高聚类效果。</p><h4 id="常用聚类评估指标">常用聚类评估指标</h4><h5 id="内部评估指标">内部评估指标</h5><p><strong>1. 轮廓系数（Silhouette Coefficient）</strong></p><p><strong>轮廓系数</strong>结合了簇内距离和簇间距离，用来评估每个数据点的聚类效果。其定义为：<span class="math display">\[s(i) = \frac{b(i) - a(i)}{\max(a(i), b(i))}\]</span> 其中： - $ a(i) $：数据点 $ i $到同簇内其他数据点的平均距离（簇内距离）。 - $ b(i) $：数据点 $ i $到最近的其他簇的质心的平均距离（簇间距离）。</p><p>轮廓系数的取值范围为 <span class="math inline">\([-1, 1]\)</span>： -$ s(i) $：表示该点聚类效果良好，距离同簇点近，离其他簇远。 - $ s(i)$：表示该点处于两个簇的边界上。 - $ s(i)$：表示该点可能被错误分类到其他簇。</p><p>通过计算所有数据点的平均轮廓系数来评估整个聚类模型的效果，轮廓系数越大越好。</p><p><strong>2. SSE（Sum of Squared Errors，簇内误差平方和）</strong></p><p>SSE 是 K-means等聚类算法中常用的评估指标，表示簇内所有数据点与其质心的距离平方和。公式为：<span class="math display">\[SSE = \sum_{j=1}^{K} \sum_{x \in C_j} ||x - \mu_j||^2\]</span> SSE 越小，表示数据点与质心越接近，聚类效果越好。SSE 随着 $ K $值的增加通常会减小，因此不能单独依赖 SSE 来选择最优 $ K $。</p><p><strong>3. Calinski-Harabasz 指数（方差比准则）</strong></p><p><strong>Calinski-Harabasz 指数</strong>衡量簇的分离度和紧密度之比：<span class="math display">\[CH = \frac{\text{簇间方差}}{\text{簇内方差}} \times \frac{n - K}{K - 1}.\]</span> 其中，$ n $ 是数据点总数，$ K $是簇的数量。簇间方差越大、簇内方差越小，表示聚类效果越好。Calinski-Harabasz指数越大，聚类效果越好。</p><p><strong>4. Davies-Bouldin 指数</strong></p><p><strong>Davies-Bouldin指数</strong>衡量的是簇间相似性和簇内相似性的比值： <span class="math display">\[DB = \frac{1}{K} \sum_{i=1}^{K} \max_{j \neq i} \left( \frac{\sigma_i +\sigma_j}{d(\mu_i, \mu_j)} \right)\]</span> 其中： - $ _i $ 是簇 $ i $ 的簇内数据点与质心的平均距离。 - $d(_i, _j) $ 是簇 $ i $ 和 $ j $ 质心之间的距离。</p><p>Davies-Bouldin指数越小，表示簇内距离较小且簇间距离较大，聚类效果越好。</p><p><strong>5. 轮廓系数图（Elbow Method）</strong></p><p>轮廓系数图或肘部法（Elbow Method）是一种用于确定最优 $ K $值的方法。绘制不同 $ K $ 值对应的 SSE曲线，曲线通常是先快速下降，然后变得平缓。最优的 $ K $通常位于曲线的“肘部”位置，即 SSE 开始平缓下降的点。</p><h5 id="外部评估指标">外部评估指标</h5><p>外部评估指标用于在有真实标签的情况下评估聚类结果的效果。它通过将聚类结果与真实的类别标签进行对比，计算聚类效果的准确性。这类指标适用于有标注的数据集，常见的外部评估指标有：</p><p><strong>1. 调整兰德指数（Adjusted Rand Index, ARI）</strong></p><p><strong>调整兰德指数</strong>是基于<strong>兰德指数（RandIndex）</strong>的一种改进，用来衡量聚类结果与真实标签之间的一致性。其计算依据是聚类结果中的点对是否被正确地分配到同一簇或不同簇。</p><p><strong>RandIndex</strong>计算所有点对之间的组合，统计两点是否被正确地分配到同一簇或不同簇：<span class="math display">\[  RI = \frac{a + b}{a + b + c + d}  \]</span> 其中： - $ a $：同属于一个簇，且真实标签也相同的点对数。 - $b $：属于不同簇，且真实标签也不同的点对数。 - $ c$：属于同一个簇，但真实标签不同的点对数。 - $ d$：属于不同簇，但真实标签相同的点对数。</p><p>兰德指数的取值范围是<span class="math inline">\([0,1]\)</span>。兰德指数的一个主要问题是，即使聚类结果是随机的，RI也往往会得到一个较高的值，而不是真正反映聚类效果。</p><p>由于 Rand Index没有考虑随机聚类可能带来的效果，<strong>调整兰德指数（ARI）</strong>引入了随机化校正：<span class="math display">\[ARI = \frac{RI - E[RI]}{\max(RI) - E[RI]}\]</span></p><p><strong>2. 互信息（Mutual Information, MI）与归一化互信息（NormalizedMutual Information, NMI）</strong></p><p><strong>互信息（Mutual Information, MI）</strong>用于衡量聚类结果与真实标签之间的信息共享程度。它基于信息论，反映了一个聚类结果中有多少信息可以解释真实的标签分布。</p><p>互信息的公式为： <span class="math display">\[MI(U, V) = \sum_{i=1}^{|U|} \sum_{j=1}^{|V|} P(U_i, V_j) \log\frac{P(U_i, V_j)}{P(U_i) P(V_j)}\]</span> 其中： - $ U $ 是聚类结果的簇分布，$ V $ 是真实标签的分布。 -$ P(U_i) $ 是聚类簇 $ U_i $ 的概率。 - $ P(V_j) $ 是真实标签 $ V_j $的概率。 - $ P(U_i, V_j) $ 是数据同时属于簇 $ U_i $ 和真实标签 $ V_j $的联合概率。</p><p>互信息的取值越高，说明聚类结果与真实标签之间的关联性越强。</p><p>为了消除样本数量对互信息的影响，可以对 MI进行归一化，得到<strong>归一化互信息（NMI）</strong>，其定义为： <span class="math display">\[NMI(U, V) = \frac{MI(U, V)}{\sqrt{H(U) H(V)}}\]</span> 其中，$ H(U) $ 和 $ H(V) $分别是聚类结果和真实标签的熵，表示其不确定性。 NMI 的取值范围是 <span class="math inline">\([0, 1]\)</span>，$ 1 $表示聚类结果与真实标签完全匹配。$ 0 $表示聚类结果与真实标签没有相关性。</p><p><strong>3. 同质性、完整性和 V-measure</strong></p><p>这三者是相互关联的聚类评估指标，分别用于衡量聚类结果与真实标签之间的不同特性。</p><p><strong>同质性</strong>表示每个簇内部的所有数据点都属于同一个真实类别。若每个聚类的簇仅包含单一类别的数据点，则该聚类是同质的。公式为：<span class="math display">\[H = 1 - \frac{H(C|K)}{H(C)}\]</span> 其中，$ H(C|K) $ 是给定聚类结果 $ K $ 后的真实标签 $ C $的条件熵。$ H(C) $ 是真实标签 $ C $ 的熵。</p><p>同质性越高，表示同簇数据点的真实类别越统一，理想情况下，应该接近1。</p><p><strong>完整性</strong>表示真实类别的所有数据点都被划分到同一个簇中。若每个真实类别的所有数据点都集中在某个簇中，则聚类具有完整性。公式为：<span class="math display">\[C = 1 - \frac{H(K|C)}{H(K)}\]</span> 其中： - $ H(K|C) $ 是给定真实标签 $ C $ 后的聚类结果 $ K $的条件熵。 - $ H(K) $ 是聚类结果 $ K $ 的熵。</p><p>完整性越高，表示聚类结果能更好地包含每个真实类别的数据点。</p><p><strong>V-measure</strong>是同质性和完整性的调和平均数，用来综合衡量聚类的整体效果。公式为：<span class="math display">\[V = 2 \times \frac{H \times C}{H + C}\]</span> V-measure 的取值范围是 <span class="math inline">\([0,1]\)</span>，其含义与同质性和完整性相似，越接近1，表示聚类效果越好。V-measure兼顾了同质性和完整性，因此是一个较为平衡的评估指标。</p><p><strong>4. 纯度</strong>（Purity）</p><p>把每个簇中最多的类作为这个簇所代表的类，然后计算正确分配的类的数量，然后除以<span class="math inline">\(N\)</span> 。 <span class="math display">\[  (\Omega, \mathbb{C})=\frac{1}{N} \sum_{k} \max _{j}\left|\omega_{k}\cap c_{j}\right|  \]</span> 其中 <span class="math inline">\(\Omega=\left\{\omega_{1},\omega_{2}, \ldots, \omega_{K}\right\}\)</span> 是聚类结果的集合 <span class="math inline">\(\omega_{k}\)</span>表示第k个聚类的集合；<span class="math inline">\(\mathbb{C}=\left\{c_{1}, c_{2}, \ldots,c_{J}\right\}\)</span> 是原始分类的集合，<span class="math inline">\(c_j\)</span>表示第j个分类的集合。</p><p><img src="/2024/10/08/statistic/pure.png"></p><p>purity优点是方便计算，值在0~1之间；缺点：当簇的数量很多的时候，容易达到较高的纯度——特别是，如果每个文档都被分到独立的一个簇中，那么计算得到的纯度就会是1。因此，不能简单用纯度来衡量聚类质量与聚类数量之间的关系。</p><h4 id="选择评估指标的依据">选择评估指标的依据</h4><ol type="1"><li><strong>无监督聚类</strong>：如果没有真实标签，推荐使用<strong>轮廓系数</strong>、<strong>Calinski-Harabasz指数</strong>和<strong>Davies-Bouldin 指数</strong>等内部评估指标。</li><li><strong>有监督聚类</strong>：如果有真实标签，可以使用<strong>ARI</strong>、<strong>NMI</strong>、<strong>V-measure</strong>等外部评估指标。</li><li><strong>最优簇数量</strong>：使用<strong>肘部法</strong>和<strong>轮廓系数图</strong>来帮助选择最优簇数量$ K $。</li></ol><h4 id="常见的聚类算法">常见的聚类算法</h4><ul><li>基于划分的算法：如K-means，简单高效，但对簇形状和初始条件敏感。</li><li>基于层次的算法：如层次聚类，自上而下或者自下而上。能构建簇的层次结构，但计算复杂度较高。</li><li>基于密度的算法：如DBSCAN，高密度区域找到簇，适合发现任意形状的簇并处理噪声，但对参数敏感。</li><li>基于网格的算法：如STING，适合大规模数据集，速度快，但依赖网格划分方式。</li><li>基于模型的算法：如高斯混合模型GMM，假设数据来自多个高斯分布的混合，每个簇对应一个高斯分布。通过<strong>期望最大化算法（EM算法）</strong>来估计簇的参数和分布。但对模型假设和参数选择敏感。</li><li>谱聚类。基于图论，通过构造相似度矩阵并对其进行谱分解，从而将数据映射到低维空间，在低维空间中进行聚类。</li></ul><h3 id="主成分分析-pca">2.9. 主成分分析 PCA</h3><p><strong>主成分分析（PCA, Principal Component Analysis）</strong>是一种常用的<strong>降维</strong>和<strong>数据分析</strong>技术。它的主要目标是通过线性变换，将原始的高维数据映射到较低维度的空间中，同时尽可能保持数据的<strong>方差</strong>或信息量。这种方法在高维数据集中的应用非常广泛，可以帮助减少特征数量、可视化数据结构，或是去除噪声等。</p><p>PCA的核心思想是通过构建一组新的<strong>正交基向量</strong>，即<strong>主成分（PrincipalComponents）</strong>，这些主成分是数据集中特征方差最大的方向。在降维的过程中，PCA会根据数据的方差信息，选取前几个主成分来表示数据，从而达到降维的目的。</p><h4 id="pca的步骤">PCA的步骤</h4><p>PCA 的计算过程大致可以分为以下几个步骤：</p><p><strong>1. 标准化数据。</strong> 在使用 PCA之前，通常需要将每个特征进行标准化处理，即将数据归一化为均值为 0、方差为1 的形式。这是因为 PCA依赖于特征的方差，而不同尺度的特征（如米和千克）会对结果产生不公平的影响。</p><p><strong>2. 计算协方差矩阵。</strong> PCA的核心是对数据进行线性变换，因此需要计算数据的<strong>协方差矩阵</strong>，以了解各特征之间的线性关系。协方差矩阵的元素描述了两个特征间的协方差，即它们的联合变化情况。</p><p><strong>3. 计算特征值和特征向量。</strong>对协方差矩阵进行<strong>特征值分解</strong>，得到特征值和特征向量。特征向量代表主成分的方向，而特征值表示沿着该方向的方差大小。特征值越大，说明主成分捕捉到的方差越多。<span class="math display">\[\text{Cov}(X) v = \lambda v\]</span> 其中，$ v $ 是特征向量，$ $ 是特征值。</p><p><strong>4. 选择主成分。</strong>根据特征值的大小对特征向量排序，选择其中最大的 $ k $个特征向量作为主成分。这些主成分构成新的低维空间，数据将被投影到这些主成分上，从而实现降维。</p><p><strong>5. 将数据投影到主成分空间。</strong>通过选取的主成分矩阵，将原始数据投影到新空间中。假设我们选择了前 $ k $个主成分，则数据 $ X $ 被投影后的新数据表示为： <span class="math display">\[Z = X W,\]</span> 其中， <span class="math inline">\(W\)</span>是主成分向量矩阵。</p><h4 id="pca的几何解释">PCA的几何解释</h4><p>PCA的几何解释是，它通过找到新的坐标轴（即主成分），使得这些新坐标轴是原始数据的最佳线性组合。在新的坐标系下，数据沿着第一个主成分（最大方差方向）投影最多的方差，第二个主成分与第一个正交，捕捉剩余的最大方差，依此类推。</p><ul><li><strong>第一主成分</strong>：方差最大的方向。</li><li><strong>第二主成分</strong>：与第一主成分正交且方差次大的方向。</li><li>以此类推。</li></ul><h4 id="pca的性质">PCA的性质</h4><ul><li><strong>正交性</strong>：各主成分之间是相互正交的，即彼此独立不相关。</li><li><strong>方差解释率</strong>：每个主成分解释了原始数据的方差总量的某一比例。通过特征值的比率可以衡量主成分的重要性。通常会选择能解释大部分方差的前几个主成分，达到降维的效果。</li><li><strong>最大方差方向</strong>：PCA总是试图在数据中找到方差最大的方向作为新的坐标轴，从而保证保留最多的信息。</li></ul><h4 id="优缺点-6">优缺点</h4><p><strong>优点</strong></p><ol type="1"><li><strong>降维效果好</strong>：可以有效减少数据维度，保留尽可能多的有用信息。</li><li><strong>特征解耦</strong>：主成分是线性不相关的，有助于去除多重共线性。</li><li><strong>可视化</strong>：将高维数据映射到低维空间，方便进行可视化分析。</li></ol><p><strong>缺点</strong></p><ol type="1"><li><strong>线性假设</strong>：PCA假设主成分是数据的线性组合，不能捕捉到非线性结构。</li><li><strong>信息损失</strong>：降维过程中可能会丢失部分信息，特别是方差较小的主成分对应的信息。</li><li><strong>解释性差</strong>：PCA转换后的主成分没有原始特征的具体意义，难以解释。</li></ol><h4 id="线性判别分析和主成分分析有何区别和联系">线性判别分析和主成分分析有何区别和联系？</h4><ul><li><p>PCA 是无监督的。PCA忽略类别信息，专注于保持数据的总方差，寻找能捕捉最多信息的方向。选择的是投影后数据方差最大的方向。由于PCA是无监督的，因此假设方差越大，信息量越多，用主成分来表示原始数据可以去除冗余的维度，达到降维。</p></li><li><p>LDA 是有监督的。LDA利用类别标签信息，目标是找到一个投影向量w，使得数据投影后，类间距离最大化，同时类内距离最小化。</p></li></ul><h1 id="二数理统计和优化">二、数理统计和优化</h1><h2 id="常见的分布">1. 常见的分布</h2><p>伯努利分布（0-1分布），Beta分布，二项分布，泊松分布，t分布，多项式分布。详见教材## 2. 参数估计有哪些方法？</p><p><strong>极大似然估计MLE</strong></p><p>在统计学中，常常使用极大似然估计法来估计参数。即找到一组参数，使得在这组参数下，我们数据的似然度（概率）最大。<strong>(极大似然估计：就是利用已知的样本结果信息，反推最具有可能（最大概率）导致这些样本结果出现的模型参数值，即‘模型已定，参数未知’</strong>)</p><p><strong>极大似然估计的前提一定是要假设数据总体的分布，如果不知道数据分布，是无法使用极大似然估计的</strong></p><p>求极大似然估计的步骤</p><p>（1）写出似然函数；</p><p>（2）对似然函数取对数，并整理；</p><p>（3）求导数，令导数为 0，得到似然方程；</p><p>（4）解似然方程，得到的参数。</p><p><strong>最大后验概率估计MAP</strong></p><p><strong>极大似然估计中采样需满足一个重要的假设，就是所有的采样都是独立同分布的。</strong></p><p>那么我们就知道了极大似然估计的核心关键就是对于一些情况，样本太多，无法得出分布的参数值，可以采样小样本后，利用极大似然估计获取假设中分布的参数。</p><p>极大似然估计就是经验风险最小化的一个例子。当模型是条件概率分布、损失函数是对数损失函数时，经验风险最小化就等价于极大似然估计。</p><p>最大后验概率是计算给定数据条件下模型的条件概率，即后验概率。使用模型的先验分布是贝叶斯学习的特点。</p><p><strong>期望极大化EM</strong></p><p>EM算法解决这个的思路是使用启发式的迭代方法，既然我们无法直接求出模型分布参数，那么我们可以先猜想隐含参数（EM算法的 E步），接着基于观察数据和猜测的隐含参数一起来极大化对数似然，求解我们的模型参数（EM算法的M步)。由于我们之前的隐含参数是猜测的，所以此时得到的模型参数一般还不是我们想要的结果。我们基于当前得到的模型参数，继续猜测隐含参数（EM算法的E步），然后继续极大化对数似然，求解我们的模型参数（EM算法的M步)。以此类推，不断的迭代下去，直到模型分布参数基本无变化，算法收敛，找到合适的模型参数。</p><p>一个最直观了解 EM 算法思路的是 K-Means 算法。在 K-Means聚类时，每个聚类簇的质心是隐含数据。我们会假设 K 个初始化质心，即 EM算法的 E步；然后计算得到每个样本最近的质心，并把样本聚类到最近的这个质心，即 EM算法的 M 步。重复这个 E 步和 M 步，直到质心不再变化为止，这样就完成了K-Means 聚类。</p><p>EM算法和极大似然估计的前提是一样的，都要假设数据总体的分布，如果不知道数据分布，是无法使用EM算法的。</p><p>EM算法是通过不断求解下界的极大化逼近求解对数似然函数极大化的算法</p><h2 id="频率学派和贝叶斯学派什么区别">3.频率学派和贝叶斯学派什么区别？</h2><p><strong>频率学派</strong></p><p>频率学派是上帝视角，认为频率是固定的，事件在多次重复实验中趋于一个稳定的值p，那么这个值就是该事件的概率。</p><p>他们认为模型参数是个定值，希望通过类似解方程组的方式从数据中求得该未知数。这就是频率学派使用的参数估计方法-<strong>极大似然估计（MLE）</strong>，这种方法往往在<u>大数据量的情况</u>下可以很好的还原模型的真实情况。</p><p><strong>贝叶斯派</strong></p><p>他们认为世界是不确定的，因获取的信息不同而异。假设对世界先有一个预先的估计，然后通过获取的信息来不断调整之前的预估计。他们认为模型参数源自某种潜在分布，希望从数据中推知该分布。对于数据的观测方式不同或者假设不同，那么推知的该参数也会因此而存在差异。这就是贝叶斯派视角下用来估计参数的常用方法-<strong>最大后验概率估计（MAP）</strong></p><p>这种方法在先验假设比较靠谱的情况下效果显著，随着数据量的增加，先验假设对于模型参数的主导作用会逐渐削弱，相反真实的数据样例会大大占据有利地位。极端情况下，比如把先验假设去掉，或者假设先验满足均匀分布的话，那她和极大似然估计就如出一辙了。</p><h2 id="大数定理和中心极限定理">4. 大数定理和中心极限定理</h2><h2 id="假设检验">5. 假设检验</h2><h2 id="最优化问题">6. 最优化问题</h2><p>详见教材 &lt;&lt;数据科学优化方法&gt;&gt;孙怡帆，中国人民大学出版社，2024.</p><h2 id="优化器总结">7. 优化器总结</h2><h4 id="梯度下降-gradient-descent-gd">1. 梯度下降 (Gradient Descent,GD)</h4><p>梯度下降是一种基于梯度信息来更新参数的优化方法。假设损失函数为 <span class="math inline">\(J(\theta)\)</span>，对于每次迭代，更新权重的方式为：<span class="math display">\[\theta_{t+1} = \theta_t - \eta \nabla J(\theta_t),\]</span> 其中，$ _t $ 是第 $ t $ 次迭代时的参数，$ $ 是学习率，<span class="math inline">\(\nabla J(\theta_t)\)</span>是损失函数对参数的梯度。</p><ul><li><strong>是否收敛到最优值</strong>：在凸问题中，只要学习率 $$选得合适，梯度下降可以收敛到全局最优解。但对于<strong>非凸问题</strong>，它可能会收敛到局部最优解。</li><li><strong>优点</strong>：简单且易于实现。</li><li><strong>缺点</strong>：对于批量梯度下降，计算梯度会涉及整个训练集，计算成本高。</li></ul><h4 id="随机梯度下降-stochastic-gradient-descent-sgd">2. 随机梯度下降(Stochastic Gradient Descent, SGD)</h4><p>SGD是梯度下降的一个变种，它在每次更新时仅使用一个样本的梯度，而不是整个训练集的梯度：<span class="math display">\[\theta_{t+1} = \theta_t - \eta \nabla J(\theta_t; x_i, y_i)\]</span> 其中 $ (x_i, y_i) $ 是随机选择的训练样本。</p><ul><li><strong>是否收敛到最优值</strong>：在凸问题中，SGD在学习率逐渐衰减的情况下可以收敛到全局最优值，但波动较大。在非凸问题中，SGD可能会陷入局部最优，但随机性有时会帮助跳出局部最优。</li><li><strong>优点</strong>：计算开销低，每次迭代只计算一个样本的梯度。</li><li><strong>缺点</strong>：更新频繁，带有随机性，会造成损失函数在收敛过程中严重震荡。收敛较慢，更新过程存在噪声。</li></ul><h4 id="小批量梯度下降法mini-batch-gradient-descent-mbgd">3.<strong>小批量梯度下降法（Mini-batch Gradient Descent,MBGD）</strong></h4><p>小批量梯度下降是批量梯度下降和随机梯度下降的折中，使用一部分数据计算梯度，然后更新参数。这种方式可以降低参数更新时的方差，使得收敛更加稳定。但是对于非凸问题，依旧无法保证得到全局最优解。</p><p><strong>在梯度下降公式中，可以从两个角度进行改进。一是自适应选择学习率；二是梯度（动量）。</strong></p><p>首先，在修正梯度方面，主要有momentum动量法和nesterov 加速法。</p><h4 id="动量梯度下降-momentum-gd-和-nagnesterov-accelerated-gradient">4.<strong>动量梯度下降 (Momentum GD) 和 NAG（Nesterov acceleratedgradient）</strong></h4><p>动量法：参数更新时在一定程度上保留之前更新的方向，同时又利用当前batch的梯度微调最终的更新方向，简言之就是通过积累之前的动量来(previous_sum_of_gradient)加速当前的梯度，可能更加稳定、更有利于跳出局部最优。</p><p>动量法的更新公式为： <span class="math display">\[v_{t+1} = \gamma v_t + \eta \nabla J(\theta_t), \\\theta_{t+1} = \theta_t - v_{t+1}\]</span> 其中， $ $ 是动量因子（通常取值接近于 1），$ v_t $是动量向量。</p><ul><li><strong>是否收敛到最优值</strong>：在凸问题中，动量法可以比标准梯度下降更快收敛。在非凸问题中，它同样可能收敛到局部最优，但动量项可能有助于避免一些局部最优点。</li><li><strong>优点</strong>：加快收敛速度，减少震荡。</li><li><strong>缺点</strong>：动量项的选取较为敏感。</li></ul><p>NAG 进一步引入了nesterov动量，先在计算梯度更新前做一个矫正，更新公式为： <span class="math display">\[v_{t+1} = \gamma v_t + \eta \nabla J(\theta_t - \gamma v_t), \\\theta_{t+1} = \theta_t - v_{t+1}.\]</span></p><p>传统的优化算法要么将学习率设置为常数要么根据训练次数调节学习率。往往忽视了学习率其他变化的可能性。然而，学习率对模型的性能有着显著的影响，因此需要采取一些策略来想办法更新学习率，从而提高训练速度。如果学习率太小，则梯度很大的参数会有一个很慢的收敛速度；如果学习率太大，则已经优化得差不多的参数可能会出现不稳定的情况。</p><p><strong>自适应学习率算法主要有：AdaGrad算法，RMSProp算法，Adam算法以及AdaDelta算法等。</strong></p><h4 id="adagrad-adaptive-gradient-algorithm">5. <strong>AdaGrad(Adaptive Gradient Algorithm)</strong></h4><p>AdaGrad根据历史梯度信息来调整学习率，能够自动缩放每个参数反比于其所有梯度历史总和的平方根。更新公式为：<span class="math display">\[\theta_{t+1, i} = \theta_{t,i}- \frac{\eta}{\sqrt{G_{t,ii} + \epsilon}}g_{t,i}.\]</span> 其中，<span class="math inline">\(g_{t,i}\)</span> 为 <span class="math inline">\(t\)</span>时刻，参数 <span class="math inline">\(\theta_{t,i}\)</span> 的梯度。<span class="math inline">\(G_t\)</span> 是对角矩阵，<span class="math inline">\((i,i)\)</span>元素为到第$ t $次迭代为止，参数<span class="math inline">\(\theta_{t,i}\)</span> 的累积梯度平方和。</p><ul><li><strong>是否收敛到最优值</strong>：AdaGrad在凸问题中可以收敛到最优解，但在非凸问题中，学习率可能会变得非常小，导致无法继续有效更新。</li><li><strong>优点</strong>：具有损失函数最大梯度的参数相应地有个快速下降的学习率，而具有小梯度的参数在学习率上有相对较小的下降。</li><li><strong>缺点</strong>：中后期，分母上梯度累加的平方和会越来越大，学习率会逐渐减小到接近0，使得训练提前结束，无法学习。</li></ul><h4 id="rmsprop-root-mean-square-propagation">6. <strong>RMSProp (RootMean Square Propagation)</strong></h4><p>RMSProp通过调整每个参数的学习率来解决梯度震荡问题。其核心思想是对每个参数的梯度平方值进行指数加权平均，并使用这个平均值来调整每个参数的更新步长：<span class="math display">\[E[g^2]_t = \beta E[g^2]_{t-1} + (1 - \beta) g_t^2, \qquad\theta_{t+1} = \theta_t - \frac{\eta}{\sqrt{E[g^2]_t + \epsilon}} g_t\]</span> 其中，$ g_t $ 是梯度，$ E[g^2]_t $ 是梯度平方的移动平均，<span class="math inline">\(\beta\)</span>是衰减因子，<span class="math inline">\(\epsilon\)</span> 是防止除零的小量。</p><ul><li><strong>是否收敛到最优值</strong>：RMSProp能够在一定程度上控制学习率的大小，使得在深度学习中的表现较好。在非凸问题中，它能够有较好的局部收敛表现。</li><li><strong>优点</strong>：能够动态调整学习率，对稀疏数据有较好的处理能力。</li><li><strong>缺点</strong>：可能会在学习率过小的情况下导致收敛变慢。</li></ul><h4 id="adadelta">7. <strong>Adadelta</strong></h4><p>Adadelta 是 <strong>AdaGrad</strong> 的改进版，旨在解决 AdaGrad中学习率逐渐衰减至过小的问题。</p><p>Adadelta的主要思想是通过使用<strong>指数加权移动平均</strong>（ExponentialMoving Average, EMA）来代替 AdaGrad中的累积平方梯度和累计学习率。通过这种方式，它能够更稳定地调整学习率，同时避免学习率在训练过程中过度减小。</p><p>Adadelta不仅对梯度平方进行加权平均，还对参数更新的量进行加权平均，因此它不依赖于预设的全局学习率。</p><p>(1). <strong>梯度平方的指数加权移动平均</strong>： <span class="math display">\[   E[g^2]_t = \rho E[g^2]_{t-1} + (1 - \rho) g_t^2   \]</span></p><p>其中，$ g_t $ 是在第 $ t $ 次迭代中计算的梯度，<span class="math inline">\(\rho\)</span> 是衰减率（通常取值在 0.9 左右），$E[g^2]_t $ 是梯度平方的移动平均值。</p><p>(2). <strong>参数更新的移动平均</strong>： <span class="math display">\[   \Delta \theta_t = - \frac{\sqrt{E[\Delta \theta^2]_{t-1} +\epsilon}}{\sqrt{E[g^2]_t + \epsilon}} g_t   \]</span> 其中，$ E[^2]_{t-1} $ 是之前参数更新量的移动平均值，$ $是一个用于防止除零的小量（通常取 $ 10^{-6} $）。</p><p>(3). <strong>更新移动平均</strong>： <span class="math display">\[   E[\Delta \theta^2]_t = \rho E[\Delta \theta^2]_{t-1} + (1 - \rho)(\Delta \theta_t)^2  \]</span></p><p>(4). <strong>参数更新</strong>： <span class="math display">\[   \theta_{t+1} = \theta_t + \Delta \theta_t  \]</span></p><ul><li><p><strong>是否收敛到最优值</strong>：在凸优化问题中，Adadelta可以收敛到全局最优解。在非凸问题中，它的表现依然较好，能够避免陷入局部最优点。不过，类似于其他基于梯度的优化方法，Adadelta在非凸问题中并不能保证一定收敛到全局最优解。</p></li><li><p><strong>AdaGrad</strong>使用的是累积平方梯度求和来更新学习率，导致学习率在训练过程中逐渐趋近于零，尤其是在处理长时间训练或大量数据时。这会使得AdaGrad 训练过程后期的学习率非常小，进而导致参数几乎无法更新。</p></li><li><p><strong>Adadelta</strong> 通过引入指数加权移动平均（EMA）代替了AdaGrad 中的累积平方梯度求和，避免了学习率过早衰减的现象。同时，Adadelta不再需要预设学习率，因为它会自动调整学习率。</p></li><li><p><strong>依赖于衰减率的选择</strong>：虽然不需要手动设置学习率，但衰减率$ $的选择依然是影响模型收敛速度的一个关键因素。对于不同的数据集和任务，可能需要针对衰减率进行调优。</p></li></ul><h4 id="adam-adaptive-moment-estimation">8. <strong>Adam (AdaptiveMoment Estimation)</strong></h4><p>Adam 是 RMSProp和动量法的结合，通过同时计算梯度的一阶和二阶矩的指数加权平均来调整学习率：<span class="math display">\[m_t = \beta_1 m_{t-1} + (1 - \beta_1) g_t, \\v_t = \beta_2 v_{t-1} + (1 - \beta_2) g_t^2.\]</span> <span class="math display">\[\hat{m}_t = \frac{m_t}{1 - \beta_1^t}, \quad \hat{v}_t = \frac{v_t}{1 -\beta_2^t}.\]</span> <span class="math display">\[\theta_{t+1} = \theta_t - \frac{\eta}{\sqrt{\hat{v}_t} + \epsilon}\hat{m}_t\]</span> 其中，$ m_t $ 和 $ v_t $ 分别是梯度的一阶和二阶矩，$ _1 $ 和 $_2 $ 是超参数。</p><ul><li><strong>是否收敛到最优值</strong>：Adam在许多实际问题中表现优越，但在某些情况下，Adam可能会收敛到次优解。理论上，它能收敛到局部最优，但是否能达到全局最优取决于问题的性质。</li><li><strong>优点</strong>：能够动态调整学习率，对稀疏数据和噪声鲁棒性强。</li><li><strong>缺点</strong>：较为复杂，依赖超参数的设置。</li></ul><h4 id="adamw-adaptive-moment-estimation">9. <strong>AdamW (AdaptiveMoment Estimation)</strong></h4><p><strong>AdamW</strong> 是 <strong>Adam</strong>优化算法的改进版本，它的主要改进是在 Adam的基础上引入了<strong>权重衰减（WeightDecay）</strong>的正确实现。这种权重衰减是通过将 L2正则化直接应用于<strong>参数更新公式</strong>，而不是像 Adam那样对梯度进行修正。这种改进旨在提高模型的泛化能力，尤其是避免深度学习模型中过拟合的问题。</p><ul><li><p><strong>Adam 中的错误正则化实现</strong>：在原版的 Adam中，权重衰减实际上是通过将梯度中的 L2 惩罚项添加到更新公式中。这种做法在Adam 中并不完全等同于对参数的惩罚，因为 Adam依赖于动量和梯度的调整，它使得实际的正则化效果被稀释或扭曲，导致权重衰减效果不理想。</p></li><li><p><strong>AdamW 的提出</strong>：为了解决这个问题，AdamW提出了更正的权重衰减实现。AdamW将权重衰减项直接应用到参数本身的更新步骤，而不是施加在梯度上。这种做法能够更加有效地抑制模型的过拟合，提高泛化能力。</p></li></ul><p>AdamW 基本上继承了 Adam的大部分更新过程，但在参数更新时引入了独立的权重衰减项。</p><p>(1). <strong>梯度的移动平均</strong>（一阶矩估计）： <span class="math display">\[   m_t = \beta_1 m_{t-1} + (1 - \beta_1) g_t   \]</span> 其中，$g_t $是在第 $ t $ 次迭代中计算的梯度，$ m_t <span class="math inline">\(是梯度的移动平均，\)</span>_1 $是动量衰减因子（通常取 0.9）。</p><p>(2). <strong>梯度平方的移动平均</strong>（二阶矩估计）： <span class="math display">\[   v_t = \beta_2 v_{t-1} + (1 - \beta_2) g_t^2  \]</span> 其中，$ v_t $ 是梯度平方的移动平均，$ _2 $是衰减因子（通常取 0.999）。</p><p>(3). <strong>偏差修正</strong>：为了消除初期时矩估计的偏差，需要进行偏差校正： <span class="math display">\[   \hat{m}_t = \frac{m_t}{1 - \beta_1^t}, \quad \hat{v}_t = \frac{v_t}{1- \beta_2^t}   \]</span></p><p>(4). <strong>参数更新</strong>（AdamW 核心改进部分）： AdamW的更新步骤不仅包含 Adam的参数更新公式，还直接在参数更新时引入了权重衰减项： <span class="math display">\[   \theta_{t+1} = \theta_t - \eta \frac{\hat{m}_t}{\sqrt{\hat{v}_t} +\epsilon} - \eta \lambda \theta_t   \]</span> 其中，$ $ 是权重衰减系数（即 L2 正则化系数），$ $是学习率。</p><p>AdamW 的关键在于第二个项 $_t$，它直接将权重衰减施加在参数更新上，而不是施加在梯度上。这种方式与传统SGD 中的权重衰减更一致。</p><ul><li><strong>Adam</strong>：权重衰减通过 L2正则化实现，并作用在梯度上。这种实现可能会导致正则化效果受到 Adam的梯度调整机制的干扰，导致模型参数更新不充分，特别是在学习率较小时。</li><li><strong>AdamW</strong>：权重衰减直接作用于参数本身，即在每次参数更新时独立加入一个基于参数的衰减项。这样可以保证权重衰减的效果更加直接和有效，避免了Adam对梯度的干扰。此外，这种权重衰减更加显式地对模型参数产生作用，从而能够更好地抑制模型过拟合，提高泛化性能。</li><li><strong>需要调优的超参数增加</strong>：相比 Adam，AdamW多了一个权重衰减系数 $ $，这增加了模型调优的复杂性。</li></ul><h1 id="三常见vqa">三、常见VQA</h1><h2 id="section">1.</h2><h1 id="reference">Reference</h1><p><a href="https://github.com/315386775/DeepLearing-Interview-Awesome-2024?tab=readme-ov-file">DeepLearing-Interview-Awesome-2024</a></p><p><a href="https://oi-wiki.org/basic/radix-sort/">几种排序算法</a></p><p><a href="https://github.com/zhengjingwei/machine-learning-interview?tab=readme-ov-file#1-1">machine-learning-interview</a></p><p><a href="https://aman.ai/primers/ai/">Distilled AI</a></p>]]></content>
      
      
      <categories>
          
          <category> Machine Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Machine Learning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Blip, Blip2 (Q-former), InstructBlip, CONCH, PULSE</title>
      <link href="/2024/09/12/multi_modalv2/"/>
      <url>/2024/09/12/multi_modalv2/</url>
      
        <content type="html"><![CDATA[<h2 id="a-very-nice-blog-blip系列文章小结">A very nice blog: <a href="http://www.myhz0606.com/article/blip_hub">BLIP系列文章小结</a></h2><h1 id="blip">BLIP</h1><p><a href="https://arxiv.org/pdf/2201.12086">BLIP: BootstrappingLanguage-Image Pre-training for Unified Vision-Language Understandingand Generation</a></p><h2 id="model-architecture">Model Architecture</h2><p><img src="/2024/09/12/multi_modalv2/blip1.jpg"></p><p>整体上看，对于图像部分，有一个 <span class="math inline">\(N\)</span>层的 ViT。对于文本部分，分别使用三个 text encoder去计算三个不同的目标函数。在 blip 中，同种颜色代表同样的共享参数，</p><p>对于第一个 text encoder，具有 <span class="math inline">\(N\)</span>层，主要是将文本特征和图像特征进行对比学习，计算 ITC loss来进行分类任务。第二个 image-grounded textencoder。提取得到的图像特征通过 cross attention 进入模型，文本特征通过self-attention 得到，然后进行融合得到多模态的特征，计算 ITM loss 来判断image-text pairs 是否匹配。相较于第一个 text encoder，第二个 encoder只需要学习额外的 cross attention 层。为了能够执行 生成式 任务，blip添加了一个 decoder。由于 decoder 不能看到完整的句子，因此将 causalself-attention 替换掉前面 encoder 中的 bi self-attention，不过cross-attention 和 feed forward 层依旧和前面的共享参数。这里 decoder使用的 language modeling loss (LMloss)，根据前面的文本去预测后面的文本，而不是进行文本的完形填空 (i.e.,MLM loss)。</p><p>不同 text encoder 使用不同的 token，分别是[CLS]，[Encode]，[Decode]。</p><h2 id="capfilt">CapFilt</h2><p><img src="/2024/09/12/multi_modalv2/blip2.jpg"></p><p>对于从网络上获取的图文数据质量比较糟糕，图片对应的文本描述往往不够准确。针对这种情况，blipfinetune 了一个 filter 来筛选图文对，一个 captioner来生成合成的文本。<span class="math inline">\(\{I_w, T_w\}\)</span> 是从web 上获取的 noisy image-text pairs，<span class="math inline">\(\{I_h,T_h\}\)</span> 是人工标注的 image-textpairs，通常认为是高质量的。对于预训练好的 blip 模型，首先基于 <span class="math inline">\(\{I_h, T_h\}\)</span>数据对两个预训练好的 textencoder 进行 finetune 得到 filter model。然后对 noisy data <span class="math inline">\(\{I_w, T_w\}\)</span> 进行筛选。同时，基于 <span class="math inline">\(\{I_h, T_h\}\)</span>数据对预训练好的 decoder 进行finetune，用来生成合成的 caption。由于生成的 caption质量并不确定，因此将 <span class="math inline">\(\{I_w, T_s\}\)</span>再通过 filter 进行筛选。最终得到数据 <span class="math inline">\(D =\{I_w, T_w\} + \{I_w, T_s\} + \{I_h, T_h\}\)</span>。</p><h1 id="conch">CONCH</h1><p><a href="https://www.nature.com/articles/s41591-024-02856-4">Avisual-language foundation model for computational pathology</a></p><p>论文介绍了一种视觉-语言基础通用模型-CONCH，利用不同来源的组织病理学图像、生物医学文本和超过117 万个图像标题对等数据，通过任务识别进行预训练。CONCH以最先进的视觉语言基础预训练框架 CoCa为基础，使用一个图像编码器、一个文本编码器和一个多模态融合解码器，并结合使用对齐目标函数和标题目标函数进行训练。其中，对齐目标损失的目的是在模型的表征空间中对齐图像和文本模态，而标题目标则是学习预测与图像相对应的标题。论文总共使用14 种不同的基准数据集，研究了 CONCH在一系列任务中的能力，包括图像分类、图像到文本和文本到图像检索、图像分割和图像标题生成。</p><h2 id="数据处理">数据处理</h2><p>为便于整理，论文将数据源分为两类：（1）EDU，包含从教育笔记中提取的数据；（2）PMCOA，从 PubMed Central Open Access Datase 下载的数据。</p><p>数据整理的挑战有两个： 1.筛选组织病理学图像：下载的原始数据包含了组织病理学和非组织病理学的图像。2.处理图像面板：大量数据以图像面板的形式呈现，面板中的图像由多个子图像组成，图像标题文本中有时同时或分别包含了多个子图像的描述。</p><p>为了应对这些挑战，数据清理分为三个步骤： 1.检测组织病理学图像：使用YOLOv5对象检测模型生成边界框 bounding boxes以提取检测到的图像。这一步之前，作者首先通过生成合成数据来训练该对象检测模型。2. 分割图像标题：作者在整理 EDU数据集时收集了一个包含图像标题说明和拆分后标题说明的数据集，对GPT模型进行微调，原始图像标题为输入，拆分后的图像标题为输出，最终使得模型具有实现自动拆分图像标题的能力。3.子图像和标题进行对齐：首先在干净的EDU数据集上训练一个CLIP模型，将检测到的子图像与拆分后的标题说明进行对齐。使用训练后的模型，给定一组图像面板中的<span class="math inline">\(m\)</span> 幅检测到的图像和 <span class="math inline">\(n\)</span> 个分割的文本，得到模型中的图像嵌入表征<span class="math inline">\({u0, u1, ..., um}\)</span> 和文本嵌入表征<span class="math inline">\({v0, v1, ..., vn}\)</span>。然后两两计算余弦相似度，将相似度最高的作为一对图文数据。</p><p>通过以上三步以及进一步数据清理，形成了一个包含<span class="math inline">\(117\)</span>万对人类组织病理学图像-说明的数据集。</p><h2 id="visual-language-pretraining">Visual-language pretraining</h2><p>在训练过程中，作者同时考虑了两种loss，一种是图文对比损失（image-to-textand text-to-image contrastive loss)，另一种是针对标题的loss。</p><p><img src="/2024/09/12/multi_modalv2/conch1.jpg"></p><p>模型框架：模型包括一个 image encoder <span class="math inline">\(f(\cdot; \theta)\)</span>，一个 text encoder <span class="math inline">\(g(\cdot; \phi)\)</span> 和一个图文融合的 decoder<span class="math inline">\(h(\cdot; \psi)\)</span>。</p><p>Image encoder 包含了一个 backbone (参数为 <span class="math inline">\(\theta_{\text{backbone}}\)</span>) 和 两个attention pooler 模块，参数分别为 <span class="math inline">\(\theta_{\text{contrast}}\)</span> 和 <span class="math inline">\(\theta_{\text{caption}}\)</span>。 Backbone使用的是标准的 ViT，具有12层的 transformer层，12 个 attentionheads，embedding 的维度为 <span class="math inline">\(768\)</span>，hidden dimension 是 <span class="math inline">\(3072\)</span>。Image 划分为 <span class="math inline">\(16 \times 16\)</span> 个 image tokens （<span class="math inline">\(256\)</span>个），并在每个token上面添加可学习的绝对位置编码。ViT将RGB图像转换为 feature maps. 基于从 ViT最后一层输出的image token的特征表示 （其实也是输入decoder cross-attention 中的 quey)，每一个attention pooler 从不同数量的 image tokens上去学习相应的信息。具体来说，第一个 attention pooler <span class="math inline">\(f_{\text{contrast}(\cdot;\theta_{\text{contrast}})}\)</span> 使用一个 query 去学习一个 imagetoken，用于捕捉image的全局特征。第二个 attention pooler <span class="math inline">\(f_{\text{caption}(\cdot;\theta_{\text{caption}})}\)</span> 使用 <span class="math inline">\(n =256\)</span> 个 queries 去生成 <span class="math inline">\(256\)</span>个 image tokens，用于获取 image的细颗粒度的局部信息，进而生成caption。</p><p>Text encoder 和 multimodal decoder 分别包括 12个 transformerlayers，embedding dimension 为 768，hidden dimension 为 3072。Textencoder通过嵌入表将离散的单词token映射为连续的嵌入向量，并添加了可学习的绝对位置embeddings。Textencoder 为每个tokenized的 caption 添加了一个&lt;CLS&gt;token，用于在Transformer注意力过程中提取文本说明的全局表征。</p><p>Multimodal decoder在每个多头自注意力层之后插入了交叉注意力层，以整合来自图像token的信息。最后结合语言模型输出预测下一个token在支持的词汇表中的分布。</p><p>假设有 <span class="math inline">\(M\)</span> 个 image-caption 图文对<span class="math inline">\((x_i, w_i)_{i=1}^{M}\)</span>，其中caption $w_i = (<bos>, w_{i,1}, ..., w_{i,T}, <eos>)$ 包含 <span class="math inline">\(T\)</span> 个 word tokens。对于一个图文对 <span class="math inline">\((x_i, w_i)\)</span>，假设通过 <span class="math inline">\(f_{\text{contrast}(\cdot;\theta_{\text{contrast}})}\)</span> 得到的输出是 <span class="math inline">\(u_i\)</span>，通过 text encoder <span class="math inline">\(g(\cdot;\phi)\)</span> 在 &lt;CLS&gt; token 处经过l2 normalization 后得到的输出是 <span class="math inline">\(v_i\)</span>，那么loss是：</eos></bos></p><p><img src="/2024/09/12/multi_modalv2/itc.jpg"></p><p>其中前两项为 image-to-text and text-to-image contrastive loss,respectively, to maximize the cosine-similarity scores between pairedimage and text embeddings relative to remaining negative pairings in themini-batch. The last term seeks to maximize the log-likelihood of eachobserved token under the multimodal autoregressive language model (jointly parameterized by the image encoder, text encoder and multimodaldecoder), conditioned on previous tokens in the caption, as well as thecorresponding image.</p><p>具体训练设置，主要包括以下几点：</p><ul><li>训练轮数：每个视觉-语言预训练实验都运行了 40 个epoch。</li><li>硬件配置：实验分布式运行在 8个NVIDIA A100 80-GB GPU上，每个GPU上的本地批量大小为48。</li><li>梯度累积：为了达到更大的有效全局批量大小，使用了 梯度累积，实现了1,536 的全局批量大小（48 × 8 GPU × 4次梯度累积）。</li><li>图像大小：输入图像大小为 448 ×448像素，其中：对较大的图像，首先将其较短边调整为448像素，并对其进行中心裁剪。对较小的图像，按需进行零填充 以达到所需的尺寸。</li></ul><h1 id="quilt-llava-visual-instruction-tuning-by-extracting-localized-narratives-from-open-source-histopathology-videos">Quilt-LLaVA:Visual Instruction Tuning by Extracting Localized Narratives fromOpen-Source Histopathology Videos</h1><h2 id="abstract">Abstract</h2><p>组织病理学诊断需要对整张切片图像（WSI）进行全局分析，这就要求病理学家从不同的WSI patch 中复合信息。然而，高分辨率的 WSI对组织病理学多模式模型提出了挑战。训练组织病理学多模态模型需要用于微调的数据集，而目前的数据集包含单个图像patch的信息，没有每个patch 间的空间概念，也没有更广泛的 WSI视图。为了弥补这一不足，本文推出了 QUILT- INSTRUCT，这是一个包含 107,131个组织病理学特定指令问/答对的大型数据集，这些指令以构成 WSI的诊断相关图像patch为基础。数据集是利用 YouTube上的组织病理学教育视频收集的，该视频通过自动提取叙述者的光标位置来提供叙述的局部定位。QUILT-INSTRUCT支持上下文推理，从整个 WSI 中提取诊断和支持事实。利用QUILT-INSTRUCT，我们训练出了QUILT-LLAVA，它的推理能力超越了给定的单个图像patch，能够跨patch进行诊断推理。为了评估QUILT-LLAVA，我们提出了一个全面的评估数据集，该数据集由 985 幅图像和1283 个人工生成的问题解答组成。我们还使用公开的组织病理学数据集对QUILT-LLAVA 进行了全面评估，结果显示 QUILT-LLAVA 在相对 GPT-4分数上明显优于 SOTA <span class="math inline">\(10%\)</span>以上，在开放集和封闭集 VQA 上分别优于 SOTA <span class="math inline">\(4%\)</span> 和 <span class="math inline">\(9%\)</span>。</p><h2 id="quilt--instruct-数据集构建">QUILT- INSTRUCT 数据集构建</h2><p>从 4149 个 YouTube 教育视频中构建了 QUILT-INSTRUCT，总时长超过 1000小时。这些视频是最近组织病理学数据集 QUILT 的一部分。</p><p>在教育视频中，专家在讲述高分辨率 WSI时往往会停顿一下，然后再用光标指示重点突出区域。我们通过三个步骤将非结构化视频转换为可用的视觉教学数据：首先，我们在视频中定位叙述者的光标。然后，对光标的位置进行时空聚类，以便在图像中将组织病理学概念视觉化。最后，利用提取的标题，使用LLM 生成指令调整数据集 -QUILT-INSTRUCT。这一过程涉及提示（prompting）技术，从为每个图像patch生成不同 Q/A 对的独立提示，到结合 WSI中各patch信息的基于推理的提示，从而生成推理诊断的 Q/A 对。</p><p><img src="/2024/09/12/multi_modalv2/quilt1.jpg"></p><p>论文介绍了两种不同类型的问答生成方法，用于处理基于病理学的 WholeSlide Images (WSI，整个切片图像) 的文本生成任务。</p><ul><li><p>Independent Prompts（独立提示）：</p><p>这一方法基于单个切片级别（patch-level）的文本输入进行问答生成。切片级别文本是与病理图像的某一小块相关的描述。这种提示生成的问答对是独立于整个图像或视频的上下文，仅依赖于该切片的内容，类似于文献[17]中的对话式和详细描述的生成方式。因为这些提示不依赖于其他信息源，因此被称为“独立提示”（Independentprompts）。</p></li><li><p>Reasoning-based Prompts（推理提示）：</p><p>这种方法利用整个视频中的上下文线索，特别是该视频围绕单个WSI的诊断展开，通过逐步揭示概念和线索。输入不仅包含切片级别的文本，还包括整个WSI的全局信息。因此，模型不仅仅基于当前的切片，还可以考虑到全局的诊断信息。通过这种方法，模型（如GPT-4）可以超越当前上下文进行推理，但仍然依靠从视频或图像中提取的事实信息，这样可以减少生成内容的虚构或不准确（即减少幻觉现象）。</p><p>简单来说，独立提示只基于局部信息生成问答，而推理提示则结合了全局的诊断线索，帮助模型更合理地推理并减少错误。</p></li></ul><h2 id="training-quilt-llava-evaluating-with-quilt-vqa">TrainingQUILT-LLAVA &amp; evaluating with QUILT-VQA</h2><p>论文使用 QUILT-INSTRUCT 来训练 QUILT-LLAVA。在 QUILT-INSTRUCT之外单独设计 QUILT-VQA，以评估 QUILT-LLAVA。最后，从 QUILT-VQA中生成指令遵循测试集，以评估 QUILT-LLAVA 的指令遵循能力。</p><h3 id="training-quilt-llava">Training QUILT-LLAVA</h3><p>LLAVA 由一个vision模块、多层感知机（MLP）和大语言模型（LLM）组成。这个设计允许语言模型处理视觉信息。1. 首先，MLP 最初作为一个投影器被训练，直到收敛。在这个阶段，LLM和视觉模块都被冻结，不会更新权重。 2. 随后，MLP 和 LLM都会结合指令跟随数据进行微调，使模型与人类病理学家的诊断过程保持一致。</p><p>LLAVA 使用预训练的 CLIP图像编码器，但在这个特定领域中，使用了在公共病理学数据集（如 QUILT-NET[9] 和 PLIP [8]）上训练的预训练 CLIP模型。作者还通过不同的图像编码器、训练策略和视觉提示进行消融实验，以测试其效果。</p><p><img src="/2024/09/12/multi_modalv2/quilt2.jpg"></p><p>具体来说， - 对齐视觉和语言模型:</p><p>作者首先在病理学领域内对视觉和语言模型进行对齐。为此，从 QUILT数据集中提取了 723K图像-文本对，并将描述文本转换为问答格式。问答对的生成方法是随机选择一个预定义的问题（见附录图18），将其添加到图像描述前，形成问答对。问题设计用于描述图像中可见的视觉信息。</p><pre><code>在这一阶段，视觉和语言模型被冻结，仅训练 MLP 层，其任务是将来自图像编码器的嵌入映射到语言模型中，以便语言模型根据问题预测图像的描述。这一阶段的训练将病理学图像的嵌入与相应的文本嵌入对齐，确保了视觉信息能够被语言模型处理。</code></pre><ul><li><p>组织病理学数据集指令微调</p><p>论文使用 QUILT-INSTRUCT对模型进行微调。在此阶段，冻结视觉编码器权重，继续训练 MLP层和语言模块。</p></li></ul><h3 id="evaluation-data-generation-quilt-vqa">Evaluation DataGeneration: QUILT-VQA</h3><p>在组织病理学领域，研究人员依靠 PathVQA [7] 和 PMC-VQA [31]等评估数据集来评估其模型的性能。然而，这些数据集表现出明显的缺点，包括由于转述相同的问题而造成的严重重复。更糟糕的是，同一个问题经常会有相互矛盾的答案（见附录第3.4节）。相比之下，教育视频内容提供了一种宝贵的资源：解说员在解说过程中经常提出问题，然后自己给出答案，从而引入了互动元素。例如，解说员会说："你知道我们面对的是哪种器官吗？"然后接着详细说明："是的，这是一个结肠"。视频中的这种问答形式提供了丰富的有机问答数据集，可以提取并重新用于评估。</p><p>PathVQA: 包含从教科书和数字图书馆中的 4998 个病理图像标题对中提取的32799 个问题-答案对。问题分为开放式问题和封闭式问题，前者包括"什么"、"哪里"、"何时"、"谁的"、"如何"、"多少 "等问题，后者包括 "是"/"否"等问题。我们使用了评估集中的 6761 个样本。</p><p>PMC-VQA: 包含一个由 34823 对图像组成的 VQA测试集，这些图像涵盖各种模式或病症。该数据集是从 PMC-OA文章中的图像标题对中整理出来的，采用多选格式。论文从该数据集中检索到PMC-VQA 子集，其中包括 2318 对组织病理学 VQA 对。</p><p>QUILT-VQA: 首先，视频的文字转录内容会被处理，识别出问号 ("?")所在的位置。如果问号出现在某个稳定文本块（视频中关联图像的描述文本）45秒的时间范围内，作者将扩展该文本块，确保其包含带有问号的完整句子。这种方法确保了问题和视频中的视觉内容相匹配。在数据预处理和问号映射完成后，作者使用GPT-4直接从文本中提取问答对。具体地，GPT-4的输入是经过处理的稳定文本块以及其中带有问号的句子，表明这些句子包含提问内容。在GPT-4初步提取完问答对后，作者进行了手动验证，以确保每个问答对不仅在医学上具有相关性，还与该文本块的内容紧密对应。</p><p>在提取完成后，作者将问题分为两类：依赖图像的问答对（Image-dependentQ/Apairs）：共1055对，这类问题引用了讲述者对特定图像的描述。基于一般医学知识的问答对（General-knowledgeQ/Apairs）：共228对，这类问题与更广泛的医学知识相关，而不仅仅依赖于某一特定图像。</p><p><img src="/2024/09/12/multi_modalv2/quilt3.jpg"></p><h3 id="evaluation-data-generation-instruction-following-test-set">Evaluationdata generation: Instruction Following Test Set</h3><p>QUILT-VQA 的重点是评估 QUILT-LLAVA的医学知识，除此之外，我们还旨在评估该模型在多模态对话中遵循指令的能力。为此，我们构建了一组326 个问题，其中包括 256 个会话问题和 70 个详细描述问题，所有问题均来自QUILT-VQA中从未曾看过的视频中提取的图像-文本对话。为了生成这个评估集，我们采用了与生成QUILT-INSTRUCT 时相同的基于会话和详细描述的提示。</p><h2 id="experiments">Experiments</h2><p>本节将介绍 QUILT-LLAVA 在组织病理学 VQA 基准测试中与现有 SOTA多模态模型的性能对比情况。首先，我们使用 GPT-4对生成结果与真实答案进行了比对。其次，执行开放式和封闭式 VQA任务。最后，使用视觉提示和不同的训练模型进行消融实验。</p><ul><li><p>使用GPT-4 评估生成结果</p><p>评估的主要维度包括回答的帮助性、相关性、准确性和详细程度。</p><p>评估方法：使用 GPT-4 来对比不同模型的输出：候选模型（QUILT-LLAVA） 和GPT-4。GPT-4会根据这些维度（帮助性、相关性、准确性和详细程度）对两个模型的回答进行评分，评分范围为1到10分，分数越高表示整体表现越好。除了分数外，GPT-4还提供详细的解释，以帮助理解每个模型在生成回答时的表现，便于更好地分析模型的优势和不足。</p></li><li><p>可视化问题解答结果</p><p>这些VQA数据集包括开放式和封闭式问答题对。对于封闭式问题，准确率被用来衡量模型给出的正确答案的比例。与此相反，对于开放式问题，我们侧重于召回率，以评估模型的回答中包含真实tokens的频率。</p></li></ul><h2 id="conclusion-and-limitations">Conclusion and Limitations</h2><p>GPT-4 仍然容易生成不准确的信息，导致 QUILT-LLAVA产生错误陈述或“幻觉”现象（即模型生成的信息与真实内容不符）。此外，尽管我们明确指示GPT-4 不要这样做，但 GPT-4有时还是会只从标题中获取信息，而不是从图像中提取信息。</p><h1 id="xray-pulse">xray-pulse</h1><h2 id="维度变换">维度变换</h2><ul><li>图像输入：图像大小为[3, 224, 224].</li><li>Image encoder: Image 划分为 <span class="math inline">\(16 \times16\)</span> 个 image tokens（<span class="math inline">\(256\)</span>个），并在每个token上面添加可学习的绝对位置编码，大小为[1,257, 768].</li><li>projector: 输出 [1, 197, 1408]</li><li>q-former：输入[1, 197, 1408]， 输出： [1, 32, 768]；num_query_token=32</li><li>projector:</li><li>pulse: 1, 32, 4096</li></ul><h1 id="references">References:</h1><ul><li><a href="https://github.com/mli/paper-reading">沐神,论文精读</a></li><li><a href="http://www.myhz0606.com/article/blip_hub">BLIP系列文章小结</a></li></ul>]]></content>
      
      
      <categories>
          
          <category> Foundation Model </category>
          
      </categories>
      
      
        <tags>
            
            <tag> multi-modal </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Llama 3.1</title>
      <link href="/2024/09/04/llama3.1/"/>
      <url>/2024/09/04/llama3.1/</url>
      
        <content type="html"><![CDATA[<h1 id="llama-3.1">Llama 3.1</h1><p><a href="https://arxiv.org/pdf/2407.21783">The Llama 3 Herd ofModels</a></p><h2 id="abstract">Abstract</h2><p>Modern artificial intelligence (AI) systems are powered by foundationmodels. This paper presents a new set of foundation models, called Llama3. It is a herd of language models that natively supportmultilinguality, coding, reasoning, and tool usage. <strong>Our largestmodel is a dense Transformer with 405B parameters and a context windowof up to 128K tokens.</strong> This paper presents an extensiveempirical evaluation of Llama 3. We find that Llama 3 deliverscomparable quality to leading language models such as GPT-4 on aplethora of tasks. We publicly release Llama 3, including pre-trainedand post-trained versions of the 405B parameter language model and ourLlama Guard 3 model for input and output safety. The paper also presentsthe results of experiments in which we integrate image, video, andspeech capabilities into Llama 3 via a compositional approach. Weobserve this approach performs competitively with the state-of-the-arton image, video, and speech recognition tasks. The resulting models arenot yet being broadly released as they are still under development.</p><h2 id="introduction">Introduction</h2><h2 id="pre-training">Pre-Training</h2><h2 id="section"></h2><!-- ![](./multi_modalv1/blip2.jpg) --><h1 id="references">References:</h1><ul><li><a href="https://github.com/mli/paper-reading">沐神,论文精读</a></li><li><a href="https://llama.meta.com/docs/overview">Meta</a></li></ul>]]></content>
      
      
      <categories>
          
          <category> LLM Model </category>
          
      </categories>
      
      
        <tags>
            
            <tag> gpt </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Clip, ViLT, ALBEF, VLMO, Blip, CoCa, BeiTV</title>
      <link href="/2024/09/01/multi_modalv1/"/>
      <url>/2024/09/01/multi_modalv1/</url>
      
        <content type="html"><![CDATA[<h1 id="clip">CLIP</h1><p><a href="https://arxiv.org/pdf/2103.00020">Learning TransferableVisual Models From Natural Language Supervision</a></p><h2 id="approach">Approach</h2><p>核心思想：利用自然语言的监督信号来训练一个较好的视觉模型。</p><p>利用自然语言的监督信号去训练一个视觉模型的好处在于：</p><ol type="1"><li><p>以往的视觉模型训练需要对图片进行类别标注，消耗大量的人力资源。结合可以直接获取的文本信息，不需要对图片进行额外标注，数据规模更大，模型的输入输出不再是单一的标签，自由度更高。</p></li><li><p>相较于单一模态特征（比如单一视觉特征），使用多模态的特征，很容易进行zero shot 的迁移学习。</p></li></ol><p>目标函数的选择：如果使用预测目标函数，即根据图片去预测对应的文本，由于一张图片对应的文本描述具有多样性，从而会导致模型训练效率较低。对比之下，如果只考虑图片和文本是否匹配，这种对比目标函数可以将约束放松，提高模型的训练效率。</p><p><img src="/2024/09/01/multi_modalv1/clip1.jpg"></p><ol type="1"><li><p>contrastive pre-trainng: 模型的输入是 <span class="math inline">\(N\)</span> 个配对的图文，图像通过一个 imageencoder，文本通过一个 text encoder，对应得到 <span class="math inline">\(N\)</span> 个文本特征和 <span class="math inline">\(N\)</span>个图像特征。然后通过计算余弦相似度进行对比学习。矩阵对角线都属于正样本，剩余<span class="math inline">\((N^2 - N)\)</span> 个都是负样本。</p></li><li><p>creating dataset classifier from text:考虑到用于预训练的图文中的文本通常是一个句子，因此，在推理的时候会将label 转换为一个句子，即使用一个 prompt template，然后 fed into textencoder。</p></li><li><p>zero-shot prediction: 对于一张新的图片，通过 image encoder得到图片特征。所有感兴趣的标签通过 prompt engineering 后会变成句子， fedinto 预训练好的 textencoder，会得到相应的文本特征。将图像特征和若干个文本特征计算余弦相似度，然后通过softmax得到概率分布，最大概率对应的句子（标签）即为相应的物体。</p></li></ol><p>对于 image encoder， 可以选择 ResNets 或 vision transforemr。对于text encoder，可以选择 transformer。</p><p>伪代码：首先，文本使用 text encoder 进行特征提取，图像使用 imageencoder进行特征提取。然后投射层将不同模态的特征转换为相同纬度的向量，再进行<span class="math inline">\(l_2\)</span> norm的标准化处理得到用于对比的两个特征。通过计算余弦相似度得到 logits，和ground truth 计算交叉熵目标函数得到 loss。对于 clip而言，正样本都在对角线上，所以通过 labels = np.arange(n) 创建 groundtruth。这里分别针对 image 和 text 计算了两个对称的loss，再计算平均。</p><p><img src="/2024/09/01/multi_modalv1/clip2.jpg"></p><h2 id="prompt-engineering-and-ensembling">prompt engineering andensembling</h2><p>Prompt: 提示，文本的引导作用。</p><p>为什么需要 prompt engineering 和 prompt ensembling?</p><ol type="1"><li><p>词具有多义性。一个单词具有多个不同的含义。比如 'remote'该词可以作为 ‘遥控器’，也具有‘遥远的’含义。如果不结合上下文信息，textencoder 很难抽取正确的特征。</p></li><li><p>在预训练时，图像匹配的文本通常是一个句子，但是在推理时，文本输入通常是label 对应的一个单词，此时会出现 distribution gap 的问题。</p></li></ol><p>基于上述问题，作者提出一种 prompt template，即 "A photo of a{label}"。这种方式可以将标签转换为一个句子，避免了 distribution gap的问题，同时 label在句中的位置通常表示是个名词，一定程度上解决了多义性问题。</p><p>此外，也可以将一些先验信息加入 prompt template中，比如食物、动物的数据。</p><p>prompt ensembing: 使用多个 prompt template，然后将结果综合起来。</p><h2 id="模型评价">模型评价</h2><p>优势：适用于图像文本匹配。可以提前对数据库内的图像、文本进行特征抽取。对于新来的文本或者图像，只需要做一个简单的点乘，具有灵活性和高效性。</p><p>缺点：</p><ol type="1"><li>如果推理的数据集相较于预训练的数据集 out ofdistribution，Clip的泛化能力也会很差。</li><li>从给定的类别中进行判别选择，而不是生成新的输出。</li><li>...</li></ol><h1 id="how-to-train-really-large-models-on-many-gpus">How to TrainReally Large Models on Many GPUs?</h1><p><a href="https://lilianweng.github.io/posts/2021-09-25-train-large/">Blog</a></p><h1 id="vilt">ViLT</h1><p><a href="https://arxiv.org/pdf/2102.03334">ViLT: Vision-and-LanguageTransformer Without Convolution or Region Supervision</a></p><p>已有 vision-and-language pre-training (VLP) 工作的不足：</p><ol type="1"><li><p>抽取图像特征的效率较低，需要的时间远远高于模态融合部分。</p></li><li><p>使用已预训练好的模型抽取特征，可能泛化能力较弱，不是 end-to-end形式。</p></li></ol><p><img src="/2024/09/01/multi_modalv1/vilt3.jpg?300x300"></p><p>传统的 VLP 框架和 ViLT：</p><ol type="1"><li><p>使用特征检测的模型，对于给定的图片，通过 CNN backbone进行特征提取，然后基于这些特征使用 roi等抽取属于物体的特征，得到若干个离散物体的特征向量。文本通过 linearembedding得到文本特征。对于得到的图像序列和文本序列，再进行模态融合。使用目标检测抽取的特征的方式效率较低。</p></li><li><p>基于grid 特征检测的模型，对于给定的图片，通过 CNN backbone得到特征图，然后将特征图拉伸得到相应的序列。</p></li><li><p>ViLT，借鉴 ViT 的 patch embedding layer, ViLT 将图像划分为若干patches，然后通过一个 linear 投影层得到 patchembeddings。文本也是通过一个 linear 投影层得到 wordembeddings。最后两个序列都 fed into 一个 transformer。</p></li></ol><p>从时间上看，ViLT计算高效，参数量更少。基于目标检测的模型性能最好，基于grid特征检测的模型性能最差，ViLT处于两者中间。在使用较少参数量的前提下，效果也不相上下。</p><h2 id="taxonomy-of-vision-and-language-models">Taxonomy ofVision-and-Language Models</h2><p>We propose a taxonomy of vision-and-language models based on twopoints: (1) whether the two modalities have an even level ofexpressiveness in terms of dedicated parameters and/or computation; and(2) whether the two modalities interact in a deep network.</p><p><img src="/2024/09/01/multi_modalv1/vilt1.jpg"></p><p>图中，VE 表示如何抽取 visual embedding，TE 表示如何抽取 textembedding，MI 表示如何进行模态融合 (modality interaction)。</p><ol type="a"><li><p>轻量的文本 encoder，昂贵的视觉 encoder，轻量的 modalityinteraction。</p></li><li><p>visual encoder 和 text encoder的特征提取能力一样，计算量上基本等价，modality interaction部分使用简单的 dot-product。代表性模型为 CLIP。</p></li></ol><ol start="3" type="1"><li><p>text encoder 非常轻量，visual encoder使用目标检测模型，计算昂贵，modality interaction也很昂贵。代表性算法为：ViLBERT, UNITER。</p></li><li><p>轻量的 text encoder 和 visual encoder，复杂的 modalityinteraction。代表性算法为 ViLT。</p></li></ol><h2 id="modality-interaction-schema">Modality Interaction Schema</h2><p>模态融合主要包括两类：</p><ol type="1"><li><p>single-stream approaches:将抽取的文本特征序列和图像特征序列进行拼接作为输入。</p></li><li><p>dual-stream approaches:两个模型分别对两个序列进行处理，提取单一模态信息，然后再进行融合。</p></li></ol><p>本文使用 single-stream approach，避免引入更多的参数。</p><h2 id="vision-and-language-transformer">Vision-and-LanguageTransformer</h2><p><img src="/2024/09/01/multi_modalv1/vilt2.jpg"></p><p>对于多模态的 sing -streamapproaches，需要将两个模态的序列进行拼接，因此需要一个 modal-typeembedding 告诉模型该 token 属于哪个模态。此外，每个模态前面都需要加上[CLS] 特殊标记符。然后， patch embedding + position embedding +modal-type embedding 作为 transformer encoder 的输入。</p><p>本文主要使用了两类loss，分别是文本距离和语言完形填空部分，具体而言：</p><ol type="1"><li><p>image text matching loss:判断对于给定的配对图文，哪个是真的图文对，哪个是假的图文对。</p></li><li><p>word patch alignmentloss：计算文本特征输出和图像特征输出之间的距离。</p></li><li><p>masked language modeling loss: 单词重建的 loss。</p></li></ol><h1 id="albef-align-before-fuse">ALBEF, Align before Fuse</h1><p><a href="https://arxiv.org/pdf/2107.07651">Align before Fuse: Visionand Language Representation Learning with Momentum Distillation</a></p><p>https://blog.salesforceairesearch.com/align-before-fuse/https://nancyyanyu.github.io/posts/paper-albef/</p><p>模型设计方面： -在多模态学习中，视觉特征远远大于文本特征，因此需要使用较为强大的视觉模型(bigViT)。 -此外，模态之间的融合也至关重要，因此模态融合模型也需要足够大（bigmodality interaction）。</p><p>loss 方面： - Clip 使用的是 image text contrastive (ITC)loss，效果不错，可以采用。 - ViLT使用的是 Image TextMatching的loss（ITM），word patch alignment (WPA) loss，文本的单词和image 的 patch 之间进行对应关系。但是 WPA loss 计算非常慢，因此不考虑。- Bert中常用的计算 loss 方式是 Mask Language Modeling (MLM)，mask掉某个词然后再去预测这个词。比较常用。</p><p>总之，直观上来说，结合 ITC、MLM 和 ITM 的 loss 应该效果不错。</p><p>结合上面的考虑，ALBEF 使用复杂的 image encoder (12 blocks) 和multimodal encoder (6 blocks) ，相对轻量的 text encoder (6blocks)。并且，考虑使用 image-text contrastive loss (ITC) 对 imageembedding 和 text embedding 进行对齐，最后还使用了 ITM 和 MLM loss。</p><h2 id="introduction">Introduction</h2><p>已有的工作使用同一个 transformer-based multimodal encoder 去同时model visual tokens 和 word tokens。并且对于 visual tokens还是基于目标检测的模型 (region-based image features)。由于使用的 visualencoder 提取器是基于<strong>预训练</strong>的目标检测器，而不是end-to-end 训练得到的，这种方式得到的 visual tokens 和 word tokens并不匹配，从而使得模态融合 encoder 训练困难。(注意，ALBEF 和 ViLT都想丢弃使用目标检测器的 encoder，但是二者出发点不同， ViLT是从提升计算效率角度出发。)</p><p>因此，本文贡献： 1. 提出一种对比学习的 loss，对 image 和 text 在fusing 之前进行对齐。 2. 针对 noisy web data，提出 momentumdistillation，通过生成伪标签达到自训练。noisy web data:从网络上获取的图像-文本是具有噪音的。比如从搜索引擎获取的图文，文本中包含搜索引擎需要的关键词，但是文本并没有对图像进行很好的描述。</p><h2 id="model-architecture">Model Architecture</h2><p><img src="/2024/09/01/multi_modalv1/albef1.jpg"></p><p>image: 对于给定图像，将其划分为若干个 patches，然后 fed into 一个 12层的 vision transformer encoder。</p><p>原始 bert 有 12 层，考虑到 image encoder 要比文本模型大，用于融合的multimodal encoder 也是越复杂越好。这里将 bert model进行拆分，维持原始计算参数量。 text: bert model 的前六层用作 textencoder。</p><p>融合部分: bert 的后六层用作 multimodal encoder。</p><p>除了 ViT 和 BERT 模型外，还有一个 momentum model。该模型也包含了 ViT和 BERT 模型，只不过是将左侧模型的参数进行移动平均得到，用来产生更多的负样本对 以及 momentum distilation。</p><p>目标函数：ITC loss</p><h1 id="vlmo">VLMO</h1><p>使用一个共享的 self-attention 层，然后使用不同的 feed forward层去学习不同的模态特征。</p><h1 id="blip">BLIP</h1><p><a href="https://arxiv.org/pdf/2201.12086">BLIP: BootstrappingLanguage-Image Pre-training for Unified Vision-Language Understandingand Generation</a></p><h2 id="model-architecture-1">Model Architecture</h2><p><img src="/2024/09/01/multi_modalv1/blip1.jpg"></p><p>整体上看，对于图像部分，有一个 <span class="math inline">\(N\)</span>层的 ViT。对于文本部分，分别使用三个 text encoder去计算三个不同的目标函数。在 blip 中，同种颜色代表同样的共享参数，</p><p>对于第一个 text encoder，具有 <span class="math inline">\(N\)</span>层，主要是将文本特征和图像特征进行对比学习，计算 ITC loss来进行分类任务。第二个 image-grounded textencoder。提取得到的图像特征通过 cross attention 进入模型，文本特征通过self-attention 得到，然后进行融合得到多模态的特征，计算 ITM loss 来判断image-text pairs 是否匹配。相较于第一个 text encoder，第二个 encoder只需要学习额外的 cross attention 层。为了能够执行 生成式 任务，blip添加了一个 decoder。由于 decoder 不能看到完整的句子，因此将 causalself-attention 替换掉前面 encoder 中的 bi self-attention，不过cross-attention 和 feed forward 层依旧和前面的共享参数。这里 decoder使用的 language modeling loss (LMloss)，根据前面的文本去预测后面的文本，而不是进行文本的完形填空 (i.e.,MLM loss)。</p><p>不同 text encoder 使用不同的 token，分别是[CLS]，[Encode]，[Decode]。</p><h2 id="capfilt">CapFilt</h2><p><img src="/2024/09/01/multi_modalv1/blip2.jpg"></p><p>对于从网络上获取的图文数据质量比较糟糕，图片对应的文本描述往往不够准确。针对这种情况，blipfinetune 了一个 filter 来筛选图文对，一个 captioner来生成合成的文本。<span class="math inline">\(\{I_w, T_w\}\)</span> 是从web 上获取的 noisy image-text pairs，<span class="math inline">\(\{I_h,T_h\}\)</span> 是人工标注的 image-textpairs，通常认为是高质量的。对于预训练好的 blip 模型，首先基于 <span class="math inline">\(\{I_h, T_h\}\)</span>数据对两个预训练好的 textencoder 进行 finetune 得到 filter model。然后对 noisy data <span class="math inline">\(\{I_w, T_w\}\)</span> 进行筛选。同时，基于 <span class="math inline">\(\{I_h, T_h\}\)</span>数据对预训练好的 decoder 进行finetune，用来生成合成的 caption。由于生成的 caption质量并不确定，因此将 <span class="math inline">\(\{I_w, T_s\}\)</span>再通过 filter 进行筛选。最终得到数据 <span class="math inline">\(D =\{I_w, T_w\} + \{I_w, T_s\} + \{I_h, T_h\}\)</span>。</p><h1 id="coca">Coca</h1><h1 id="beitv">BeiTV</h1><h1 id="references">References:</h1><ul><li><a href="https://github.com/mli/paper-reading">沐神,论文精读</a></li></ul>]]></content>
      
      
      <categories>
          
          <category> Foundation Model </category>
          
      </categories>
      
      
        <tags>
            
            <tag> multi-modal </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Transformer, KV Cache, MHA, MQA, GQA, BERT, ViT, MAE, Swin Transformer</title>
      <link href="/2024/08/26/Transformer/"/>
      <url>/2024/08/26/Transformer/</url>
      
        <content type="html"><![CDATA[<h1 id="attention-is-all-you-need">Attention is all you need</h1><h2 id="comparison-with-rnn-and-cnn">Comparison with RNN and CNN</h2><p>RNN: 对于给定一个序列，从左向右进行计算。对于第<span class="math inline">\(t\)</span>个词，会对应一个隐藏状态向量 <span class="math inline">\(h_t\)</span>。该隐藏状态向量 <span class="math inline">\(h_t\)</span> 是由前一个词的隐藏状态向量 <span class="math inline">\(h_{t-1}\)</span> 和当前位置 <span class="math inline">\(t\)</span>的输入词决定的。因此，历史信息可以通过隐藏状态 <span class="math inline">\(h_{t-1}\)</span> 传送到当下。</p><ul><li><p>优点：可以处理时序信息。</p></li><li><p>缺点：（1）由于是序列计算，无法进行并行计算，计算性能较差。（2）如果时序很长，历史信息可以无法有效传输到后面。虽然可以设置较大的<span class="math inline">\(h_{t}\)</span> 缓解该问题，但是存储 <span class="math inline">\(h_t\)</span> 会提升内存的需求。</p></li></ul><p>CNN:</p><ul><li><p>优点：具有多个输出通道(多个卷积核)，每个输出通道可以识别不同的模式。</p></li><li><p>缺点：对于较长的序列，卷积核只能观察到距离较近的像素点，否则需要进行多层卷积操作。</p></li></ul><h2 id="model-architecture">Model Architecture</h2><p>当前的时序模型主要是encoder-decoder的架构。对于一个序列表示 <span class="math inline">\((x_1, ...,x_n)\)</span>，encoder将该序列映射为一个连续表征 <span class="math inline">\(\mathbf z = (\mathbf z_1,..., \mathbfz_n)\)</span>，其中 <span class="math inline">\(\mathbf z_i \inR^d\)</span>, <span class="math inline">\(d\)</span>为隐藏向量维度。对于encoder输出的 <span class="math inline">\(\mathbfz\)</span>，decoder <strong>依次</strong> 生成输出序列 <span class="math inline">\((y_1, ..., y_m)\)</span>。</p><p>注意，对于encoder而言，可以看到整个输入句子。但是，对于decoder而言，无法观察到序列后面的词，因此词是按照自回归模式一个一个生成的，<strong>过去时刻的输出也作为当前时刻的输入</strong>。</p><p><img src="/2024/08/26/Transformer/architecture.jpg?400x310"></p><h3 id="encoder-and-decoder-stacks">Encoder and Decoder Stacks</h3><p>Encoder: 包含 <span class="math inline">\(N=6\)</span>个layer，每个layer具有两个 sub-layers，其中第一个子层是一个 multi-headself-attention，第二个子层是一个position-wise fully connectedfeed-forward network(MLP)。对于每个子层，使用残差连接+layernorm，即Layernorm(<span class="math inline">\(x\)</span> + sublayer(<span class="math inline">\(x\)</span>))。每层的输出维度统一为<span class="math inline">\(d_{model}=512\)</span>。</p><p>Decoder: <span class="math inline">\(N=6\)</span>个layers。每个layer具有三个sub-layers，并且每个子层都使用残差连接+layernorm。对于第一个子层的self-attention，由于不能获取之后的输入，因此使用maskedMHA。</p><h3 id="attention">Attention</h3><p>Query: 需要查询的内容向量；</p><p>Key: 可以认为是用于被查询的关键信息向量；</p><p>Value: 通过将 query和key进行匹配对比，可以获得不同value的权重，然后基于该权重对value进行加权获得输出向量。</p><p>Scaled dot-product attention: <span class="math display">\[\begin{aligned}\text{Attention}(Q,K,V) = \text{softmax}(\frac{QK^T}{\sqrt{d_k}}V),\end{aligned}\]</span> 其中，query Q，key K 以及value V是等长的，都是 <span class="math inline">\(d_k\)</span>.</p><p>对于encoder，使用<strong>self-attention</strong>，query, key andvalue都是来自input embedding投影得到。</p><p>对于decoder, 使用<strong>masked self-attention</strong> 和<strong>cross-attention</strong>。对于cross-attention，key和value来自encoder的输出，query是来自decoder的下一刻的输入得到。通过计算query和key的相似度，对value进行加权得到输出。</p><h2 id="position-wise-feed-forward-networks"><strong>Position-wise</strong>feed-forward networks</h2><p>对于attention 外的sub-layers, 对于每一个position的输入使用<strong>同一个</strong>MLP进行映射： <span class="math display">\[\begin{aligned}\text{FFN}(x) = \text{max}(0, xW_1 + b_1)W_2 + b_2.\end{aligned}\]</span> 其中, <span class="math inline">\(x\)</span>是一个 <span class="math inline">\(512\)</span> 的向量，inner-layer hasdimensionality <span class="math inline">\(d_{ff}=2048\)</span>，输出也是一个 <span class="math inline">\(512\)</span> 向量。</p><h2 id="为什么需要除以sqrtd_k">为什么需要除以<span class="math inline">\(\sqrt{d_k}\)</span></h2><p><a href="https://mp.weixin.qq.com/s/h-24XRdJDDZDg65LTjXA0w">ref1</a></p><ol type="1"><li>当维度 <span class="math inline">\(d_k\)</span>比较大时，点积的大小会增大，元素的相对距离增大，进行softmax操作时，会推动softmax函数往仅有很小的梯度的方向靠拢，导致softmax函数容易导致梯度消失问题。</li><li>假设Q和K的均值为0，方差为1。它们的矩阵乘积将有均值为0，方差为<span class="math inline">\(d_k\)</span>。因此，<span class="math inline">\(d_k\)</span>的平方根被用于缩放（而非其他数值），因为，Q和K的矩阵乘积的均值本应该为0，方差本应该为1，这样会获得一个更平缓的softmax。</li><li>也可以使用其他缩放方式，只要能做到每层参数的梯度保持在训练敏感的范围内，不要太大，不要太小。那么这个网络就比较好训练。</li></ol><h2 id="mask-self-attention">Mask self-attention</h2><p>为了不看到 <span class="math inline">\(t\)</span>时刻之后的内容，对于点积矩阵的上半部分添加一个较小的数字，比如 <span class="math inline">\(-1e10\)</span>，这样经过softmax函数后对应位置会变成零。</p><h2 id="mha">MHA</h2><p><img src="/2024/08/26/Transformer/mha.jpg?400x300"></p><p>对原始的Q, K, V，先通过一个linear layer映射到低维向量，然后进行scaleddot-productattention操作，得到h个输出向量，再将h个输出向量进行拼接，最后再通过一个linearlayer回到<span class="math inline">\(d_{model}\)</span>维度。</p><p>直接进行dot-product时，没有什么需要学习的参数。而使用MHA时，linearlayer的投影参数 <span class="math inline">\(W^Q, W^K, W^V\)</span>是需要学习的，因此可以学习到不同的模式信息。</p><p>计算公式： <span class="math display">\[\begin{aligned}\text{MultiHead}(Q,K,V) &amp;= \text{concat}(\text{head}_{1},\text{head}_{2},...,\text{head}_{h})W^O,\\\text{head}_{i} &amp;= \text{Attention}(QW_i^Q, KW^K_i, VW^V_i),\end{aligned}\]</span> 其中, <span class="math inline">\(W_i^Q \in\mathbb{R}^{d_{model} \times d_k}\)</span>, <span class="math inline">\(W_i^K \in \mathbb{R}^{d_{model} \timesd_k}\)</span>, <span class="math inline">\(W_i^V \in\mathbb{R}^{d_{model} \times d_v}\)</span>, <span class="math inline">\(W_i^O \in \mathbb{R}^{hd_v \timesd_{model}}\)</span>. In this paper, they set <span class="math inline">\(h=8\)</span>, <span class="math inline">\(d_k =d_v = d_{model}/h\)</span>.</p><h2 id="kv-cache-原理-mha-mqa-gqa">KV Cache 原理, MHA, MQA, GQA</h2><p><a href="https://mp.weixin.qq.com/s/mKdliGu4WhUx4PHatBpewA">ref1</a></p><p><a href="https://www.linsight.cn/3dc22f96.html">ref2</a></p><h2 id="positional-encoding">Positional Encoding</h2><p>对于rnn而言，当前时刻的输入包含了上一时刻的输出，依次传递序列信息。而attention是考虑所有词之间的关联性，权重与序列信息无关，并没有将序列/位置信息考虑进去。如果将句子的词打乱，语义可能有所不同，但是attention无法捕捉这种情况。在transformer中，通过将position进行encoding记录时序信息，然后和词的embedding相加作为输入。</p><p><img src="/2024/08/26/Transformer/positional.png"> <!-- $$\begin{aligned}PE(pos, 2i) &= sin(pos/10000^{2i/d_{model}}) \\PE(pos, 2i+1) &= cos(pos/10000^{2i/d_{model}}) \\\end{aligned}$$ --></p><p>pos is the index of the word in the sentence. (0-30) <span class="math inline">\(2i\)</span> and <span class="math inline">\(2i+1\)</span> is the index of the column, d_modelis the number of columns, it is a hyper-parameter(120). For eachword(token), we encode it to a vector with dimension d_model accordingto its position.</p><p>Here we use denominator <span class="math inline">\(10000^{2i/d_{model}}\)</span> to make sure thepositional encoding is different for different tokens. The sin and cosare periodic functions, if we don't use the denominator, then thepositional encoding could be same for different tokens.</p><ul><li>If there are two different sentences with the same size, will thepositional encodings be the same?? yes.</li></ul><h2 id="complexity">Complexity</h2><p><img src="/2024/08/26/Transformer/complexity.jpg"></p><h1 id="bert-bidirectional-encoder-representations-from-transformers">BERT:Bidirectional Encoder Representations from Transformers</h1><ul><li>GPT: 单向，使用过去的信息预测未来。</li><li>ELMo: 基于rnn的架构，双向rnn,在用到一些下游任务时，需要对架构进行调整。</li><li>BERT: 相较于gpt, 可以使用左右侧信息，进行双向预测。相较于ELMo,基于transformer架构，结构简单，只需要修改最上层。</li></ul><p>Bert结合了ELMo的双向性和gpt的transformer架构，将预测未来变成<strong>完形填空</strong>。</p><h2 id="framework">Framework</h2><p>Bert主要包括两部分，pre-training andfine-tuning。在pre-training阶段，模型在一个没有进行标注的数据上进行训练，是一个self-supervised pre-trainingtask。在fine-tuning阶段，用同一个Bert模型，但是模型首先被初始化为预训练的权重，然后再在有标注的数据上进行微调。每一个下游任务都会创建一个不同的模型进行微调。</p><p><img src="/2024/08/26/Transformer/bert.jpg"></p><p>Model architecture: a multi-layer bidirectional transformerencoder.</p><p><strong>主要包括三个参数</strong></p><ol type="1"><li><p>number of layers/ transformer blocks, i.e., <span class="math inline">\(L\)</span>.</p></li><li><p>hidden dimension, i.e., <span class="math inline">\(H\)</span>(<span class="math inline">\(d_{model}\)</span>).</p></li><li><p>the number of attention heads, i.e., <span class="math inline">\(A\)</span>.</p></li></ol><p>两个模型： 1. Bert <span class="math inline">\(_{base}\)</span>:<span class="math inline">\(L=12, H=768, A=12\)</span>, total parametersis <span class="math inline">\(110M\)</span>.</p><ol start="2" type="1"><li>Bert <span class="math inline">\(_{large}\)</span>: <span class="math inline">\(L=24, H=1024, A=16\)</span>, total parameters is<span class="math inline">\(340M\)</span>.</li></ol><p><strong>如何根据超参数设置计算所需要训练的参数量？</strong></p><p>对于transformer架构，输入是字典（句子）的大小，这里假设为<span class="math inline">\(30k\)</span>。通过嵌入层得到输出，输出维度为 <span class="math inline">\(H\)</span> (<span class="math inline">\(d_{model}\)</span>)。输出的embedding会喂给transformer blocks，transformer block中包括两部分，分别是self-attention和 mlp。 对于self-attention，dot-production 没有学习参数，但是对于MHA，会对 Q, K, V分别通过一个 linear layer 映射到低维向量，然后进行scaled dot-product attention 操作，得到 <span class="math inline">\(A\)</span> 个输出向量，再将 <span class="math inline">\(A\)</span> 个输出向量进行拼接，最后再通过一个linear layer 回到 <span class="math inline">\(H\)</span> (<span class="math inline">\(d_{model}\)</span>) 维度。在 MHA中，头的个数乘以低维投影的维度 = <span class="math inline">\(H\)</span>(<span class="math inline">\(d_{model}\)</span>)，因此低维投影部分的参数量为<span class="math inline">\(3 \times H \times H\)</span>。这里乘以 <span class="math inline">\(3\)</span> 的原因是 Q, K, V分别通过一个 linearlayer进行投影操作。同样，对于得到的低维投影向量进行拼接后还会进行一次投影，可学习参数量是<span class="math inline">\(H \times H\)</span>。因此，一个self-attention 层的可学习参数量为 $ 4 H H = 4 H^2$（观察上文中的MHA结构图，可以发现有 4 个linear模块）。接下来是 mlp, mlp具有两个全连接层，第一个全连接层的输入输出是 <span class="math inline">\(H \times 4H\)</span>，第二个全连接层的输入输出是<span class="math inline">\(4H \times H\)</span>，总共为 <span class="math inline">\(8H^2\)</span>。因此，一个transformerblock的可学习参数总共为 <span class="math inline">\(12H^2\)</span>。</p><p>假设模型有 <span class="math inline">\(L\)</span>个blocks，那么该模型的可学习参数总量为<span class="math inline">\(30k \times H + L \times H^2 \times12\)</span>。</p><p>对于Bert <span class="math inline">\(_{base}\)</span>，<span class="math inline">\(L=12, H=768, A=12\)</span>，根据公式计算得到：$30k+ 12 ^2 = 108,514,656 110M $。</p><p><strong>输入输出</strong></p><p>对于transformer而言，输入是一个序列对，编码器和解码器会分别输入一个序列。Bert只有一个编码器，输入是一个序列，可以是一段连续的文字，未必是真正语义上的句子，也可以包含两个句子。</p><p>论文使用 <strong>WordPiece</strong>embeddings。通常来讲，如果根据空格对句子进行划分，每个词为一个token，那么词字典会非常大，输入的嵌入层的token很多，增加可学习参数。WordPiece根据词出现的频率进行划分，如果一个词出现的频率不大，那么将该词切开，看它的一个子序列。如果它的某一个子序列出现的概率较大，那么只保留这个子序列。这种方式可以将一个较长的句子切分成出现频率较高的几个子序列，类似于词根，从而减少词典大小。</p><p>对于一个序列，序列的第一个词token永远是一个特殊的记号 [CLS]，表示classification。这个词的作用是用来表示整个序列层面的信息。虽然该token被放在序列的开头，但是由于Bert使用的是transformer的编码器，依旧可以注意到整个序列中的词，所以可以放在第一个位置。</p><p>由于两个句子被连接到一起作为一个序列进行输入，有两种方式来区分不同的句子。一种是在句子后面添加一个特殊的标记token[SEP]。其次，添加一个可学习的嵌入层来表示每个token所属于的句子id。具体来说，如下图所示，对于一个输入的序列，共有三个嵌入层。首先第一个嵌入层tokenembedding 是对每个词元输出一个向量；第二个是 segmentembedding层，该层的输入是 2，表示该词元属于哪个句子。第三个是<strong><em>可学习的</em></strong> positionembedding层，输入的是每个词元在该序列里的位置信息，从 0开始。最终得到的输出是词元本身的嵌入+所属句子的嵌入+位置嵌入。</p><p><img src="/2024/08/26/Transformer/bert1.jpg"></p><h2 id="pre-training-bert">Pre-training BERT</h2><p>We do not use traditional left-to-right or right-to-left languagemodels to pre-train BERT. Instead, we pre-train BERT using twounsupervised tasks, described in this section.</p><h3 id="task-1-masked-lm">Task #1: Masked LM</h3><p>如果一个词元是由WordPiece生成，那么该词元有<span class="math inline">\(15\%\)</span>的概率被随机替换为掩码[Mask]。对于特殊词元，比如第一个词元[CLS]和中间分割词元[SEP]，就不进行替换。但是这种操作会带来一个问题，在训练的时候会有序列中的词元被替换为特殊token[Mask]，但是在 fine-tune 的时候，输入的序列不会包含 [Mask] 这种token，从而导致两个输入数据的分布不一致。</p><p>为了减少这种情况，预训练时并不总是用实际的 [MASK]标记来替换被掩码的词。训练数据生成器会随机选择 <span class="math inline">\(15\%\)</span> 的词元位置进行预测。如果第 <span class="math inline">\(i\)</span> 个标记被选中，那么该词元 (1) 有 <span class="math inline">\(80\%\)</span> 的概率被替换为 [MASK]；（2）<span class="math inline">\(10\%\)</span>的概率替换为字典中的随机词元；（3）<span class="math inline">\(10\%\)</span> 的概率保持不变。模型会预测这 <span class="math inline">\(15\%\)</span> 的词元，而不是构建整个输入序列。用<span class="math inline">\(T_i\)</span> 表示预测原始标记的<span class="math inline">\(15\%\)</span>词元的交叉熵损失。</p><h3 id="task-2-next-sentence-prediction-nsp">Task #2: Next SentencePrediction (NSP)</h3><p>许多重要的下游任务，如问题解答（QA）和自然语言推断（NLI），都是基于对两个句子之间关系的理解。为了训练一个能理解句子关系的模型，本文预先训练了一个binarizednext sentence predictiontask。该任务可以从任何语料库中生成。具体来说，每个预训练的序列包含两个句子A 和 B，<span class="math inline">\(50\%\)</span> 的情况下, B 是 A后面的实际下一句（标记为 IsNext），<span class="math inline">\(50\%\)</span> 的情况下，B是语料库中的随机句子（标记为 NotNext）。如下图所示。</p><p><img src="/2024/08/26/Transformer/bert3.jpg?400x300"></p><p>其中，‘flight ##less’是一个词，但是该词出现的概率较低，所以WordPiece中被划分为两个词，'##'表示与前面词相连。 如图 1 所示，特殊标记[CLS]对应的输出向量 <span class="math inline">\(C\)</span> 用来进行下一个句子预测。</p><h2 id="fine-tuning-bert">Fine-tuning BERT</h2><p>输入：对于每项任务，只需将任务的输入输出转换为Bert要求格式的输入，然后end-to-end微调所有参数即可。预训练阶段的句子A 和句子 B 类似于：(1) 解析中的句子对；(2) 推断中的假设-前提对；(3)问题解答中的问题-答案对；(4) 当进行文本分类时，句子+<span class="math inline">\(\emptyset\)</span>。</p><p>输出：对于token-level tasks，比如问题解答。将每个词元的embedding fedinto 一个输出层；对于分类任务，[CLS] 表示被fed into一个输出层。然后end-to-end进行调参。</p><h2 id="transformer-位置编码的几种方式">Transformer位置编码的几种方式</h2><p><a href="https://www.kexue.fm/archives/8130">苏剑林：让研究人员绞尽脑汁的Transformer位置编码</a></p><h1 id="vit-iclr-2021">ViT (ICLR 2021)</h1><p>paper: AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGERECOGNITION AT SCALE</p><h2 id="introduction">Introduction</h2><p>Transformer在cv中应用的难点：Transformer中一个self-attention的计算复杂度为<span class="math inline">\(O(n^2)\)</span>。对于2d图像，如果简单地将图像拉成一维序列，每个像素点作为一个词元。对于一个<span class="math inline">\(256 \times256\)</span>的图像，那么self-attention的计算复杂度将会是 <span class="math inline">\((224 \times 224)^2 =50176^2\)</span>。对于较大的图像，序列长度很长，计算复杂度很高。</p><p>在中小型数据集上，ViT的效果较弱，弱于ResNets。主要原因是Transformer不具备归纳偏置(inductive bias)能力。卷积神经网络具有归纳偏置能力，该能力类似于一种先验知识/假设。比如对于卷积神经网络来说，具有两个inductive bias:</p><ol type="1"><li><p>locality。卷积神经网络假设相邻区域具有相邻特征，因此通过滑动窗口进行学习。</p></li><li><p>translation equivariance，平移等价性。<span class="math inline">\(f(g(x)) =g(f(x))\)</span>。在卷积神经网络中，无论是对图片中物体先进行平移再卷积，还是先卷积再平移，只要是对于同一个不变的输入，结果不变。卷积神经网络具备这两个归纳偏置能力，等同于拥有很多先验信息，可以在中小型数据集上表现不错。</p></li></ol><p>通过在大数据集上进行预训练，作者发现，即使transformer不具备归纳偏置能力，依旧具有很好的表现，且可以扩展到下游任务中。</p><p>以往的工作要么是将卷积神经网络和自注意力结合，要么直接用自注意力取代卷积神经网络。但是没有工作直接只用transformer到图像领域。因此，本文直接使用标准的transformer结构，只是对图片进行预处理，划分成块。</p><h2 id="model-architecture-1">Model Architecture</h2><p><img src="/2024/08/26/Transformer/vit.jpg"></p><p>如 Figure 1所示，首先将原始图片划分为 <span class="math inline">\(3\times 3 = 9\)</span> 个patch(token)，然后按照顺序组成一个序列，通过一个 linear layer得到patchembedding。为了表示每个 patch在原始图片中的位置信息，类似于transformer，添加了一个 positionembedding。然后通过一个标准的transformer encoder得到输出。对于分类任务，仿照 BERT，在序列的开头添加一个特殊标记[CLS]，<strong>位置为 0</strong>。因为使用的 transformer 架构，该 token可以注意到序列中其他 patch 的信息，所以可以根据该 token的输出进行判断得到有效信息。</p><p>具体来说，对于大小为 <span class="math inline">\(224 \times 224\times 3\)</span> 的图片，将其划分为 $16 $ 大小的 patch，那么可以得到<span class="math inline">\(224^2/16^2 = 196\)</span> 个 patch，每个patch 的大小为 <span class="math inline">\(16 \times 16 \times 3 =768\)</span>。<span class="math inline">\(196\)</span> 个维度为 <span class="math inline">\(768\)</span> 的 token 通过 linear projection 得到<span class="math inline">\(196\)</span> 个 patchembeddings。再加上特殊字符 [CLS] 对应的embedding，共有 <span class="math inline">\(197\)</span> 个 embeddings。通过将 patchembeddings 和 position embeddings 相加，得到的 transformer encoder输入大小为 <span class="math inline">\(197 \times 768\)</span>。</p><p>对于 transformer block，假设有 <span class="math inline">\(12\)</span> 个 head。通过将输入进行投影得到的<span class="math inline">\(Q\)</span>, <span class="math inline">\(K\)</span>, <span class="math inline">\(V\)</span>的大小为 <span class="math inline">\(197 \times 64\)</span> (<span class="math inline">\(768/12 = 64\)</span>)。最后将 <span class="math inline">\(12\)</span> 个头的输出向量进行拼接得到 <span class="math inline">\(197 \times 768\)</span>。过一层 layernorm 后将输出fed into MLP layer。在 MLP 这层，会将维度放大 <span class="math inline">\(4\)</span> 倍，然后缩小投射回去。即 <span class="math inline">\(197 \times 3012\)</span> --&gt; <span class="math inline">\(197 \times 768\)</span>。</p><p>具体计算公式如下： <img src="/2024/08/26/Transformer/vit1.jpg"></p><p><strong><em>How can I use ViT with a different resolution?</em></strong></p><p>当输入更高分辨率的图像时 (e.g., <span class="math inline">\(512\times 512\)</span>)，论文会保持 patch size不变，这将导致分割后得到的序列长度增加。尽管理论上 ViT可以处理任意长度的序列，但是，位置编码会不同，预训练时的位置编码就没有用。针对这种情况，作者对预训练好的位置编码进行2D 插值来扩充序列。</p><p><a href="https://github.com/huggingface/transformers/issues/12167">githubref</a></p><p><strong><em>为什么不直接对 transformer 的 <span class="math inline">\(n\)</span> 个输出embeddings 进行 global averagepooling，然后基于得到的特征进行分类预测？</em></strong></p><p>作者通过实验表示， global average pooling 方式和加 class token [CLS]两种方式都可以。</p><h2 id="position-embedding">Position Embedding</h2><p>在本文中，作者对比了四种对2D图像使用 position embedding进行编码的方式：</p><ol type="1"><li><p>不提供位置信息；</p></li><li><p>一维positional embedding：将输入的patch 按照栅格顺序（从左到右按行读取） 进行编码。</p></li><li><p>二维positional embedding：将输入视为二维的 patch网格。在这种情况下，需要学习两组嵌入，每组嵌入的大小为 D/2 (<span class="math inline">\(D = d_{model}\)</span>)，分别针对 X 轴和 Y轴。然后，根据输入路径上的坐标，我们将 X 嵌入和 Y 嵌入拼接起来，得到该patch 的最终位置嵌入 <span class="math inline">\(D\)</span>维。</p></li><li><p>Relative positional embeddings：详见 <a href="https://www.kexue.fm/archives/8130">苏剑林：让研究人员绞尽脑汁的Transformer位置编码</a>。</p></li></ol><h2 id="模型参数量以及显存间的计算">模型参数量以及显存间的计算</h2><p><a href="https://mp.weixin.qq.com/s/OfgEoh5UXSqNBTMuTSC12w">现在 LLM的大小为什都设计成 6/7B、13B 和 130B 几个档次？</a></p><p><a href="https://mp.weixin.qq.com/s/bhPo3FO_3AFQpgff-wmDoQ">如何根据模型参数量估计大模型微调和推理所需显存?</a></p><h1 id="mae">MAE</h1><p><a href="https://arxiv.org/pdf/2111.06377">Masked Autoencoders AreScalable Vision Learners</a></p><p>随机遮住大量的 patches， 然后在 pixel space 重构这些缺失的patches，得到原始完整的图片。使用一个非对称的编码器和解码器的架构，非对称的原因是指：编码器只看到未被遮挡的patches，这样会提高计算效率，降低内存需要。</p><p><img src="/2024/08/26/Transformer/mae.jpg?300x300"></p><p><strong>Masking:</strong> 将图像划分为一个个不重叠的patches，然后进行随机均匀采样少量patches，并屏蔽（即去除）剩余的部分。</p><p><strong>Encoder:</strong> Our encoder is a ViT but applied only onvisible, unmasked patches. Just as in a standard ViT, our encoder embedspatches by a linear projection with added positional embeddings, andthen processes the resulting set via a series of Transformer blocks.However, our encoder only operates on a small subset (e.g., 25%) of thefull set. Masked patches are removed; no mask tokens are used. Thisallows us to train very large encoders with only a frac- tion of computeand memory</p><p><strong>Decoder:</strong> The input to the MAE decoder is the fullset of tokens consisting of (i) encoded visible patches, and (ii) masktokens. See Figure 1. Each mask token is a shared, learned vector thatindicates the presence of a missing patch to be predicted(每一个被盖住的块都被表示为同一个可学习的向量). We add<strong>positional embeddings</strong> to all tokens in this full set;without this, mask tokens would have no information about their locationin the image. The decoder has another series of Transformer blocks. TheMAE decoder is only used during pre-training to perform the imagereconstruction task (only the encoder is used to produce imagerepresentations for recognition)</p><h1 id="swin-transformer">Swin Transformer</h1><p><a href="https://arxiv.org/pdf/2103.14030">Swin Transformer:Hierarchical Vision Transformer using Shifted Windows</a></p><h2 id="introduction-1">Introduction</h2><p>ViT始终都是在全图上计算自注意力，计算复杂度是图片大小（像素数量）的平方级。Swintransformer则是在小窗口内计算自注意力，只要窗口大小固定，自注意力的计算复杂度固定。SwinTransformer的复杂度始终与图像的像素数量（而非图像的边长）成线性关系。</p><p>具体来说，假设图片大小为 <span class="math inline">\(N \timesN\)</span>，共有 <span class="math inline">\(L = N^2\)</span>个像素。标准的 transformer 的计算复杂度为 <span class="math inline">\((N^2)^2 = L^2 = O(L^2)\)</span>。假设 swintransformer 的每个窗口固定大小为 <span class="math inline">\(M \timesM\)</span>，单个窗口的计算复杂度为 <span class="math inline">\((M^2)^2\)</span>。共有 <span class="math inline">\((N/M)^2\)</span> 个窗口，那么总计算复杂度为 <span class="math inline">\((N/M)^2 * (M^2)^2 = N^2 M^2 = L \times M^2 =O(L)\)</span>。因此，swin transformer 可以将复杂度降低到 linearcomputation complexity （相较于图片像素数量）。</p><p><img src="/2024/08/26/Transformer/swin1.jpg?300x300"></p><p><img src="/2024/08/26/Transformer/swin2.jpg?300x300"></p><p>捕捉多尺度的特征。</p><h1 id="references">References:</h1><ul><li><p><a href="https://github.com/mli/paper-reading">沐神,论文精读</a></p></li><li><p><a href="http://nlp.seas.harvard.edu/2018/04/03/attention.html">TheAnnotated Transformer</a></p></li><li><p><a href="https://arxiv.org/pdf/1706.03762">Attention is all youneed</a></p></li><li><p><a href="https://jalammar.github.io/illustrated-transformer/" class="uri">https://jalammar.github.io/illustrated-transformer/</a></p></li><li><p><a href="https://arxiv.org/pdf/2010.11929">ViT</a></p></li><li><p><a href="https://arxiv.org/pdf/2103.14030">Swin Transformer:Hierarchical Vision Transformer using Shifted Windows</a></p></li></ul>]]></content>
      
      
      <categories>
          
          <category> Foundation Model </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Transformer </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>行测</title>
      <link href="/2024/08/26/%E8%A1%8C%E6%B5%8B-%E8%A8%80%E8%AF%AD%E7%90%86%E8%A7%A3/"/>
      <url>/2024/08/26/%E8%A1%8C%E6%B5%8B-%E8%A8%80%E8%AF%AD%E7%90%86%E8%A7%A3/</url>
      
        <content type="html"><![CDATA[<h1 id="言语理解">言语理解</h1><p>题型：</p><ol type="1"><li>中心理解，包括概括中心和文段中心有关。</li><li>细节判断。</li><li>语句连贯，语句排序、语句填空、接语选择</li><li>逻辑填空</li></ol><h2 id="中心理解">中心理解</h2><ol type="1"><li>提问方式：</li></ol><ul><li>这段文字主要/旨在/重在/意在...</li><li>这段文字想要说明/论述/强调的是...</li><li>这段文字的主旨/主题/观点是... ...</li></ul><ol start="2" type="1"><li><p>解决思路：<strong><em>寻找中心句 (谁做了什么？)注意表述扩大范围、无中生有的不要选择。</em></strong></p></li><li><p>解决技巧：关键词；关键句。</p></li></ol><h3 id="关键词">关键词</h3><p>关联词帮助快速理顺文段脉络、分清句子主次。<strong><em>谁做了什么</em></strong></p><ol type="1"><li>转折关联词 -<strong><em>转折之后是重点，转折之前不能选。</em></strong></li></ol><p>虽然...,但是/然而/事实上/不过/其实/可是/却...;</p><ol start="2" type="1"><li>关键词之主题词</li></ol><ul><li><p>主题词是文段的切入点和叙述的核心话题，抓住主题词就能快速、准确找到文段中心，对应正确选项。</p></li><li><p>如何找主题词：所围绕的词；高频出现的词。</p></li><li><p>偏离主题的不能选择。</p></li></ul><ol start="3" type="1"><li>因果关联词 - <strong><em>结果是重点</em></strong></li></ol><p>标志词包括： - 因果：因为...,所以...。因此，因而，故而，于是等等。 -指代词：对此、这，这种、在这个意义上等。 -同意替换：换而言之、换句话说。</p><p>以上后面的关联词表示结果关系，结果是重点！</p><ol start="4" type="1"><li>递进关联词 - <strong><em>递进之后内容是重点</em></strong></li></ol><p>不但...,而且...。更，特别是，尤其，致命，核心，突出，根本的是等常见词。</p><ol start="5" type="1"><li>并列关联词 - <strong><em>识别并列，归纳概括总结</em></strong></li></ol><ul><li><p>有标志词：标点符号（；，、）；和、与、也、同时、另外、此外、加上等。一方面、另一方面等等。</p></li><li><p>无标志词：句式相同，排比句，</p></li></ul><h2 id="句子成分">句子成分</h2><ol type="1"><li><p>背景铺垫，交代事件事物发生的背景，通过背景引出话题。<strong><em>围绕背景的选项不用选择</em></strong></p></li><li><p>概念类，对文段中的概念进行解释说明。</p></li><li><p>举例类，通过列举具体实例来论证自己的观点。常见表述：（1）标志词：比如/譬如/例如/拿什么来说；（2）非标志词：人名、专业术语、数据等。<strong><em>举例子解释的是观点，围绕举例子的选项不选。</em></strong></p></li><li><p>原因解释。对某个结果或者某个现象进行具体的解释。<strong><em>所解释的结果/对象是重点。原因解释内的选项，一般不能选择。</em></strong></p></li><li><p>问题类，指出某些不足或矛盾，是需要解决的对象。</p></li><li><p>对策类。描述中有问题，如果有对策选项，那么选对策，否则选问题。</p></li></ol><h2 id="细节判断题">细节判断题</h2><ol type="1"><li>看清提问，选是还是选非；</li><li>勾画文段句子主体；</li><li>勾画选项主体；</li><li>文选对比。</li></ol><p>有可能出现： 1. 无中生有。某个话题未提及。 2.偷换概念。包括扩大、缩小、替换、混搭。 3.偷换/强加逻辑。将某一逻辑关系偷换为另一逻辑关系。 4.偷换时态。时态词。过去时、完成时、将来时。 5.偷换语气、程度。可能/必须，一定/或许。</p><p>细节判断，如果出现中心句，属于中心理解式细节判断。</p><h2 id="语句填空">语句填空</h2><p>要保证文段核心话题的一致性，内容的一致性。 1. 横线开头，总领全文。 2.横线在中间，承上启下。 3. 横线在结尾，总结全文。</p><h2 id="接语选择">接语选择</h2><p>围绕尾句话题展开的为正确选项。</p><p>最不可能讲述的是。。， 选择文中已经讲述过的。</p><ol type="1"><li>尾句有关联标志词的，比如转折、递进、结果。直接围绕尾句话题展开即可。</li><li>尾句无关联标志词的，如果为对策性表述，则围绕对策展开；如果是问题性表述，则围绕问题展开。</li></ol><h2 id="语句排序">语句排序</h2><ol type="1"><li>先观察选项布局。</li><li>确定、对比首句：背景引入、下定义、话题概念范围（概念范围大者比小者更适合做首句）</li><li>尾句对比。</li><li>代入验证。</li></ol><h2 id="逻辑填空">逻辑填空</h2><ol type="1"><li>尊重语义，尊重文段。</li><li>关注搭配主体。比如，成绩不断<strong>逼近</strong>人体能力的极限。</li></ol><h1 id="资料分析">资料分析</h1><h2 id="速算技巧">速算技巧</h2><h3 id="估算的原则">估算的原则</h3><p>保留三位有效数字的误差上限为 <span class="math inline">\(5‰\)</span>，保留两位有效数字的误差上限为 <span class="math inline">\(5\%\)</span>。</p><p>判断原则： 1. 首位均不同，保留两位即可；首位有相同的，保留三位。 2.看选项。选项之间误差在 <span class="math inline">\(10\%\)</span>以上，保留两位即可；选项之间误差在<span class="math inline">\(10\%\)</span>以内，保留三位。 3.看选项。如果选项首位都不同，只关注首位是什么即可。</p><h3 id="截位直除法">截位直除法</h3><p>一步除法：<strong>只估算分母</strong>即可。</p><p>多步连除：分子分母同时截位。比如 <span class="math inline">\((a/b)/(c/d)\)</span>.</p><h3 id="估算法">估算法</h3><p>乘法估算：乘法一大一小，按照比例增加或减少。</p><p>特殊分数：<span class="math inline">\(1/6=16.7\%, 1/7=14.3\%,1/8=12.5\%, 1/9=11.1\%\)</span>.</p><h1 id="判断推理">判断推理</h1><h2 id="图形推理">图形推理</h2><p>图像推理的考点：位置规律，样式规律，属性规律，数量规律，特殊规律，空间重构，立体图形。</p><h3 id="位置规律">位置规律</h3><p><strong>元素组成相同，优先考虑位置规律。</strong></p><p>位置主要有两种： 1.平移。方向（直线（上下、左右、斜对角线）、绕圈（顺逆时针））。常见步数（恒定、等差递增）。2.旋转（顺时针、逆时针，常见角度：45，90，180）、翻转（左右翻转、上下翻转）。</p><p>上下、左右翻转属于转动。</p><h3 id="样式规律">样式规律</h3><p><strong>元素组成相似时，比如相同线条重复出现。</strong></p><ol type="1"><li>加减同异</li></ol><ul><li>相加、相减；</li><li>去同存异；（重点，主要是线的变化，“消消乐”）</li><li>去异求同；</li></ul><p>先横着看，没有规律的话再竖着看，再从下往上看或者米字形/s型开。</p><ol start="2" type="1"><li>黑白运算</li></ol><ul><li>特征：图形轮廓和分割区域相同，内部的颜色不同</li><li>方法：相同位置运算</li></ul><p>同颜色方块数量相同，优先考虑平移；同颜色方块数量不同，优先考虑黑白运算。比如黑块+白块=白块，黑块+黑块=黑块等类似定义运算。</p><h3 id="属性规律">属性规律</h3><p><strong>元素组成不相同、不相似时，优先考虑属性规律。先看是否具有对称性，如果没有看曲直性、开闭性。</strong></p><ol type="1"><li>对称性。（对称轴方向（依次旋转）和数量），轴对称，中心对称。</li></ol><p>中心对称：把一个图形绕着某个点旋转180度，如果图形重合，那么该图形叫做中心对称图形，旋转点叫做对称中心。</p><p>有可能需要注意对称轴和图形有没有重合，是否自带对称轴。</p><p>如果有一个不是对称图形，那么该题不考察对称性。</p><ol start="2" type="1"><li>曲直性。</li></ol><p>全部由曲线构成、全部由直线构成、全部由直线+曲线构成，外直内曲、外曲内直。</p><ol start="3" type="1"><li>开闭性。</li></ol><p>图形全部封闭、全部开放、半开放半封闭</p><p>如果完整的图形留了小开口，可以考虑开闭性。</p><p>线面连接：几个面由线进行连接。</p><h3 id="数量规律">数量规律</h3><p>元素组成不相同、不相似时，优先考虑属性规律。如果数量规律明显，那么考虑。</p><p>考点包括：点、线、面、素、角。</p><ol type="1"><li>点：线与线的交点，切点也是交点。直线与直线、曲线与曲线、直线与曲线的交点。图形特征：（1）线条交叉明显；（2）一团线比较乱；（3）相切较多时。</li><li>线：</li></ol><ul><li>直线数特征图看直线数量：多边形、单一直线（不考虑射线 ）。</li><li>曲线数特征图看曲线：全曲线图、圆、弧（一条平滑的曲线、没有折点）。</li><li>直线和曲线数量的差。第一条直线和最后一条直线之间的垂直或者平行或者相对或者相向关系。</li><li>数笔画数目，是否一笔画（图形由一笔画成，线条不能重复来回画），可以观察线条之间是否连通。奇点数为0或者2时为一笔画。奇点：由一个点发射出奇数条线。<strong>端点属于奇点，奇点数一定是偶数个。笔画数=奇点数/2；非联通图，分开数。</strong>常见的笔画特征图及其变形：五角星、‘日’及其变形、‘田’及其变形、圆和相切/相交。</li></ul><ol start="3" type="1"><li><p>面：<strong>封闭</strong>空间。面是白色的，黑色不是。适用场景：（1）图形被分割、封闭面明显；（2）生活化图形、粗线条图形中留空白区域。</p></li><li><p>素：元素的种类；元素的个数；元素替换；适用场景：多个独立小图形。</p></li><li><p>角：考虑内角（0-180度之间的角）。钝角，锐角，直角的数量。适用场景：扇形图、改造图、折线图。</p></li></ol><h2 id="类比推理">类比推理</h2><h2 id="定义判断">定义判断</h2><h2 id="逻辑推理">逻辑推理</h2><h1 id="数量关系">数量关系</h1><h2 id="代入排除法">代入排除法</h2><p>可以用于：固定题型如年龄问题、余数问题、多位数、不定方程以及不会的题目。</p><h2 id="数字特性">数字特性</h2><ol type="1"><li>奇偶特性。</li></ol><p>奇数+-奇数=偶数，偶数+-偶数=偶数，奇数+-偶数=奇数。<strong>和差同性（无论是差还是和，得到的数字性质一样）</strong></p><p>奇数 <span class="math inline">\(\times\)</span> 奇数=奇数，偶数<span class="math inline">\(\times\)</span> 偶数=偶数，奇数 <span class="math inline">\(\times\)</span> 偶数=偶数。</p><p>适用于 知差求和，知和求差，2倍类，平均分，不定方程。</p><p>例题：y =8x-15，那么y一定是奇数。直接看选项。可以进一步代入选项，x一定是正整数。</p><p>例题：x+y=50，那么x-y一定是偶数。</p><ol start="2" type="1"><li>整除特性。</li></ol><p>一个数能被 2（5）整除，末一位能被 2（5）整除。</p><p>一个数能被 4 （25）整除，末两位能被 4（25）整除。</p><p>一个数能被 8（125）整除，末三位能被 8（125）整除。</p><p>一个数能被 3（9）整除，其各位数字之和能被 3（9）整除。</p><ol start="3" type="1"><li>倍数特性。</li></ol><p>普通倍数，因子倍数，比例倍数。适用于倍数、百分数、分数、比例等题型。</p><p>因子倍数： <span class="math inline">\(x = a \times b \timesc\)</span>.</p><p>比例倍数：<span class="math inline">\(a:b = m:n\)</span>(m和n互质)，那么 <span class="math inline">\(a\)</span> 是 <span class="math inline">\(m\)</span> 的倍数， <span class="math inline">\(b\)</span> 是 <span class="math inline">\(n\)</span> 的倍数， <span class="math inline">\(a-b\)</span> 是 <span class="math inline">\(m-n\)</span> 的倍数， <span class="math inline">\(a+b\)</span> 是 <span class="math inline">\(m+n\)</span> 的倍数。<span class="math inline">\(a/(a+b) = m/(m+n)\)</span>.</p><h2 id="方程法">方程法</h2><p>原则：1. 优先设所求量； 2. 设小不设大； 3. 设中间变量。</p><p>对于不定方程组，具有无数组解。但是要根据具体问题考虑可行解。可以利用奇偶特性、因子倍数、尾数法等。</p><p>例题： <span class="math inline">\(3x+10y=41\)</span>，那么可行解包括$x = 7, y = 2$。</p><p><strong>先看尾数，其次看倍数，再看奇偶特性，最后考虑代入。</strong></p><h2 id="工程问题">工程问题</h2><p>工程总量 = 效率 x 时间</p><h2 id="行程问题">行程问题</h2><h2 id="排列组合">排列组合</h2><h2 id="概率问题">概率问题</h2><h2 id="经济利润">经济利润</h2><h1 id="references">References:</h1><ul><li><p>B站 老闻言语基础精讲班</p></li><li><p>李梦娇 常识判断</p></li><li><p>刘文超 资料分析/判断推理/数量关系</p></li></ul>]]></content>
      
      
      <categories>
          
          <category> 行测 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 行测 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Generative Models</title>
      <link href="/2023/11/29/Generative-Models/"/>
      <url>/2023/11/29/Generative-Models/</url>
      
        <content type="html"><![CDATA[<h1 id="generative-models">Generative Models</h1><h2 id="autoregressive-models-gan-flow-based-models-vae">AutoregressiveModels, GAN, Flow-based Models, VAE</h2><p>GAN: refer to <a href="https://lilianweng.github.io/lil-log/2017/08/20/from-GAN-to-WGAN.html">FromGAN to WGAN</a></p><p>VAE: refer to <a href="https://lilianweng.github.io/lil-log/2018/08/12/from-autoencoder-to-beta-vae.html">FromAutoencoder to Beta-VAE</a></p><p>Flow-based Models: refer to <a href="https://lilianweng.github.io/lil-log/2018/10/13/flow-based-deep-generative-models.html">Flow-basedDeep Generative Models</a></p><p>Autoregressive Models: refer to <a href="https://lrjconan.github.io/UBC-EECE571F-DL-Structures/">AutoregressiveModels</a></p><p>Reference: <a href="https://deepgenerativemodels.github.io/syllabus.html">CS236 - Fall2023 Deep Generative Models</a></p><p>Reference: <a href="https://lrjconan.github.io/UBC-EECE571F-DL-Structures/">EECE 571F(2023 Winter Term 1): Deep Learning with Structures</a></p><h2 id="energy-based-models-ebms">Energy-based Models (EBMs)</h2><h3 id="parameterizing-probability-distributions">Parameterizingprobability distributions</h3><p>In generating models, we want to learn a probability distribution<span class="math inline">\(p_{\theta}(x)\)</span>, which closelymatches the true data distribution <span class="math inline">\(p_{data}(x)\)</span>. The probability shouldsatisfy the following two conditions:</p><ul><li><p>non-negative: <span class="math inline">\(p_{\theta}(x) \geq0\)</span>.</p></li><li><p>sum to one: <span class="math inline">\(\int p_{\theta}(x)dx =1\)</span> or <span class="math inline">\(\sum_{x}p(x) =1\)</span>.</p></li></ul><p>It's not hard to choose a non-negative function, for example, givenany function <span class="math inline">\(f_{\theta}(x)\)</span>, we canchoose <span class="math inline">\(g_{\theta}(x) = f_{\theta}(x)^2,g_{\theta} = \exp(f_{\theta}(x)), g_{\theta}(x) =|f_{\theta}(x)|\)</span>, etc. However, <span class="math inline">\(g_{\theta}(x)\)</span> might not sum to one. Thesolution is to normalize <span class="math inline">\(g_{\theta}(x)\)</span> by dividing the sum of<span class="math inline">\(g_{\theta}(x)\)</span> over all possible<span class="math inline">\(x\)</span>. <span class="math display">\[p_{\theta}(x) = \frac{g_{\theta}(x)}{\sum_{x}g_{\theta}(x)} =\frac{g_{\theta}(x)}{\int g_{\theta}(x) \text{d}x} =\frac{g_{\theta}(x)}{Z(\theta)},\]</span> where <span class="math inline">\(Z(\theta)\)</span> is calledthe Partition function / Normalization constant.</p><p>Example:</p><ul><li><p>Gaussian: <span class="math inline">\(g_{(\mu, \sigma)}(x) =e^{-\frac{(x-\mu)^2}{2 \sigma^2}}\)</span>, volume is <span class="math inline">\(Z(\mu, \sigma) = \int e^{-\frac{(x-\mu)^2}{2\sigma^2}} \text{d}x = \sqrt{2 \pi \sigma^2}\)</span>.</p></li><li><p>Exponential: <span class="math inline">\(g_{\lambda}(x) =e^{-\lambda x}\)</span>, volume is <span class="math inline">\(Z(\lambda) = \int_0^{\infty} e^{-\lambda x}\text{d}x = 1/\lambda\)</span>.</p></li><li><p>Exponential family: <span class="math inline">\(g_{\theta}(x) =h(x) e^{\theta^T T(x)}\)</span>, volume is <span class="math inline">\(Z(\theta) = \int h(x) e^{\theta^T T(x)}\text{d}x\)</span>.</p></li><li><p>Beta, Poisson, Gamma, Dirichlet, etc.</p></li></ul><p>Generally, we can choose <span class="math inline">\(g_{\theta}(x)\)</span> so that <span class="math inline">\(Z(\theta)\)</span> is analytically. But how aboutusing the models that <span class="math inline">\(Z(\theta)\)</span> isnot easy to compute analytically?</p><h3 id="energy-based-models">Energy-based Models</h3><p>EBMs has the following form: <span class="math display">\[p_{\theta}(x) = \frac{1}{\int \exp(f_{\theta}(x))\text{d}x}e^{f_{\theta}(x)} = \frac{1}{Z(\theta)} e^{f_{\theta}(x)}.\]</span></p><p>Why do we choose <span class="math inline">\(f_{\theta}(x)\)</span>as the form of <span class="math inline">\(e^{f_{\theta}(x)}\)</span>?</p><ul><li>We want to capture large variations in probability. We usually touse log-probability.</li><li>Exponential families. Many distributions can be written in thisform.</li><li>Some physical meaning. <span class="math inline">\(-f_{\theta}(x)\)</span> is called the energy.</li></ul><p>Pros:</p><ul><li>We can use any function <span class="math inline">\(f_{\theta}(x)\)</span> to parameterize theprobability distribution.</li><li>Stable training.</li><li>Relatively high sample quality.</li></ul><p>Cons:</p><ul><li>Sampling from <span class="math inline">\(p_{\theta}(x)\)</span> ishard.</li><li>Evaluating and optimizing likelihood is <span class="math inline">\(p_{\theta}(x)\)</span> is hard.</li><li>Curse of dimensionality. Computing <span class="math inline">\(Z(\theta)\)</span> numerically scalesexponentially with the dimensionality of <span class="math inline">\(x\)</span>.</li></ul><h4 id="ebms-with-discrete-observable-variables-and-discrete-latent-variables-restricted-boltzmann-machinerbm">EBMswith Discrete Observable Variables and Discrete Latent Variables:Restricted Boltzmann machine(RBM)</h4><p>Suppose we have binary visible units <span class="math inline">\(x\)</span>, binary hidden units(latent variables)<span class="math inline">\(h\)</span>, the energy function is: <span class="math display">\[E(x, h) = - a^T x - b^T h - x^T W h\]</span> where <span class="math inline">\(a, b, W\)</span> areparameters.</p><p>The probability distribution is: <span class="math display">\[p(x, h) = \frac{1}{Z} e^{-E(x, h)} = \frac{1}{Z} e^{a^T x + b^T h + x^TW h}\]</span> where <span class="math inline">\(Z = \sum_{x, h} e^{-E(x,h)}\)</span>.</p><p><img src="/2023/11/29/Generative-Models/bipartite_model.jpg"> Whyrestricted? - Only one layer of hidden units. - No connections betweenhidden units.</p><p>Bipartite graph: conditional independence <span class="math display">\[\begin{aligned}p(x|h) &amp;= \prod_{i=1}^D p(x_i|h) \\p(h|x) &amp;= \prod_{j=1}^H p(h_j|x)\end{aligned}\]</span></p><p>Formally, we have <span class="math display">\[\begin{aligned}p(x|h = \tilde{h}) \propto \exp(- E_{\theta}(x, h = \tilde{h})) \propto\exp(-\tilde{a}^Tx) = \prod_{i} \exp(-\tilde{a}_i x_i)\end{aligned}\]</span></p><h4 id="inference-gibbs-sampling">Inference: Gibbs Sampling</h4><p>In inference, we want to compute the maximum a posterior(MAP) <span class="math inline">\(p(h|x)\)</span> and computing the marginals <span class="math inline">\(p(x)\)</span>.</p><ul><li>Due to the conditional independence, we can compute <span class="math inline">\(p(h|x)\)</span> in parallel.</li><li>But, the marginal <span class="math inline">\(p(x)\)</span> isintractable. We need to use Markov Chain Monte Carlo(MCMC) to samplefrom <span class="math inline">\(p(x)\)</span>.</li></ul><p>Gibbs sampling is a special case of MCMC. It can draw samples from<span class="math inline">\(p(x_1, x_2,...,x_n)\)</span> by iterativelysampling from the conditional distributions <span class="math inline">\(p(x_i|x_1, x_2,...,x_{i-1},x_{i+1},...,x_n)\)</span>.</p><p>In RBM, we do not iterative over individual variables. Instead, we doblock-Gibbs sampling, i.e., sampling a block of variables conditioned onthe other block.</p><blockquote><p>Given initial sample <span class="math inline">\((x^{(0)},h^{(0)})\)</span>,</p><p>for <span class="math inline">\(t = 1, 2, ..., T\)</span>:</p><p><span class="math display">\[\begin{aligned}h^{(t)} &amp;\sim p(h|x = x^{(t-1)}), \\x^{(t)} &amp;\sim p(x|h = h^{(t)}),\end{aligned}\]</span></p><p>Return <span class="math inline">\((x^{(T)}, h^{(T)})\)</span>.</p></blockquote><p>For both <span class="math inline">\(p(h|x)\)</span> and <span class="math inline">\(p(x|h)\)</span>, we can sampling in parallel.</p><p>Remark: <strong>the Gibbs sampler can generate random variables froma (marginal) distribution indirectly.</strong> After sampling manyiterations, <span class="math inline">\((x^{(T)}, h^{(T)})\)</span>follows the distribution <span class="math inline">\(p(x, h)\)</span>,<span class="math inline">\(x^{(T)}\)</span> follows the marginaldistribution <span class="math inline">\(p(x)\)</span>, and <span class="math inline">\(h^{(T)}\)</span> follows the marginal distribution<span class="math inline">\(p(h)\)</span>. For more detials:</p><p><a href="https://uh.edu/~cmurray/courses/econ_7395/Explaining%20the%20Gibbs%20Sampler.pdf">Explainingthe Gibbs sampler</a></p><p><a href="https://edisciplinas.usp.br/pluginfile.php/7733433/mod_resource/content/1/aula9slidesT.pdf#:~:text=%E2%80%9CThe%20Gibbs%20sampler%20is%20a,this%20scheme%20may%20seem%20mysterious.">MarkovChain Monte Carlo.Gibbs Sampler.</a></p><h4 id="learning-contrastive-divergence">Learning: ContrastiveDivergence</h4><p>In RBMs, we want to learn the parameters <span class="math inline">\(\theta\)</span> by maximizing the summedlog-likelihood of the training data <span class="math inline">\(\logp_{\theta}(x)\)</span>. The problem is that the partition function <span class="math inline">\(Z(\theta)\)</span> is intractable. Contrastivedivergence(CD) is a method to approximate the gradient of thelog-likelihood.</p><p>Since <span class="math display">\[\begin{aligned}    \frac{\partial p_{\theta}(x)}{\partial \theta} &amp;=\frac{1}{p_{\theta}(x)} \frac{\partial p_{\theta}(x)}{\partial \theta}\\    &amp;= \frac{1}{p_{\theta}(x)} \frac{\partial \int p_{\theta}(x,h)\text{d}h}{\partial \theta} \\    &amp;= \frac{1}{p_{\theta}(x)} \int \frac{\partial p_{\theta}(x,h)}{\partial \theta}\text{d}h \\    &amp;=  \frac{1}{p_{\theta}(x)} \int \frac{\frac{1}{Z}\exp(-E_{\theta}(x, h))}{\partial \theta} \text{d}h \\    &amp;= \frac{1}{p_{\theta}(x)} \int (-\frac{1}{Z^2}\exp(-E_{\theta}(x, h)) \frac{\partial Z}{\partial \theta} - \frac{1}{Z}\exp(-E_{\theta}(x, h)) \frac{\partial E_{\theta}(x, h)}{\partial\theta}) \text{d}h \\    &amp;= -\frac{1}{p_{\theta}(x)} \int \frac{1}{Z}\frac{\partial  Z}{\partial \theta} p_{\theta}(x, h) \text{d}h  -\frac{1}{p_{\theta}(x)}  \int  \frac{\partial E_{\theta}(x, h)}{\partial\theta} p_{\theta}(x, h) \text{d}h \\    &amp;= - \int \frac{1}{Z} \frac{\partial  Z}{\partial \theta}p_{\theta}(h | x) \text{d}h  - \int \frac{\partial E_{\theta}(x,h)}{\partial \theta} p_{\theta}(h | x) \text{d}h  \\    &amp;= - \frac{1}{Z} \frac{\partial  Z}{\partial \theta} - \int\frac{\partial E_{\theta}(x, h)}{\partial \theta} p_{\theta}(h | x)\text{d}h  \\    &amp;= - \frac{1}{Z} \frac{\partial  \int \int  \exp(-E_{\theta}(x,h)) \text{d}x \text{d}h}{\partial \theta} -\mathbb{E}_{p_{\theta}(h|x)}[\frac{\partial E_{\theta}(x, h)}{\partial\theta}] \\    &amp;=  -\int \int   (-\frac{\partial E_{\theta}(x, h)}{\partial\theta}) p_{\theta}(x,h) \text{d}x \text{d}h -\mathbb{E}_{p_{\theta}(h|x)}[\frac{\partial E_{\theta}(x, h)}{\partial\theta}] \\    &amp;= \mathbb{E}_{p_{\theta}(h|x)}[-\frac{\partial E_{\theta}(x,h)}{\partial \theta}] - \mathbb{E}_{p_{\theta}(x, h)}[-\frac{\partialE_{\theta}(x, h)}{\partial \theta}]  \\\end{aligned}\]</span> Here we don't know the distribution <span class="math inline">\(p_{\theta}(h|x)\)</span>. Maximizing the summedlog-likelihood of the training data <span class="math inline">\(\logp_{\theta}(x)\)</span> is equivalent to minimizing the KL divergencebetween the real data distribution <span class="math inline">\(p_{data}(x)\)</span> and the model distribution<span class="math inline">\(p_{\theta}(x)\)</span>: <span class="math display">\[\begin{aligned}    \min_{\theta} \text{KL}(p_{data}(x) || p_{\theta}(x)) =\min_{\theta} \int p_{data}(x) \log p_{data}(x) \text{d}x - \intp_{data}(x) \log p_{\theta}(x) \text{d}x.\end{aligned}\]</span> Since the entropy of <span class="math inline">\(p_{data}(x)\)</span> is: <span class="math display">\[\begin{aligned}    H(p_{data}(x)) &amp;= - \int p_{data}(x) \log p_{data}(x) \text{d}x\end{aligned}\]</span> The cross-entropy of <span class="math inline">\(p_{data}(x)\)</span> and <span class="math inline">\(p_{\theta}(x)\)</span> is: <span class="math display">\[\begin{aligned}    H(p_{data}(x), p_{\theta}(x)) &amp;= -\int p_{data}(x) \logp_{\theta}(x) \text{d}x \\\end{aligned}\]</span> And <span class="math inline">\(H(p_{data}(x), p_{\theta}(x))= H(p_{data}(x)) + \text{KL}(p_{data}(x) || p_{\theta}(x))\)</span>.</p><p>The entropy of <span class="math inline">\(p_{data}(x)\)</span> is aconstant, so minimizing the KL divergence is equivalent to minimizingthe cross-entropy of <span class="math inline">\(p_{data}(x)\)</span>and <span class="math inline">\(p_{\theta}(x)\)</span>, which isequivalent to maximizing： <span class="math display">\[\begin{aligned}    \max_{\theta} \int p_{data}(x) \log p_{\theta}(x) \text{d}x.\end{aligned}\]</span></p><p>We can use stochastic gradient ascent to maximize the above equation.The gradient is: <span class="math display">\[\begin{aligned}    \frac{\partial}{\partial \theta} \int p_{data}(x) \log p_{\theta}(x)\text{d}x &amp;= \int p_{data}(x) \frac{\partial}{\partial \theta} \logp_{\theta}(x) \text{d}x \\    &amp;= \mathbb{E}_{p_{\theta}(h|x)p_{data}(x)}[-\frac{\partialE_{\theta}(x, h)}{\partial \theta}] - \mathbb{E}_{p_{\theta}(x,h)}[-\frac{\partial E_{\theta}(x, h)}{\partial \theta}].\end{aligned}\]</span> We can use Monte Carlo to approximate the above equation.</p><ul><li>For the first expectation <span class="math inline">\(\mathbb{E}_{p_{\theta}(h|x)p_{data}(x)}[-\frac{\partialE_{\theta}(x, h)}{\partial \theta}]\)</span>, we can first sample <span class="math inline">\(x\)</span> from <span class="math inline">\(p_{data}(x)\)</span> (we don't know thedistribution of real data, but we have training data.), then sample<span class="math inline">\(h\)</span> from <span class="math inline">\(p_{\theta}(h|x)\)</span>.</li><li>For the second expectation <span class="math inline">\(\mathbb{E}_{p_{\theta}(x, h)}[-\frac{\partialE_{\theta}(x, h)}{\partial \theta}]\)</span>, we can use<strong>finite-step</strong> Gibbs sampler.</li></ul><p>In this way, we don't need to compute the partition function <span class="math inline">\(Z(\theta)\)</span>. This method is calledContrastive Divergence(CD).</p><h4 id="ebms-with-continuous-observable-variables-and-discrete-latent-variables-grbms">EBMswith Continuous Observable Variables and Discrete Latent Variables:GRBMs</h4><p>Here, we consider continuous observable variables <span class="math inline">\(v\)</span> and binary units (latent variables)<span class="math inline">\(h\)</span>. The energy function is: <span class="math display">\[E_{\theta}(v, h) = \frac{1}{2}(\frac{v-\mu}{\sigma})^T(\frac{v-\mu}{\sigma}) - (\frac{v}{\sigma^2}) Wh - b^T h.\]</span> The conditional independence still holds: <span class="math display">\[\begin{aligned}p(v|h) &amp;= \mathcal{N}(v | Wh + \mu, \text{diag}(\sigma^2)) \\p(h_j = 1|v) &amp;= [\text{Sigmoid}(W^T \frac{v}{\sigma^2} + b)]_j\end{aligned}\]</span></p><h3 id="modern-ebms">Modern EBMs</h3><h4 id="ebms-with-learnable-energy-functions">EBMs with Learnable EnergyFunctions</h4><p>For RBMs, we designed the energy function in advance, and it impliedconditional independence. But, in general, it's hard to design theenergy function in advance.Thus, we want to learn the energy function<span class="math inline">\(E_{\theta}(x)\)</span> from data.</p><p>One way is to use deep neural networks to parameterize the energyfunction <span class="math inline">\(E_{\theta}(x)\)</span>. Forexample, we can use U-Net architecture：</p><p><img src="/2023/11/29/Generative-Models/u-net.png"></p><p>The energy obtained by the energy function is a scalar and the outputof the U-Net is a tensor. Thus, we need to design some readout choicesto get the scalar energy. For example, <span class="math display">\[\begin{aligned}    E_{\theta}(x) &amp;= x^T f_{\theta}(x), \\    E_{\theta}(x) &amp;= (x - f_{\theta}(x))^2, \\    E_{\theta}(x) &amp;= f_{\theta}(x)^2 .\\\end{aligned}\]</span> Empirically, the first choice is better.</p><h4 id="inference-langevin-monte-carlo">Inference: Langevin MonteCarlo</h4><p>After learning the energy function <span class="math inline">\(E_{\theta}(x)\)</span>, how to sample from <span class="math display">\[p_{\theta}(x) =\frac{1}{Z}\exp(-E_{\theta}(x))\]</span></p><p>One way is to use Langevin Monte Carlo(LMC). The stochasticdifferential equation(SDE) of LMC is: <span class="math display">\[\begin{aligned}    \text{d}x = \nabla \log p_{\theta} (x) \text{d}t + \sqrt{2} \text{d}B_t\end{aligned}\]</span> where <span class="math inline">\(B_t\)</span> is a standardBrownian motion. The first term <span class="math inline">\(\nabla \logp_{\theta} (x) \text{d}t\)</span> is called the drift term, whichdominates the movement of the particle. The second term <span class="math inline">\(\sqrt{2} \text{d} B_t\)</span> is called thediffusion term, which includes the stochasticity of the process.</p><p>One can prove Langevin diffusion is irreducible, strong Feller, andaperiodic. Thus, <strong>the stationary distribution of the Langevindiffusion is <span class="math inline">\(p_{\theta}(x)\)</span>, and wecan use Langevin diffusion to sample from <span class="math inline">\(p_{\theta}(x)\)</span>.</strong></p><p>To turn the Langevin diffusion into a sampling algorithm, we need todiscretize the SDE. The simplest way is to use Euler-Maruyamadiscretization: <span class="math display">\[\begin{aligned}    \text{d}x &amp;= \nabla \log p_{\theta} (x) \text{d}t + \sqrt{2}\text{d} B_t \\    x_{t+\eta} &amp;= x_t + \nabla \log p_{\theta} (x_t) (t+\eta - t) +\sqrt{2} (B_{t+\eta} - B_t) \\    &amp;= x_t + \eta \nabla \log p_{\theta} (x_t) + \sqrt{2 \eta}\epsilon, \quad \epsilon \sim N(0, I)\end{aligned}\]</span> where <span class="math inline">\(\eta\)</span> is the stepsize.</p><ul><li>If we ignore the noise term, we are using gradient ascent tomaximize the density, this means we are trying to fing the 'mode' of<span class="math inline">\(x\)</span>. The 'mode' is the mean of thedistribution. In order to generate more samples, we add noise term.</li></ul><p>Sampling algorithm: - Given initial sample <span class="math inline">\(x^{(0)}\)</span> and step size <span class="math inline">\(\eta\)</span>. - for <span class="math inline">\(t= 1, 2, ..., T\)</span>: <span class="math inline">\(x^{(t)} = x^{(t-1)}+ \eta \nabla \log p_{\theta} (x^{(t-1)}) + \sqrt{2 \eta} \epsilon,\quad \epsilon \sim N(0, I)\)</span> - Return <span class="math inline">\(x^{(T)}\)</span>.</p><p>In EBMs, the score function <span class="math inline">\(\nabla \logp_{\theta} (x)\)</span> is the derivative of the energy function <span class="math inline">\(\log p_{\theta} (x)\)</span> with respect to <span class="math inline">\(x\)</span>, and <span class="math display">\[\nabla_x \log p_{\theta} (x) = \nabla_x (-E_{\theta}(x) - \log Z)=-\nabla E_{\theta}(x).\]</span> here, <span class="math inline">\(Z\)</span> doesn't depend on<span class="math inline">\(x\)</span>.</p><p>Noticed that there is another score function <span class="math inline">\(\nabla \log p_{\theta} (x)\)</span>, which is thederivative of the probability distribution <span class="math inline">\(\log p_{\theta} (x)\)</span> with respect to <span class="math inline">\(\theta\)</span>.</p><h4 id="learning-contrastive-divergence-1">Learning: ContrastiveDivergence</h4><p>Similar to RBMs, we can use contrastive divergence to update <span class="math inline">\(\theta\)</span>. The gradient is <span class="math display">\[\int p_{data} \frac{\partial \log p_{\theta}(x)}{\partial \theta}\text{d} x = \mathbb{E}_{p_{data}(x)}[- \frac{\partialE_{\theta}(x)}{\partial \theta}] -\mathbb{E}_{p_{\theta}(x)}[-\frac{\partial E_{\theta}(x)}{\partial\theta}].\]</span></p><p>For the second expectation, we can use Langevin Monte Carlo samplingto sample from <span class="math inline">\(p_{\theta}(x)\)</span>, andthen estimate the expectation.</p><h4 id="score-matching">Score Matching</h4><p>In contrastive divergence, <strong>at each trainingiteration</strong>, we use Langevin Monte Carlo to sample from <span class="math inline">\(p_{\theta}(x)\)</span>, and then estimate theexpectation. However, the Langevin Monte Carlo sampling is notefficient, expecially in high-dimensional space. Thus, we need to trainthe model without sampling.</p><p>Score matching is a method to train the model without sampling. Theidea is to minimize the difference between the score function <span class="math inline">\(\nabla \log p_{\theta} (x)\)</span> and the scorefunction of the data distribution <span class="math inline">\(\nabla\log p_{data} (x)\)</span>.</p><p>The (stein) score function is: <span class="math display">\[s_{\theta}(x) = \nabla \log p_{\theta} (x) = -\nabla E_{\theta}(x),\]</span> which is independent of the partition function <span class="math inline">\(Z(\theta)\)</span> and needs <strong>the pdf isdifferentiable</strong>.</p><p><strong>Fisher divergence</strong> between two distributions <span class="math inline">\(p(x)\)</span> and <span class="math inline">\(q(x)\)</span> is: <span class="math display">\[D_F(p(x), q(x)) = \frac{1}{2} \mathbb{E}_{x \sim p(x)}[||\nabla_x \logp(x) - \nabla_x \log q(x)||_2^2].\]</span> Score matching is to minimize the Fisher divergence between<span class="math inline">\(p_{\theta}(x)\)</span> and <span class="math inline">\(p_{data}(x)\)</span>: <span class="math display">\[\begin{aligned}    \min_{\theta} D_F(p_{\theta}(x), p_{data}(x)) &amp;= \min_{\theta}\frac{1}{2} \mathbb{E}_{x \sim p_{data}(x)}[||\nabla_x \log p_{data}(x)- s_{\theta}(x)||_2^2] \\    &amp;= \min_{\theta} \frac{1}{2} \mathbb{E}_{x \simp_{data}(x)}[||\nabla_x \log p_{data}(x) - (-\nabla_x E_{\theta}(x))||_2^2]\end{aligned}\]</span> Since we don't know the real data distribution, we need todeal with <span class="math inline">\(\nabla_x \logp_{data}(x)\)</span>. Assume that $ p_{data}(x)$ decays to 0sufficiently rapidly as <span class="math inline">\(x \rightarrow \pm\infty\)</span>, one can derive the following equation: <span class="math display">\[\begin{aligned}   &amp;\frac{1}{2} \mathbb{E}_{x \sim p_{data}(x)}[||\nabla_x \logp_{data}(x) - \nabla_x \log p_{\theta}(x)||_2^2] \\    =&amp;\mathbb{E}_{x \sim p_{data}(x)}[\frac{1}{2} ||\nabla_x \logp_{\theta}(x)||^2_2 + \text{tr}(\nabla^2_x \log p_{\theta}(x))] +\text{const},\end{aligned}\]</span> where <span class="math inline">\(tr(\nabla^2_x \logp_{\theta}(x))\)</span> is the trace of the Hessian matrix of <span class="math inline">\(\log p_{\theta}(x)\)</span>. Therefore, we can usemonte carlo to estimate the above loss: <span class="math display">\[\begin{aligned}   &amp;\mathbb{E}_{x \sim p_{data}(x)}[\frac{1}{2} ||\nabla_x \logp_{\theta}(x)||^2_2 + \text{tr}(\nabla^2_x \log p_{\theta}(x))], \\   =&amp; \frac{1}{n} \sum_{i=1}^n [\frac{1}{2} ||\nabla_x \logp_{\theta}(x_i)||^2_2 + \text{tr}(\nabla^2_x \log p_{\theta}(x_i))] \\   =&amp; \frac{1}{n} \sum_{i=1}^n [\frac{1}{2} ||\nablaE_{\theta}(x_i)||^2_2 + \text{tr}(\nabla^2_x \log p_{\theta}(x_i))] \\\end{aligned}\]</span> Then, we can use stochastic gradient descent to minimize theabove loss.</p><p>Note: computing the trace of the Hessian matrix <span class="math inline">\(\text{tr}(\nabla^2_x \log p_{\theta}(x))\)</span>is expensive.</p><p>Conclusions:</p><ul><li>we have used two distances for training EBMs:<ul><li>KL divergence, which is equal to maximum likelihood. (contrastivedivergence).</li><li>Fisher divergence, which is equal to score matching.</li></ul></li><li>Energy-based models are very felxible probabilistic models withintracable partition functions.</li><li>Sampling is hard and requires MCMC.</li><li>Computing the likelihood is hard.</li><li>Comparing the likelihood/probability of two different points istractable.</li><li>Contrastive divergence is a good approximation to maximumlikelihood. But, it needs sampling for each iteration.</li><li>Sampling free methods: score matching, noise contrastive estimation,adversarial optimization, etc.</li></ul><h2 id="score-based-models">Score-Based Models</h2><p>How to represent probability distribution function <span class="math inline">\(p(x)\)</span> in different models:</p><ul><li><p>GAN: min-max loss</p></li><li><p>Autoregressive models: <span class="math inline">\(p_{\theta}(x)= \prod_{i=1}^{d} p_{\theta}(x_i | x_{&lt;i})\)</span></p></li><li><p>Flow-based models: <span class="math inline">\(p_{\theta}(x) =p(z) |\det(J_{f_{\theta}}(x))|\)</span>, <span class="math inline">\(z =f_{\theta}(x)\)</span>.</p></li><li><p>VAE: use ELBO obj and latent variables</p></li><li><p>EBMs: <span class="math inline">\(p_{\theta}(x) =\frac{1}{Z}\exp(-E_{\theta}(x))\)</span></p></li></ul><p>Pros: except for GAN, these models are maximizing the likelihood.</p><p>Cons: They need special atchitectures or surrogate losses.</p><p>Remember that the score function is: <span class="math display">\[s_{\theta}(x) = \nabla \log p_{\theta} (x).\]</span> As shown in the following figure, score function is thegradient of the log probability function <span class="math inline">\(\log p_{\theta}(x)\)</span> and the direction ofthe score function is the vector to the mode of the distribution. Thismeans that the score function directly models the vector field ofgradients.</p><p><img src="/2023/11/29/Generative-Models/score.jpg"></p><p>Score matching is not limited to EBMs. We can use score matching totrain other models, such as autoregressive models, flow-based models,etc.</p><p><img src="/2023/11/29/Generative-Models/scorebased-models.jpg"></p><p>We want to train a score-based model <span class="math inline">\(s_{\theta}\)</span> to estimate the score <span class="math inline">\(\nabla_{x} \log p_{data}(x)\)</span>, we use theaverage Euclidean distance between the score function <span class="math inline">\(s_{\theta}(x)\)</span> and the score <span class="math inline">\(\nabla_{x} \log p_{data}(x)\)</span> over thewhole space as the loss function: <span class="math display">\[\begin{aligned}    \frac{1}{2} \mathbb{E}_{x \sim p_{data}(x)}[||s_{\theta}(x) -\nabla_{x} \log p_{data}(x)||_2^2],  (\text{Fisher divergence})\end{aligned}\]</span> which is equal to minimize: <span class="math display">\[\begin{aligned}    \mathbb{E}_{x \sim p_{data}(x)}[\frac{1}{2} ||s_{\theta}(x) ||^2_2 +\text{tr}(\nabla_x s_{\theta}(x))],  (\text{Score matching})\end{aligned}\]</span></p><p>We need to compute the value of the score function <span class="math inline">\(s_{\theta}(x)\)</span> and the trace of theJacobian matrix <span class="math inline">\(\text{tr}(\nabla_xs_{\theta}(x))\)</span>. Thus, the score model must be efficient toevaluate. Since the score models is not scalable, computing the trace ofthe Jacobian matrix <span class="math inline">\(\text{tr}(\nabla_xs_{\theta}(x))\)</span> in the backpropagation is order of <span class="math inline">\(O(d)\)</span>, where <span class="math inline">\(d\)</span> is the dimension of <span class="math inline">\(x\)</span>. We need to find an efficient way totrain the score model.</p><h3 id="denoising-score-matching">Denoising Score Matching</h3><p>Consider the perturbed distribution: <span class="math display">\[q_{\sigma}(\tilde{x}|x) = \mathcal{N}(\tilde{x}|x, \sigma^2 I), \qquadq_{\sigma}(\tilde{x}) = \int p(x)q_{\sigma}(\tilde{x}|x) \text{d}x.\]</span> Instead of estimating <span class="math inline">\(\nabla_x\log q_{\theta}(x)\)</span>, we can estimate <span class="math inline">\(\nabla_{\tilde{x}} \logq_{\sigma}(\tilde{x})\)</span>. It's easier to estimate and when thenoise level is small, <span class="math inline">\(q_{\sigma}(\tilde{x})\approx p(\tilde{x})\)</span>.</p><p>Therefore, we can use denoising score matching to match the score ofa noise-perturbed distribution: <span class="math display">\[\begin{aligned}    &amp;\frac{1}{2}E_{\tilde{x} \simq_{\sigma}}[\|\nabla_{\tilde{x}}\log q_{\sigma}(\tilde{x}) -s_{\theta}(\tilde{x})\|_2^2] \\    =&amp; \frac{1}{2} E_{x \sim p_{data}(x), \tilde{x} \simq_{\sigma}(\tilde{x}|x)}[\|s_{\theta}(\tilde{x})- \nabla_{\tilde{x}}\log q_{\sigma}(\tilde{x}|x)\|_2^2] + \text{const}.\end{aligned}\]</span> In this form, we don't need to compute the trace of theJacobian matrix <span class="math inline">\(\text{tr}(\nabla_xs_{\theta}(x))\)</span>. Since <span class="math inline">\(q_{\sigma}(\tilde{x}|x) = \mathcal{N}(\tilde{x}|x,\sigma^2 I)\)</span>, <span class="math inline">\(\nabla_{\tilde{x}}\log q_{\sigma}(\tilde{x}|x) = -\frac{\tilde{x} - x}{\sigma^2}\)</span>.It's more efficient to optimize for high dimensional data.</p><p>Con: notice that, we use score function to estimate thenoise-perturbed distribution, which means <strong>we cannot estimate thescore of the clean data</strong>.</p><h4 id="denoising">Denoising</h4><p>Denoising: after training a score model, we can use langevin MCsampling to get noise samples from <span class="math inline">\(q_{\sigma}(\tilde{x})\)</span>. According toTweedie's formula: <span class="math display">\[E_{x \sim p(x|\tilde{x})}[x] = \tilde{x} + \sigma^2 \nabla_x \logq_{\sigma}(\tilde{x}) \approx \tilde{x} + \sigma^2s_{\theta}(\tilde{x}).\]</span> [remember <span class="math inline">\(s_{\theta}(\tilde{x}) =\nabla_{\tilde{x}}\log q_{\sigma}(\tilde{x})\)</span>]</p><p>Langevin MCMC: from scores to samples:</p><ul><li>Given initial sample <span class="math inline">\(x^{(0)}\)</span>.</li><li>for <span class="math inline">\(t = 1, 2, ..., T\)</span>:</li><li><span class="math inline">\(\qquad z^{(t)} \sim \mathcal{N}(0,I)\)</span></li><li><span class="math inline">\(\qquad\tilde{x}^{(t)} =\tilde{x}^{(t-1)} + \frac{\epsilon}{2} \nabla s_{\theta}(\tilde{x}) +\sqrt{\epsilon} z^t\)</span></li><li>Return <span class="math inline">\(\tilde{x}^{(T)}\)</span>.</li></ul><p>If noise <span class="math inline">\(\epsilon \to 0\)</span> and<span class="math inline">\(T \to \infty\)</span>, <span class="math inline">\(\tilde{x}^{(T)} \sim p_{data}(x)\)</span>.</p><h4 id="multi-scale-noise-perturbation">Multi-scale NoisePerturbation</h4><p>When using the denoising score matching, we only use the observedsamples to estimate the scores. Thus, the estimated scores in lowdensity regions are not accurate. Moreover, langevin MCMC converges veryslowly.</p><p><img src="/2023/11/29/Generative-Models/low_density.png"></p><p>One way to improve the accuracy of the estimated scores in lowdensity regions is to increase the noise level <span class="math inline">\(\sigma\)</span>. As shown in the following figure,the estimated scores in low density regions are more accurate when thenoise level <span class="math inline">\(\sigma\)</span> is large.</p><p><img src="/2023/11/29/Generative-Models/add_noise.png"></p><ul><li>High noise provides useful directional information for Langevindynamics.</li><li>But perturbed density no longer approximates the true datadensity.</li></ul><p>Multi-scale noise perturbation: perturb data with different levels ofnoise simulteanously, and aggregate the information from all noiselevels.</p><p><img src="/2023/11/29/Generative-Models/multi_noise.png"></p><p>If the noise levle <span class="math inline">\(\sigma\)</span> islarge, the perturbed data quality is worse, but the estimated score ismore close to the perturbed scores. It's a trade-off between the dataquality and estimated score accuracy.</p><p>When the noise is small, the perturbed distribution is close to theoriginal data distribution, but the estimation errors in low densityregions are still high.</p><p>When we add larger and larger noise, the estimation score is close tothe perturbed data score, but the perturbed data score differs from theoriginal data score.</p><p>In order to achieve the best data quality and estimation accuracy atthe same time, we should consider all perturbations jointly instead offocusing on only one perturbation.</p><p><strong>Training procedure</strong>: Assume we have <span class="math inline">\(L\)</span> noise levels <span class="math inline">\(\sigma_1, \sigma_2, ..., \sigma_L\)</span> andcorresponding perturbed data distributions <span class="math inline">\(q_{\sigma_1}(\tilde{x}), q_{\sigma_2}(\tilde{x}),..., q_{\sigma_L}(\tilde{x})\)</span>. For each perturbed datadistribution, we can easily sample from them, and use score estimationto estimate the corresponding scores. However, this method requires alarge number of separate score models to be learned independently, whichis very costly, exspecially when the number of noise levels <span class="math inline">\(L\)</span> is large.</p><p>One way is to train a single conditional score network for all noiselevels. The score model will take <span class="math inline">\(\sigma\)</span> as an input. This model is namedthe <strong>Noise Conditional Score Network</strong>.</p><p><img src="/2023/11/29/Generative-Models/noise-score.png"></p><p>The loss function is a weighted combination of denoising scorematching loss with different noise levels: <span class="math display">\[\begin{aligned}    &amp;\frac{1}{L}\sum_{l=1}^L \lambda(\sigma_i) E_{\tilde{x} \simq_{\sigma_i}}[\|\nabla_{\tilde{x}}\log q_{\sigma_i}(\tilde{x}) -s_{\theta}(\tilde{x}, \sigma_i)\|_2^2]  \\    =&amp; \frac{1}{L}\sum_{l=1}^L \lambda(\sigma_i)    E_{x \sim p_{data}(x), z \sim N(0, I)}[\|s_{\theta}(x+\sigma_i z,\sigma_i) + \frac{z}{\sigma_i}\|_2^2] + \text{const}.\end{aligned}\]</span></p><p>[compute the loss in parallel?]</p><p>About the weighting function <span class="math inline">\(\lambda(\sigma_i)\)</span>, we can set <span class="math inline">\(\lambda(\sigma_i) = \sigma_i^2\)</span>.</p><p>About choosing the noise level <span class="math inline">\(\sigma_i\)</span>:</p><ul><li><p>The largest noise level <span class="math inline">\(\sigma_1\)</span> approximates the maximumpairwise distance between data points.</p></li><li><p>The smallest noise level <span class="math inline">\(\sigma_L\)</span> should be small enough so thatthe noise in final samples is negligible.</p></li><li><p>Adjacent noise scales should have sufficient overlap tofacilitate transitioning across noise scales in annealed Langevindynamics. One way is to use geometric sequence: <span class="math display">\[\frac{\sigma_1}{\sigma_2} = \frac{\sigma_2}{\sigma_3} = ... =\frac{\sigma_{L-1}}{\sigma_L},  \quad \sigma_1 &gt; \sigma_2 &gt; ...&gt; \sigma_L.\]</span></p></li></ul><p><strong>Sampling procedure</strong>: we use annealed Langevindynamics to sample from the noise conditional score network using scoresof different noise levels.</p><p>We first use Langevin dynamics to sample from the most perturbed datadistribution. Then, the resulting samples will be used as initialsamples for sampling from the next noise level. We continue in thisfashion and finally use Langevin dynamics to sample from the leastperturbed data distribution.</p><p><img src="/2023/11/29/Generative-Models/annealed.png"></p><p><strong><span class="math inline">\(s_{\theta}\)</span> is shared ateach iteration, since we only have one network.</strong></p><p>Conclusions:</p><ul><li>Gradients of distributions (scores) can be estimated easily</li><li>Flexible architecture choices — no need to benormalized/invertible</li><li>Stable training — no minimax optimization</li><li>Better or comparable sample quality to GANs</li><li>Exact likelihood computation</li></ul><h2 id="introduction-about-diffusion-model">Introduction about diffusionmodel</h2><p>Reference: <a href="https://lilianweng.github.io/posts/2021-07-11-diffusion-models/">Whatare Diffusion Models?</a></p><h3 id="ddpm-denoising-diffusion-probabilistic-models">DDPM: DenoisingDiffusion Probabilistic Models</h3><p>Diffusion model is a generative model:</p><p><img src="/2023/11/29/Generative-Models/image.png"></p><ul><li>diffusion process: add noise to a real image, finally we get a noiseimage.</li><li>reverse process: from noise image to generate real image.</li></ul><ol type="1"><li><p>training phase from a real image datasets ---&gt; throughdiffusion process ---&gt; noise images ---&gt; through reverse process---&gt; real images</p></li><li><p>inference phase</p></li></ol><p>sampling noise images from a gaussian distribution, then use thepre-trained reverse process to generate images.</p><h4 id="diffusion-process">Diffusion process</h4><p>add noise to a clean image <span class="math inline">\(X_0\)</span>and then we get noisy image <span class="math inline">\(X_1, X_2, ...,X_T\)</span>.</p><p>Now, we focus on the process from image <span class="math inline">\(X_{t-1}\)</span> to <span class="math inline">\(X_t\)</span>. <span class="math display">\[X_t = \sqrt {1 - \beta_t}X_{t-1} + \sqrt {\beta_t} Z_{t}, \quad Z_t \simN(0, I)\]</span> &gt;Remark: the noise scale <span class="math inline">\(\beta_t\)</span> will be increased gradually. $_t$ increases from <span class="math inline">\(10^{-4}\)</span> to <span class="math inline">\(2*10^{-2}\)</span> linearly. <span class="math inline">\(T = 2000\)</span>.</p><p>Let <span class="math inline">\(1 - \beta_t = \alpha_t\)</span>, thenwe have <span class="math display">\[X_t = \sqrt{\alpha_t}X_{t-1} + \sqrt{1 - \alpha_t} Z_t \\X_{t-1} = \sqrt{\alpha_{t-1}}X_{t-2} + \sqrt{1 - \alpha_{t-1}} Z_{t-1}\\\]</span> Combine these two equlities, we have <span class="math display">\[\begin{aligned}X_t &amp;= \sqrt{\alpha_t}X_{t-1} + \sqrt{1 - \alpha_t} Z_t \\&amp;= \sqrt{\alpha_t}(\sqrt{\alpha_{t-1}}X_{t-2} + \sqrt{1 -\alpha_{t-1}} Z_{t-1})+ \sqrt{1 - \alpha_t} Z_t \\&amp;= \sqrt{\alpha_t \alpha_{t-1}}X_{t-2} + \sqrt{\alpha_t(1 -\alpha_{t-1})} Z_{t-1}+ \sqrt{1 - \alpha_t} Z_t \\&amp; = \sqrt{\alpha_t \alpha_{t-1}}X_{t-2} + + \sqrt{1 - \alpha_t\alpha_{t-1}} Z,   \quad Z \sim N(0, I) \\&amp;= ... \\&amp;= \sqrt{\alpha_t \alpha_{t-1}...\alpha_1}X_{0} + + \sqrt{1 -\alpha_t \alpha_{t-1}... \alpha_{1}} Z \\&amp;= \sqrt{\bar{\alpha}_t}X_{0} + + \sqrt{1 - \bar{\alpha}_t}Z,   \qquad \bar{\alpha}_t = \prod_{i = 1}^{t}\alpha_i.\end{aligned}\]</span></p><h4 id="the-relation-between-ddpm-and-sde">The relation between DDPM andSDE</h4><p>DDPM: <span class="math inline">\(x_i = \sqrt{1 - \beta_i} x_{i-1} +\sqrt{\beta_i}z_{i-1}\)</span></p><p>SDE: <span class="math inline">\(\text{d}x = f(x, t)\text{d}t + g(x,t)\text{d}w\)</span>,</p><p>the solution is <span class="math display">\[x_t = x_0 + \int_0^t f(x, t)dt + \int_0^t g(x, t)dw\]</span> where <span class="math inline">\(w\)</span> is a Brownianmotion.</p><p>How to get the mean and convariance of <span class="math inline">\(x_t\)</span> ?</p><p>We can use FPK equation. See more on <a href="https://users.aalto.fi/~ssarkka/course_s2014/handout3.pdf">FPKequation</a>.</p><p>The problem is the mean and covariance of <span class="math inline">\(x_t\)</span> is dependent on the expectation of<span class="math inline">\(p(x(t))\)</span>, which we don't know.</p><h4 id="ddpm-variance-preserving-sde">DDPM / Variance preservingSDE</h4><p><span class="math display">\[x_i = \sqrt{1 - \beta_i} x_{i-1} +\sqrt{\beta_i}z_{i-1}\]</span></p><p>Let <span class="math inline">\(\beta(t=i/N) = N \beta_i, X(t = i/N)= x_i, Z(i/N) = z_i, \Delta t = 1/N\)</span>.</p><p>Then we have <span class="math display">\[\begin{aligned}    X(t + \Delta t) &amp;= \sqrt{1 - \beta(t+\Delta t)\Delta t}X(t) +\sqrt{\beta(t+\Delta t)\Delta t}Z(t) \\    &amp; \approx X(t)  - \frac{1}{2} \beta(t) \Delta t X(t) +\sqrt{\beta(t)\Delta t}Z(t) \\\end{aligned}\]</span></p><p><span class="math display">\[\text{d} x = - \frac{1}{2} \beta(t) x(t) dt + \sqrt{\beta(t)} \text{d}w,\]</span> where <span class="math inline">\(w\)</span> is a Brownianmotion.</p><p>The mean and covariance of <span class="math inline">\(x_t\)</span>is <span class="math display">\[\begin{aligned}    \mathbb{E}[x(t)] &amp;= \mathbb{E}[x(0)]e^{-\frac{1}{2}\int_0^t\beta(s)ds} \\    \text{cov}[x(t)] &amp;= I -  I \times e^{-\frac{1}{2}\int_0^t\beta(s)ds} \leq I.\end{aligned}\]</span></p><h4 id="score-based-model-variance-exploding-sde">SCORE-based model /Variance Exploding SDE</h4><p><span class="math display">\[x_i = x_{i-1} + \sqrt{\sigma_{i}^2 -\sigma^2_{i-1}}z_{i-1}\]</span></p><p>Let <span class="math inline">\(X(t = i/N) = x_i\)</span>, <span class="math inline">\(\sigma(t = i/N) = \sigma_i, Z(t = 1/N) = z_i,\Delta t = 1/N\)</span>.</p><p><span class="math display">\[\begin{aligned}    X(t + \Delta t) &amp;= X(t) + \sqrt{\sigma(t+\Delta t)^2 -\sigma(t)^2}Z(t) \\    &amp; \approx X(t) + \sqrt{\frac{\Delta \sigma(t)^2}{\Delta t}\Delta t}Z(t) \\\end{aligned}\]</span> where <span class="math inline">\(w\)</span> is a Brownianmotion. <span class="math display">\[d x = \sqrt{\frac{d \sigma(t)^2}{dt}} d w\]</span></p><p>The mean and covariance of <span class="math inline">\(x_t\)</span>is <span class="math display">\[\begin{aligned}    \mathbb{E}[x(t)] &amp;= \mathbb{E}[x(0)]  \\    \text{cov}[x(t)] &amp;= \sigma^2(t) I.\end{aligned}\]</span> Here, <span class="math inline">\(\sigma^2(t)\)</span> isnon-decreasing variance. Thus, the variance will be exploded.</p><h2 id="evaluating-generative-models">Evaluating Generative Models</h2><h3 id="model-families">Model families</h3><ul><li><p>Probability density/mass functions</p><ul><li>Autoregressive models: <span class="math inline">\(p_{\theta}(x) =\prod_{i=1}^{d} p_{\theta}(x_i | x_{&lt;i})\)</span>.</li><li>Normalizing flow models: <span class="math inline">\(p_{\theta}(x) =p(z) |\det(J_{f_{\theta}}(x))|\)</span>, <span class="math inline">\(z =f_{\theta}(x)\)</span>.</li><li>Latent variable models(VAEs): <span class="math inline">\(p_{\theta}(x) = \int p_{\theta}(x|z)p(z)\text{d}z\)</span>.</li><li>Energy-based models: <span class="math inline">\(p_{\theta}(x) =\frac{1}{Z}\exp(-E_{\theta}(x))\)</span>.</li></ul></li><li><p>Sample generation processes</p><ul><li>GANs: <span class="math inline">\(x = G_{\theta}(z)\)</span>, <span class="math inline">\(z \sim p(z)\)</span>.</li></ul></li><li><p>Score functions</p><ul><li>Score-based models: <span class="math inline">\(s_{\theta}(x) =\nabla_x \log p_{\theta} (x)\)</span>.</li></ul></li></ul><h3 id="distances-of-probability-distributions">Distances of probabilitydistributions</h3><ul><li><p>KL divergence(maximum likelihood): <span class="math inline">\(D_{KL}(p||q) = \int p(x) \log \frac{p(x)}{q(x)}\text{d}x\)</span></p><ul><li>Autoregressive models.</li><li>Normalizing flow models.</li><li>ElBO in VAEs.</li><li>Contrastive divergence in EBMs.</li></ul></li><li><p>f-divergences, Wasserstein distances</p><ul><li>GANs (f-GANs, WGANs)</li></ul></li><li><p>Fisher divergence(score matching): denoising score matching,sliced score matching</p><ul><li>Score-based models</li><li>Energy-based models</li></ul></li><li><p>Noise-contrastive estimation</p><ul><li>Energy-based models</li></ul></li></ul><h3 id="evaluation---density-estimation">Evaluation - Densityestimation</h3><p>We can use likelihood as a matric for density estimation:</p><ul><li>Split dataset into train, validation, test sets.</li><li>Learn model <span class="math inline">\(p_{\theta}(x)\)</span> usingthe train set.</li><li>Tune hyperparameters using the validation set.</li><li>Evaluate likelihood on the test set: <span class="math inline">\(\mathbb{p_{data}}[\logp_{\theta}(x)]\)</span>.</li></ul><p>However, the likelihood is intractable for many models. Not allmodels have tractable likelihoods. For example, GANs, VAEs, EBMs,etc.</p><p>For VAEs, we can compare the ELBO to log-likelihood. But, how aboutGANs and EBMs?</p><p>In general, unbiased estimation of probability density functions fromsamples is impossible. We can use approximation methods, such as kerneldensity estimation.</p><h4 id="kernel-density-estimation">Kernel density estimation:</h4><p>Given an intractable density model <span class="math inline">\(p_{\theta}(x)\)</span> and limited samples <span class="math inline">\(S = \{x_i\}_{i=1}^n\)</span>, we can use kerneldensity estimation to estimate the density function <span class="math inline">\(p_{\theta}(x)\)</span>. For a new data point <span class="math inline">\(x\)</span>, we can estimate the density of <span class="math inline">\(x\)</span> as: <span class="math display">\[\hat{p}_{\theta}(x) = \frac{1}{n} \sum_{i \in S} K(\frac{x -x_i}{\sigma}),\]</span> where <span class="math inline">\(K\)</span> is a kernelfunction, <span class="math inline">\(\sigma\)</span> is thebandwidth.</p><ul><li>Gaussian kernel: <span class="math inline">\(K(x) =\frac{1}{\sqrt{2\pi}}\exp(-\frac{1}{2}x^2)\)</span>.</li><li>A kernel is a function that satisfies the following properties:<ul><li><span class="math inline">\(K(x) \geq 0\)</span>.</li><li><span class="math inline">\(\int K(x) \text{d}x = 1\)</span>.</li><li><span class="math inline">\(K(x) = K(-x)\)</span>.</li></ul></li><li>Bandwidth <span class="math inline">\(\sigma\)</span> controls thesmoothness of the density estimate.<ul><li>Small <span class="math inline">\(\sigma\)</span>:undersmoothed</li><li>Large <span class="math inline">\(\sigma\)</span>: oversmoothed</li><li><span class="math inline">\(\sigma\)</span> is a hyperparameter. Wecan use cross-validation to choose <span class="math inline">\(\sigma\)</span>.</li></ul></li><li>KDE is very unreliable in higher dimensions.</li></ul><h3 id="importance-sampling-for-latent-variable-models">Importancesampling for latent variable models</h3><p>For likelihood <span class="math inline">\(p(x)\)</span>, we can uselikehood weighting to estimate the likelihood: <span class="math display">\[p(x)= \mathbb{E}_{p(z)}[p(x|z)].\]</span></p><p>Monte Carlo sampling is one way to estimate the expectation. However,if <span class="math inline">\(p(z)\)</span> is far from <span class="math inline">\(p(z|x)\)</span>, the variance of the likehoodweighting is very large. For example, the probability of <span class="math inline">\(p(z)\)</span> is very small, but the probabilityof <span class="math inline">\(p(z|x)\)</span> is very large at someregions. Thus, we need another distribution <span class="math inline">\(q(z)\)</span>, which is close to <span class="math inline">\(p(z|x)\)</span>, to estimate the expectation.</p><p>Importance sampling is another way to estimate the expectation. Theidea is to sample from a proposal distribution <span class="math inline">\(q(z)\)</span>, then <span class="math display">\[\begin{aligned}\mathbb{E}_{p(z)}[p(x|z)] = \int p(z) p(x|z) \text{d}z = \int q(z)\frac{p(z)}{q(z)} p(x|z) \text{d}z = \mathbb{E}_{q(z)}[\frac{p(z)}{q(z)}p(x|z)].\end{aligned}\]</span> Then, we can use Monte Carlo sampling to estimate theexpectation.</p><p>Pros:</p><ul><li>Still unbiased.</li><li>We can choose <span class="math inline">\(q(z)\)</span> to havelower variance. One can prove that <span class="math inline">\(q(z)\)</span> should be high where <span class="math inline">\(|p(z)p(x|z)|\)</span> is high.</li></ul><p>Cons:</p><ul><li>We need to choose a good proposal distribution <span class="math inline">\(q(z)\)</span>.</li><li>It's unreliable in high dimensions.</li></ul><p>When to use importance sampling?</p><ul><li><span class="math inline">\(p(z)\)</span> is difficult to samplefrom.</li><li>We can evaluate <span class="math inline">\(p(z)\)</span>.</li><li><span class="math inline">\(q(x)\)</span> is easy to evaluate andsample from.</li><li>We can choose <span class="math inline">\(q(z)\)</span> to be highwhere <span class="math inline">\(|p(z)p(x|z)|\)</span> is high.</li></ul><p>Annealed importance sampling is another method to estimate thelikelihood.</p><h3 id="sample-quality">Sample quality</h3><h4 id="inception-score-is">Inception score (IS)</h4><p>Inception score is a metric for evaluating the quality of generatedimages. The idea is to use a pretrained Inception network to classifythe generated images.</p><p>Assumption 1: we are evaluating sample quality for generative modelstrained on labeled datasets.</p><p>Assumption 2: We have a good probabilistic classifier <span class="math inline">\(c(y|x)\)</span> for predicting the label <span class="math inline">\(y\)</span> of any point <span class="math inline">\(x\)</span>.</p><p>(A classifier can be trained on a large dataset, such asImageNet.)</p><p>We want a good generative model to satisfy two properties:</p><ul><li><p>Sharpness: the generated images should be sharp.</p><p><img src="/2023/11/29/Generative-Models/sharpness.png"></p><p><span class="math display">\[S = \exp(E_{x \sim p}[\int c(y | x)\logc(y|x) \text{d}y])\]</span> High sharpness implies classifier isconfident in making predictions for generated images, and <span class="math inline">\(c(y|x)\)</span> has low entropy.</p></li><li><p>Diversity: the generated images should be diverse.</p><p><img src="/2023/11/29/Generative-Models/diversity.png"></p><p><span class="math display">\[D = \exp(E_{x \sim p}[\int c(y|x) \logc(y) \text{d} y]), \qquad c(y) = E_{x \sim p} [c(y|x)]\]</span></p><p>High diversity implies the generated images are diverse, and <span class="math inline">\(c(y)\)</span> has high entropy.</p></li></ul><p>Inception score combines these two properties: <span class="math display">\[IS = D \times S.\]</span></p><p>Higher IS implies better sample quality.</p><h4 id="frechet-inception-distance-fid">Frechet inception distance(FID)</h4><p>Inception score only considers the samples from <span class="math inline">\(p_{\theta}(x)\)</span>, but ignores the real datadistribution <span class="math inline">\(p_{data}(x)\)</span>.</p><p>FID is a metric for evaluating the quality of generated images. Theidea is to use a pretrained Inception network to extract features fromthe generated images and real images. Then, we can compute the Frechetdistance between the two feature distributions.</p><ul><li>Let <span class="math inline">\(\mathcal{G}\)</span> be thegenerated samples and <span class="math inline">\(\mathcal{T}\)</span>be the test dataset.</li><li>Compute feature representations <span class="math inline">\(F_{\mathcal{G}}\)</span> and <span class="math inline">\(F_{\mathcal{T}}\)</span>.</li><li>Fit a multivariate Gaussian to <span class="math inline">\(F_{\mathcal{G}}\)</span> and <span class="math inline">\(F_{\mathcal{T}}\)</span>. Let <span class="math inline">\(\mu_{\mathcal{G}}\)</span> and <span class="math inline">\(\mu_{\mathcal{T}}\)</span> be the mean vectors and<span class="math inline">\(\Sigma_{\mathcal{G}}\)</span> and <span class="math inline">\(\Sigma_{\mathcal{T}}\)</span> be the covariancematrices.</li><li>FID is defined as the Wasserstein-2 distance between the twoGaussians: <span class="math display">\[FID(\mathcal{G}, \mathcal{T}) = ||\mu_{\mathcal{G}} -\mu_{\mathcal{T}}||_2^2 + \text{tr}(\Sigma_{\mathcal{G}} +\Sigma_{\mathcal{T}} -2(\Sigma_{\mathcal{G}}\Sigma_{\mathcal{T}})^{1/2}).\]</span></li></ul><p>Lower FID implies better sample quality.</p><h4 id="kernel-inception-distance-kid">Kernel inception distance(KID)</h4><p>Maximum mean discrepancy (MMD) is a two-sample test statistic thatmeasures the distance between two distributions by computing differencesin their moments. Using the kernel trick, we can compute the MMD betweentwo distributions: <span class="math display">\[MMD(p, q) = E_{x, x' \sim p} [K(x, x')] + E_{y, y' \sim q}[K(y, y')] - 2E_{x \sim p, y \sim q} [K(x, y)].\]</span></p><p>Kernel inception distance (KID) is a metric for evaluating thequality of generated images. The idea is to use a pretrained Inceptionnetwork to extract features from the generated images and real images.Then, we can compute the MMD between the two feature distributions.</p><p>FID VS. KID:</p><ul><li>FID can only be positive, and it's biased, KID is unbiased.</li><li>The computation time of FID is <span class="math inline">\(O(n)\)</span>, but the computation time of KID is<span class="math inline">\(O(n^2)\)</span>.</li></ul><h3 id="evaluation---latent-representations">Evaluation - latentrepresentations</h3><p>What is a good latent representation? For downstream tasks, we canevaluate the quality of latent representations by evaluating theperformance of the downstream tasks, such as reconstruction,classification, etc.</p><p>For unsupervised learning, there is no one-size-fits-all metric forevaluating the quality of latent representations. We can use thefollowing metrics to evaluate the quality of latent representations:</p><h4 id="clustering">clustering</h4><p>Representations that can be grouped into clusters are potentiallyuseful. For example, the representations of a generated model for MNISTcan be grouped into different clusters, where each cluster correspondsto one or more digits.</p><p>For labelled datasets, there are many evaluation metrics. The lablesare only used for evaluation, not for clustering.</p><pre><code>from sklearn.metrics.cluster import completeness_score, homogeneity_score, v_measure_score</code></pre><h4 id="compression-or-reconstruction">compression orreconstruction</h4><p>Latent representations can be evaluated based on the maximumcompression they can achieve without significant loss in reconstructionquality.</p><p>Some metrics: Mean Squared Error (MSE), Peak signal-to-noise ratio(PSNR), Structural similarity (SSIM), etc.</p><h4 id="disentanglement">disentanglement</h4><p>We want representations that disentangle independent andinterpretable factors of variation in the observed data.</p><p>Some quantitative metrics:</p><ul><li>Beta-VAE metric: accuracy of a linear classifier that predicts afixed factor of variation.</li><li>Factor-VAE, Mutual Information Gap (MIG), SAP score, DCIdisentanglement, Modularity, etc.</li></ul><h2 id="reference">Reference</h2><ul><li><p><a href="https://users.aalto.fi/~asolin/sde-book/sde-book.pdf">SDEBOOK</a></p></li><li><p><a href="https://arxiv.org/abs/2006.11239">Denoising DiffusionProbabilistic Models. Jonathan et al.</a></p></li><li><p><a href="https://yang-song.net/blog/2021/score/">GenerativeModeling by Estimating Gradients of the Data Distribution</a></p></li><li><p><a href="https://lilianweng.github.io/posts/2021-07-11-diffusion-models/">Whatare Diffusion Models?</a></p></li><li><p><a href="https://deepgenerativemodels.github.io/syllabus.html">CS236 - Fall2023 Deep Generative Models</a></p></li><li><p><a href="https://lrjconan.github.io/UBC-EECE571F-DL-Structures/">EECE 571F(2023 Winter Term 1): Deep Learning with Structures</a></p></li><li><p><a href="https://arxiv.org/abs/2101.03288">How to Train YourEnergy-Based Models. Yang Song and Durk Kingma.</a></p></li><li><p><a href="https://glizen.com/radfordneal/ftp/ais-rev.pdf">Importancesampling</a></p></li></ul>]]></content>
      
      
      <categories>
          
          <category> Generative Model </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Diffusion </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>The Effect of Data Centering on PCA Models</title>
      <link href="/2023/11/07/The%20Effect%20of%20Data%20Centering%20on%20PCA%20Models/"/>
      <url>/2023/11/07/The%20Effect%20of%20Data%20Centering%20on%20PCA%20Models/</url>
      
        <content type="html"><![CDATA[<p>A common misconception in PCA is that PC 1 is the mean of the datawhen the data are not centered. It was shown in this paper that PC 1 isnot the mean of the data but it can point in the direction of the mean.The extent to which PC 1 points in the direction of the mean depends onhow far away the data set mean is from the origin. More details can befound in the paper [1].</p><h2 id="references">References</h2><p>[1] <a href="https://eigenvector.com/wp-content/uploads/2020/06/EffectofCenteringonPCA.pdf">TheEffect of Data Centering on PCA Models</a></p>]]></content>
      
      
      <categories>
          
          <category> Statistic </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Statistic </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Monte Carlo Gradient Estimation in Machine Learning</title>
      <link href="/2023/10/21/Monte-Carlo-Gradient-Estimation/"/>
      <url>/2023/10/21/Monte-Carlo-Gradient-Estimation/</url>
      
        <content type="html"><![CDATA[<h1 id="monte-carlo-methods-and-stochastic-optimisation">Monte CarloMethods and Stochastic Optimisation</h1><p>The mean-value analysis problem: <span class="math display">\[\mathcal{F}(\theta) := \int p(x; \theta) f(x; \phi) \text{d}x =\mathbb{E}_{p(x; \theta)} f(x; \phi),  \enspace (1)\]</span> where <span class="math inline">\(f(x; \phi)\)</span> is thecost with parameters <span class="math inline">\(\phi\)</span>, <span class="math inline">\(p(x;\theta)\)</span> is the measure, a probabilitydistribution that is continuous in its domain and differentiable with<span class="math inline">\(\theta\)</span>.</p><p>This objective is very common in variational inference, reinforcementlearning, etc.</p><p>To learn the distribution parameter <span class="math inline">\(\theta\)</span>, we need to consider the gradient:<span class="math display">\[\eta := \nabla_{\theta} \mathcal{F}(\theta) = \nabla_{\theta}\mathbb{E}_{p(x; \theta)} f(x; \phi). \enspace  (2)\]</span> <span class="math inline">\(\eta\)</span> is called thesensitivity analysis of <span class="math inline">\(\mathcal{F}\)</span>.</p><p>Problems:</p><ul><li>not able to evaluate the expectation in closed form;</li><li><span class="math inline">\(x\)</span>: high-dimensional, <span class="math inline">\(\theta\)</span>: high-dimensional;</li><li>the cost funtion may not be differential or a black-boxfunction.</li></ul><h2 id="monte-carlo-estimators">Monte Carlo Estimators</h2><p>We can numerically evaluate the integral by first drawing<strong>independent</strong> samples <span class="math inline">\(\hat{x}^{(1)}, ..., \hat{x}^{(N)}\)</span> fromthe distribution <span class="math inline">\(p(x; \theta)\)</span>, andthen computing the averaged of the function evaluated at these samples:<span class="math display">\[\bar{\mathcal{F}}_N = \frac{1}{N} \sum_{n=1}^{N} f(\hat{x}^{(n)}),\qquad \hat{x}^{(n)} \sim p(x; \theta), \quad n = 1,..., N.\]</span> <span class="math inline">\(\bar{\mathcal{F}}_N\)</span> is arandom variable and is called Monte Carlo estimator of eq. (1).</p><p>Remark 1. As long as we can write an integral in the form of eq. (1)(a product of a function and a distribution that we can easily samplefrom), we can apply the Monte Carlo method.</p><h3 id="four-properties">Four Properties</h3><ul><li><p>Consistency. As <span class="math inline">\(N \to\infty\)</span>, the estimator <span class="math inline">\(\bar{\mathcal{F}}_N \to \mathbb{E}_{p(x; \theta)}f(x; \phi)\)</span>. This can be easily satisfied according to thestrong law of large number.</p></li><li><p>Unbiasedness. <span class="math display">\[\mathbb{E}_{p(x; \theta)} [\bar{\mathcal{F}}_N] = \mathbb{E}_{p(x;\theta)} [\frac{1}{N} \sum_{n=1}^{N} f(\hat{x}^{(n)})] = \frac{1}{N}\sum_{n=1}^{N} \mathbb{E}_{p(x; \theta)} [f(\hat{x}^{(n)})] =\mathbb{E}_{p(x; \theta)}[f(x)].\]</span> (if we repeat the estimation process many times, the estimateis centered on the the actual value of the integral on average)</p></li><li><p>Minimum variance. If we consider two <strong>unbiased</strong>estimators using the same number of sampling <span class="math inline">\(N\)</span>, we will prefer the estimator that haslower variance. [<strong>if MC estimator has the minimumvariance?</strong>]</p></li><li><p>Computational efficiency. for example: a computational costlinear in the number of parameters, can be computed inparallel.</p></li></ul><h3 id="the-central-role-of-gradient-estimation-eq.-2">The central roleof Gradient Estimation eq. (2)</h3><p>some examples in different areas.</p><h4 id="variational-inference">Variational Inference</h4><p>Variational inference is a general method for approximating complexand unknown distributions by the closest distribution within a tractablefamily. Consider a generic probabilistic model <span class="math inline">\(p(x|z)p(z)\)</span> that defines a generativeprocess in which observed data <span class="math inline">\(x\)</span> isgenerated from a set of unobserved variables z using a data distribution<span class="math inline">\(p(x|z)\)</span> and a prior distribution<span class="math inline">\(p(z)\)</span>. The posterior distribution ofthis generative process <span class="math inline">\(p(z|x)\)</span> isunknown, and is approximated by a variational distribution <span class="math inline">\(q(z|x; \theta)\)</span> with variationalparameters <span class="math inline">\(\theta\)</span>. The objective is<span class="math display">\[\max_{\theta, \phi} \mathbb{E}_{q(z|x;\theta)} [\log p(x|z;\phi) - \log\frac{q(z|x;\theta)}{p(z)}].\]</span> Optimising the distribution <span class="math inline">\(q\)</span> requires the gradient of the objectivewith respect to the variational parameters <span class="math inline">\(\theta\)</span>: <span class="math display">\[\eta = \nabla_{\theta}\mathbb{E}_{q(z|x;\theta)} [\log p(x|z;\phi) -\log \frac{q(z|x;\theta)}{p(z)}].\]</span></p><h4 id="reinforcement-learning">Reinforcement Learning</h4><p>Model-free policy search is an area of reinforcement learning wherewe learn a policy|a distribution over actions|that on average maximisesthe accumulation of long-term rewards. Through interaction in anenvironment, we can generate trajectories <span class="math inline">\(\tau = (s_1, a_1, s_2, a_2, ... , s_T , a_T)\)</span> that consist of pairs of states st and actions at for timeperiod <span class="math inline">\(t = 1, ... , T\)</span>. A policy islearnt by following the policy gradient:</p><p><span class="math display">\[\eta = \nabla_{\theta} \mathbb{E}_{p(\tau;\theta)} [\sum_{t=0}^{T}\gamma^t r(s_t,a_t)]\]</span> The cost is the return over the trajectory, which is aweighted sum of rewards obtained at each time step <span class="math inline">\(r(s_t, a_t)\)</span>, with the discount facthor<span class="math inline">\(\gamma \in [0, 1]\)</span>. The measure isthe joint distribution over states and actions <span class="math inline">\(p(\tau;\theta) = \prod_{t=0}^{T-1} [p(s_{t+1}|s_t,a_t)p(a_t|s_t; \theta)]p(a_T |s_T ; \theta)\)</span>.</p><h2 id="intuitive-analysis-of-gradient-estimators">Intuitive Analysis ofGradient Estimators</h2><p>The gradients <span class="math inline">\(\nabla_{\theta}\mathbb{E}_{p(x;\theta)}[f(x)]\)</span> can be computed in two ways:</p><ul><li><p>Derivatives of Measure. The gradient can be computed bydifferentiation of the measure <span class="math inline">\(p(x;\theta)\)</span>. Gradient estimators in this class include the scorefunction estimator and the measure-valued gradient.</p></li><li><p>Derivatives of Paths. The gradient can be computed bydifferentiation of the cost <span class="math inline">\(f(x)\)</span>,which encodes the pathway from parameters <span class="math inline">\(\theta\)</span>, through the random variable <span class="math inline">\(x\)</span>, to the cost value, such as thepathwise gradient, harmonic gradient estimators and finite dfferences,and Malliavin-weighted estimators.</p></li></ul><p>We focus on three classes of gradient estimators: the score function,pathwise and measure-valued gradient estimators. <strong>All threeestimators satisfy two desirable properties: consistent and unbiased;but they differ in their variance behaviour and in their computationalcost.</strong></p><h3 id="intuitive-comparision">Intuitive comparision</h3><p>Consider the stochastic gradient problem (2) that uses Gaussianmeasures for three simple families of cost functions, quadratics,exponentials and cosines: <span class="math display">\[\eta = \nabla_{\theta} \int \mathcal{N}(x | \mu, \sigma^2) f(x; k)\text{d}x; \quad \theta \in \{\mu, \sigma \}; \quad f \in \{ (x-k)^2,\exp(-kx^2), \cos(kx)\}.\]</span></p><p><img src="/2023/10/21/Monte-Carlo-Gradient-Estimation/figure2.png"></p><p><img src="/2023/10/21/Monte-Carlo-Gradient-Estimation/figure3.png"></p><p>The computational cost:</p><ul><li><p>Both of the score-function and pathwise estimators can becomputed using a single sample in the Monte Carlo estimator (N = 1),even for multivariate distributions, making them computationallycheap.</p></li><li><p>The measure-valued derivative estimator will require 2Devaluations of the cost function for D dimensional parameters, and forthis the reason will typically not be preferred in high-dimensionalsettings.</p></li><li><p>If the cost function is not differentiable, then the pathwisegradient will not be applicable.</p></li></ul><p>Several criteria to be judged when choosing an unbiased gradientestimator:</p><ul><li>computational cost;</li><li>implications on the use of differentiable and non-differentiablecost functions;</li><li>the change in behaviour as the cost itself changes;</li><li>the availability of effective variance reduction techniques toachieve low variance.</li></ul><h1 id="score-function-gradient-estimators-likelihood-ratio-method-reinforce-estimator">ScoreFunction Gradient Estimators (likelihood ratio method, REINFORCEestimator)</h1><h2 id="score-function">Score function</h2><p>The score function is the derivative of the log-probability of thedistribution <span class="math inline">\(\nabla_{\theta} \log p(x;\theta)\)</span> with respect to its parameters <span class="math inline">\(\theta\)</span>: <span class="math display">\[\nabla_{\theta} \log p(x; \theta) = \frac{\nabla_{\theta} p(x;\theta)}{p(x; \theta)}.\]</span></p><p>properties: 1, Its expectation is zero: <span class="math display">\[\mathbb{E}_{p(x; \theta)} [\nabla_{\theta} \logp(x; \theta)] = \int p(x; \theta) \frac{\nabla_{\theta} p(x;\theta)}{p(x; \theta)} \text{d}x =  \nabla_{\theta} \int p(x; \theta)\text{d}x = \nabla_{\theta} 1 = 0.\]</span></p><p>2, Its variance is the Fisher information matrix.</p><p>Using the score function, we can derive a general-purpose estimatorfor the sensitivity analysis of eq. (2): <span class="math display">\[\begin{aligned}\eta &amp;= \nabla_{\theta} \mathbb{E}_{p(x; \theta)} [f(x)] \\&amp;= \nabla_{\theta} \int p(x; \theta) f(x) \text{d}x \\&amp;= \int f(x) \nabla_{\theta} p(x; \theta) \text{d}x \\&amp;= \int p(x; \theta) f(x) \nabla_{\theta} \log p(x; \theta)\text{d}x \\&amp;= \mathbb{E}_{p(x; \theta)} [f(x) \nabla_{\theta} \log p(x;\theta)].\end{aligned}\]</span> The form is what we need - a product of a distribution we cansample from and a function we can evaluate. Then, use the Monte Carloestimator, we have <span class="math display">\[\bar{\eta} = \frac{1}{N} \sum_{n=1}^{N} f(\hat{x}^{(n)}) \nabla_{\theta}\log p(\hat{x}^{(n)}; \theta), \quad \hat{x}^{(n)} \sim p(x; \theta).\]</span></p><p><strong>Notice that, in the third line, we have exchanged the orderof the integral and the derivative. We will discuss the validity of thisexchange later.</strong></p><h2 id="estimator-properties">Estimator Properties</h2><h3 id="unbiasedness">Unbiasedness</h3><p>When the interchange between differentiation and integration isvalid, we will obtain an <strong>unbiased</strong> estimator of thegradient. Intuitively, since differentiation is a process of limits, thevalidity of the interchange will relate to the conditions for which itis possible to exchange limits and integrals, in such cases most oftenrelying on the use of the <strong>dominated convergence theorem or theLeibniz integral rule</strong> (Flanders, 1973; Grimmett and Stirzaker,2001). The interchange will be valid if the following conditions aresatisfied:</p><ol type="a"><li><p>The measure <span class="math inline">\(p(x; \theta)\)</span> iscontinuously differentiable with respect to <span class="math inline">\(\theta\)</span>;</p></li><li><p>The product <span class="math inline">\(f(x) p(x;\theta)\)</span> is both integrable and differentiable for <span class="math inline">\(\theta\)</span>;</p></li><li><p>There exists an integrable function <span class="math inline">\(g(x)\)</span> such that <span class="math inline">\(\sup_{\theta} \|f(x) \nabla_{\theta} p(x; \theta)\|_1 \leq g(x)\)</span> for <span class="math inline">\(\forallx\)</span>.</p></li></ol><p>These assumptions usually hold in machine learning applications.</p><h3 id="abosolute-continuity">Abosolute Continuity</h3><ul><li><p>Example (Bounded support). Consider the score-function estimatorfor a cost <span class="math inline">\(f(x) = x\)</span> anddistribution <span class="math inline">\(p(x; \theta) = \frac{1}{\theta}1_{0 &lt; x &lt; \theta}\)</span>, which is differential in <span class="math inline">\(\theta\)</span> when <span class="math inline">\(x\in (0, \theta)\)</span>; the score function is <span class="math inline">\(\nabla_{\theta} \log p(x; \theta) =-\frac{1}{\theta}\)</span>. The true gradient is: <span class="math inline">\(\nabla_{\theta} \mathbb{E}_{p(x;\theta)} [x] =\nabla_{\theta} (\frac{1}{\theta} \int_{0}^{\theta} \frac{x^2}{2}) =\frac{1}{2}\)</span>. The score-funtion gradient is: <span class="math inline">\(\mathbb{E}_{p(x;\theta)} [x \frac{-1}{\theta}] =-\frac{\theta/2}{\theta} = -\frac{1}{2}\)</span>.</p><p>Why the score-function estimator fails to provide the correctgradient? The reason is that the measure is not absolutely continuouswith respect to <span class="math inline">\(\theta\)</span> at theboundary of the support.</p></li></ul><p>Let's explain the absolute continuity in detail. <span class="math display">\[\begin{aligned}  \nabla_{\theta} \mathbb{E}_{p(x;\theta)}[f(x)] &amp;= \int\nabla_{\theta} p(x;\theta) f(x) \text{d}x \\&amp;= \int \lim_{h \to 0} \frac{p(x; \theta + h) - p(x;\theta)}{h} f(x)\text{d}x \\&amp;= \lim_{h \to 0} \frac{1}{h} \int p(x; \theta) \frac{p(x; \theta +h) - p(x;\theta)}{p(x;\theta)} f(x) \text{d}x\\&amp;= \lim_{h \to 0} \frac{1}{h} \int p(x; \theta) (\frac{p(x; \theta +h)}{p(x;\theta)}-1) f(x)  \text{d}x\\&amp;= \lim_{h \to 0} \frac{1}{h}(\mathbb{E}_{p(x;\theta)}[\omega(\theta, h) f(x)] -\mathbb{E}_{p(x;\theta)}[f(x)])   \end{aligned}\]</span> where the ratio <span class="math inline">\(\omega(\theta, h):= \frac{p(x; \theta+h)}{p(x;\theta)}\)</span>. The estimator makes animplicit assumption of absolute continuity, where <strong>we require<span class="math inline">\(p(x; \theta+h) &gt; 0\)</span> for allpoints where <span class="math inline">\(p(x; \theta) &gt;0\)</span>.</strong> Not all distributions of interest satisfy thisproperty, and failures of absolute continuity can result in a biasedgradient.</p><h3 id="estimator-variance">Estimator Variance</h3><p>Define the estimator mean as <span class="math inline">\(\mu(\theta):= \mathbb{E}_{p(x;\theta)}[\bar{\eta}_N]\)</span>, for <span class="math inline">\(N=1\)</span>, The variance of the score functionestimator is: <span class="math display">\[\begin{aligned}\text{Var}_{p(x;\theta)}(\bar{\eta}_1) &amp;=\mathbb{E}_{p(x;\theta)}[(f(x) \nabla_{\theta} \log p(x; \theta))^2] -\mu(\theta)^2,\end{aligned}\]</span> or <span class="math display">\[\begin{aligned}\text{Var}_{p(x;\theta)}(\bar{\eta}_1) &amp;= \lim_{h \to 0} \frac{1}{h}\mathbb{E}_{p(x;\theta)}[(\omega(\theta, h) - 1)^2f(x)^2] -\mu(\theta)^2, \\&amp; \geq \sup_{h} \frac{(\mu(\theta + h) -\mu(\theta))^2}{\mathbb{E}_{p(x;\theta)}[\omega(\theta, h) - 1]^2}.\end{aligned}\]</span> Three sources of variance:</p><ul><li><p>importance ratio <span class="math inline">\(\omega(\theta,h)\)</span> (the need for absolute continuity);</p></li><li><p>The dimensionality of the parameters <span class="math inline">\(x\)</span>;</p></li><li><p>The variance of the cost function <span class="math inline">\(f(x)\)</span>.</p></li></ul><p><img src="/2023/10/21/Monte-Carlo-Gradient-Estimation/figure4.jpg"></p><h3 id="computational-cost">Computational Cost</h3><p>The computational cost of the score function estimator is low, it isthe order of <span class="math inline">\(O(N(D+L))\)</span> for <span class="math inline">\(N\)</span> samples, <span class="math inline">\(D\)</span> dimensional distributional parameters<span class="math inline">\(\theta\)</span> and <span class="math inline">\(L\)</span> is the cost of evaluating the costfunction.</p><h3 id="conclusion">Conclusion</h3><ul><li><p>The score funtion only need the final value of the cost in itscomputation and it makes no assumptions about the internal structure ofthe cost function. Any type of cost function can be used.</p></li><li><p>The measure must be differentiable with respect to the parameters<span class="math inline">\(\theta\)</span>, and we can easily samplefrom the measure. It is applicable to both discrete and continuousdistributions.</p></li><li><p>The estimator can be implemented using only a single sample,making it computationally efficient.</p></li><li><p>There are too many sources of variance, we can use some variancereduction techniques to reduce the variance.</p></li></ul><h1 id="pathwise-gradient-estimators">Pathwise Gradient Estimators</h1><h2 id="sampling-paths-sampling-process">Sampling Paths (samplingprocess)</h2><p>For continuous distribution, the alternative way to generate samples<span class="math inline">\(\hat{x}\)</span> from the distribution <span class="math inline">\(p(x; \theta)\)</span> is to sample from a simplerbase distribution <span class="math inline">\(p(\epsilon)\)</span> whichis independent of the parameters <span class="math inline">\(\theta\)</span>, and then transform the samplesthrough a deterministic <strong>path</strong> <span class="math inline">\(g(\epsilon;  \theta)\)</span>: <span class="math display">\[\hat{x} \sim p(x; \theta) \quad \equiv \quad \hat{x} = g(\epsilon;\theta), \quad \epsilon \sim p(\epsilon).\]</span></p><p>This transformation is described by the rule for the change ofvariables for probability: <span class="math display">\[p(x; \theta) = p(\epsilon) |\nabla_{\epsilon} g(\epsilon;\theta)|^{-1}.\]</span></p><h3 id="one-liners-reparameterization-trick">One-liners[Reparameterization Trick]</h3><p>One whidely-known example is sampling from a multivariate Gaussiandistribution <span class="math inline">\(p(\bf x; \bf \theta) =\mathcal{N}(\mathbf x|\mathbf \mu, \mathbf \Sigma)\)</span>:</p><ol type="1"><li>First sample from a standard Gaussian distribution <span class="math inline">\(p(\mathbf \epsilon) = \mathcal{N}(\mathbf\epsilon|\mathbf 0, \mathbf I)\)</span>;</li><li>Then transform the samples through the local-scale transformation<span class="math inline">\(g(\epsilon; \theta) = \mu + \mathbf L\epsilon\)</span>, where <span class="math inline">\(\mathbf{LL}^T =\mathbf \Sigma\)</span>.</li></ol><p><span class="math display">\[\hat{x} = \mu + \mathbf L \epsilon, \quad \epsilon \sim\mathcal{N}(\mathbf 0, \mathbf I), \quad \mathbf L \mathbf L^T = \mathbf\Sigma.\]</span></p><p><img src="/2023/10/21/Monte-Carlo-Gradient-Estimation/figure7.png"></p><p>Many such transformations exist for common distributions, includingDirichlet, Gamma, and many others. These types of transformations arecalled <strong>one-liners</strong> because they can be implemented in asingle line of code.</p><p>The expectation of eq. (1) is then: <span class="math display">\[\begin{aligned}\mathbb{E}_{p(x; \theta)} [f(x)] = \mathbb{E}_{p(\epsilon)}[f(g(\epsilon; \theta))]\end{aligned}\]</span> It is often used in Monte Carlo methods, and is called<strong>reparameterisation</strong> trick.</p><h2 id="gradient-estimators">Gradient Estimators</h2><p>Assume that we have a distribution <span class="math inline">\(p(x;\theta)\)</span> with known <strong>differentiable</strong> samplingpath <span class="math inline">\(g(\epsilon; \theta)\)</span> and basedistribution <span class="math inline">\(p(\epsilon)\)</span>. Thegradient estimator for the sensitivity analysis of eq. (2) is: <span class="math display">\[\begin{aligned}\eta &amp;= \nabla_{\theta} \mathbb{E}_{p(x; \theta)} [f(x)] \\&amp;= \nabla_{\theta} \int p(\epsilon) f(g(\epsilon; \theta))\text{d}\epsilon \\&amp;= \mathbb{E}_{p(\epsilon)}[\nabla_{\theta} f(g(\epsilon;\theta))]\\\bar{\eta} &amp;= \frac{1}{N} \sum_{n=1}^{N} \nabla_{\theta}f(g(\hat{\epsilon}^{(n)}; \theta)), \quad \hat{\epsilon}^{(n)} \simp(\epsilon).   \qquad (3)\end{aligned}\]</span></p><h3 id="decoupling-sampling-and-gradient-computation">DecouplingSampling and Gradient Computation</h3><p>The pathwise estimator (3) is limited to those distributions forwhich we simultaneously have a differential path and use this same pathto generate samples. But, <strong>sampling from a distribution may notprovide a differentiable path</strong>. Thus, we can expand theapplicability of the pathwise gradient by <strong>decoupling</strong>these two processes.</p><p>The pathwise estimator can be rewritten in a more general form:</p><p><span class="math display">\[\begin{aligned}  \eta &amp;= \nabla_{\theta} \mathbb{E}_{p(\mathbf x;\theta)}[f(\mathbfx)] \\  &amp;= \mathbb{E}_{p(\epsilon)}[\nabla_{\theta} f(\mathbf x)|_{\mathbfx = g(\epsilon;\theta)}] \\  &amp;= \int p(\epsilon) \nabla_{\mathbf x} f(\mathbf x)|_{\mathbf x =g(\epsilon;\theta)} \nabla_{\theta} g(\epsilon; \theta) \text{d}\epsilon \\  &amp;= \int p(\mathbf x;\theta) \nabla_{\mathbf x} f(\mathbfx)\nabla_{\theta} \mathbf x \text{d} \mathbf x \\  &amp;= \mathbb{E}_{p(\mathbf x;\theta)}[\nabla_{\mathbf x} f(\mathbfx) \nabla_{\theta} \mathbf x].\end{aligned}\]</span></p><p>One way to compute <span class="math inline">\(\nabla_{\theta}\mathbf x\)</span> is to use <span class="math inline">\(\nabla_{\theta}g(\epsilon; \theta)\)</span>, but this form is not alwaysconvenient.Another way to compute <span class="math inline">\(\nabla_{\theta} \mathbf x\)</span> is to use theinverse of the path <span class="math inline">\(g^{-1}(x;\theta)\)</span>. <span class="math inline">\(g^{-1}(x; \theta)\)</span>can be thought as the 'standardisation path' of the random variable--that is the transformation that removes the dependence of the samplingon the distribution parameters, standardising it to a zero mean unitvariance-like form.</p><p>Consider the equation <span class="math inline">\(\epsilon =g^{-1}(x; \theta)\)</span>, evaluating the total derivative on bothsides: <span class="math display">\[\begin{aligned}  \nabla_{\theta} \epsilon &amp;= \nabla_{\theta} g^{-1}(x;\theta) \\  0 &amp;= \nabla_{x} g^{-1}(x;\theta) \nabla_{\theta} x +\nabla_{\theta} g^{-1}(x;\theta) \\  \text{thus,} \nabla_{\theta} x &amp;= - (\nabla_{x}g^{-1}(x;\theta))^{-1} \nabla_{\theta} g^{-1}(x; \theta).\end{aligned}\]</span></p><p>In this form, we can apply pathwise gradient estimator to a far widerset of distributions and paths, such as for the Beta, Gamma, andDirichlet distributions.</p><ul><li>Example (Univariate distributions). For univariate distribution<span class="math inline">\(p(x;\theta)\)</span>, we can use thesampling path given by the inverse CDF: <span class="math inline">\(x =g(\epsilon;\theta) = F^{-1}(\epsilon; \theta)\)</span>, where <span class="math inline">\(\epsilon \sim \mathcal{U}[0, 1]\)</span>.Computing the derivative <span class="math inline">\(\nabla_{\theta} x =\nabla_{\theta} F^{-1} (\epsilon; \theta)\)</span> is often complicatedand expensive. We can obtain an alternative expression by consideringthe inverse path <span class="math inline">\(g^{-1}(x;\theta) = F(x;\theta)\)</span>, we have: <span class="math display">\[\nabla_{\theta} x = -\frac{\nabla_{\theta} F(x;\theta)}{\nabla_x F(x;\theta)} = - \frac{\nabla_{\theta} F(x;\theta)}{p(x; \theta)}.\]</span></li></ul><h3 id="bias-and-variance">Bias and variance</h3><p>When deriving the pathwise estimator, we exploited an interchangebetween differentiation and integration. If this interchange is valid,then the estimator is <strong>unbiased</strong>.</p><p>The variance of the pathwise estimator can be shown to be bounded bythe squared Lipschitz constant of the cost function <span class="math inline">\(f(x)\)</span>. (1) The variance bounds that existare independent of the dimensionality of the parameters <span class="math inline">\(\theta\)</span>, which means we can getlow-variance gradient estimates, even in high-dimensional space. (2) Asthe cost funtion becomes highly-variable, i.e., the Lipschitz constantincreases, the variance of the pathwise estimator can be higher thanthat of the score function methods.</p><p>The pathwise estimator will not always have lower variance whencompared to other methods since the variance is bounded by the Lipschitzconstant of the cost function.</p><h3 id="computational-cost-1">Computational cost</h3><p>The pathwise gradient estimator is restricted to differentiable costfunctions, which is a limitation when compared to the score functionestimator. Rapid convergence can be obtained even when using only asingle sample to compute the gradient, as is often done in practice.There is a trade-off between the number of samples used and theLipschitz constant of the cost function, and may require more samples tobe used for functions with higher Lipschitz constants. Thisconsideration is why we will find that regularisation that promotessmoothness of the functions we learn is important for successfulapplications.</p><p>The computational cost of the pathwise estimator is the same as thescore function estimator and is of the order <span class="math inline">\(O(N(D+L))\)</span> for <span class="math inline">\(N\)</span> samples, <span class="math inline">\(D\)</span> dimensional distributional parameters<span class="math inline">\(\theta\)</span> and <span class="math inline">\(L\)</span> is the cost of evaluating the costfunction and its gradient.</p><h3 id="conclusion-1">Conclusion</h3><ul><li>The pathwise estimator is only applicable to differentiable costfunctions.</li><li>When using the pathwise estimator, we do not need to know themeasure <span class="math inline">\(p(x; \theta)\)</span>, but we needto know the deterministic and differentiable sampling path <span class="math inline">\(g(\epsilon; \theta)\)</span> and the basedistribution <span class="math inline">\(p(\epsilon)\)</span>.</li><li>The estimator can be implemented using only a single sample, makingit computationally efficient.</li><li>We might need to control the smoothness of the cost function toensure that the variance of the estimator is low, and may need to employvariance reduction techniques.</li></ul><h3 id="gumbel-softmax-estimator">Gumbel softmax estimator</h3><h2 id="measure-valued-gradient-estimators">Measure-Valued GradientEstimators</h2><h3 id="weak-derivatives-measure-valued-derivatives">Weak Derivatives(measure-valued derivatives)</h3><p>Consider the derivative of a density function <span class="math inline">\(p(x; \theta)\)</span> with respect to a singleparameter <span class="math inline">\(\theta_i\)</span>, with <span class="math inline">\(i\)</span> the index on the set of distributionalparameters. The derivative <span class="math inline">\(\nabla_{\theta_i}p(x;\theta)\)</span> is not a density, since it may have negative valuesand does not integrate to one. Using the properties of signed measures,we can always decompose this derivative into a difference of twodensities, each multiplied by a constant: <span class="math display">\[\nabla_{\theta_i} p(x;\theta) = c_{\theta_i}^+ p^+(x;\theta) -c_{\theta_i}^- p^-(x;\theta),\]</span> where <span class="math inline">\(p^+, p^-\)</span> aredensities, referred to as the positive and negative parts of <span class="math inline">\(p\)</span>. By integrating both sides of theequation, we can obtain the constants <span class="math inline">\(c_{\theta_i}^+, c_{\theta_i}^-\)</span>: <span class="math display">\[\begin{aligned}  &amp;\int \nabla_{\theta_i} p(x;\theta) \text{d}x = \nabla_{\theta_i}\int p(x;\theta) \text{d}x  = 0; \\  &amp;\int c_{\theta_i}^+ p^+(x;\theta) - c_{\theta_i}^- p^-(x;\theta)\text{d}x = c_{\theta_i}^+ - c_{\theta_i}^- .\end{aligned}\]</span> Thus, we have: <span class="math display">\[c_{\theta_i}^+ = c_{\theta_i}^- := c_{\theta_i}\]</span> The decomposition of the derivative becomes: <span class="math display">\[\nabla_{\theta_i} p(x;\theta) = c_{\theta_i} (p^+(x;\theta) -p^-(x;\theta)).\]</span> The triple <span class="math inline">\((c_{\theta_i}, p^+,p^-)\)</span> is called the i-th <strong>weak derivative</strong> of<span class="math inline">\(p\)</span> with respect to <span class="math inline">\(\theta_i\)</span>.</p><p>For multivariate parameters <span class="math inline">\(\theta\)</span>, each dimension has onetriple.</p><p><img src="/2023/10/21/Monte-Carlo-Gradient-Estimation/figure5.png"></p><ul><li>The derivative is weak because we do not require the density to bedifferentiable.</li><li>The weak derivative is not unique, but always exists and can beobtained using the Hahn-Jordan decomposition of a signed measure intotwo measures that have complementary support. see <a href="https://www.math.uwaterloo.ca/~beforres/PMath451/Course_Notes/Chapter4.pdf">signedmeasure</a>.</li></ul><h3 id="deriving-the-estimator">Deriving the estimator</h3><p>For D-dimensional parameters <span class="math inline">\(\theta\)</span>, we can write the gradient of theexpectation as: <span class="math display">\[\begin{aligned}\eta_i &amp;= \nabla_{\theta_i} \mathbb{E}_{p(x;\theta)}[f(x)] =\nabla_{\theta_i} \int p(x;\theta) f(x) \text{d}x \\&amp;= \int \nabla_{\theta_i} p(x;\theta) f(x) \text{d}x \\&amp;= \int c_{\theta_i} (p_i^+(x;\theta) - p_i^-(x;\theta)) f(x)\text{d}x \\&amp;= c_{\theta_i} (\mathbb{E}_{p_i^+(x;\theta)}[f(x)] -\mathbb{E}_{p_i^-(x;\theta)}[f(x)]). \\\bar{\eta}_{i, N} &amp;=  \frac{c_{\theta_i}}{N} (\sum_{n=1}^{N}f(\hat{x}^{+(n)}) - \sum_{n=1}^{N} f(\hat{x}^{-(n)})), \quad\hat{x}^{+(n)} \sim p_i^+(x;\theta), \quad \hat{x}^{-(n)} \simp_i^-(x;\theta).\end{aligned}\]</span> The positive and negative components may be differentdepending on which parameter of the measure the derivative is taken withrespect to. The constant <span class="math inline">\(c_{\theta_i}\)</span> will also change dependingthe parameter being differentiated.</p><ul><li>Example (Bernoulli measure-valued gradient). Consider the Bernoullidistribution <span class="math inline">\(p(x;\theta) = \theta^x(1-\theta)^{1-x}\)</span>, with <span class="math inline">\(x \in \{0,1\}\)</span> and <span class="math inline">\(\theta \in [0, 1]\)</span>.By taking the derivative with respect to <span class="math inline">\(\theta\)</span>, we have: <span class="math display">\[\begin{aligned}  \nabla_{\theta} \int p(x;\theta) f(x) \text{d}x&amp;=  \nabla_{\theta} (\theta f(1) + (1-\theta) f(0))\\  &amp;= f(1) - f(0).\end{aligned}\]</span> By weak derivative, we have: <span class="math display">\[\begin{aligned}\nabla_{\theta} \int p(x;\theta) f(x) \text{d}x &amp;= \int\nabla_{\theta} p(x;\theta) f(x) \text{d}x \\&amp;= \int \delta_1 f(x) - \delta_0 f(x) \text{d}x \\&amp;= f(1) - f(0).\end{aligned}\]</span> which is the same as the original gradient.</li></ul><h4 id="vector-case">Vector case</h4><p>If the measure is a factorised distribution <span class="math inline">\(p(x;\theta) = \prod_d p(x_d | \theta_d)\)</span>,then the positive component and negative component of the weakderivative will itself factorise across the dimensions. For the positivecomponent, this decomposition will be <span class="math inline">\(p^+_i(x;\theta) =p(x_{-i})p^+_i(x_i;\theta_i)\)</span>, which is the product of themarginal distribution <span class="math inline">\(p(x_{-i})\)</span> andthe positive component of the weak derivative with respect to <span class="math inline">\(\theta_i\)</span>. The negative component will be<span class="math inline">\(p^-_i(x;\theta) =p(x_{-i})p^-_i(x_i;\theta_i)\)</span>, which is the product of themarginal distribution <span class="math inline">\(p(x_{-i})\)</span> andthe negative component of the weak derivative with respect to <span class="math inline">\(\theta_i\)</span>.</p><h3 id="estimator-properties-1">Estimator Properties</h3><h4 id="domination">Domination</h4><p>Remember in the score function estimator, we need the measure to beabsolutely continuous with respect to <span class="math inline">\(\theta\)</span>. We explored one example where wewere unable to ensure domination, because no bounding constant appliesat the boundaries of the domain. For weak derivatives, we can alwaysensure the correctness of the interchange between differentiation andintegration： the fundamental property of weak derivatives states thatif the triple <span class="math inline">\((c, p^+, p^-)\)</span> is theweak derivative of <span class="math inline">\(p(x;\theta)\)</span>,then for every <strong>bounded continuous</strong> function <span class="math inline">\(f(x)\)</span>, we have: <span class="math display">\[\nabla_{\theta} \int f(x)  p(x;\theta) \text{d}x = c_{\theta} [\int f(x)p^+(x;\theta) \text{d}x - \int f(x) p^-(x;\theta) \text{d}x].\]</span></p><ul><li>Example (Bounded support). Consider the measure-valued estimator fora cost function <span class="math inline">\(f(x) = x\)</span> anddistribution <span class="math inline">\(p(x; \theta) = \frac{1}{\theta}1_{\{0 &lt; x &lt; \theta\}}\)</span>, which is differential in <span class="math inline">\(\theta\)</span> when <span class="math inline">\(x\in (0, \theta)\)</span>; The measure-valued derivative is <span class="math display">\[\begin{aligned}    \nabla_{\theta} \int f(x) \mathcal{U}_{[0, \theta]}(x) \text{d}x&amp;= \nabla_{\theta} (\frac{1}{\theta} \int_{0}^{\theta} f(x)\text{d}x) = \frac{1}{\theta} f(\theta) - \frac{1}{\theta^2}\int_{0}^{\theta} f(x) \text{d} x \\    &amp;= \frac{1}{\theta} (\int f(x) \delta_{\theta}(x) \text{d}x -\int f(x) \mathcal{U}_{[0, \theta]}(x) \text{d}x) \\\end{aligned}\]</span></li></ul><p>The measure-valued derivative is given by the triple <span class="math inline">\((\frac{1}{\theta}, \delta_{\theta},\mathcal{U}_{[0, \theta]})\)</span>. For specific cost function <span class="math inline">\(f(x) = x\)</span>, we have:</p><p>The true gradient is: <span class="math inline">\(\nabla_{\theta}\mathbb{E}_{p(x;\theta)} [x] = \nabla_{\theta} (\frac{1}{\theta}\int_{0}^{\theta} \frac{x^2}{2}) = \frac{1}{2}\)</span>. Themeasure-valued gradient is: <span class="math inline">\(\frac{1}{\theta}(\mathbb{E}_{\delta_{\theta}} [x] - \mathbb{E}_{\mathcal{U}_{[0,\theta]}[x]}) = \frac{1}{\theta} (\theta - \frac{\theta}{2}) =\frac{1}{2}.\)</span></p><h4 id="bias-and-variance-1">Bias and variance</h4><p>For bounded and continuous cost functions <span class="math inline">\(f\)</span>, by using the fundamental property ofweak derivatives, the measure-valued gradient estimator is<strong>unbiased</strong>.</p><p>The variance of the measure-valued gradient estimator is: <span class="math display">\[\begin{aligned}\text{Var}_{p(x;\theta)}[\bar{\eta}_1] =\text{Var}_{p^+(x;\theta)}[f(x)] + \text{Var}_{p^-(x;\theta)}[f(x)] - 2\text{Cov}_{p^+(x';\theta)p^-(x;\theta)} [f(x'), f(x)].\end{aligned}\]</span></p><ul><li>The variance depends on the choice of decomposition of the weakderivative into positive and negative components.</li><li>If the random variables can be 'coupled' in some way, where theyshare the same underlying source of randomness, this will reduce thevariance of the gradient estimator by increasing the covariance term.The most common way is to sample the variables <span class="math inline">\(x'\)</span> and <span class="math inline">\(x\)</span> using common random numbers. Anotherway is to use variance reduction techniques.</li><li>Figure 5 shows that the measure valued estimator is not sensitive tothe dimensionality of the parameters <span class="math inline">\(\theta\)</span>. It is however sensitive to themagnitude of the function.</li></ul><p><img src="/2023/10/21/Monte-Carlo-Gradient-Estimation/figure6.png"></p><h4 id="computational-cost-2">Computational cost</h4><p>Measure-valued gradients are much more computationally expensive thanthe score-function or pathwise gradients. This is because the gradientwe computed is the gradient for a single parameter: for every parameterwe require two evaluations of the cost function to compute its gradient.It is this structure of adapting the underlying sampling distributionsfor each parameter that leads to the low variance of the estimator butat the same time makes its application to high-dimensional parameterspaces prohibitive.</p><p>The computational cost of the measure-valued gradient estimator isthe order of <span class="math inline">\(O(2NDL)\)</span> for <span class="math inline">\(N\)</span> samples, <span class="math inline">\(D\)</span> dimensional distributional parameters<span class="math inline">\(\theta\)</span> and <span class="math inline">\(L\)</span> is the cost of evaluating the costfunction.</p><h3 id="conclusion-2">Conclusion</h3><ul><li>The measure-valued estimator can be used with any type of costfunction, differentiable or not. As long as we can evaluate the costfunction repeatedly for different inputs.</li><li>It is applicable to both discrete and continuous distributions.</li><li>Computationally expensice in high-dimensional parameter spaces.</li><li>We need methods to sample from the positive and negativemeasures.</li><li>Using the weak derivative <strong>requires manual derivation of thedecomposition at first</strong>, although for many common distributionsthe weak-derivative decompositions are known.</li></ul><h2 id="variance-reduction-techniques">Variance ReductionTechniques</h2><p>The gradient variance is one of the principal sources of performanceissues. This paper introduces four common methods to reduce the varianceof gradient estimators: large-samples, coupling, conditioning, andcontrol variates.</p><h3 id="large-samples">Large-Samples</h3><p>The simplest way to reduce the variance of the gradient estimator isto use more samples. The variance of an estimators will shrinks as <span class="math inline">\(O(\frac{1}{N})\)</span>, where <span class="math inline">\(N\)</span> is the number of samples. However, thecomputational cost will increase linearly with the number of samples.The computational cost can be reduces by parallelising the computationof the gradient across multiple processors. Sometimes, increasing thenumber of Monte Carlo samples will not be an option, such as when thecost function involves a real-world experiment or interaction with auser.</p><h3 id="coupling-and-common-random-numbers">Coupling and Common randomnumbers</h3><p>When consider the difference between two expectations of a function<span class="math inline">\(f(x)\)</span> under different butclosely-related distributions <span class="math inline">\(p_1(x)\)</span> and <span class="math inline">\(p_2(x)\)</span>: <span class="math display">\[\begin{aligned}\eta = \mathbb{E}_{p_1(x)}[f(x)] - \mathbb{E}_{p_2(x)}[f(x)]\end{aligned}\]</span></p><p>The direct method to compute the difference is to estimate eachexpectation separately using Monte Carlo sampling, and then compute thedifference between the two estimates： <span class="math display">\[\begin{aligned}\bar{\eta}_{ind} = \frac{1}{N} \sum_{n=1}^{N} f(\hat{x}_1^{(n)}) -\frac{1}{N} \sum_{n=1}^{N} f(\hat{x}_2^{(n)}),\end{aligned}\]</span> where <span class="math inline">\(\hat{x}_1^{(n)} \simp_1(x)\)</span> and <span class="math inline">\(\hat{x}_2^{(n)} \simp_2(x)\)</span>.</p><p>We can achieve a simple form of variance reduction by coupling <span class="math inline">\(\hat{x}_1^{(n)}\)</span> and <span class="math inline">\(\hat{x}_2^{(n)}\)</span>, so that each pair <span class="math inline">\((\hat{x}_1^{(n)}, \hat{x}_2^{(n)})\)</span> issampled from some joint distribution <span class="math inline">\(p_{12}(x_1, x_2)\)</span> with marginals <span class="math inline">\(p_1(x)\)</span> and <span class="math inline">\(p_2(x)\)</span>. The variance of the coupledestimator is: <span class="math display">\[\begin{aligned}\text{Var}_{p_12(x_1, x_2)}[\bar{\eta}_{cpl}] &amp;=\text{Var}_{p_12(x_1, x_2)}[f(x_1) - f(x_2)] \\&amp;= \text{Var}_{p_1(x_1)}[f(x_1)] + \text{Var}_{p_2(x_2)}[f(x_2)] - 2\text{Cov}_{p_12(x_1, x_2)}[f(x_1), f(x_2)] \\&amp;= \text{Var}_{p_1(x_1)p_2(x_2)}[\bar{\eta}_{ind}] - 2\text{Cov}_{p_1(x_1)}[f(x_1), f(x_2)].\end{aligned}\]</span> Thus, to reduce the variance we need to choose a coupling<span class="math inline">\(p_{12}(x_1, x_2)\)</span> such that <span class="math inline">\(f(x_1)\)</span> and <span class="math inline">\(f(x_2)\)</span> are positively correlated. Themost common way to achieve this is to use <strong>common randomnumbers</strong> when <span class="math inline">\(p_1(x_1)\)</span> and<span class="math inline">\(p_2(x_2)\)</span> are close or in a relatedfamily of distributions. This means that the random numbers used togenerate <span class="math inline">\(x_1\)</span> and <span class="math inline">\(x_2\)</span> are the same. For example, in theunivariate case, we can first sample <span class="math inline">\(u \sim\mathcal{U}[0,1]\)</span> and then apply the inverse CDF transformationto obtain <span class="math inline">\(x_1 = F_{p_1}^{-1}(u)\)</span> and<span class="math inline">\(x_2 = F_{p_2}^{-1}(u)\)</span>.</p><ul><li>Coupling may not always reduce the variance of the measure valuedestimator, depending on the cost function.</li></ul><h3 id="conditioning">Conditioning</h3><p>Rao-Blackwellisation is a variance reduction technique thatprobabilistically conditions our estimator on a subset of dimensions andintegrates out the remaining dimensions.</p><p>Assume that the dimensions <span class="math inline">\(\{1,...,D\}\)</span> of <span class="math inline">\(x\)</span> are partitionedinto a set of dimensions <span class="math inline">\(\mathcal{S}\)</span> and its complement <span class="math inline">\(\mathcal{S}^c = {1,...,D}\ \mathcal{S}\)</span>.The expectation <span class="math inline">\(g(x_{\mathcal{S}^c}) =\mathbb{E}_{p(x_{\mathcal{S}})}[f(x)|x_{\mathcal{S}^c}]\)</span>. We canestimate <span class="math inline">\(\mathbb{E}_{p(x)}[f(x)]\)</span> byperforming Monte Carlo integration over the dimensions <span class="math inline">\(\mathcal{S}^c\)</span>: <span class="math display">\[\begin{aligned}\bar{g}_{N} &amp;= \frac{1}{N} \sum_{n=1}^{N}g(\hat{x}_{\mathcal{S}^c}^{(n)}) \\&amp;= \frac{1}{N} \sum_{n=1}^{N}\mathbb{E}_{p(x_{\mathcal{S}})}[f(x)|\hat{x}_{\mathcal{S}^c}^{(n)}],  \quad\hat{x}_{\mathcal{S}^c}^{(n)} \sim p(x_{\mathcal{S}^c}).\end{aligned}\]</span></p><p>By law of total expectation <span class="math inline">\(\text{Var}(Y)= \mathbb{E}[\text{Var}(Y | X)] + \text{Var}(\mathbb{E}[Y |X])\)</span>, we have: <span class="math display">\[\begin{aligned}\text{Var}_{p(x)}[f(x)] &amp;=\mathbb{E}_{p(x_{\mathcal{S}^c})}[\text{Var}_{p(x_{\mathcal{S}})}[f(x)|x_{\mathcal{S}^c}]]+\text{Var}_{p(x_{S^c})}[\mathbb{E}_{p(x_{\mathcal{S}})}[f(x)|x_{\mathcal{S}^c}]]\\&amp;=\mathbb{E}_{p(x_{\mathcal{S}^c})}[\text{Var}_{p(x_{\mathcal{S}})}[f(x)|x_{\mathcal{S}^c}]]+ \text{Var}_{p(x_{\mathcal{S}^c})}[g(x_{\mathcal{S}^c})] \\&amp; \geq \text{Var}_{p(x_{\mathcal{S}^c})}[g(x_{\mathcal{S}^c})]\end{aligned}\]</span></p><p>Thus, for unconditional one <span class="math inline">\(\bar{f}_N =\frac{1}{N}\sum_{n=1}^{N} f(\hat{x}^{(n)})\)</span>, we have: <span class="math display">\[\begin{aligned}\text{Var}_{p(x)}[\bar{f}_N] = \frac{1}{N} \text{Var}_{p(x)}[f(x)] \geq\frac{1}{N}  \text{Var}_{p(x_{\mathcal{S}^c})}[g(x_{\mathcal{S}^c})] =\text{Var}_{p(x_{\mathcal{S}^c})}[\bar{g}_{N}].\end{aligned}\]</span></p><ul><li>Conditional estimator has lower variance than the unconditionalestimator.</li><li>This technique is useful in practice only if we can compute theconditional expectation <span class="math inline">\(g(x_{\mathcal{S}^c})\)</span> efficiently.</li></ul><h2 id="control-variates">Control Variates</h2><p>Since all the gradient estimators have the same form <span class="math inline">\(\mathbb{E}_{p(x;\theta)}[f(x)]\)</span>, we willfocus on this general form. The strategy is to replace the function<span class="math inline">\(f(x)\)</span> in the expectation with asubstitute function <span class="math inline">\(\tilde{f}(x)\)</span>whose expectation is the same as <span class="math inline">\(\mathbb{E}_{p(x;\theta)}[f(x)]\)</span>, but whosevariance is lower.</p><p>If we have a function <span class="math inline">\(h(x)\)</span> witha known expectation <span class="math inline">\(\mathbb{E}_{p(x;\theta)}[h(x)]\)</span>, then wecan construct a new function <span class="math display">\[\tilde{f}(x) =f(x) - \beta(h(x)-\mathbb{E}_{p(x;\theta)}[h(x)]).\]</span> Here <span class="math inline">\(h(x)\)</span> is a control variate. <span class="math inline">\(\beta\)</span> is a coefficient that affects thestrength of the control variate. Then we can get a control variateestimator: <span class="math display">\[\begin{aligned}\bar{\eta}_{N} &amp;= \frac{1}{N} \sum_{n=1}^{N}\tilde{f}(\hat{x}^{(n)}) \\&amp;= \bar{f} - \beta (\bar{h} - \mathbb{E}_{p(x;\theta)}[h(x)]).\end{aligned}\]</span></p><h3 id="bias-consistency-and-variance">Bias, consistency andvariance</h3><ol type="1"><li><p>Unbiasedness. The control variate estimator is unbiased： <span class="math display">\[\begin{aligned}\mathbb{E}_{p(x;\theta)}[\bar{\eta}_{N}] &amp;=\mathbb{E}_{p(x;\theta)}[\bar{f} - \beta (\bar{h} -\mathbb{E}_{p(x;\theta)}[h(x)]) ]\\&amp;= \mathbb{E}_{p(x;\theta)}[\bar{f}] \\&amp;= \mathbb{E}_{p(x;\theta)}[f(x)].\end{aligned}\]</span></p></li><li><p>Consistency. The control variate estimator is consistent: <span class="math display">\[\lim_{N \to \infty} \bar{\eta}_{N} =\mathbb{E}_{p(x;\theta)}[\tilde{f}(x)] = \mathbb{E}_{p(x;\theta)}[f(x)].\]</span></p></li><li><p>Variance. The variance of the control variate estimator is (N=1):<span class="math display">\[\begin{aligned}\text{Var}_{p(x;\theta)}[\tilde{f}] &amp;= \text{Var}_{p(x;\theta)}[f -\beta (h - \mathbb{E}_{p(x;\theta)}[h(x)]) ]\\&amp;= \text{Var}_{p(x;\theta)}[f] + \beta^2 \text{Var}_{p(x;\theta)}[h]- 2 \beta \text{Cov}_{p(x;\theta)}[f, h].\end{aligned}\]</span> By minimising the right-hand side of the equation with respectto <span class="math inline">\(\beta\)</span>, we can obtain the optimalvalue of <span class="math inline">\(\beta\)</span>: <span class="math display">\[\beta^* = \frac{\text{Cov}_{p(x;\theta)}[f,h]}{\text{Var}_{p(x;\theta)}[h]} =\sqrt{\frac{\text{Var}_{p(x;\theta)}[f]}{\text{Var}_{p(x;\theta)}[h]}}\text{Corr}(f, h).\]</span> Using the optimal value of <span class="math inline">\(\beta\)</span>, the potential variance reductionis: <span class="math display">\[\begin{aligned}\frac{\text{Var}_{p(x;\theta)}[\tilde{f}]}{\text{Var}_{p(x;\theta)}[f]}= \frac{\text{Var}_{p(x;\theta)}[f - \beta (h -\mathbb{E}_{p(x;\theta)}[h(x)])]}{\text{Var}_{p(x;\theta)}[f]} = 1 -\text{Corr}(f, h)^2 \leq 1.\end{aligned}\]</span></p></li></ol><ul><li>The stronger the correlation between <span class="math inline">\(f\)</span> and <span class="math inline">\(h\)</span>, the greater the potential variancereduction.</li><li>In practice, the optimal <span class="math inline">\(\beta^*\)</span> will not be known and it can beestimated using the same <span class="math inline">\(N\)</span> samples.But the samples used to estimate <span class="math inline">\(\bar{h}\)</span> will introduce a bias because<span class="math inline">\(\bar{\beta}_N\)</span> and <span class="math inline">\(\bar{h}\)</span> will no longer be independent. Inpractice, thi bias is often negligible and can be controlled since itdecreases quickly as the number of samples <span class="math inline">\(N\)</span> increases.</li></ul><h3 id="multiple-and-non-linear-controls">Multiple and Non-linearControls</h3><h3 id="designing-control-variates">Designing Control Variates</h3><ol type="1"><li><p>Baselines. One simple way to reduce the variance of ascore-function gradient estimator is to use the score function itself asa control variate, since its expectation under the measure is zero. Themodified estimator is: <span class="math display">\[\begin{aligned}\bar{\eta}_{N} &amp;= \frac{1}{N} \sum_{n=1}^{N} (f(\hat{x}^{(n)}) -\beta)\nabla_{\theta} \log p(\hat{x}^n;\theta), \hat{x}^{(n)} \simp(x;\theta) \\\end{aligned}\]</span> In reinforcement learning, <span class="math inline">\(\beta\)</span> is called a baseline and it can beestimated with a running average of the cost. While this approach iseasier to implement than optimising <span class="math inline">\(\beta\)</span> to minimise variance, it is notoptimal and does not guarantee lower variance compared to the vanillascore-function estimator.</p></li><li><p>Bounds. We can use bounds on the cost function <span class="math inline">\(f\)</span> as ways of specifying the form of thecontrol variate <span class="math inline">\(h\)</span>. This isintuitive because it maintains a correlation between <span class="math inline">\(f\)</span> and <span class="math inline">\(h\)</span>, and if chosen well, may be easilyintegrable against the measure and available in closed form. Thisapproach requires more knowledge of the cost function, since we willneed to characterise the cost analytically in some way to bound it. Ingeneral, unless the bounds used are tight, they will not be effective ascontrol variates, since the gap between the bound and the true functionis not controllable and will not necessarily give the information neededfor variance reduction.</p></li><li><p>Delta method. The delta method is a way of constructing a controlvariate by using the Taylor expansion of the cost function. Thisrequires a cost function that is differentiable so that we can computethe second-order Taylor expansion, but can be an effective and verygeneral approach for variance reduction that allows easy implementation.It can be used for variance reduction in both the score-functionestimator (Paisley et al., 2012) and the pathwise estimator(Miller etal., 2017).</p></li></ol><ul><li><p>Example. Define <span class="math inline">\(\gamma(x)\)</span> isthe gradient of the cost function <span class="math inline">\(f(x)\)</span>, <span class="math inline">\(H(x)\)</span> is the Hessian of the cost function<span class="math inline">\(f(x)\)</span>. The second-order Taylorexpansion of a cost function expand around point <span class="math inline">\(\mu\)</span> and its derivate are <span class="math display">\[\begin{aligned}  h(x) &amp;= f(\mu) + (x - \mu)^T \gamma(\mu) + \frac{1}{2} (x - \mu)^TH(\mu) (x - \mu), \\  \nabla_{x} h(x) &amp;= \gamma(\mu)^T + (x - \mu)^T H(\mu).\end{aligned}\]</span></p><p>We can use this expansion directly as a control variate for thescore-function estimator:</p></li></ul><p><span class="math display">\[\begin{aligned}\bar{\eta}_{SF} &amp;= \nabla_{\theta} \mathbb{E}_{p(x;\theta)}[f(x)] \\&amp;= \nabla_{\theta} \mathbb{E}_{p(x;\theta)}[f(x) - \beta^T h(x)] +\beta^T \nabla_{\theta} \mathbb{E}_{p(x;\theta)}[h(x)] \\&amp;= \mathbb{E}_{p(x;\theta)}[(f(x) - \beta^T h(x))\nabla_{\theta}\log p(x;\theta)] + \beta^T \nabla_{\theta}\mathbb{E}_{p(x;\theta)}[h(x)].\end{aligned}\]</span> In the Gaussian mean-field variational inference that Paisleyet al. (2012) consider, the second term is known in closed-form andhence does not require Monte Carlo approximation. <span class="math inline">\(\beta\)</span> is a multivariate controlcoefficient and is estimated separately.</p><p>For the pathwise estimator, using the sampling path <span class="math inline">\(x = g(\epsilon; \theta)\)</span>, we have: <span class="math display">\[  \begin{aligned}    \bar{\eta}_{PW} &amp;= \nabla_{\theta}\mathbb{E}_{p(x;\theta)}[f(x)] \\    &amp;= \nabla_{\theta} \mathbb{E}_{p(x;\theta)}[f(x) - \beta^T h(x)]+ \beta^T \nabla_{\theta} \mathbb{E}_{p(x;\theta)}[h(x)] \\    &amp;= \nabla_{\theta} \mathbb{E}_{p(x;\theta)}[f(g(\epsilon;\theta)) - \beta^T h(g(\epsilon; \theta))] + \beta^T \nabla_{\theta}\mathbb{E}_{p(x;\theta)}[h(x)] \\    &amp;= \mathbb{E}_{p(\epsilon)}[\nabla_{x}f(x) \nabla_{\theta}g(\epsilon; \theta) - \beta \nabla_x h(x)\nabla_{\theta} g(\epsilon;\theta)] + \beta^T \nabla_{\theta} \mathbb{E}_{p(x;\theta)}[h(x)].  \end{aligned}  \]</span> Assume that the final term is known in closed-form and doesnot require stochastic approximation.</p><h2 id="guidance-in-choosing-gradient-estimators">Guidance in ChoosingGradient Estimators</h2><p>The authors provide some guidance in choosing gradientestimators.</p><ul><li><p>If our estimation problem involves continuous functions andmeasures that are continuous in the domain, then using the pathwiseestimator is a good default. It is relatively easy to implement and itsdefault implementation, without additional variance reduction, willtypically have variance that is low enough so as not to interfere withthe optimisation.</p></li><li><p>If the cost function is not differentiable or is a black-boxfunction then the score-function or the measure-valued gradients areavailable. If the number of parameters is low, then the measure-valuedgradient will typically have lower variance and would be preferred. Butif we have a high-dimensional parameter set, then the score-functionestimator should be used.</p></li><li><p>If we have no control over the number of times we can evaluate ablack-box cost function, effectively only allowing a single evaluationof it, then the score function is the only estimator of the three wereviewed that is applicable.</p></li><li><p>The score-function estimator should, by default, always beimplemented with at least some basic variance reduction. The simplestoption is to use a baseline control variate estimated with a runningaverage of the cost value. • When using the score-function estimator,some attention should be paid to the dynamic range of the cost functionand its variance, and ways found to keep its value bounded within areasonable range, e.g. by transforming the cost so that it is zero mean,or using a baseline.</p></li><li><p>For all estimators, track the variance of the gradients ifpossible and address high variance by using a larger number of samplesfrom the measure, decreasing the learning rate, or clipping the gradientvalues. It may also be useful to restrict the range of some parametersto avoid extreme values, e.g. by clipping them to a desiredinterval.</p></li><li><p>The measure-valued gradient should be used with some couplingmethod for variance reduction. Coupling strategies that exploitrelationships between the positive and negative components of thedensity decomposition, and which have shared sampling paths, are knownfor the commonly-used distributions.</p></li><li><p>If we have several unbiased gradient estimators, a convexcombination of them might have lower variance than any of the individualestimators.</p></li><li><p>If the measure is discrete on its domain then the score-functionor measure-valued gradient are available. The choice will again dependon the dimensionality of the parameter space.</p></li><li><p>In all cases, we strongly recommend having a broad set of teststo verify the unbiasedness of the gradient estimator whenimplemented.</p></li></ul><h1 id="reference">Reference</h1><p>(https://pages.stat.wisc.edu/~shao/stat609/stat609-07.pdf)</p><p>https://bochang.me/blog/posts/measure-val-grad/</p><p><a href="https://www.math.uwaterloo.ca/~beforres/PMath451/Course_Notes/Chapter4.pdf">signedmeasure</a></p>]]></content>
      
      
      <categories>
          
          <category> Machine Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Monte Carlo Gradient Estimator </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Introduction to Deep Learning</title>
      <link href="/2023/09/11/deep-learning-with-structures/"/>
      <url>/2023/09/11/deep-learning-with-structures/</url>
      
        <content type="html"><![CDATA[<h2 id="brief-intro-to-deep-learning">Brief Intro to Deep Learning</h2><p>Deep learning: - Data: large datasets, e.g., ImageNet, etc.; - Model:deep neural networks, e.g., ResNet-152, etc.; - Learning algorithm:backpropagation, i.e., stochastic gradient descent (SGD).</p><p>In recurrent neural networks (RNNs): <strong>same</strong> neuralnetwork gets reused many times. <span class="math display">\[h^t = F(x^t, h^{t-1}, W).\]</span></p><p>Back propagation: - Loss is always a scalar value; - For the backwardpropogation, the gradient is in the form: <span class="math inline">\(J^T v\)</span>, where <span class="math inline">\(v\)</span> is a vector, and <span class="math inline">\(J\)</span> is a Jacobian matrix, <span class="math inline">\(T\)</span> means transpose; - For the forwardpropogation, the gradient is in the form: <span class="math inline">\(Jv\)</span>. - In BF process, the shape of a Jacobian matrix is <span class="math inline">\(m \times n\)</span>, where <span class="math inline">\(m\)</span> is the dimension of output, and <span class="math inline">\(n\)</span> is the dimension of input.</p><p>Consider Vector-by-Matrix Gradients:</p><p>Case 1: <span class="math inline">\(\bm{z} =\mathbf{W}\bm{x}\)</span>, where <span class="math inline">\(\bm x \in\mathbb{R}^{n \times 1}\)</span>, <span class="math inline">\(\mathbf{W}\in \mathbb{R}^{m \times n}\)</span>, <span class="math inline">\(\bm z\in \mathbb{R}^{m \times 1}\)</span>. <span class="math display">\[\begin{aligned}\frac{\partial \bm{z}}{\partial \bm{x}} &amp;= \mathbf{W}, \\\end{aligned}\]</span></p><p>Case 2： <span class="math inline">\(\bm{z} = \bm{x}\mathbf{W}\)</span>, where <span class="math inline">\(\bm x \in\mathbb{R}^{1 \times m}\)</span>, <span class="math inline">\(\mathbf{W}\in \mathbb{R}^{m \times n}\)</span>, <span class="math inline">\(\bm z\in \mathbb{R}^{1 \times n}\)</span>. <span class="math display">\[\begin{aligned}\frac{\partial \bm{z}}{\partial \bm{x}} &amp;= \mathbf{W}^T, \\\end{aligned}\]</span></p><p>Case 3: <span class="math inline">\(\bm z = \bm x\)</span>, then<span class="math inline">\(\frac{\partial \bm z}{\partial \bm x} =\mathbf{I}\)</span>.</p><p>Case 4: <span class="math inline">\(\mathbf{X} \in \mathbb{R}^{m\times n}\)</span>, then <span class="math inline">\(\frac{\partialscalar}{\partial \mathbf{X}}  \in \mathbb{R}^{m \times n}\)</span>, justas the same as the shape of <span class="math inline">\(\mathbf{X}\)</span>.</p><p>Case 5: <span class="math inline">\(\mathbf{Z} =\mathbf{X}\mathbf{W}\)</span>, where <span class="math inline">\(\mathbf{X} \in \mathbb{R}^{m \times n}\)</span>,<span class="math inline">\(\mathbf{W} \in \mathbb{R}^{n \timesw}\)</span>. Assume <span class="math inline">\(\frac{\partialLoss}{\partial \mathbf{Z}} = \mathbf{\delta} \in \mathbb{R}^{m \timesw}\)</span>. <span class="math display">\[\begin{aligned}\frac{\partial Loss}{\partial \mathbf{X}} &amp;= \mathbf{\delta}\mathbf{W}^T, \\\frac{\partial Loss}{\partial \mathbf{W}} &amp;= \mathbf{X}^T\mathbf{\delta}.\end{aligned}\]</span></p><p><img src="/2023/09/11/deep-learning-with-structures/bp.jpg"></p><p>Here we consider the simple example: <span class="math display">\[\begin{aligned}\bm h_1 = \bm x \mathbf{W}_1,  \qquad (1 \times 4) = (1 \times 3) \times(3 \times 4)\\\hat{\bm y} = \bm h_1 \mathbf{W}_2, \qquad (1 \times 2) = (1 \times 4)\times (4 \times 2)\\L = \|\hat{\bm y}\|_2^2. \qquad (1 \times 1) = (1 \times 2) \times (2\times 1)\end{aligned}\]</span> Then, for the backward propagation process, we have: <span class="math display">\[\begin{aligned}\frac{\partial L}{\partial \hat{\bm y}} &amp;= 2 \hat{\bm y}, \qquad (1\times 2) = (1 \times 2) \\\frac{\partial L}{\partial \mathbf{W}_2} &amp;=  (\frac{\partial\hat{\bm y}}{\partial \mathbf{W}_2})^T \frac{\partial L}{\partial\hat{\bm y}}= \bm h_1^T  \frac{\partial L}{\partial \hat{\bm y}} = 2 \bmh_1^T \hat{\bm y}, \qquad (4 \times 2) = (4 \times 1) \times (1 \times2)\\\frac{\partial L}{\partial \bm h_1} &amp;=  \frac{\partial L}{\partial\hat{\bm y}}\frac{\partial \hat{\bm y}}{\partial \bm h_1}=\frac{\partialL}{\partial \hat{\bm y}} \mathbf{W}_2^T = 2 \hat{\bm y} \mathbf{W}_2^T, \qquad (1 \times 4) = (1 \times 2) \times (2 \times 4)\\\frac{\partial L}{\partial \mathbf{W}_1} &amp;=  (\frac{\partial \bmh_1}{\partial \mathbf{W}_1})^T [\frac{\partial L}{\partial \hat{\bmy}}\frac{\partial \hat{\bm y}}{\partial \bm h_1}] = \bm x^T\frac{\partial L}{\partial \bm h_1} = 2 \bm x^T \hat{\bm y}\mathbf{W}_2^T, \qquad (3 \times 4) = (3 \times 1) \times (1 \times 4)\\\frac{\partial L}{\partial \bm x} &amp;=  \frac{\partial L}{\partial\hat{\bm y}}\frac{\partial \hat{\bm y}}{\partial \bm h_1} \frac{\partial\bm h_1}{\partial \bm x} = 2\hat{\bm y} \mathbf{W}_2^T \mathbf{W}_1^T,\qquad (1 \times 3) = (1 \times 2) \times (2 \times 4) \times (4 \times3)\end{aligned}\]</span></p><p>We can get similar results if <span class="math inline">\(\bm x \in\mathbb{R}^{3 \times 1}\)</span>.</p><h2 id="invariant-and-equivariant">Invariant and Equivariant:</h2><p><strong>Invariant</strong>: A mathematical object (or a class ofmathematical objects) remains unchanged after operations ortransformations of a certain type are applied to the objects <span class="math inline">\(F(g(x)) = F(x)\)</span>, e.g., max pooling;</p><ul><li>Symmetry Group: all transformations under which the object isinvariant</li></ul><p><strong>Equivariant</strong>: Applying a transformation and thencomputing the function produces the same result as computing thefunction and then applying the transformation <span class="math inline">\(F(g(x)) = g(F(x))\)</span>.</p><ul><li><p>Convolution is translation equivariant, i.e., Conv(Shift(X)) =Shift(Conv(X))!</p></li><li><p>Global pooling gives you shift-invariance!</p></li></ul><h3 id="permutation-invariance">Permutation Invariance</h3><p>Birkhoff Polytope: <span class="math display">\[B_n = \{P \in \mathbb{R}^{n \times n} | \forall i, j, P_{ij} \geq 0,\sum_{i=1}^n P_{ij} = 1, \sum_{j=1}^n P_{ij} = 1\}\]</span> This type of matrix is Doubly Stochastic Matrix.</p><p>Birkhoff–von Neumann Theorem: 1. Birkhoff Polytope is the convex hullof permutation matrices 2. Permutation matrices = Vertices of BirkhoffPolytope (S_n):</p><p><img src="/2023/09/11/deep-learning-with-structures/Birkhoff.png"></p><p>Assume <span class="math inline">\(\mathbf{X} \in \mathbb{R}^{n\times 3}\)</span>, permutation matrix <span class="math inline">\(\mathbf{P} \in \mathbb{R}^{n \times n}\)</span>.<span class="math inline">\(\bm Y \in \mathbb{R}^{1 \times K}\)</span>is the output probability of classes.</p><p>Permutation Invariance : <span class="math inline">\(\bm Y =f(\mathbf{PX})\)</span>, <span class="math inline">\(\forall \mathbf{P}\in S_n\)</span>.</p><p>Assume <span class="math inline">\(\mathbf{H} \in \mathbb{R}^{n\times d}\)</span> is the representations, and <span class="math inline">\(\mathbf{H} = f(\mathbf{X})\)</span>, then,</p><p>Permutation Equivariance: <span class="math inline">\(\mathbf{PH} =\mathbf{P}f(\mathbf{X}) = f(\mathbf{PX})\)</span>, where <span class="math inline">\(\mathbf{P} \in S_n\)</span> is a permutationmatrix.</p><ul><li>equivariant first, then move to invariant.</li></ul><p>Valid set funtions: the function is invariant to the order of theinput.</p><p>Theorem: A function <span class="math inline">\(f\)</span> operatingon a set <span class="math inline">\(X\)</span> having elements from acountable <strong>universe</strong>. If <span class="math inline">\(f\)</span> is a valid set function, then thereexists a function <span class="math inline">\(g\)</span> such that <span class="math inline">\(f(X) = \rho(\sum_{x \in X} \phi(x))\)</span>.</p><p>Proof: For the mapping: <span class="math inline">\(f(X) \toR\)</span>, the domain of <span class="math inline">\(f\)</span> is allsubsets in <span class="math inline">\(X\)</span>. For example: if <span class="math inline">\(X = \{a, b, c\}\)</span>, then the domain is a<strong>power set</strong> <span class="math inline">\(\{\phi, a, b, c,\{a, b\}, \{a, c\}, \{b, c\}, \{a, b, c\}\}\)</span>.</p><p>Sufficiency: summation is permutation invariant!</p><p>Necessity: find an unique representation of any set and then mapit:</p><p>why choose 4: base 3,4 ... are ok, but base 2 is not. Since base 2cannot guarantee the uniqueness of the representation.</p><h2 id="deep-learning-for-sequences">Deep learning for Sequences</h2><p>Applications:</p><ul><li><p>Language Model: <span class="math inline">\(P(x^{t+1}| x^{t},\cdots, x^1)\)</span>, where <span class="math inline">\(x^{t+1}\)</span> is the word we want topredict.</p></li><li><p>Machine Translation.</p></li></ul><p>Key challenges:</p><ul><li>Variable length input and output;</li><li>Order change may be crucial for cognition;</li><li>complex statistical dependencies (e.g. long-range ones).</li></ul><h3 id="transformer">Transformer:</h3><p><img src="/2023/09/11/deep-learning-with-structures/transformer.png"> Theoutput representation of the final encoder is the input of eachdecoder.</p><p><img src="/2023/09/11/deep-learning-with-structures/encoder_decoder.png">The decoder includes self-attention and encoder-decoder attention. Forthe self-attention, it uses <strong>masked multi-headattention</strong>, why????</p><p><strong>How to encode the input sequence?</strong></p><p><strong>Input embedding:</strong> Construct the one-hot vector foreach word. N words, each word will be mapping to a D dimension vector.Then, we can get a NxD matrix. D is the hyper-parameter.</p><ul><li><p>Will the input embedding manners affect the performance of themodel?</p><p>In general, we use the same vocabulary dictionary for the inputembedding.</p></li></ul><p><strong>positional encoding:</strong> <img src="/2023/09/11/deep-learning-with-structures/positional.png"> <span class="math display">\[\begin{aligned}PE(pos, 2i) &amp;= sin(pos/10000^{2i/d_{model}}) \\PE(pos, 2i+1) &amp;= cos(pos/10000^{2i/d_{model}}) \\\end{aligned}\]</span></p><p>pos is the index of the word in the sentence. (0-30) <span class="math inline">\(2i\)</span> and <span class="math inline">\(2i+1\)</span> is the index of the column, d_modelis the number of columns, it is a hyper-parameter(120). For eachword(token), we encode it to a vector with dimension d_model accordingto its position.</p><p>Here we use denominator <span class="math inline">\(10000^{2i/d_model}\)</span> to make sure thepositional encoding is different for different tokens. The sin and cosare periodic functions, if we don't use the denominator, then thepositional encoding could be same for different tokens.</p><ul><li>If there are two different sentences with the same size, will thepositional encodings be the same?? yes.</li></ul><p><strong>Self attention:</strong> <span class="math inline">\(X \inR^{tokes \times dim}\)</span>, <span class="math inline">\(W_Q \inR^{dim \times dim}\)</span>, <span class="math inline">\(Q = XW_Q \inR^{tokens \times dim}\)</span>.</p><p><span class="math inline">\(softmax(\frac{QK^T}{\sqrt{dim}}) \inR^{tokens \times tokens}\)</span> is the <strong>attentionmatrix</strong>, the dimension must be the same as the number oftokens.</p><p>we apply softmax in individual row, then the output of softmax is<span class="math inline">\(tokens \times tokens\)</span>.</p><p>The final output dimension is <span class="math inline">\(tokens\times dim\)</span>.</p><ul><li><p>Why does it need to divide by <span class="math inline">\(\sqrt{dim}\)</span>?</p><p>To keep the variance of each entry <span class="math inline">\(QK^T[i,k]\)</span> to be 1. <strong>[approach to1]</strong>. if we don't preserve the variance, then the gradient willbe larger and larger, and the model will be unstable.</p></li></ul><p><strong>Multi-head attention</strong> it can capture differentdependency of the input sequence. One choice is to input the same inputembedddings for each attention head, and then aggregate the output ofeach attention head. Another choice is to split the input embeddingsinto different parts, and then input different parts to differentattention heads. (The problem is not a convex problem, thus the weightsof each attention head may be different.)</p><p><strong>Layer normalization</strong> <img src="/2023/09/11/deep-learning-with-structures/layernorm.png"> it isapplied to each row of the output of multi-head attention. It is similarto batch normalization, but it is applied to each row, not each column.we want to learn <span class="math inline">\(\gamma\)</span> and <span class="math inline">\(\beta\)</span>, because we want to learn thedistribution of each row.</p><p><strong>Masked multi-head attention in decoder</strong> The decodermust be autoregressive. We need to input the previous words to predictthe next word and prevent attending from future. For a attention matrix,we can mask the upper triangle, then the values masked are zeros. But,if we mask the upper triangle, the sum of each row is not equal to 1,thus we need to adjust the attention matrix. Normally, the attentionmatrix is: <span class="math display">\[\begin{aligned}A = softmax(\frac{QK^T}{\sqrt{dim}})\end{aligned}\]</span> The dimension of the attention matrix is <span class="math inline">\(outputtokens \times outputtokens\)</span>. If wemask <span class="math inline">\(A_{ij}\)</span>, then the input ofsoftmax function will be: <span class="math display">\[\begin{aligned}A_{ij} = \frac{\exp \frac{ \sum_k Q_{ik}K_{jk} -\infty}{\sqrt{dim}}}{\sum \exp()}\end{aligned}\]</span></p><p>Shifted right: we already generate one token, and we want to predictthe next token, then we always focus on the right part of the outputsequence.</p><p><strong>Cross attention</strong> it is used in the decoder. The inputof the decoder is the output of the encoder. The encoder-decoderattention is similar to the self attention, but the query is the outputof the decoder, and the key and value are the output of the encoder.Here the output of encoder is the embaddings, and the docoder cangenerate the key and value from the embeddings. Cross attention cancapture the relationship between the input sentence and the outputsentence.</p><h2 id="graph-neural-networks-message-passing-models">Graph NeuralNetworks Message Passing Models</h2><p>Graph: multi-edges, nodes have types, edges have types.</p><ul><li>connectivity: adjacency list <span class="math inline">\(G = (V,E)\)</span> and adjacency matrix <span class="math inline">\(A\)</span>.<span class="math inline">\(|V| = n, |E| = m\)</span>. <span class="math inline">\(A \in \mathbb{R}^{n \times n}\)</span>.</li><li>features: node features <span class="math inline">\(X\)</span>, edgefeatures, graph features.</li></ul><p>if you want to permute a graph, then you need to<strong>left</strong> multiply the permutation matrix to the adjacencymatrix (change rows) and also right multiply the transpose permutationmatrix to the adjacency matrix(change columns): <span class="math display">\[\begin{aligned}A' = PAP^T\end{aligned}\]</span></p><p>For a graph <span class="math inline">\(A_1\)</span>, if there existsa permutation matrix <span class="math inline">\(P\)</span>, such that<span class="math inline">\(A_2 = PA_1P^T\)</span>, then <span class="math inline">\(A_1\)</span> and <span class="math inline">\(A_2\)</span> are graph isomorphic.</p><p>For a graph <span class="math inline">\(A\)</span>, if there exists apermutation matrix <span class="math inline">\(P\)</span>, such that<span class="math inline">\(A = PAP^T\)</span>, then <span class="math inline">\(A\)</span> is graph automorphism.</p><p>Given graph data <span class="math inline">\((A, X)\)</span> and<span class="math inline">\(f(A, X) \in \mathbb{R}^{n \timesd}\)</span>:</p><ul><li><p>invariance: <span class="math inline">\(f(PAP^T, PX)=f(A,X)\)</span>, <span class="math inline">\(\forall P \inS_n\)</span>.</p></li><li><p>equivariance: <span class="math inline">\(f(PAP^T, PX) = Pf(A,X)\)</span>, <span class="math inline">\(\forall P \inS_n\)</span>.</p></li></ul><p>Key challenges:</p><ul><li>unordered neighbors;</li><li>variable size of neighbors;</li><li>varying graph partitions.</li></ul><h3 id="message-passing-in-gnns">Message Passing in GNNs</h3><p>Feedforward networks: layers do not share message passing module.[don't share weights]. Usually, we call it 'layers'.</p><p>Recurrent networks: layers share message passing module.[reuseweights]. Usually, we call it 'steps' instead of 'layers'.</p><p>Even if we increase the number of nodes and edges, the model canstill work.</p><p><strong>ONLY USE ONE NETWORK FOR ALL NEIGHBORS</strong>, same acrossedges. if we use different networks for each neighbor, changing thenumber of nodes will affect the model. The model is not invariant to thenumber of nodes.</p><p>For undirected graph, the message passing is symmetric, i.e., m_ij =m_ji. It doesn't related to the order of nodes. For directed graph, m_ijmay not equal to m_ji.</p><p>We can also use transformers or gcn or LSTM to implement the messagepassing. Be careful that LSTM is not permutation invariant.</p><p>Tips: parallel message passing: compute messages for all nodes/edgesand compute updates for all nodes in parallel. Use dense operators onGPUs.</p><p>privacy and robust to attack in gnn.</p><p>we can also use transformers in encoding graph structure as anattention mask.</p><h2 id="graph-convolutional-networks">Graph Convolutional Networks</h2><p>laplace operator is the eigenfuction, why?</p><p>For <strong>undirected</strong> graph, Graph Laplacian: <span class="math inline">\(L = D - A\)</span>, where <span class="math inline">\(D\)</span> is the degree matrix, <span class="math inline">\(A\)</span> is the adjacency matrix. Laplacianmatrix can compute the difference between the node and its neighbors. Itis symmetric, diagonally dominant, positive semi-definite(eigenvaluesare nonnegative), and the number of zero eigenvalues is the number ofconnected components.</p><ul><li>Translation group;</li><li>Roto-translation group: SO(n): <span class="math inline">\(Q \in\mathbb{R}^{n \times n}, Q^TQ = Q Q^T = I, det(Q) = 1.\)</span></li></ul><p><span class="math inline">\(g = (X, R_\theta), g' = (X',R_{\theta'})\)</span></p><p><span class="math inline">\(g \cdot g' = g \cdot g' (x_0) =g(R_{\theta'}x_0 + X') = R_{\theta}R_{\theta'}x_0 +R_{\theta}X' + X = (R_{\theta'}X' + X, R_{\theta +\theta'})\)</span></p><ul><li>Scale-translation group.</li><li>Affine group.</li></ul><p>cross-correlations: <span class="math inline">\(f \star g (x) = \intf(x'-x)g(x')dx'\)</span>, in mathmetics, the order ofconvolution is inverse to the order in DL.</p><h2 id="autoregressive-models">Autoregressive Models</h2><p>Autoregressive model： <span class="math display">\[\begin{aligned}P(x_1, \cdots, x_n) = \prod_{i=1}^n P(x_i|x_1, \cdots, x_{i-1}) =\prod_{i=1}^n P(x_i|x_{&lt;i})\end{aligned}\]</span></p><p>For images, each <span class="math inline">\(x_i\)</span> is a pixelvalue, e.g., {0,...,255}. n = height * width. Each term <span class="math inline">\(P(x_i|x_{&lt;i})\)</span> can be modeled by asingle CNN/RNN/....</p><p>Why do we consider the same model for each term? Otherwise, thenumber of model is O(n).</p><p>PixelCNNs: conditioned on the pixels above and to the left of thepixel being predicted. At each step, we will mask the pixels below andright of the pixel being predicted. Then we use convolution on theimage, but it will yield high computation cost.</p><p>PixelRNNs: vectorize the image as a sequence of pixels, and then useRNN to model the sequence.</p><p>Masked Filter: we mask the filter to make sure the convolution isautoregressive. But it will yield blind spots.</p><p>blind spots: the model cannot see the pixels below and right of thepixel being predicted.</p><p>resovle blind spots: use a stack of masked convolutions.</p><p>The cons of softmax: if the number of dimension is very large, thenthe softmax will be very small. Thus we use the discretized mixturelogistic distribution: <span class="math display">\[\begin{aligned}P(x) = \sum_{k=1}^K \pi_k \sigma(\frac{x - \mu_k}{s_k})\end{aligned}\]</span> where <span class="math inline">\(\sigma\)</span> is thesigmoid function, <span class="math inline">\(\pi_k\)</span> is theweight of the <span class="math inline">\(k\)</span>-th component, <span class="math inline">\(\mu_k\)</span> is the mean of the <span class="math inline">\(k\)</span>-th component, <span class="math inline">\(s_k\)</span> is the scale of the <span class="math inline">\(k\)</span>-th component.</p><p>Due to the sequential nature of autoregressive sampling, it isslow.</p><ul><li><p>will autoregressive model focus more on the nearby locations?yes, because we use masked convolutions.</p></li><li><p>For directed graphs, we can generate the lower triangle of theadjacency matrix, and then generate the upper triangle.</p></li></ul><h2 id="generative-adversarial-networks-gans">Generative AdversarialNetworks (GANs)</h2><p>Generative models: generate data from noise.</p><p>Min-max loss: <span class="math display">\[\begin{aligned}\min_{\theta} \max_{\phi}\mathbb{E}_{x \sim p_{data}(x)}[\logD_{\phi}(x)] + \mathbb{E}_{z \sim p_z(z)}[\log(1 -D_{\phi}(G_{\theta}(z)))]\end{aligned}\]</span> where <span class="math inline">\(D\)</span> is thediscriminator, <span class="math inline">\(G\)</span> is the generator,<span class="math inline">\(x\)</span> is the real data, <span class="math inline">\(z\)</span> is the noise, <span class="math inline">\(p_{data}\)</span> is the distribution of the realdata, <span class="math inline">\(p_z\)</span> is the distribution ofthe noise.</p><p>The fake images are generated from noise (normal distribution), wecan not get the distribution of fake images. Why? Because the mappingfrom noise to fake images is not invertible. Thus, we cannot get thelikelihood of the fake images.</p><p>GAN is also called likihood-free model. We cannot get the likelihoodof the fake images.</p><p>For GANs, we can get images through one forward pass, but forautoregressive model, we need to generate images pixel in a sequentialmanner.</p><p>The output of the discriminator is a scalar, the probability of theinput image being real. The output of the generator is an image.</p><p>Fix generator, the optimal discriminator is: <span class="math display">\[\begin{aligned}D_{\phi}^*(x) = \frac{p_{data}(x)}{p_{data}(x) + p_{G_{\theta}}(x)}\end{aligned}\]</span> where <span class="math inline">\(p_{G_{\theta}}(x)\)</span>is the distribution of the fake images. When <span class="math inline">\(p_{G_{\theta}}(x) = p_{data}(x)\)</span>, then<span class="math inline">\(D_{\phi}^*(x) = 0.5\)</span>. However, wedon't know the distribution of the fake images and the distribution ofthe real images.</p><p>inner-loop is the discriminator, outer-loop is the generator. why?focus more on the generator, because the discriminator is easy to train??</p><h2 id="reference">Reference</h2><p><a href="https://lrjconan.github.io/UBC-EECE571F-DL-Structures/">EECE571F (2023 Winter Term 1): Deep Learning with Structures</a></p><p>http://cs231n.stanford.edu/slides/2018/cs231n_2018_ds02.pdf</p><p><a href="https://jalammar.github.io/illustrated-transformer/" class="uri">https://jalammar.github.io/illustrated-transformer/</a></p><p><a href="https://uvagedl.github.io/">UvA - An Introduction to GroupEquivariant Deep Learning</a></p>]]></content>
      
      
      <categories>
          
          <category> Graph </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Deep Learning with Structures </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Basic knowledge</title>
      <link href="/2023/09/06/Basic%20knowledge/"/>
      <url>/2023/09/06/Basic%20knowledge/</url>
      
        <content type="html"><![CDATA[<h2 id="graph-convolutional-autoencoders-with-co-learning-of-graph-structure-and-node-attributes">Graphconvolutional autoencoders with co-learning of graph structure and nodeattributes</h2><p>In this paper, they design the special graph encoder and decoder forthe tasks undertaken by the graph autoencoders. The task of the encoderis to embed the nodes into a new space, and then the latentrepresentation of each node is close to its neighbors[the encoder is alow-pass filter]. The decoder restores the original space from theembedded space by making the latent representation of each node awayfrom its neighbors[the decoder is a high-pass filter].</p><p>In this paper, they encode both the graph structure and the nodeattributes in the latent space with an improved GCN, which is a<strong>completely low-pass graph filter</strong>. Then, to reconstructthe node attributes X , they design a new <strong>high-pass graphdecoder</strong>. At the same time, we use the inner product layer toreconstruct the graph structure information. Last, the graph encoder andtwo sub-decoders are jointly optimized in a unified framework in such away that each can be beneficial to the other and finally lead to abetter graph embedding.</p><h3 id="normalized-adjacency-matrix-and-laplacian-matrices">Normalizedadjacency matrix and Laplacian matrices</h3><p>1, The normalized adjacency matrix is defined as: <span class="math display">\[\hat{A} = D^{-1/2}A D^{-1/2},\]</span> where<span class="math inline">\(A\)</span> is the adjacency matrix of graph<span class="math inline">\(G\)</span>. <span class="math inline">\(D =diag(d)\)</span>, <span class="math inline">\(d(i)\)</span> is thedegree of node <span class="math inline">\(i\)</span>.</p><p>2, The normalized Laplacian matrix is defined as: <span class="math display">\[L_s = I - \hat{A} = I - D^{-1/2}AD^{-1/2}.\]</span> Note that <span class="math inline">\(L_s = I -\hat{A} = D^{-1/2}(D - A) D^{-1/2} = D^{-1/2}L D^{-1/2}\)</span>, where<span class="math inline">\(L = D - A\)</span> is the unnormalizedLaplacian matrix of graph <span class="math inline">\(G\)</span>.</p><p>For the largest eigenvalue <span class="math inline">\(\lambda^s\)</span> of <span class="math inline">\(A\)</span> and the maximum degree <span class="math inline">\(\Delta\)</span> of a node in a graph, we have $d_{avg} ^s $. Normalizing the adjacency matrix can make its largesteigenvalue 1.</p><p>3, Let <span class="math inline">\(\alpha_1 \geq \alpha_2 \geq ...\geq \alpha_n\)</span> be the eigenvalues of <span class="math inline">\(\hat{A}\)</span>, <span class="math inline">\(\lambda^s_1 \leq \lambda^s_2 \leq ... \leq\lambda^s_n\)</span> be the eigenvalues of <span class="math inline">\(L_s\)</span>, then <span class="math display">\[ 1= \alpha_1 \geq ... \geq \alpha_n \geq -1, \quad 0=\lambda^s_1 \leq ...\leq \lambda^s_n \leq 2.\]</span></p><h3 id="graph-convolutional-networks">Graph convolutional networks</h3><p>GCN generalizes the convolutional neural networks on non-Euclideandomains. It uses the first-order approximation of Chebyshev polynomials:<span class="math display">\[g_{\theta} \star x \approx \theta (I_N + D^{-1/2}AD^{-1/2})X.\]</span> The spectral radius of <span class="math inline">\((I_N +D^{-1/2}AD^{-1/2})\)</span> is 2, and repeated application of thisoperator will cause numerical instabilities. To solve this problem, GCNuses a renormalization trick by adding a self-loop to each node, whichis equivalent to adding the identity matrix <span class="math inline">\(I_N\)</span> to the adjacency matrix <span class="math inline">\(A\)</span>: <span class="math inline">\(\tilde{A}= A + I\)</span>, the associated degree matrix <span class="math inline">\(\tilde{D} = D + I\)</span>. The new symmetricallynormalized matrix is <span class="math inline">\(\tilde{A}_{GCN} =\tilde{D}^{-1/2} \tilde{A} \tilde{D}^{-1/2}\)</span>. The one-layer GCNis <span class="math display">\[Z^{(m+1)} = \sigma(\tilde{A}_{GCN}Z^{(m)}W^{(m)}),\]</span> where <span class="math inline">\(Z^{(m)}\)</span> is thelatent representation matrix learned by the <span class="math inline">\(m\)</span>-th layer, <span class="math inline">\(Z^{(0)} = X\)</span>.</p><h3 id="graph-signal-processing">Graph signal processing</h3><p>In graph signal processing , the eigenvalues and eigenvectors of thegraph Laplacian correspond to the frequencies and Fourier basis.</p><p>The graph laplacian is defined as <span class="math inline">\(L =D-A\)</span>. By eigen-decomposition, <span class="math inline">\(L = U\Lambda U^{-1}\)</span>, where <span class="math inline">\(\Lambda =diag(\lambda_1, ..., \lambda_n)\)</span>, <span class="math inline">\(U= (u_1, u_2, ..., u_n)\)</span>. The eigenvalues <span class="math inline">\(\lambda_i, i \in [n]\)</span> can be considered tobe frequencies, and the associated eigenvectors <span class="math inline">\(u_i, i \in [n]\)</span> can be considered to be aFourier basis.</p><p>A graph signal <span class="math inline">\(f\)</span> can bedecomposed into a linear combination of basis signals <span class="math inline">\(u_i\)</span>: <span class="math display">\[f = Uc = \sum_{i=1}^n c_i u_i,\]</span> where <span class="math inline">\(c = (c_1, ...,c_n)^T\)</span>, <span class="math inline">\(c_i\)</span> is thecoefficient of <span class="math inline">\(u_i\)</span>, the magnitudeof <span class="math inline">\(c_i\)</span> represents the importance of<span class="math inline">\(u_i\)</span> in <span class="math inline">\(f\)</span>.</p><p>The smoothness of the basis signal <span class="math inline">\(u_i\)</span> is measured by the correspondingeigenvalue <span class="math inline">\(\lambda_i\)</span>. The smallerthe eigenvalue <span class="math inline">\(\lambda_i\)</span>, thesmoother the basis signal <span class="math inline">\(u_i\)</span>.<span class="math display">\[\sum_{e_{j,k} \in E} a_{j,k}[u_i(j) - u_i(k)]^2 = u_i^T L u_i =\lambda_i u_i^T u_i = \lambda_i.\]</span></p><p>The basic idea of graph filtering is to design a proper graph filterto produce the required signals for the downstream tasks. A graph filteris a function that takes a graph signal as input and <strong>outputs anew signal</strong>. A linear graph filter can be represented as amatrix <span class="math inline">\(G \in \mathbb{R}^{N \timesN}\)</span>, which is defined as <span class="math display">\[G = U p(\Lambda) U^{-1},\]</span> where <span class="math inline">\(p(\Lambda) =diag(p(\lambda_1), ..., p(\lambda_n))\)</span>. <span class="math inline">\(p(\cdot)\)</span> is the frequency responsefunction.</p><p>The output signal can be written as <span class="math display">\[y = Gf = U p(\Lambda) U^{-1} Uc = U p(\Lambda) c = \sum_{i=1}^np(\lambda_i) c_i u_i.\]</span></p><p>Definition 1 (completely low-pass graph filter). A completelylow-pass graph filter is a graph filter whose frequency responsefunction <span class="math inline">\(p(\cdot): \mathbb{R} \to\mathbb{R}^{+}\)</span> is a decreasing function with <span class="math inline">\(\lambda\)</span>.</p><ul><li>According to definition 1, the completely low-pass graph filterobtains a smooth graph output signal <span class="math inline">\(y\)</span> that consists of mostly low-frequencybasis signals, and as a result, the latent representation of each nodeis close to its neighbors.</li></ul><p>Definition 2 (completely high-pass graph filter). A completelyhigh-pass graph filter is a graph filter whose frequency responsefunction <span class="math inline">\(p(\cdot): \mathbb{R} \to\mathbb{R}^{+}\)</span> is an increasing function with <span class="math inline">\(\lambda\)</span>.</p><p>According to definition 2, the completely high-pass graph filterobtains an unsmooth graph output signal <span class="math inline">\(y\)</span> that consists of mostly high-frequencybasis signals, which makes the latent representation of each node faraway from its neighbors.</p><p>For GCN, the graph filter of GCN is <span class="math display">\[\tilde{A}_{GCN} = \tilde{D}^{-1/2} \tilde{A} \tilde{D}^{-1/2} = I - L_s= U (I - \Lambda^s) U^{-1}.\]</span> The frequency response function of GCN is <span class="math inline">\(p(\lambda^s_i) = 1 - \lambda_i^s\)</span>. Sincethe range of <span class="math inline">\(\lambda_i^s\)</span> is <span class="math inline">\([0, 2]\)</span>, the frequency response functionof GCN is a decreasing function with <span class="math inline">\(\lambda_i^s\)</span>. GCN is completely low-passgraph filter when <span class="math inline">\(\lambda_i^s \in [0,1]\)</span>, but not in <span class="math inline">\([1, 2]\)</span>.When <span class="math inline">\(\lambda_i^s \in [1, 2]\)</span>, <span class="math inline">\(p(\lambda^s_i)\)</span> will take a negative valuethat will introduce noise and undermine the performance. Thus, GCN isnot a completely low-pass graph filter.</p><h3 id="reference">Reference</h3><p>Jie Wang, Jiye Liang, Kaixuan Yao, Jianqing Liang, Dianhui Wang,Graphconvolutional autoencoders with co-learning of graph structure and nodeattributes,Pattern Recognition,Volume 121,2022,108215,ISSN 0031-3203, <a href="https://doi.org/10.1016/j.patcog.2021.108215" class="uri">https://doi.org/10.1016/j.patcog.2021.108215</a>.</p><p><a href="https://people.orie.cornell.edu/dpw/orie6334/Fall2016/lecture7.pdf" class="uri">https://people.orie.cornell.edu/dpw/orie6334/Fall2016/lecture7.pdf</a></p><h2 id="the-difference-between-adam-and-adamw">The difference betweenAdam and AdamW</h2><p><a href="https://towardsdatascience.com/why-adamw-matters-736223f31b5d" class="uri">https://towardsdatascience.com/why-adamw-matters-736223f31b5d</a></p><h2 id="why-regularization-can-reduce-overfitting">Why regularizationcan reduce overfitting?</h2><p><a href="http://neuralnetworksanddeeplearning.com/chap3.html#regularization" class="uri">http://neuralnetworksanddeeplearning.com/chap3.html#regularization</a></p><h2 id="cosine-decay-schedule-with-warm-up-period">Cosine decay schedulewith warm up period</h2><p>Loshchilov and Hutter, SGDR: Stochastic Gradient Descent with WarmRestarts. ICLR 2017. <a href="https://arxiv.org/abs/1608.03983" class="uri">https://arxiv.org/abs/1608.03983</a></p><p><a href="https://scorrea92.medium.com/cosine-learning-rate-decay-e8b50aa455b" class="uri">https://scorrea92.medium.com/cosine-learning-rate-decay-e8b50aa455b</a></p>]]></content>
      
      
      <categories>
          
          <category> Graph </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Graph </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Kolmogorov-Smirnov statistic</title>
      <link href="/2023/08/27/KS_test/"/>
      <url>/2023/08/27/KS_test/</url>
      
        <content type="html"><![CDATA[<h1 id="kolmogorov-smirnov-statistic">Kolmogorov-Smirnov statistic</h1><p>Consider any distribution <span class="math inline">\(D\)</span> on<span class="math inline">\(\mathbb{R}\)</span>, and CDF <span class="math inline">\(F(t) = \mathbb{P}_{x \sim D}(x \leqt)\)</span>.</p><p>Let <span class="math inline">\(X = (x_j)_{i \in [n]}\)</span> be<span class="math inline">\(n\)</span> samples drawn from <span class="math inline">\(D\)</span>.</p><p>Def: The empirical CDF (eCDF) of <span class="math inline">\(X\)</span> is defined as <span class="math inline">\(\hat{F}_n(t) = \frac{1}{n} \sum_{j=1}^n\mathbb{1}_{x_j \leq t}\)</span>.</p><p>Def: The Kolmogorov-Smirnov statistic is defined as <span class="math inline">\(D_n = \sup_{t \in \mathbb{R}} \{|\hat{F}_n(t) -F(t)|\}\)</span>.</p><p>Theorem [DKW, 1956]: <span class="math inline">\(\mathbb{P}(D_n &gt;\epsilon) \leq c e^{-2n\epsilon^2}\)</span>, where <span class="math inline">\(c\)</span> is a constant.</p><p>Theorem [Massart, 1990]: <span class="math inline">\(\mathbb{P}(D_n&gt; \epsilon) \leq 2 e^{-2n\epsilon^2}\)</span>.</p><p>Theorem [Harvey, 2020]: <span class="math inline">\(\mathbb{P}(D_n&gt; \epsilon) \leq \frac{4}{\epsilon}e^{-\frac{1}{2}n\epsilon^2}\)</span>.</p>]]></content>
      
      
      <categories>
          
          <category> Statistic </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Statistic </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Itô Calculus and Stochastic Differential Equations</title>
      <link href="/2023/08/24/Heuristic%20Solutions%20of%20SDEs/"/>
      <url>/2023/08/24/Heuristic%20Solutions%20of%20SDEs/</url>
      
        <content type="html"><![CDATA[<h1 id="the-stochastic-integral-of-itô">The Stochastic Integral ofItô</h1><p>A stochastic differential equation can be transformed into a vectordifferential equation of the form <span class="math display">\[\begin{aligned}\frac{\text{d}x}{\text{d} t}  &amp;= f(x, t) + \mathbb{L}(X, t) w(t), \\\end{aligned}\]</span> where <span class="math inline">\(w(t)\)</span> is a whiteGaussian noise with zero mean. Since <span class="math inline">\(w(t)\)</span> is discontinuous, we can not use theordinary differential equation to solve the above equation. Fortunately,we can reduce the problem to definition of a new king of integral, thestochastic integral of Itô.</p><p>We can integrate the SDE from initial time <span class="math inline">\(t_0\)</span> to final time <span class="math inline">\(t\)</span>: <span class="math display">\[\begin{aligned}x(t) - x(t_0) &amp;= \int_{t_0}^{t} f(x, t) \text{d} t + \int_{t_0}^{t}\mathbb{L}(x, t) w(t) \text{d} t. \\\end{aligned}\]</span> The first integral with respect to time on the right-hand sidecan be solved by Riemann integral or Lebesgue integral. The secondintegral is the problem we need to solve. We will first discuss thereason why we can not use the Riemann integral, Lebesgue integral andStieltjes integral to solve the second integral.</p><p>First, it cannot be solved by Riemann integral. The Riemann integralis defined as <span class="math display">\[\int_{t_0}^{t} \mathbb{L}(X, t) w(t) \text{d} t = \lim_{n \to \infty}\sum_{k=1}^{n} \mathbb{L}(x(t_k^{*}), t_k^{*}) w(t_k^{*}) (t_{k+1} -t_k),\]</span> where <span class="math inline">\(t_0 &lt; t_1 &lt; ...&lt;t_n= t\)</span>, and <span class="math inline">\(t_k^{*} \in [t_k,t_{k+1}]\)</span>. In Riemann integral, the upper and lower bounds ofthe integral are defined as the selections of <span class="math inline">\(t_k^*\)</span> such that the integral is maximizedand minimized. If the upper bound and lower bound converge to the samevalue, the Riemann integral exists. However, the white Gaussian noise isdiscontinuous and not bounded, it can take arbitrarily small and largevalues at every finite interval, so the upper and lower bounds of theintegral are not convergent. Therefore, the Riemann integral does notexist.</p><p>For Stieltjes integral, we need to interpret the increment <span class="math inline">\(w(t) \text{d}t\)</span> as an increment of anotherprocess <span class="math inline">\(\beta(t)\)</span>, thus the intergalbecomes <span class="math display">\[\int_{t_0}^{t} \mathbb{L}(x(t), t) w(t) \text{d} t = \int_{t_0}^{t}\mathbb{L}(x(t), t) \text{d} \beta(t).\]</span> Here <span class="math inline">\(\beta(t)\)</span> is aBrownian motion. Brownian motion is a continuous process. However, theBrownian motion is not differentiable, so the Stieltjes integral doesnot converge.</p><p>Both Stieltjes and Lebesgue integrals are defined as limits of theform <span class="math display">\[\int_{t_0}^{t} \mathbb{L}(x(t), t) \text{d} \beta = \lim_{n \to \infty}\sum_{k=1}^{n} \mathbb{L}(x(t_k^{*}), t_k^{*}) (\beta(t_{k+1}) -\beta(t_k)),\]</span> where <span class="math inline">\(t_0 &lt; t_1 &lt; ...&lt;t_n= t\)</span>, and <span class="math inline">\(t_k^{*} \in [t_k,t_{k+1}]\)</span>. Both of these definitions would require the limit tobe independent of the position on the interval <span class="math inline">\(t_k^{*} \in [t_k, t_{k+1}]\)</span>. However, inthis case, the limit is not independent of the position on the interval<span class="math inline">\(t_k^{*} \in [t_k, t_{k+1}]\)</span>, so theStieltjes and Lebesgue integrals do not exist.</p><h2 id="definition-of-the-stochastic-integral-of-itô">Definition of theStochastic Integral of Itô</h2><p>For Itô integral, it fixed the choice of <span class="math inline">\(t_k^{*}\)</span> to be <span class="math inline">\(t_k\)</span>, thus the limit becomes unique. TheItô integral is defined as <span class="math display">\[\int_{t_0}^{t} \mathbb{L}(x(t), t) \text{d} \beta = \lim_{n \to \infty}\sum_{k=1}^{n} \mathbb{L}(x(t_k), t_k) (\beta(t_{k+1}) - \beta(t_k)).\]</span></p><p>The SDE can be defined to be the Itô integral of the form <span class="math display">\[\begin{aligned}x(t) - x(t_0) &amp;= \int_{t_0}^{t} f(x(t), t) \text{d} t +\int_{t_0}^{t} \mathbb{L}(x, t) \text{d} \beta(t).\end{aligned}\]</span></p><p>The differential form is <span class="math display">\[\begin{aligned}\text{d} x &amp;= f(x, t) \text{d} t + \mathbb{L}(x, t) \text{d}\beta(t).\end{aligned}\]</span> or <span class="math display">\[\begin{aligned}\frac{\text{d} x}{\text{d} t } &amp;= f(x, t) + \mathbb{L}(x, t) \frac{\text{d} \beta(t)}{\text{d} t}.\end{aligned}\]</span></p><ul><li>Why don't we consider more general SDEs of the form <span class="math display">\[\begin{aligned}\frac{\text{d} x}{\text{d} t } &amp;= f(x(t), w(t), t),\end{aligned}\]</span> where the white noise <span class="math inline">\(w(t)\)</span> enters the system through anonlinear transformation. We can not rewrite this equation as astochastic integral with respect to a Brownian motion, and thus wecannot define the mathematical meaning of this equation.</li></ul><h2 id="itô-formula">Itô Formula</h2><p>Consider the stochastic integral <span class="math display">\[\int_{t_0}^{t} \beta(t) \text{d} \beta(t),\]</span> where <span class="math inline">\(\beta(t)\)</span> is astandard Brownian motion with zero mean and diffusion constant <span class="math inline">\(q = 1\)</span>. Based on the definition of the Itôintegral, we have <span class="math display">\[\begin{aligned}\int_{t_0}^{t} \beta(t) \text{d} \beta(t) &amp;= \lim_{n \to \infty}\sum_{k=1}^{n} \beta(t_k) (\beta(t_{k+1}) - \beta(t_k)) \\&amp;= \lim_{n \to \infty} \sum_{k=1}^{n} [-\frac{1}{2}(\beta(t_{k+1}) -\beta(t_k))^2 +　\frac{1}{2}(\beta^2(t_{k+1}) - \beta^2(t_k))]\\&amp;= -\frac{1}{2}t + \frac{1}{2}\beta^2(t).\end{aligned}\]</span> where <span class="math inline">\(0 = t_0 &lt; t_1 &lt; ...&lt; t_n = t\)</span> and <span class="math inline">\(\lim_{n \to\infty} \sum_{k=1}^{n} (\beta(t_{k+1}) - \beta(t_k))^2\)</span>. That isbecause <span class="math inline">\(\beta(t_{k+1}) - \beta(t_k) \simN(0, t_{k+1} - t_k) \sim N(0, \frac{t}{n})\)</span>.</p><p>So the Itô differential of <span class="math inline">\(\beta^2(t)/2\)</span> is <span class="math display">\[\begin{aligned}\text{d} \frac{\beta^2(t)}{2} &amp;= \beta(t) \text{d}\beta(t) +\frac{1}{2} \text{d}t.\end{aligned}\]</span> It is not the same as the ordinary differential of <span class="math inline">\(\beta^2(t)/2\)</span>: <span class="math display">\[\begin{aligned}\frac{\text{d} \beta^2(t)}{2} &amp;= \beta(t) \text{d}\beta(t).\end{aligned}\]</span> That is because the Itô integral fixes the choice of <span class="math inline">\(t_k^{*}\)</span> to be <span class="math inline">\(t_k\)</span>.</p><p>Theorem Itô formula: Let <span class="math inline">\(x(t)\)</span> bean Itô process(note: <span class="math inline">\(x(t)\)</span> is avector process) which is the solution of an SDE of the form <span class="math display">\[\begin{aligned}\text{d} x &amp;= f(x, t) \text{d} t + \mathbb{L}(x, t) \text{d}\beta(t),\end{aligned}\]</span> where <span class="math inline">\(\beta(t)\)</span> is aBrownian motion. Consider an arbitrary <strong>scalar</strong> function<span class="math inline">\(\phi(x(t), t)\)</span> of the process, theItô SDE of <span class="math inline">\(\phi\)</span> is <span class="math display">\[\begin{aligned}\text{d} \phi &amp;= \frac{\partial \phi}{\partial t} \text{d} t +\sum_{i}\frac{\partial \phi}{\partial x_i} \text{d} x_i + \frac{1}{2}\sum_{i,j}\frac{\partial^2 \phi}{\partial x_i \partial x_j} \text{d} x_i\text{d} x_j \\&amp;= \frac{\partial \phi}{\partial t} \text{d} t + (\nabla \phi)^T\cdot \text{d} x + \frac{1}{2} tr\{ \nabla \nabla^T \phi\} \text{d} x\text{d} x^T\end{aligned}\]</span> provided that the required partial derivatives exist, wherethe mixed partial derivatives are combined according to the rules <span class="math display">\[\begin{aligned}\text{d} \beta \text{d} t &amp;= 0, \\\text{d} t \text{d} \beta &amp;= 0, \\\text{d} \beta \text{d} \beta^T &amp;= Q \text{d} t.\end{aligned}\]</span> (Q is the diffusion matrix(covariance matrix) of the Brownianmotion). It can be derived from the Taylor expansion of <span class="math inline">\(\phi(x(t), t)\)</span>. Usually, in deterministiccase, we could ignore the second-order, we have <span class="math display">\[\begin{aligned}\text{d} \phi &amp;= \frac{\partial \phi}{\partial t} \text{d} t +\frac{\partial \phi}{\partial x} \text{d} x.\end{aligned}\]</span> In stochastic case, because <span class="math inline">\(\text{d} \beta \text{d} \beta^T = Q \text{d}t\)</span>, which is order one, the <span class="math inline">\(\text{d}x \text{d} x^T\)</span> is potentially of order one, so we need toconsider the second-order term.</p><ul><li>Here the Itô formula is derived for a scalar function <span class="math inline">\(\phi(x(t), t)\)</span>. However, for vectorfunction, it works for each of the components of a vector-valuedfunction separately and thus also includes the vector case.</li></ul><p>Example: We can apply the Itô formula to the function <span class="math inline">\(\phi(x(t), t) = x^2(t)/2\)</span>, with <span class="math inline">\(x(t) = \beta(t)\)</span>, where <span class="math inline">\(\beta(t)\)</span> is a standard Brownian motion(q=1). The Itô SDE of <span class="math inline">\(\phi\)</span> is <span class="math display">\[\begin{aligned}\text{d} \phi &amp;= \beta \text{d} \beta + \frac{1}{2} \text{d} \beta\text{d} \beta \\&amp;= \beta \text{d} \beta +\frac{1}{2} \text{d} t.\end{aligned}\]</span></p><h1 id="reference">Reference</h1><p>Simo Särkkä and Arno Solin (2019). Applied Stochastic DifferentialEquations. Cambridge University Press.</p>]]></content>
      
      
      <categories>
          
          <category> Generative Model </category>
          
      </categories>
      
      
        <tags>
            
            <tag> SDE </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Gaussian processes</title>
      <link href="/2023/08/06/gaussianprocess/"/>
      <url>/2023/08/06/gaussianprocess/</url>
      
        <content type="html"><![CDATA[<h1 id="jointly-gaussian-random-variables">Jointly Gaussian randomvariables</h1><p>Definition: Random variables (RV) <span class="math inline">\(X_1,..., X_n\)</span> are jointly Gaussian if any linear combination of themis Gaussian.</p><p>RV <span class="math inline">\(X = [X_1, ..., X_n]^{T}\)</span> isGaussian <span class="math inline">\(\leftrightarrows\)</span> Given anyscalars <span class="math inline">\(a_1,... a_n\)</span>, the RV <span class="math inline">\(Y = a_1 X_1 + a_2 X_2 + ... + a_n X_n\)</span> isGaussian distributed.</p><h2 id="pdf-of-jointly-gaussian-rvs-in-n-dimensions">Pdf of jointlyGaussian RVs in n dimensions</h2><p>Let <span class="math inline">\(X \in \mathbb{R}^n\)</span>, <span class="math inline">\(\mu = \mathbb{E}[X]\)</span>,</p><p>covariance matrix <span class="math display">\[C:= \mathbb{E}[(X -\mu)(X - \mu)^T] =\begin{pmatrix}    \sigma_{11}^2 &amp; \sigma_{12}^2 &amp; \cdots &amp; \sigma_{1n}^2\\    \sigma_{21}^2 &amp; \sigma_{22}^2 &amp; \cdots &amp; \sigma_{2n}^2\\    \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\    \sigma_{n1}^2 &amp; \sigma_{n2}^2 &amp; \cdots &amp; \sigma_{nn}^2\end{pmatrix}\]</span> Then, the pdf of RV <span class="math inline">\(X\)</span> canbe defined as <span class="math display">\[p(x) = \frac{1}{(2\pi)^{n/2} \text{det}^{1/2}(C)} \exp(-\frac{1}{2}(x -\mu)^T C^{-1} (x - \mu)).\]</span></p><ul><li><span class="math inline">\(C\)</span> is invertible</li><li>We can verify all linear combinations is Gaussian.</li><li>To fully specify the probability distribution of a Gaussian vector<span class="math inline">\(X\)</span>, the mean vector <span class="math inline">\(\mu\)</span> and covariance matrix <span class="math inline">\(C\)</span> suffice.</li></ul><h1 id="gaussian-processes">Gaussian processes</h1><p>Gaussian processes (GP) generalize Gaussian vectors to<strong>infinite</strong> dimensions.</p><p>Definition. <span class="math inline">\(X(t)\)</span> is a GP if anylinear combination of values <span class="math inline">\(X(t)\)</span>is Gaussian. That is, for arbitrary <span class="math inline">\(n &gt;0\)</span>, times <span class="math inline">\(t_1, ..., t_n\)</span> andconstants <span class="math inline">\(a_1, ..., a_n\)</span>, <span class="math inline">\(Y = a_1 X(t_1) + a_2 X(t_2) + ... + a_nX(t_n)\)</span> is Gaussian distributed.</p><ul><li><p>Time index <span class="math inline">\(t\)</span> can becontinuous or discrete.</p></li><li><p>Any linear functional of <span class="math inline">\(X(t)\)</span> is Gaussian distributed. Forexample, the integral <span class="math inline">\(Y = \int_{t_1}^{t_2}X(t) \text{d}t\)</span> is Gaussian distributed.</p></li></ul><h2 id="jointly-pdf-in-a-gaussian-process">Jointly pdf in a Gaussianprocess</h2><p>Consider times <span class="math inline">\(t_1,..., t_n\)</span>, themean value <span class="math inline">\(\mu(t_i)\)</span> is <span class="math display">\[\mu(t_i) = \mathbb{E}[X(t_i)].\]</span></p><p>The covariance between values at time <span class="math inline">\(t_i\)</span> and <span class="math inline">\(t_j\)</span> is <span class="math inline">\(C(t_i,t_j) := \mathbb{E}[(X(t_i) - \mu(t_i))(X(t_j) -\mu(t_j))^T]\)</span>.</p><p>The covariance matrix for values <span class="math inline">\(X(t_1),..., X(t_n)\)</span> is <span class="math display">\[C(t_1,..., t_n) =\begin{pmatrix}    C_{t_1, t_1} &amp; C_{t_1, t_2} &amp; \cdots &amp; C_{t_1, t_n}\\    C_{t_2, t_1} &amp; C_{t_2, t_2} &amp; \cdots &amp; C_{t_2, t_n} \\    \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\    C_{t_n, t_1} &amp; C_{t_n, t_2} &amp; \cdots &amp; C_{t_n, t_n}\end{pmatrix}\]</span>.</p><p>The jointly pdf of <span class="math inline">\(X(t_1),...,X(t_n)\)</span> is <span class="math inline">\(N([\mu(t_1), ...,\mu(t_n)]^T, C(t_1,..., t_n))\)</span>.</p><h2 id="mean-value-and-autocorrelation-functions">Mean value andautocorrelation functions</h2><p>To specify a Gaussian process, we only need to specify:</p><ul><li><p>Mean value function: <span class="math inline">\(\mu(t) =\mathbb{E}[x(t)]\)</span>.</p></li><li><p>Autocorrelation function (symmetric): <span class="math inline">\(R(t_1, t_2) = \mathbb{E}[X(t_1)X(t_2)]\)</span>.</p></li></ul><p>The autocovariance <span class="math inline">\(C(t_1, t_2) = R(t_1,t_2) - \mu(t_1) \mu (t_2)\)</span>.</p><p>More general, we consider GP with <span class="math inline">\(\mu(t)= 0\)</span>. [define new process <span class="math inline">\(Y(t) =X(t) - \mu(t)\)</span>]. In this case, <span class="math inline">\(C(t_1, t_2) = R(t_1, t_2)\)</span>.</p><p>All probs. in a GP can be expressed in terms of <span class="math inline">\(\mu(t)\)</span> and <span class="math inline">\(R(t, t)\)</span>. <span class="math display">\[p(x_t) = \frac{1}{\sqrt{2\pi (R(t,t) - \mu^2(t))}} \exp(- \frac{(x_t -\mu(t))^2}{2(R(t,t) - \mu^2(t))}).\]</span></p><h2 id="conditional-probabilities-in-a-gp">Conditional probabilities ina GP</h2><p>Consider a zero-mean GP <span class="math inline">\(X(t)\)</span>,two times <span class="math inline">\(t_1\)</span> and <span class="math inline">\(t_2\)</span>. The covariance matrix is <span class="math display">\[C = \begin{pmatrix}  R(t_1, t_1) &amp; R(t_1, t_2) \\  R(t_1, t_2) &amp; R(t_2, t_2)\end{pmatrix}\]</span></p><p>The jointly pdf of <span class="math inline">\(X(t_1)\)</span> and<span class="math inline">\(X(t_2)\)</span> is <span class="math display">\[p(x_{t_1}, x_{t_2}) = \frac{1}{2\pi \text{det}^{1/2} C}\exp(-\frac{1}{2}[x_{t_1}, x_{t_2}]^T C^{-1} [x_{t_1}, x_{t_2}])\]</span></p><p>The conditional pdf of <span class="math inline">\(X(t_1)\)</span>given <span class="math inline">\(X(t_2)\)</span> is <span class="math display">\[p_{X(t_1)| X(t_2)}(x_{t_1} | x_{t_2}) = \frac{p(x_{t_1},x_{t_2})}{p(x_{t_2})}. \qquad (1)\]</span></p><h1 id="brownian-motion-process-a.k.a-wiener-process">Brownian motionprocess (a.k.a Wiener process)</h1><p>Definition. A Brownian motion process (a.k.a Wiener process)satisfies</p><ol type="1"><li><p><span class="math inline">\(X(t)\)</span> is normally distributedwith zero mean and variance <span class="math inline">\(\sigma^2t\)</span>, <span class="math display">\[X(t) \sim N(0, \sigma^2t).\]</span></p></li><li><p>Independent increments. For all times <span class="math inline">\(0 &lt; t_1 &lt; t_2 &lt; \cdots &lt; t_n\)</span>,the random variables <span class="math inline">\(X(t_1), X(t_2) -X(t_1), ..., X(t_n) - X(t_{n-1})\)</span> are independent.</p></li><li><p>Stationary increments. Probability distribution of increment<span class="math inline">\(X(t+s) - X(s)\)</span> is the same asprobability distribution of <span class="math inline">\(X(t)\)</span>.<span class="math display">\[[X(t+s) - X(s)]  \sim N(0, \sigma^2t).\]</span></p></li></ol><ul><li>Brownian motion is a Markov process.</li><li>Brownian motion is a Gaussian process.</li></ul><h2 id="mean-and-autocorrelation-of-brownian-motion">Mean andautocorrelation of Brownian motion</h2><p>1, Mean funtion <span class="math inline">\(\mu(t) = \mathbb{E}[X(t)]= 0\)</span>.</p><p>2, Autocorrelation of Brownian motion <span class="math inline">\(R(t_1, t_2) = \sigma^2 \min\{t_1,t_2\}\)</span>.</p><p>Proof. Assume <span class="math inline">\(t_1 &lt; t_2\)</span>, thenautocorrelation <span class="math inline">\(R(t_1, t_2) =\mathbb{E}[X(t_1)X(t_2)] = \sigma^2 t_1\)</span>.</p><p>If <span class="math inline">\(t_1 &lt; t_2\)</span>, according toconditional expectations, we have <span class="math display">\[\begin{aligned}  R(t_1, t_2) = \mathbb{E}[X(t_1)X(t_2)] &amp;=\mathbb{E}_{X(t_1)}[\mathbb{E}_{X(t_2)}[X(t_1)X(t_2) | X(t_1)]] \\  &amp;= \mathbb{E}_{X(t_1)}[X(t_1)\mathbb{E}_{X(t_2)}[X(t_2) | X(t_1)]]\end{aligned}\]</span> According to equation (1), the condition distribution of <span class="math inline">\(X(t_2)\)</span> given <span class="math inline">\(X(t_1)\)</span> is <span class="math display">\[[X(t_2) | X(t_1)] \sim N(X(t_1), \sigma^2 (t_2 - t_1)),\]</span> thus, <span class="math inline">\(\mathbb{E}_{X(t_2)}[X(t_2) |X(t_1)] = X(t_1)\)</span>.</p><p><span class="math display">\[\begin{aligned}  R(t_1, t_2) = \mathbb{E}[X(t_1)X(t_2)] &amp;=\mathbb{E}_{X(t_1)}[\mathbb{E}_{X(t_2)}[X(t_1)X(t_2) | X(t_1)]] \\  &amp;= \mathbb{E}_{X(t_1)}[X(t_1) X(t_1)] \\  &amp;= \mathbb{E}_{X(t_1)}[X^2(t_1)] = \sigma^2 t_1.\end{aligned}\]</span></p><p>Similarly, if <span class="math inline">\(t_2 &lt; t_1\)</span>,<span class="math inline">\(R(t_1, t_2) = \sigma^2 t_2\)</span>.</p><h2 id="brownian-motion-with-drift-bmd">Brownian motion with drift(BMD)</h2><p>For Brownian motion, it is an unbiased random walk. Walker stepsright or left with the same probability <span class="math inline">\(1/2\)</span> for each direction (onedimension).</p><p>For BMD, it is a biased random walk. Walker steps right or left withdifferent probs.</p><p>For example, consider time interval <span class="math inline">\(h\)</span>, step size <span class="math inline">\(\sigma \sqrt{h}\)</span>, <span class="math display">\[p(X(t+h) = x + \sigma \sqrt{h} | X(t) = x) = \frac{1}{2} (1 +\frac{\mu}{\sigma} \sqrt{h}).\]</span> <span class="math display">\[p(X(t+h) = x - \sigma \sqrt{h} | X(t) = x) = \frac{1}{2} (1 -\frac{\mu}{\sigma} \sqrt{h}).\]</span></p><ul><li><p><span class="math inline">\(\mu &gt; 0\)</span>, biased to theright. <span class="math inline">\(\mu &lt; 0\)</span>, biased to theleft.</p></li><li><p><span class="math inline">\(h\)</span> needs to be small enoughto make <span class="math inline">\(|\frac{\mu}{\sigma} \sqrt{h} | \leq1\)</span>.</p></li></ul><p>In this BMD case, <span class="math inline">\(x(t) \sim N(\mu t,\sigma^2 t)\)</span>.</p><ul><li>Independent and stationary increments.</li></ul><p>(We omit the proof. More details can be found at <a href="https://www.hajim.rochester.edu/ece/sites/gmateos/ECE440/Slides/block_5_stationary_processes_part_a.pdf">Gaussianprocess</a> ).</p><h2 id="geometric-brownian-motion-gbm">Geometric Brownian motion(GBM)</h2><p>Definition. Suppose that <span class="math inline">\(Z(t)\)</span> isa standard Brownian motion <span class="math inline">\(Z(t) \sim N(0,t)\)</span>. Parameters <span class="math inline">\(\mu \in\mathbb{R}\)</span> and <span class="math inline">\(\sigma \in (0,\infty)\)</span>. Let <span class="math display">\[X(t) = \exp[(\mu - \frac{\sigma^2}{2})t + \sigma Z(t)], \qquad t \geq 0.\qquad (2)\]</span> The stochastic process <span class="math inline">\(\{X(t): t\geq 0\}\)</span> is geometric Brownian motion with drift parameter<span class="math inline">\(\mu\)</span> and volatility parameter <span class="math inline">\(\sigma\)</span>.</p><ul><li>The process is always positive, one of the reasons that geometricBrownian motion is used to model financial and other processes that<strong>cannot be negative</strong>.</li><li>For the stochastic process</li></ul><p><span class="math display">\[(\mu - \frac{\sigma^2}{2})t + \sigma Z(t) \sim N((\mu -\frac{\sigma^2}{2})t , \sigma^2 t),  \]</span></p><p>it is a BMD with drift parameter <span class="math inline">\(\mu -\sigma^2/2\)</span> and scale parameter <span class="math inline">\(\sigma\)</span>. Thus, the geometric Brownianmotion is just the exponential of this BMD process.</p><ul><li><p>Here <span class="math inline">\(X(0) = 1\)</span>, the processstarts at 1. For GBM starting at <span class="math inline">\(X(0) =x_0\)</span>, the process is <span class="math display">\[X(t) = x_0 \exp[(\mu - \frac{\sigma^2}{2})t + \sigma Z(t)], \qquad t\geq 0.\]</span></p></li><li><p>GBM is not a Gaussian process.</p></li></ul><p>From the definition of GBM (2), we can have the followingdifferential equation: <span class="math display">\[\begin{aligned}  \frac{\text{d}X}{\text{d} t} &amp;= \exp[(\mu - \frac{\sigma^2}{2})t +\sigma Z(t)][(\mu - \frac{\sigma^2}{2}) + \sigma\frac{\text{d}Z}{\text{d} t}] \\  &amp;= X [(\mu - \frac{\sigma^2}{2}) + \sigma\frac{\text{d}Z}{\text{d} t}] \\  &amp;= X \tilde{\mu} + \sigma X \frac{\text{d}Z}{\text{d} t},  \qquad(\tilde{\mu} := \mu - \frac{\sigma^2}{2})\end{aligned}\]</span> thus, Geometric Brownian motion <span class="math inline">\(X(t)\)</span> satisfies the stochasticdifferential equation <span class="math display">\[\begin{aligned}\frac{\text{d}X}{\text{d} t}  &amp;= X \tilde{\mu} + \sigma X\frac{\text{d}Z}{\text{d} t},  \\  \text{d}X &amp; = X \tilde{\mu} {\text{d} t} + \sigma X \text{d}Z.\end{aligned}\]</span></p><p>The second equation is the Black–Scholes model. In the Black–Scholesmodel, <span class="math inline">\(X(t)\)</span> is the stock price.</p><ul><li><a href="https://stats.libretexts.org/Bookshelves/Probability_Theory/Probability_Mathematical_Statistics_and_Stochastic_Processes_(Siegrist)/18%3A_Brownian_Motion/18.04%3A_Geometric_Brownian_Motion">GeometricBrownian motion</a></li></ul><h1 id="white-gaussian-process">White Gaussian process</h1><p>Definition. A white Gaussian noise (WGN) process <span class="math inline">\(W(t)\)</span> is a GP with</p><ol type="1"><li><p>zero mean: <span class="math inline">\(\mu(t) = \mathbb{E}[W(t)]= 0\)</span> for all <span class="math inline">\(t\)</span>.</p></li><li><p>Delta function antocorrelation: <span class="math inline">\(R(t_1, t_2) = \sigma^2 \delta(t_1 -t_2)\)</span>.</p></li></ol><p>Here the Dirac delta is often thought as a function that is 0everywhere and infinite at 0. <span class="math display">\[\delta(t) =\begin{cases}\infty, &amp; t=0 \\0, &amp; t\ne 0\end{cases}.\]</span> The Dirac delta is actually a distribution, a generalizationof functions, and it is defined through the integral of its product withan arbitrary function <span class="math inline">\(f(t)\)</span>. <span class="math display">\[\int_{a}^{b} f(t)\delta(t) \text{d} t=\begin{cases}f(0), &amp; a &lt; 0 &lt; b \\0, &amp; \text{otherwise}\end{cases}.\]</span></p><!-- since the autocorrelation function of W(t) is not really a function (it involves theDirac delta), WGN cannot model any real physical phenomena. Nonetheless, it is a convenientabstraction to generate processes that can model real physical phenomena. --><p>Properties of white Gaussian noise:</p><ol type="1"><li><p>For <span class="math inline">\(t_1 \ne t_2\)</span>, <span class="math inline">\(W(t_1)\)</span> and <span class="math inline">\(W(t_2)\)</span> are uncorrelated. <span class="math display">\[\mathbb{E}[W(t_1)W(t_2)] = R(t_1, t_2) = 0, \qquad t_1 \ne t_2.\]</span> This means <span class="math inline">\(W(t)\)</span> atdifferent times are independent.</p></li><li><p>WGN has infinite variance (large power). <span class="math display">\[\mathbb{E}[W^2(t)] = R(t, t) = \sigma^2 \delta(0) = \infty.\]</span></p></li></ol><ul><li>WGN is discontinuous almost everywhere.</li><li>WGN is unbounded and it takes arbitrary large positive and negativevalues at any finite interval.</li></ul><h2 id="white-gaussian-noise-and-brownian-motion">White Gaussian noiseand Brownian motion</h2><p>Remember that the Brownian motion is a solution to the differentialequation: <span class="math display">\[\frac{\text{d} X(t)}{\text{d}t} = W(t).\]</span> <strong>Why <span class="math inline">\(\frac{\text{d}X(t)}{\text{d}t}\)</span> is called white noise ?</strong></p><p>Proof. Assume <span class="math inline">\(X(t)\)</span> is theintegral of a WGN process <span class="math inline">\(W(t)\)</span>,i.e., <span class="math inline">\(X(t) = \int_{0}^{t} W(u) \text{d}u\)</span>.</p><p>Since integration is linear functional and <span class="math inline">\(W(t)\)</span> is a GP, <span class="math inline">\(X(t)\)</span> is also a GP.</p><p>A Gaussian process can be uniquely specified by its Mean valuefunction and Autocorrelation function.</p><ol type="1"><li>The mean function: <span class="math display">\[\mu(t) = \mathbb{E}[\int_{0}^{t} W(u) \text{d} u] = \int_{0}^{t}\mathbb{E} [W(u)] \text{d} u = 0.\]</span><br></li><li>The autocorrelation <span class="math inline">\(R_{X}(t_1,t_2)\)</span> with <span class="math inline">\(t_1 &lt; t_2\)</span>:<span class="math display">\[\begin{aligned}  R_{X}(t_1, t_2) &amp;= \mathbb{E}[(\int_{0}^{t_1} W(u_1) \text{d}u_1)(\int_{0}^{t_2} W(u_2) \text{d} u_2)] \\  &amp;= \mathbb{E}[\int_{0}^{t_1} \int_{0}^{t_2} W(u_1)  W(u_2)\text{d} u_1 \text{d} u_2] \\  &amp;= \int_{0}^{t_1} \int_{0}^{t_2} \mathbb{E}[W(u_1)  W(u_2)]\text{d} u_1 \text{d} u_2 \\  &amp;= \int_{0}^{t_1} \int_{0}^{t_2} \sigma^2 \delta(u_1 - u_2)\text{d} u_1 \text{d} u_2 \\  &amp;= \int_{0}^{t_1} \int_{0}^{t_1} \sigma^2 \delta(u_1 - u_2)\text{d} u_1 \text{d} u_2 + \int_{0}^{t_1} \int_{t_1}^{t_2} \sigma^2\delta(u_1 - u_2) \text{d} u_1 \text{d} u_2 \\  &amp;= \int_{0}^{t_1} \int_{0}^{t_1} \sigma^2 \delta(u_1 - u_2)\text{d} u_1 \text{d} u_2 + 0\\  &amp;= \int_{0}^{t_1} \sigma^2 \text{d} u_1 \\  &amp;= \sigma^2 t_1.\end{aligned}\]</span> If <span class="math inline">\(t_2 &lt; t_1\)</span>, we canobtain <span class="math inline">\(R_{X}(t_1, t_2) = \sigma^2t_2\)</span>. Thus, <span class="math inline">\(R_{X}(t_1, t_2) =\sigma^2 \min \{t_1, t_2\}\)</span>.</li></ol><p>The mean function and autocorrelation function are the same asBrownian motion!</p><p>Because a Gaussian process can be uniquely determined by its meanvalue function and autocorrelation function. We can conclude</p><ul><li>The integral of WGN is a Brownian motion process.</li><li>The derivative of Brownian motion is WGN.</li></ul><h1 id="reference">Reference</h1><p><a href="https://www.hajim.rochester.edu/ece/sites/gmateos/ECE440/Slides/block_5_stationary_processes_part_a.pdf">Gaussianprocess</a></p><p><a href="https://stats.libretexts.org/Bookshelves/Probability_Theory/Probability_Mathematical_Statistics_and_Stochastic_Processes_(Siegrist)/18%3A_Brownian_Motion/18.04%3A_Geometric_Brownian_Motion">GeometricBrownian motion</a></p><p><a href="http://www.columbia.edu/~ks20/FE-Notes/4700-07-Notes-GBM.pdf">GeometricBrownian motion</a></p><p><a href="https://www.seas.upenn.edu/~ese3030/homework/week_11/week_11_white_gaussian_noise.pdf">WhiteGaussian noise</a></p>]]></content>
      
      
      <categories>
          
          <category> Stochastic Process </category>
          
      </categories>
      
      
        <tags>
            
            <tag> SDE </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Ordinary Differential Equation</title>
      <link href="/2023/07/19/ODE/"/>
      <url>/2023/07/19/ODE/</url>
      
        <content type="html"><![CDATA[<h1 id="ordinary-differential-equation-ode">Ordinary DifferentialEquation (ODE)</h1><h2 id="the-defination-of-ode">The defination of ODE</h2><p>An ODE is an equation in which the unknown quantity is a function,and it also involves derivatives of the unknown function.</p><p>For example, the forced spring-mass system: <span class="math display">\[\frac{d^2 x(t)}{d t^2} + \gamma \frac{d x(t)}{dt} + v^2 x(t) =w(t).  \qquad (1)\]</span> In this equation:</p><ul><li><p><span class="math inline">\(v\)</span> and <span class="math inline">\(\gamma\)</span> are constants that determine theresonant angular velocity and damping of the spring.</p></li><li><p><span class="math inline">\(w(t)\)</span> is a given functionthat may or may not depend on time.</p></li><li><p>position variable <span class="math inline">\(x\)</span> iscalled dependent variable.</p></li><li><p>time <span class="math inline">\(t\)</span> is called independentvariable.</p></li><li><p>This equation is <strong>second order</strong>. It contains thesecond derivative and doesn't have higher-order terms.</p></li><li><p>This equation is <strong>linear</strong>. <span class="math inline">\(x(t)\)</span> is linearly. There is no terms like<span class="math inline">\(x^2(t), \log x(t)\)</span>...</p></li><li><p>This equation is <strong>inhomogeneous</strong>. Because itcontains forcing term <span class="math inline">\(w(t)\)</span>.</p></li></ul><h2 id="the-solution-to-ode">The solution to ODE</h2><p>It can be divided to two categories: - particular solution: afunction that satisfies the differential equation and does not containany arbitrary constants.</p><ul><li>general solution: a function that satisfies the differentialequation and contains free constants.</li></ul><p>To exactly solve the differential equation, it is necessary tocombine the general solution with some initial conditions (i.e., <span class="math inline">\(x(t_0)\)</span>, <span class="math inline">\(\frac{d x(t)}{dt} |_{t_0}\)</span>) or some other(boundary) conditions of the differential equation.</p><h2 id="different-formulation-of-ode">Different formulation of ODE</h2><p>It is common to omit the time <span class="math inline">\(t\)</span>,so equation (1) can also be writen is this form: <span class="math display">\[\frac{d^2 x}{d t^2} + \gamma \frac{d x}{dt} + v^2 x = w.  \]</span></p><p>Sometimes, time derivatives are also denoted with dots over thevariable, for example:</p><p><span class="math display">\[\ddot{x} + \gamma \dot x + v^2 x = w.  \qquad (2)\]</span></p><h2 id="state-space-form-of-the-differential-equation-first-order-vector-differential-equation">State-spaceform of the differential equation (first-order vector differentialequation)</h2><ul><li><p><strong>order</strong>: the order of a differential equation isthe order of the highest derivative that appears in theequation.</p></li><li><p>Order <span class="math inline">\(N\)</span> ODE can convert toOrder 1 vector ODE i.e., if we define a state variable <span class="math inline">\(\vec{x} = (x_1 = x, x_2 = \dot{x})\)</span>, thenwe can convert equation（2） to <span class="math display">\[\begin{pmatrix}\frac{d x_1 (t)}{dt} \\\frac{d x_2 (t)}{dt}\end{pmatrix}=\begin{pmatrix}0 &amp; 1 \\-v^2 &amp; -\gamma\end{pmatrix}\begin{pmatrix}x_1 (t) \\x_2 (t)\end{pmatrix} +\begin{pmatrix}0 \\1\end{pmatrix}w(t) \qquad (3)\]</span></p></li></ul><p>Define <span class="math inline">\(\frac{d \vec{x}(t)}{dt} =\begin{pmatrix}\frac{d x_1 (t)}{dt} \\\frac{d x_2 (t)}{dt}\end{pmatrix},\)</span> <span class="math inline">\(f(\vec{x}(t)) =\begin{pmatrix}0 &amp; 1 \\-v^2 &amp; -\gamma\end{pmatrix}\begin{pmatrix}x_1 (t) \\x_2 (t)\end{pmatrix}\)</span> and <span class="math inline">\(\mathbf{L} =\begin{pmatrix}0 \\1\end{pmatrix}.\)</span></p><p>Equation (3) can be seen to be a special case of this form: <span class="math display">\[\frac{d \vec{x}(t)}{dt} = f(\vec{x}(t), t) + \mathbf{L}(\vec{x}(t),t)\vec{w}(t)\]</span> where the vector-valued function <span class="math inline">\(\vec{x}(t) \in R^D\)</span> is the state of thesystem. <span class="math inline">\(f(\cdot, \cdot)\)</span> and <span class="math inline">\(\mathbf{L}(\cdot, \cdot)\)</span> are arbitraryfunctions. <span class="math inline">\(\vec{w}(t)\)</span> is some(vector-valued) forcing function, driving function, or input to thesystem.</p><ul><li><p>The first-order vector differential equation representation of an<span class="math inline">\(n\)</span>th-order differential equation isoften called the state-space form of the differential equation.</p></li><li><p>The theory and solution methods for first-order vectordifferential equations are easier to analyze.</p></li><li><p>And, <span class="math inline">\(n\)</span> th order differentialequations can (almost) always be converted into equivalent <span class="math inline">\(n\)</span>-dimensional vector-valued first-orderdifferential equations.</p></li></ul><h2 id="linear-odes">Linear ODEs</h2><p>Equation (3) is also a special case of the <strong>lineardifferential equations</strong>: <span class="math display">\[\frac{d \vec{x}(t)}{dt} = \mathbf{F}(t) \vec{x}(t) + \mathbf{L}\vec{w}(t).\]</span> where <span class="math inline">\(\mathbf{F}(t)\)</span> is amatrix-valued function of time. <span class="math inline">\(\mathbf{L}\)</span> is a matrix. <span class="math inline">\(\vec{w}(t)\)</span> is a vector-valued function oftime.</p><ul><li><strong>homogeneous</strong>: the equation is homogeneous if theforcing function <span class="math inline">\(\vec{w}(t)\)</span> is zerofor all <span class="math inline">\(t\)</span>.</li><li><strong>time-invariant</strong>: the equation is time-invariant if<span class="math inline">\(\mathbf{F}(t)\)</span> is constant for all<span class="math inline">\(t\)</span>.</li></ul><p>In the following sections, we first start with the simple scalarlinear time-invariant homogeneous differential equation with fixedinitial condition at <span class="math inline">\(t = 0\)</span>. Then,we will consider the multidimensional generalization of this equation.Besides, we also consider the linear time-invariant inhomogeneousdifferential equations. Finally, we will consider more generaldifferential equations.</p><h4 id="solutions-of-linear-time-invariant-differential-equations">Solutionsof Linear Time-Invariant Differential Equations</h4><p>Consider the <strong>scalar</strong> linear<strong>homogeneous</strong> differential equation with fixed initialcondition at <span class="math inline">\(t = 0\)</span>: <span class="math display">\[\frac{d x(t)}{dt} = F x(t),  x(0) = \text{given}, \qquad (4)\]</span> where <span class="math inline">\(F\)</span> is aconstant.</p><p>This equation can be solved by separation of variables: <span class="math display">\[\frac{d x(t)}{x(t)} = F dt.\]</span> Integrating the left-hand side from <span class="math inline">\(x(0)\)</span> to <span class="math inline">\(x(t)\)</span>, and right-hand side from <span class="math inline">\(0\)</span> to <span class="math inline">\(t\)</span>, we get <span class="math display">\[\log \frac{x(t)}{x(0)} = F t.\]</span> Combining the initial condition, we get <span class="math display">\[x(t) = x(0) e^{F t}.\]</span></p><p>Another way to solve this equation is to by intergrating both sidesof equation (4) from <span class="math inline">\(0\)</span> to <span class="math inline">\(t\)</span>. We get <span class="math display">\[x(t) = x(0) + \int_0^t F x(\tau) d \tau.\]</span> The we can substitute the solution back into the right-handside of the equation, and we get <span class="math display">\[\begin{aligned}x(t) &amp;= x(0) + \int_0^t F x(\tau) d \tau \\&amp;= x(0) + \int_0^t F [x(0) + \int_0^{\tau} F x(\tau') d\tau'] d \tau \\&amp;= x(0) + F x(0) t +  \int_0^t \int_0^{\tau}F^2 x(\tau') d\tau' d \tau \\&amp;= x(0) + F x(0) t +  F^2 x(0) \frac{t^2}{2} + \int_0^t\int_0^{\tau} \int_0^{\tau'}F^3 x(\tau'')  d \tau''d \tau' d \tau \\&amp;= ...\\&amp;= x(0) + F x(0) t +  F^2 x(0) \frac{t^2}{2} + F^3 x(0)\frac{t^3}{6} + \cdots\\&amp;=  (1 + Ft + F^2 t^2 /2 + F^3 t^3 /3! + \cdots) x(0)\\&amp;=  e^{F t} x(0).  \qquad (\text{Taylor expansion})\end{aligned}\]</span></p><table style="width:26%;"><colgroup><col style="width: 26%"></colgroup><thead><tr><th style="text-align: left;">For the <strong>multidimensional</strong>linear <strong>homogeneous</strong> differential equation with fixedinitial condition at <span class="math inline">\(t = 0\)</span>: <span class="math display">\[\frac{d \vec{x}(t)}{dt} = \mathbf{F} \vec{x}(t),  \vec{x}(0) =\text{given}, \qquad (5)\]</span> where <span class="math inline">\(\mathbf{F}\)</span> is aconstant matrix.</th></tr></thead><tbody><tr><td style="text-align: left;">Next, let's move to the linear<strong>time-invariant</strong> <strong>inhomogeneous</strong>differential equations: <span class="math display">\[\frac{d \vec{x}(t)}{dt} = \mathbf{F} \vec{x}(t) + \mathbf{L}\vec{w}(t),  \vec{x}(t_0) = \text{given}, \qquad (7)\]</span> where <span class="math inline">\(\mathbf{F}\)</span> is aconstant matrix, <span class="math inline">\(\mathbf{L}\)</span> is aconstant matrix, and <span class="math inline">\(\vec{w}(t)\)</span> isa vector-valued function of time.</td></tr><tr><td style="text-align: left;">First, we can move the <span class="math inline">\(\mathbf{F} \vec{x}(t)\)</span> to the left-handside of the equation and multify both sides with a integrating factor<span class="math inline">\(\exp(- \mathbf{F}t)\)</span>, and get <span class="math display">\[\begin{aligned}\exp(- \mathbf{F}t) \frac{d \vec{x}(t)}{dt} - \exp(-\mathbf{F}t)\mathbf{F} \vec{x}(t) = \exp(- \mathbf{F}t) \mathbf{L}\vec{w}(t),\end{aligned}\]</span> Since <span class="math inline">\(\frac{d}{dt} \exp(-\mathbf{F}t) \vec{x}(t) = \exp(- \mathbf{F}t) \frac{d \vec{x}(t)}{dt} -\exp(- \mathbf{F}t)\mathbf{F} \vec{x}(t)\)</span>, we can rewrite theabove equation as <span class="math display">\[\frac{d}{dt} \exp(- \mathbf{F}t) \vec{x}(t) = \exp(- \mathbf{F}t)\mathbf{L} \vec{w}(t).\]</span> Integrating both sides from <span class="math inline">\(t_0\)</span> to <span class="math inline">\(t\)</span>, we get <span class="math display">\[\exp(- \mathbf{F}t) \vec{x}(t) - \exp(- \mathbf{F}t_0) \vec{x}(t_0) =\int_{t_0}^t \exp(- \mathbf{F}\tau) \mathbf{L} \vec{w}(\tau) d \tau.\]</span> Then, we can get the solution of equation (7): <span class="math display">\[\vec{x}(t) = \exp(\mathbf{F}(t - t_0)) \vec{x}(t_0) + \int_{t_0}^t\exp(\mathbf{F}(t - \tau)) \mathbf{L} \vec{w}(\tau) d \tau. \qquad (8)\]</span></td></tr><tr><td style="text-align: left;">### Solutions of General Linear ODEs Theprevious section only consider the linear time-invariant differentialequations(<span class="math inline">\(F\)</span> is a constant). In thissection, we will consider the general linear differential equations withtime-varying coefficients.</td></tr><tr><td style="text-align: left;">For the linear<strong>homogeneous</strong> <strong>time-varying</strong> differentialequation with fixed initial condition at <span class="math inline">\(t =t_0\)</span>: <span class="math display">\[\frac{d \vec{x}(t)}{dt} = \mathbf{F}(t) \vec{x}(t), \quad \vec{x}(t_0) =\text{given}, \qquad (9)\]</span> where <span class="math inline">\(\mathbf{F}(t)\)</span> is amatrix-valued function of time. We can not use the exponential matrix tosolve this equation, because the <span class="math inline">\(\mathbf{F}(t)\)</span> is a time-varying matrix.But the solution of this equation has a general form: <span class="math display">\[\vec{x}(t) = \mathbf{\Psi}(t, t_0) \vec{x}(t_0), \qquad (10)\]</span> where <span class="math inline">\(\mathbf{\Psi}(t,t_0)\)</span> is a matrix-valued function of time, and it is called the<strong>transition matrix</strong>. The transition matrix <span class="math inline">\(\mathbf{\Psi}(t, t_0)\)</span> satisfies thefollowing differential properties: <span class="math display">\[\begin{aligned}\frac{\partial \mathbf{\Psi}(\tau, t)}{\partial \tau} &amp;=\mathbf{F}(\tau) \mathbf{\Psi}(\tau, t), \\\frac{\partial \mathbf{\Psi}(\tau, t)}{\partial t} &amp;= -\mathbf{\Psi}(\tau, t) \mathbf{F}(t), \\\mathbf{\Psi}(\tau, t) &amp;= \mathbf{\Psi}(\tau, s) \mathbf{\Psi}(s, t)\qquad (\text{11}) \\\mathbf{\Psi}(t, \tau) &amp;= \mathbf{\Psi}^{-1}(\tau, t)  \\\mathbf{\Psi}(t, t) &amp;= \mathbf{I}.\end{aligned}\]</span> In most cases, the transition matrix <span class="math inline">\(\mathbf{\Psi}(t, t_0)\)</span> does not have aclosed-form solution.</td></tr></tbody></table><p>For the linear <strong>inhomogeneous</strong><strong>time-varying</strong> differential equation with fixed initialcondition at <span class="math inline">\(t = t_0\)</span>: <span class="math display">\[\frac{d \vec{x}(t)}{dt} = \mathbf{F}(t) \vec{x}(t) + \mathbf{L}(t)\vec{w}(t), \quad \vec{x}(t_0) = \text{given}, \qquad (12)\]</span> where <span class="math inline">\(\mathbf{F}(t)\)</span> is amatrix-valued function of time, <span class="math inline">\(\mathbf{L}(t)\)</span> is a matrix-valued functionof time, and <span class="math inline">\(\vec{w}(t)\)</span> is avector-valued function of time. The solution of this equation has ageneral form: <span class="math display">\[\vec{x}(t) = \mathbf{\Psi}(t, t_0) \vec{x}(t_0) + \int_{t_0}^t\mathbf{\Psi}(t, \tau) \mathbf{L}(\tau) \vec{w}(\tau) d \tau. \qquad(13)\]</span></p><p>When <span class="math inline">\(\mathbf{F}(t)\)</span> and <span class="math inline">\(\mathbf{L}(t)\)</span> is constant, the solutionof equation (7) is a special case of (13) and we can verfiy that the<span class="math inline">\(\Psi(t, t_0) = \exp(\mathbf{F}(t -t_0))\)</span> satisfies properties (10).</p><hr><p>Fourier tansforms and Laplace transforms are two useful methods tosolve inhomogeneous linear time-invariant ODE (Note that<strong>time-invariant</strong>).</p><h2 id="fourier-transforms">Fourier Transforms</h2><p>The Fourier transform of a function <span class="math inline">\(x(t)\)</span> is defined as: <span class="math display">\[X(i w) = \mathcal{F}[x(t)] = \int_{-\infty}^{\infty} x(t) \exp(-iwt)\text{d}t, \qquad (14)\]</span> where <span class="math inline">\(i\)</span> is the imaginaryunit.</p><p>The inverse Fourier transform of (14) is: <span class="math display">\[x(t) = \mathcal{F}^{-1}[X(i w)] = \frac{1}{2\pi} \int_{-\infty}^{\infty}X(iw) \exp(i w t) \text{d}t.\]</span></p><p>Some useful properties:</p><ul><li><p>differention: <span class="math inline">\(\mathcal{F}[\frac{\text{d}^n x(t)}{\text{d} t^n}]= (iw)^{n} \mathcal{F}[x(t)]\)</span>.</p></li><li><p>convolution: <span class="math inline">\(\mathcal{F}[x(t) \asty(t)] = \mathcal{F}[x(t)] \ast \mathcal{F}[y(t)]\)</span>, where theconvolution <span class="math inline">\(\ast\)</span> is defined as<span class="math display">\[x(t) \ast y(t) = \int_{-\infty}^{\infty} x(t - \tau)y(\tau) \text{d}\tau\]</span></p></li></ul><p><strong>If we want to use Fourier transform to solve ODEs, theinitial condition must be 0.</strong></p><p>Now, let's use Fourier transform to solve the linear<strong>time-invariant</strong> <strong>inhomogeneous</strong>differential equations: <span class="math display">\[\frac{d \vec{x}(t)}{dt} = \mathbf{F} \vec{x}(t) + \mathbf{L}\vec{w}(t),  \quad \vec{x}(t_0) = 0,\]</span> where <span class="math inline">\(\mathbf{F}\)</span> is aconstant matrix, <span class="math inline">\(\mathbf{L}\)</span> is aconstant matrix, and <span class="math inline">\(\vec{w}(t)\)</span> isa vector-valued function of time.</p><p>Take Fourier tranforms componentwise give <span class="math display">\[(iw)\vec{X}(iw) = \mathbf{F} \vec{X}(iw) + \mathbf{L} \vec{W}(iw),\]</span> thus, we can get <span class="math display">\[\vec{X}(iw) = [(iw)\mathbf{I} - \mathbf{F}]^{-1} \ast \mathbf{L}\vec{W}(iw),\]</span> The solution is the inverse Fourier transform <span class="math display">\[x(t) = \mathcal{F}^{-1}[[(iw)\mathbf{I} - \mathbf{F}]^{-1} \ast\mathbf{L} \vec{W}(iw)] = \mathcal{F}^{-1}[((iw)\mathbf{I} -\mathbf{F})^{-1}]\ast \mathbf{L} \vec{w}(t), \qquad (15)\]</span></p><p>Since <span class="math inline">\(\vec{x}(t_0) = 0\)</span>, comparethe solution (15) with solution (8), we can obtain <span class="math display">\[\mathcal{F}^{-1}[((iw)\mathbf{I} - \mathbf{F})^{-1}] = \exp(\mathbf{F}t)u(t),\]</span> where <span class="math inline">\(u(t)\)</span> is theHeaviside step function, which is 0 for <span class="math inline">\(t&lt;0\)</span> and 1 for <span class="math inline">\(t \geq 0\)</span>.</p><h2 id="laplace-transforms">Laplace Transforms</h2><p>The Laplace transform of a function <span class="math inline">\(f(t)\)</span> is defined on space <span class="math inline">\(\{t | t\geq 0\}\)</span>: <span class="math display">\[F(s) = \mathcal{L}[f(t)] = \int_{0}^{\infty} f(t)\exp(-st) \text{d}t,\qquad (16)\]</span> where <span class="math inline">\(s = \sigma +iw\)</span>.</p><p>The inverse transform is <span class="math inline">\(f(t) =\mathcal{L}^{-1}[F(s)]\)</span>.</p><p>Remember that the Fourier transform needs the initial condition <span class="math inline">\(x(0) = 0\)</span>. But Laplace transform can takethe initial conditions into account. If <span class="math inline">\(x(0)= \text{given}\)</span>, then <span class="math display">\[\mathcal{L}[\frac{\text{d} x(t)}{\text{d} t}] = s \mathcal{L}[x(t)] -x(0) = s X(s) - x(0).\]</span></p><p><span class="math display">\[\mathcal{L}[\frac{\text{d}^n x(t)}{\text{d} t^n}] = s^n X(s) - s^{n-1}x(0) - \cdots - \frac{\text{d} x^{n-1}}{\text{d} t^{n-1}}(0).\]</span></p><p>Now, let's use Laplace transform to solve the linear<strong>time-invariant</strong> <strong>inhomogeneous</strong>differential equations: <span class="math display">\[\frac{d \vec{x}(t)}{dt} = \mathbf{F} \vec{x}(t) + \mathbf{L}\vec{w}(t),  \quad \vec{x}(t_0) = \text{given} \ne 0,\]</span> where <span class="math inline">\(\mathbf{F}\)</span> is aconstant matrix, <span class="math inline">\(\mathbf{L}\)</span> is aconstant matrix, and <span class="math inline">\(\vec{w}(t)\)</span> isa vector-valued function of time.</p><p>Take Laplace tranforms componentwise give <span class="math display">\[s X(s) - x(0) = \mathbf{F} X(s) + \mathbf{L} W(s).\]</span> Then, <span class="math display">\[X(s) = [s\mathbf{I}-\mathbf{F}]^{-1} x(0) +[s\mathbf{I}-\mathbf{F}]^{-1} \ast  \mathbf{L} W(s).  \qquad (17)\]</span></p><p>Compare the solution (17) with solution (8), we can obtain <span class="math display">\[\mathcal{L}^{-1}[(s\mathbf{I}-\mathbf{F})^{-1}] = \exp(\mathbf{F}t)\]</span> for <span class="math inline">\(t \geq 0\)</span>.</p><h1 id="reference">Reference</h1><p><a href="https://users.aalto.fi/~asolin/sde-book/sde-book.pdf">SimoSärkkä and Arno Solin (2019). Applied Stochastic Differential Equations.Cambridge University Press.</a></p>]]></content>
      
      
      <categories>
          
          <category> Generative Model </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Diffusion </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Convergence of Markov Chain</title>
      <link href="/2023/07/15/Convergence%20of%20MC/"/>
      <url>/2023/07/15/Convergence%20of%20MC/</url>
      
        <content type="html"><![CDATA[<h1 id="total-variation-distance">Total Variation Distance</h1><p>Define Total Variation Distance: <span class="math display">\[d_{TV}(\mu(x), v(x)) = \frac{1}{2} \sum_{x \in \Omega} |\mu(x) - v(x)|\]</span> This is equivalent to the following: <span class="math display">\[d_{TV}(\mu(x), v(x)) = \sum_{x \in \Omega^{-}} (v(x) - \mu(x))= \sum_{x \in \Omega^{+}} (\mu(x) - v(x))\]</span> where <span class="math inline">\(\Omega^{+} = \{x \in \Omega:\mu(x) \geq v(x)\}\)</span>, <span class="math inline">\(\Omega^{-} =\{x \in \Omega: \mu(x) &lt; v(x)\}\)</span>, and <span class="math display">\[d_{TV}(\mu(x), v(x)) = \max_{S \subset \Omega} |\mu(S) - v(S)|\]</span> where <span class="math inline">\(\mu(S) = \sum_{x \in S}\mu(x)\)</span>, <span class="math inline">\(v(S) = \sum_{x \in S}v(x)\)</span>.</p><p>Proof: Define <span class="math inline">\(\Omega^{+} = \{x \in\Omega: \mu(x) \geq v(x)\}\)</span>, <span class="math inline">\(\Omega^{-} = \{x \in \Omega: \mu(x) &lt;v(x)\}\)</span>, then <span class="math display">\[\begin{aligned}d_{TV}(\mu(x), v(x)) &amp;= \frac{1}{2} \sum_{x \in \Omega} |\mu(x) -v(x)| \\&amp;= \frac{1}{2} \sum_{x \in \Omega^{+}} (\mu(x) - v(x)) + \frac{1}{2}\sum_{x \in \Omega^{-}} (v(x) - \mu(x))\end{aligned}\]</span> Since <span class="math inline">\(\sum_{x \in \Omega} \mu(x) =1 = \sum_{x \in \Omega^{+}} \mu(x) + \sum_{x \in \Omega^{-}}\mu(x)\)</span>, we have <span class="math display">\[\begin{aligned}d_{TV}(\mu(x), v(x)) &amp;= \frac{1}{2} \sum_{x \in \Omega^{+}} (\mu(x)- v(x)) + \frac{1}{2} \sum_{x \in \Omega^{-}} (v(x) - \mu(x)) \\&amp;= \frac{1}{2} \sum_{x \in \Omega^{-}} (v(x) - \mu(x)) + \frac{1}{2}\sum_{x \in \Omega^{-}} (v(x) - \mu(x)) \\&amp;= \sum_{x \in \Omega^{-}} (v(x) - \mu(x)) \\&amp;= \sum_{x \in \Omega^{+}} (\mu(x) - v(x))\end{aligned}\]</span> If <span class="math inline">\(S = \Omega^{+}\)</span> or<span class="math inline">\(\Omega^{-}\)</span>, then <span class="math display">\[\begin{aligned}d_{TV}(\mu(x), v(x)) &amp;= \frac{1}{2} \sum_{x \in \Omega} |\mu(x) -v(x)| \\&amp;=  \max_{S \in \Omega} \sum_{x \in S} |\mu(x) - v(x)| \\\end{aligned}\]</span></p><p>If <span class="math inline">\(S\)</span> contains any elements <span class="math inline">\(x \in \Omega^{-}\)</span>, then <span class="math inline">\(d_{TV}(\mu(x), v(x)) = |\sum_{x \in S} (\mu(x) -v(x))| \leq \sum_{x \in \Omega^{+}} (\mu(x) - v(x))\)</span> when <span class="math inline">\(S = \Omega^{+}\)</span>.</p><h1 id="convergence-of-markov-chain">Convergence of Markov Chain</h1><p>Assume we start from state <span class="math inline">\(x\)</span>,run Markov chain for <span class="math inline">\(t\)</span> steps, thenwe get the distribution <span class="math inline">\(P_{x}^{t}\)</span>.If we want to prove <span class="math inline">\(P_{x}^{t}\)</span>converges to stationary distribution <span class="math inline">\(\pi\)</span>, we need to prove <span class="math inline">\(d_{TV}(P_{x}^{t}, \pi) \rightarrow 0\)</span> as<span class="math inline">\(t \rightarrow \infty\)</span>. Thus, we needto bound <span class="math inline">\(d_{TV}(P_{x}^{t},\pi)\)</span>.</p><p>Define <span class="math inline">\(d_{x}(t) := d_{TV}(P_{x}^{t},\pi)\)</span>.</p><p>Consider all possible initial states <span class="math inline">\(x\in \Omega\)</span>, we define <span class="math inline">\(d(t) :=\max_{x \in \Omega} d_{TV} (P_{x}^{t}, \pi)\)</span>.</p><p>Assume there are two Markov chains <span class="math inline">\(x_{t}\sim P_{x}^{t}\)</span>, <span class="math inline">\(y_{t} \simP_{y}^{t}\)</span>. <span class="math inline">\(x_{t}\)</span> and <span class="math inline">\(y_{t}\)</span> share the same transitionprobability matrix <span class="math inline">\(P\)</span> but start fromdifferent states. Then we define <span class="math display">\[\bar{d}(t):= \max_{x, y \in \Omega} d_{TV}(P_{x}^{t}, P_{y}^{t})\]</span>.</p><p>Next, we will prove this Lemma: &gt; Lemma 1. &gt; <span class="math display">\[d(t) \leq \bar{d}(t) \leq 2 d(t).\]</span></p><p>Proof: Let's prove the second inequality <span class="math inline">\(\bar{d}(t) \leq 2 d(t)\)</span>. <span class="math display">\[\begin{aligned}\bar{d}(t) &amp;= \max_{x, y \in \Omega} d_{TV}(P_{x}^{t}, P_{y}^{t}) \\&amp;= \max_{x, y \in \Omega} \frac{1}{2} \sum_{z \in \Omega}|P_{x}^{t}(z) - P_{y}^{t}(z)| \\&amp; = \max_{x, y \in \Omega} \frac{1}{2} \sum_{z \in \Omega}|P_{x}^{t}(z) - \pi(z) + \pi(z) - P_{y}^{t}(z)| \\&amp;\leq \max_{x, y \in \Omega} [\frac{1}{2} \sum_{z \in \Omega}|P_{x}^{t}(z) - \pi(z)| + \frac{1}{2} \sum_{z \in \Omega} |\pi(z) -P_{y}^{t}(z)| ]\\&amp;= \max_{x \in \Omega} d_{TV}(P_{x}^{t}, \pi) + \max_{y \in \Omega}d_{TV}(P_{y}^{t}, \pi) \\&amp;= d(t) + d(t) \\&amp;= 2 d(t)\end{aligned}\]</span></p><p>For the first inequality <span class="math inline">\(\bar{d}(t) \leq2 d(t)\)</span>, we need to prove <span class="math inline">\(d(t) \leq\bar{d}(t)\)</span>.</p><p>Define <span class="math display">\[S_{x,y}^{*} = \arg\max_{S \subset \Omega} （P_{x}^{t}(S) -P_{y}^{t}(S)）\]</span></p><p><span class="math display">\[S_{x}^{**} = \arg\max_{S \subset \Omega, y \in \Omega} （P_{x}^{t}(S) -P_{y}^{t}(S)）\]</span></p><p><span class="math display">\[S_{x}^{*} = \arg\max_{S \subset \Omega} \sum_{y \in \Omega}\pi(y)（P_{x}^{t}(S) - P_{y}^{t}(S)）\]</span></p><p><span class="math display">\[\begin{aligned}d(t) &amp;= \max_{x \in \Omega} d_{TV}(P_{x}^{t}, \pi) \\&amp;= \max_{x \in \Omega} \max_{S \subset \Omega}|P_{x}^{t}(S) -\pi(S)| \\&amp;= \max_{x \in \Omega} \max_{S \subset \Omega^{+}}(P_{x}^{t}(S) -\pi(S))  \\&amp;= \max_{x \in \Omega} \max_{S \subset \Omega^{+}}(P_{x}^{t}(S) -\sum_{y \in \Omega} \pi(y) P_{y}^{t}(S))  \qquad  (\pi(y) = \sum_{x \in\Omega} \pi(x) P_{x,y}) \\&amp;= \max_{x \in \Omega} \max_{S \subset \Omega^{+}} \sum_{y \in\Omega} \pi(y) (P_{x}^{t}(S) - P_{y}^{t}(S)).\end{aligned}\]</span> Since for all <span class="math inline">\(S\)</span>: <span class="math inline">\(（P_{x}^{t}(S_{x,y}^{*}) -P_{y}^{t}(S_{x,y}^{*})）\geq （P_{x}^{t}(S) -P_{y}^{t}(S)）\)</span></p><p>we have, <span class="math display">\[\begin{aligned}d(t) &amp;= \max_{x \in \Omega} \max_{S \subset \Omega^{+}} \sum_{y \in\Omega} \pi(y) (P_{x}^{t}(S) - P_{y}^{t}(S)) \\&amp; \leq \max_{x \in \Omega} \sum_{y \in \Omega} \pi(y)(P_{x}^{t}(S_{x,y}^{*}) - P_{y}^{t}(S_{x,y}^{*})) \\&amp;= \max_{x \in \Omega} \sum_{y \in \Omega} \pi(y)\max_{S}  (P_{x}^{t}(S) - P_{y}^{t}(S)) \\&amp; \leq  \max_{x \in \Omega} \sum_{y \in \Omega}\pi(y)   (P_{x}^{t}(S_{x}^{**}) - P_{y}^{t}(S_{x}^{**})) \\&amp; = \max_{x \in \Omega} \sum_{y \in \Omega} \pi(y) \max_{y,S}(P_{x}^{t}(S) - P_{y}^{t}(S)) \\&amp;= \max_{x, y \in \Omega}\max_{ S \subset \Omega}(P_{x}^{t}(S) -P_{y}^{t}(S))   \qquad (\sum_{y \in \Omega} \pi(y) = 1) \\&amp;= \max_{x, y \in \Omega} d_{TV}(P_{x}^{t}, P_{y}^{t}) \\&amp;= \bar{d}(t).\end{aligned}\]</span> that is, <span class="math inline">\(d(t) \leq \bar{d}(t) \leq2d(t)\)</span>.</p><p><strong>What we have proved is that, <span class="math inline">\(d(t)\)</span> is bounded by <span class="math inline">\(\bar{d}(t)\)</span>, and <span class="math inline">\(\bar{d}(t)\)</span> is controlled by <span class="math inline">\(\max_{x,y \in \Omega} d_{TV}(P_{x}^{t},P_{y}^{t})\)</span>. Let's find a way to bound <span class="math inline">\(d_{TV}(P_{x}^{t},P_{y}^{t})\)</span>!</strong></p><blockquote><p>Define 1. (Coupling) Assume <span class="math inline">\(x , y \in\Omega\)</span>, <span class="math inline">\(x \sim \mu, y \simv\)</span>, <span class="math inline">\(\mu, v\)</span> are twodistributions. A joint distribution <span class="math inline">\(w(x,y)\)</span> on <span class="math inline">\(\Omega \times \Omega\)</span>is called <strong>couping</strong> if <span class="math inline">\(\forall x \in \Omega\)</span>, <span class="math inline">\(\sum_{y}w(x, y) = \mu(x)\)</span>, <span class="math inline">\(\forall y \in \Omega\)</span>, <span class="math inline">\(\sum_{x} w(x, y) = v(y)\)</span>.</p></blockquote><blockquote><p>Lemma 2. Consider <span class="math inline">\(\mu, v\)</span> definedon <span class="math inline">\(\Omega\)</span>,<br>(a) For any coupling <span class="math inline">\(w(x, y)\)</span> of<span class="math inline">\(\mu, v\)</span>, <span class="math inline">\(d_{TV}(\mu, v) \leq P(x \ne y)\)</span>.<br>(b) There always exists a coupling <span class="math inline">\(w(x,y)\)</span> of <span class="math inline">\(\mu, v\)</span> such that<span class="math inline">\(d_{TV}(\mu, v) = P(x \ne y)\)</span>.</p></blockquote><p>Proof: (a) <span class="math inline">\(\forall z\)</span>, <span class="math inline">\(w(z, z) \leq \sum_{y \in \Omega} w(z, y) =\mu(z)\)</span>. Similarly, <span class="math inline">\(w(z, z) \leqv(z)\)</span>. Thus, <span class="math inline">\(w(z, z) \leq\min(\mu(z), v(z))\)</span>.</p><p><span class="math display">\[\begin{aligned}P(x \ne y) &amp;= 1 - P(x = y) \\&amp;= 1 - \sum_{z \in \Omega} w(z, z) \\&amp;\geq 1 - \sum_{z \in \Omega} \min(\mu(z), v(z)) \\&amp;= \sum_{z \in \Omega} \mu(z) - \sum_{z \in \Omega} \min(\mu(z),v(z)) \\&amp;= \sum_{z \in \Omega^{+}} (\mu(z) - v(z)) \\&amp;= d_{TV}(\mu, v)\end{aligned}\]</span></p><ol start="2" type="a"><li>We can construct a coupling <span class="math inline">\(w(x,y)\)</span>: <span class="math display">\[w(x, y) = \left\{\begin{aligned}&amp; \min \{\mu(x), v(y)\}, \quad x = y \\&amp;\frac{(\mu(x) - w(x, x))(v(y) - w(y,y))}{1 - \sum_{z \in\Omega}w(z,z)}, \quad x \ne y\end{aligned}\right.\]</span> For this joint distribution, we have <span class="math display">\[\begin{aligned}P(x \ne y) &amp;= 1 - P(x = y) \\&amp;= 1 - \sum_{z \in \Omega} w(z, z) \\&amp;= 1 - \sum_{z \in \Omega} \min(\mu(z), v(z)) \\&amp;= \sum_{z \in \Omega} \mu(z) - \sum_{z \in \Omega} \min(\mu(z),v(z)) \\&amp;= \sum_{z \in \Omega^{+}} (\mu(z) - v(z)) \\&amp;= d_{TV}(\mu, v)\end{aligned}\]</span> Thus, this joint distribution <span class="math inline">\(w(x,y)\)</span> satisfies <span class="math inline">\(d_{TV}(\mu, v) = P(x\ne y)\)</span>. Now, we need to prove that this joint distribution<span class="math inline">\(w(x, y)\)</span> is a coupling of <span class="math inline">\(\mu, v\)</span>.</li></ol><p><span class="math inline">\(\forall x \in \Omega\)</span>, <span class="math display">\[\begin{aligned}\sum_{y \in \Omega} w(x, y) &amp;= \sum_{y=x} \min \{\mu(x), v(y)\} +\sum_{y \in \Omega, y \ne x} \frac{(\mu(x) - w(x, x))(v(y) - w(y,y))}{1- \sum_{z \in \Omega}w(z,z)} \\&amp;= w(x, x) + \frac{(\mu(x) - w(x, x))}{1 - \sum_{z \in\Omega}w(z,z)} \sum_{y \ne x} (v(y) - w(y,y)) \\&amp;= w(x,x) + \frac{(\mu(x) - w(x, x))}{1 - \sum_{z \in \Omega}w(z,z)}(1 - v(x) - (\sum_{z} w(z,z) - w(x,x)))  \qquad (\sum_{y \ne x} v(y) = 1- v(x))\\&amp;= w(x,x) + \frac{(\mu(x) - w(x, x))}{1 - \sum_{z \in \Omega}w(z,z)}(1 - \sum_{z} w(z,z) + w(x,x) - v(x) ) \qquad (\sum_{y \ne x} w(y,y) =\sum_{z} w(z,z) - w(x, x))\end{aligned}\]</span> If $ x ^{+} = {x | (x) v(x)}$, then <span class="math inline">\(w(x,x) = v(x)\)</span>, <span class="math inline">\(\sum_{y \in \Omega} w(x, y)  = v(x) + (\mu(x) -v(x)) = \mu(x)\)</span>.</p><p>If $ x ^{-} = {x | (x) &lt; v(x)}$, then <span class="math inline">\(w(x,x) = \mu(x)\)</span>, <span class="math inline">\(\sum_{y \in \Omega} w(x, y)  = \mu(x) + 0 =\mu(x)\)</span>. Thus, <span class="math inline">\(w(x, y)\)</span> is acoupling of <span class="math inline">\(\mu, v\)</span>.</p><p>Last but not least, let's begin to prove the nonincreasing propertyof <span class="math inline">\(d(t)\)</span>!! <strong>Almost close tothe end!! ^o/</strong></p><blockquote><p>Lemma 3. Consider two Markov chains <span class="math inline">\(x^{t}\sim P_{x}^{t}\)</span>, <span class="math inline">\(y^{t} \simP_{y}^{t}\)</span>, <span class="math inline">\(x^{t}\)</span> and <span class="math inline">\(y^{t}\)</span> share the same transitionprobability matrix <span class="math inline">\(P\)</span> but start fromdifferent states. If <span class="math inline">\(x^t = y^t\)</span>,then <span class="math inline">\(x^{t+1} = y^{t+1}\)</span>, elif <span class="math inline">\(x^t \ne y^t\)</span>, then <span class="math inline">\(x^{t+1} \ne y^{t+1}\)</span>, <span class="math inline">\(x^{t+1}\)</span> and <span class="math inline">\(y^{t+1}\)</span> are independent.</p></blockquote><blockquote><p>Define 2. (Coupling of Markov chains) Consider two Markov chains<span class="math inline">\(x^{t} \sim P_{x}^{t}\)</span>, <span class="math inline">\(y^{t} \sim P_{y}^{t}\)</span>, <span class="math inline">\(x^{t}\)</span> and <span class="math inline">\(y^{t}\)</span> share the same transitionprobability matrix <span class="math inline">\(P\)</span> but start fromdifferent states. If <span class="math display">\[P(y^{t+1} | x^{t}, y^{t}) = P(y^{t+1} | y^{t})\]</span> and <span class="math display">\[P(x^{t+1} | x^{t}, y^{t}) = P(x^{t+1} | x^{t})\]</span> then we say <span class="math inline">\(x^{t}\)</span> and<span class="math inline">\(y^{t}\)</span> are coupled.</p></blockquote><p>Define <span class="math inline">\(w^{t} := w^{t}(x^t, y^t)\)</span>is a coupling of <span class="math inline">\(P_{x}^{t},P_{y}^{t}\)</span>, <span class="math inline">\(x^t \sim P_x^t, y \simP_{y}^{t}\)</span>, and <span class="math inline">\(w^{t}\)</span>satisfies Lemma 2(b).</p><p><span class="math display">\[P_{w^{t}}(x^{t+1} \ne y^{t+1}) = P_{w^{t}}(x^{t+1} \ne y^{t+1} | x^t \ney^t) p(x^t \ne y^t) + P_{w^{t}}(x^{t+1} \ne y^{t+1} | x^t = y^t) p(x^t =y^t)\]</span> If <span class="math inline">\(x^t = y^t\)</span>, accordingto Lemma 3, <span class="math inline">\(P_{w^{t}}(x^{t+1} \ne y^{t+1}) =P_{w^{t}}(x^{t+1} = y^{t+1} | x^t = y^t) p(x^t = y^t)  \leqP_{w^{t}}(x^{t} \ne y^t)\)</span>;</p><p>If <span class="math inline">\(x^t \ne y^t\)</span>, <span class="math inline">\(P_{w^{t}}(x^{t+1} \ne y^{t+1}) = P_{w^{t}}(x^{t+1}\ne y^{t+1} | x^t \ne y^t) p(x^t \ne y^t) \leq P_{w^{t}}(x^{t} \ney^t)\)</span>.</p><p><span class="math inline">\(d_x(t+1) = d_{TV}(P_{x}^{t+1},P_y^{t+1}） \leq P_{w^{t}}(x^{t+1} \ne y^{t+1}) \leq P_{w^{t}}(x^{t} \ney^t) = d_{TV}(P_x^t, P_y^{t}) = d_x(t)\)</span></p><p><span class="math inline">\(d_{x}(t) := d_{TV}(P_{x}^{t},\pi)\)</span>.</p><p><span class="math inline">\(d(t) = \max_{x \in \Omega}d_{x}(t)\)</span>.</p><p><span class="math inline">\(d(t) \leq \bar{d}(t) \leq2d(t)\)</span>.</p><p><span class="math inline">\(\bar{d}(t) := \max_{x, y \in \Omega}d_{TV}(P_{x}^{t}, P_{y}^{t})\)</span></p><p>???????????? Q: if we consider <span class="math inline">\(d_{TV}(P_x^t, \pi)\)</span>, then the coupling<span class="math inline">\(w^{t} := w^{t}(x^t, y)\)</span> should be acoupling of <span class="math inline">\(P_{x}^{t}, \pi\)</span>, <span class="math inline">\(x^t \sim P_x^t, y \sim \pi\)</span>.</p><p><span class="math inline">\(\pi\)</span> is different from <span class="math inline">\(P_y^t\)</span> at first.</p><p><span class="math display">\[d_x(t) = d_{TV}(P_{x}^{t}, \pi) \leq d(t) \leq \bar{d}(t) = \max_{x, y\in \Omega} d_{TV}(P_{x}^{t}, P_{y}^{t}) = P_{w^t}(x^t \ne y^t)\]</span></p><p>Now, we have already proved that <span class="math inline">\(d(t)\)</span> is nonincreasing. Next, we willprove that <span class="math inline">\(d(t)\)</span> converges to 0.</p><h1 id="useful-lectures">Useful Lectures:</h1><ul><li><a href="https://people.eecs.berkeley.edu/~sinclair/cs294/n7.pdf">L1</a></li><li><a href="https://courses.cs.duke.edu/spring13/compsci590.2/slides/lec5.pdf">MarkovChains and Coupling</a></li><li><a href="https://faculty.cc.gatech.edu/~vigoda/MCMC_Course/MC-basics.pdf">MarkovChains, Coupling, Stationary Distribution</a></li><li><a href="https://mpaldridge.github.io/math2750/S10-stationary-distributions.html">Stationarydistributions</a></li><li><a href="https://mpaldridge.github.io/math2750/S11-long-term-chains.html">Long-termbehaviour of Markov chains</a></li></ul>]]></content>
      
      
      <categories>
          
          <category> Stochastic Process </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Markov Chains </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Metropolis-Hastings</title>
      <link href="/2023/07/15/Metropolis-Hastings/"/>
      <url>/2023/07/15/Metropolis-Hastings/</url>
      
        <content type="html"><![CDATA[<h1 id="introduction">Introduction</h1><p>The first MCMC algorithm is the Metropolis algorithm, published byMetropolis et al. (1953). It was generalized by Hastings (1970) andPeskun (1973, 1981) towards statistical applications. After a long time,it was rediscovered by Geman and Geman (1984), Tanner and Wong (1987),and Gelfand and Smith (1990).</p><p>Assume the probability density <span class="math inline">\(\pi\)</span> is the target distribution and <span class="math inline">\(q(\cdot | \cdot)\)</span> is the proposaltransition distribution. From an initial state <span class="math inline">\(X_0\)</span>, the Metropolis-Hastings algorithmaims to generate a Markov chain <span class="math inline">\(\{X_1, X_2,...\}\)</span>, such that <span class="math inline">\(X_t\)</span>converges to distribution <span class="math inline">\(\pi\)</span>.</p><h1 id="metropolis-hastings-algorithm">Metropolis-Hastingsalgorithm</h1><p>Input: initial value <span class="math inline">\(X_0\)</span>,transition <span class="math inline">\(q(\cdot | \cdot)\)</span>, numberof iterations <span class="math inline">\(T\)</span>.</p><p>Output: Markov chain <span class="math inline">\(\{X_1, X_2, ...,X_T\}\)</span>.</p><p>For <span class="math inline">\(t = 1, 2, ..., T\)</span>:</p><ol type="1"><li>Generate <span class="math inline">\(u\)</span> from uniformdistribution <span class="math inline">\(U(0, I)\)</span>.</li><li>Generate <span class="math inline">\(X\)</span> from <span class="math inline">\(q(X | X_{t-1})\)</span>.</li><li>Compute the acceptance probability <span class="math inline">\(A(X,X_{t-1}) = \min\{1, \frac{\pi(X)q(X_{t-1} | X)}{\pi(X_{t-1})q(X |X_{t-1})}\}\)</span>.</li><li>if <span class="math inline">\(u \leq A(X, X_{t-1})\)</span>, then<span class="math inline">\(X_t = X\)</span>; else <span class="math inline">\(X_t = X_{t-1}\)</span>.</li></ol><p>Now, we prove that <span class="math inline">\(\pi\)</span> is one ofthe stationary distribution of the generated Markov chain.</p><p>Proof: Recall the detailed balance equation, for any <span class="math inline">\(i, j \in \Omega\)</span>, we have <span class="math display">\[\pi_i P_{i,j} = \pi_j P_{j,i},\]</span> then <span class="math inline">\(\pi\)</span> is a stationarydistribution of the Markov chain.</p><p>For the Metropolis-Hastings algorithm, we have the transitionprobability <span class="math display">\[K(X_t | X_{t-1}) = q(X_t| X_{t-1}) A(X_t, X_{t-1}) + \delta(X_t =X_{t-1}) (1 - \sum_{X \in \Omega} q(X|X_{t-1}) A(X, X_{t-1})).\]</span></p><p>Now we need to prove <span class="math display">\[\pi(X_{t-1})K(X_t | X_{t-1}) = \pi(X_t)K(X_{t-1} | X_t).\]</span></p><p>If <span class="math inline">\(X_t = X_{t-1}\)</span>, then theequation holds. If <span class="math inline">\(X_t \neqX_{t-1}\)</span>, then <span class="math display">\[\pi(X_{t-1})K(X_t | X_{t-1}) = \pi(X_{t-1})q(X_t| X_{t-1}) A(X_t,X_{t-1}) = \min\{\pi(X_{t-1})q(X_t| X_{t-1}), \pi(X_t)q(X_{t-1} |X_t)\}.\]</span> and <span class="math display">\[\pi(X_{t})K(X_{t-1} | X_{t}) = \pi(X_{t})q(X_{t-1}| X_{t}) A(X_{t-1},X_{t}) = \min\{\pi(X_{t})q(X_{t-1}| X_{t}), \pi(X_{t-1})q(X_{t} |X_{t-1})\}.\]</span> So, <span class="math inline">\(\pi(X_{t-1})K(X_t | X_{t-1}) =\pi(X_{t})K(X_{t-1} | X_{t})\)</span>. <span class="math inline">\(\pi\)</span> is a stationary distribution of theMarkov chain.</p><hr><p>Remark 1. The detailed balance condition is a sufficient but notnecessary condition for <span class="math inline">\(\pi\)</span> to be astationary distribution of the Markov chain. If we want to prove <span class="math inline">\(\pi\)</span> is the unique stationary distributionof the Markov chain, we need to prove the Markov chain is<strong>irreducible and positive recurrent</strong>. 【？？？】</p><p>Remark 2. The first initial state <span class="math inline">\(X_0\)</span> is randomly generated and usuallyremoved from the sample as burn-in or warm-up.</p><p>Q: <strong>Since it is recurrent, it must return to the initialvalues. Will this initial be rejected with a highprobability?</strong></p><p>Remark 3. In practice, the performances of the algorithm areobviously highly dependent on the choice of the transition <span class="math inline">\(q(\cdot | \cdot)\)</span>, since some choices seethe chain unable to converge in a manageable time.</p><p>Remark 4. We need to able to evaluate a function <span class="math inline">\(p(x) \propto \pi(x)\)</span>. Since we only needto compute the ratio <span class="math inline">\(\pi(y)/\pi(x)\)</span>,the proportionality constant is irrelevant. Similarly, we only careabout <span class="math inline">\(q(\cdot | \cdot)\)</span> up to aconstant</p><h1 id="reference">Reference</h1><ul><li>C.P. Robert. (2016). The Metropolis-Hastings algorithm.</li></ul>]]></content>
      
      
      <categories>
          
          <category> Stochastic Process </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Markov Chains </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Discrete Markov Chains</title>
      <link href="/2023/07/12/Markov-Chains/"/>
      <url>/2023/07/12/Markov-Chains/</url>
      
        <content type="html"><![CDATA[<h1 id="markov-chains-definitions-and-representations">Markov Chains:Definitions and Representations</h1><p>A stochastic process <span class="math inline">\(X = \{ x(t): t\inT\}\)</span> is a collection of random variables.</p><p>There are two elements:</p><ul><li>Time <span class="math inline">\(t\)</span>:<ul><li>discrete time (<span class="math inline">\(T\)</span> is a countablyinfinite set; under this case, we call 'Markov chain')</li><li>continuous time (under this case, we call 'Markov process')</li></ul></li><li>Space <span class="math inline">\(\Omega\)</span>:<ul><li>discrete space (<span class="math inline">\(X_{t}\)</span> comesfrom a countably infinite set)</li><li>continuous space.</li></ul></li></ul><p>Markov chain is a <strong>discrete-time</strong> process for whichthe future behaviour, given the past and the present, only depends onthe present and not on the past.</p><p>Markov process is the <strong>continuous-time</strong> version of aMarkov chain.</p><blockquote><p>Definition 1.[Markov chain] A discrete time stochastic process $ X_0,X_1, X_2, <span class="math inline">\(. . . is a Markov chainif\)</span>$ P(X_{t} = a_t | X_{t-1} = a_{t-1}, X_{t-2} = a_{t-2}, ...,X_0 = a_0) = P(X_{t} = a_t | X_{t-1} = a_{t-1}) = P_{a_{t-1}, a_{t}}$$</p></blockquote><p>Remark 1: This is time-homogeneous markov chain, for <span class="math inline">\(\forall t\)</span>, for <span class="math inline">\(\forall a_{t-1}, a_{t} \in \Omega\)</span>, thetransition probability <span class="math inline">\(P_{a_{t-1}, a_{t}}\)</span> is the same.</p><p>Remark 2: In DDPM, it is not a time-homogeneous chain, as thetransition probability at t is obtained by a network(t).</p><p>The state <span class="math inline">\(X_{t}\)</span> depends on theprevious state <span class="math inline">\(X_{t-1}\)</span> but isindependent of the particular history <span class="math inline">\(X_{t-2}, X_{t-3},...\)</span>. This is called the<strong>Markov property</strong> or <strong>memorylessproperty</strong>.</p><p>The Markov property does not imply that <span class="math inline">\(X_{t}\)</span> is independent of the randomvariables <span class="math inline">\(X_{0}\)</span>, <span class="math inline">\(X_{1}\)</span>,..., <span class="math inline">\(X_{t-2}\)</span>; it just implies that <strong>anydependency of <span class="math inline">\(X_{t}\)</span> on the past iscaptured in the value of <span class="math inline">\(X_{t-1}\)</span></strong>.</p><p>The Markov chain is <strong>uniquely</strong> defined by the one-steptransition probability matrix P: <span class="math display">\[P =\begin{pmatrix}P_{0,0} &amp; P_{0, 1} &amp; \cdots &amp; P_{0, j} &amp; \cdots\\\vdots &amp; \vdots &amp; \ddots &amp; \vdots&amp; \vdots\\P_{i,0} &amp; P_{i, 1} &amp; \cdots &amp; P_{i, j} &amp; \cdots\\\vdots &amp; \vdots &amp; \ddots &amp; \vdots&amp; \vdots\\\end{pmatrix}\]</span> where <span class="math inline">\(P_{i,j}\)</span> is theprobability of transition from state <span class="math inline">\(i\)</span> to state <span class="math inline">\(j\)</span>. <span class="math inline">\(P_{i,j} =P(X_{t} = j| X_{t-1} = i), i,j \in \Omega\)</span>. For <span class="math inline">\(i\)</span>, <span class="math inline">\(\sum_{j\geq 0} P_{i,j} = 1\)</span>.</p><h1 id="classification-of-states">Classification of States</h1><p>For simplicity, we assume that the state space <span class="math inline">\(\Omega\)</span> is finite. ## Communicatingclass</p><blockquote><p>Definition 2. [Communicating class] A state <span class="math inline">\(j\)</span> is reachable from state <span class="math inline">\(i\)</span> if there exists a positive integer<span class="math inline">\(n\)</span> such that <span class="math inline">\(P_{i,j}^{(n)} &gt; 0\)</span>. We write <span class="math inline">\(i \rightarrow j\)</span>. If <span class="math inline">\(j\)</span> is reachable from <span class="math inline">\(i\)</span>, and <span class="math inline">\(i\)</span> is reachable from <span class="math inline">\(j\)</span>, then the states <span class="math inline">\(i\)</span> and <span class="math inline">\(j\)</span> are said to<strong>communicate</strong>, denoted by <span class="math inline">\(i\leftrightarrow j\)</span>. A communicating class <span class="math inline">\(C\)</span> is a <strong>maximal</strong> set ofstates that communicate with each other. <strong>No state in <span class="math inline">\(C\)</span> communicates with any state not in<span class="math inline">\(C\)</span>.</strong></p></blockquote><h2 id="irreducible">Irreducible</h2><blockquote><p>Definition 3: A Markov chain is <strong>irreducible</strong> if allstates belong to <strong>one</strong> communicating class.</p></blockquote><p>This means that <strong>any state can be reached from any otherstate</strong>. For <span class="math inline">\(\forall i, j \in\Omega\)</span>, <span class="math inline">\(P_{i,j} &gt;0\)</span>.</p><blockquote><p>Lemma 1. A finite Markov chain is irreducible if and only if itsgraph representation is a strongly connected graph.</p></blockquote><h3 id="transient-vs-recurrent-states">Transient vs Recurrentstates</h3><p>Let <span class="math inline">\(r_{i,j}^{t}\)</span> denote theprobability that the chain, starting at state <span class="math inline">\(i\)</span>, <strong>the first time</strong>transition to state <span class="math inline">\(j\)</span> occurs attime <span class="math inline">\(t\)</span>. That is, <span class="math display">\[r_{i,j}^{t} = P(X_{t} = j, X_{s} \neq j, \forall 1 \leq s \leq t-1 |X_{0} = i)\]</span></p><blockquote><p>Definition 4. A state is <strong>recurrent</strong> if <span class="math inline">\(\sum_{t \geq 1} r_{i,i}^{t} = 1\)</span> and it is<strong>transient</strong> if <span class="math inline">\(\sum_{t \geq1} r_{i,i}^{t} &lt; 1\)</span>. A Markov chain is recurrent if everystate in the chain is recurrent.</p></blockquote><ul><li><p>If state i is recurrent then, once the chain visits that state,it will (with probability 1) eventually return to that state. Hence thechain will visit state <span class="math inline">\(i\)</span> over andover again, <strong>infinitely</strong> often.</p></li><li><p>A transient state has the property that a Markov chain startingat this state returns to this state only <strong>finitelyoften</strong>, with probability 1.</p></li><li><p>If one state in a communicating class is transient (respectively,recurrent) then all states in that class are transient (respectively,recurrent).</p></li></ul><blockquote><p>Definition 5. An irreducible Markov chain is called recurrent if atleast one (equivalently, every) state in this chain is recurrent. Anirreducible Markov chain is called transient if at least one(equivalently, every) state in this chain is transient.</p></blockquote><p>Let <span class="math inline">\(\mu_{i} = \sum_{t \geq 1} t \cdotr_{i,i}^{t}\)</span> denote the expected time to return to state <span class="math inline">\(i\)</span> when starting at state <span class="math inline">\(i\)</span>.</p><blockquote><p>Definition 6. A state <span class="math inline">\(i\)</span> is<strong>positive recurrent</strong> if <span class="math inline">\(\mu_{i} &lt; \infty\)</span> and <strong>nullrecurrent</strong> if <span class="math inline">\(\mu_{i} =\infty\)</span>.</p></blockquote><p>Here we give an example of a Markov chain that has null recurrentstates. Consider the following markov chain whose states are thepositive integers.</p><figure><img src="/2023/07/12/Markov-Chains/image.png" alt="Fig. 1. An example of a Markov chain that has null recurrent states"><figcaption aria-hidden="true">Fig. 1. An example of a Markov chain thathas null recurrent states</figcaption></figure><p>Starting at state 1, the probability of not having returned to state1 within the first <span class="math inline">\(t\)</span> steps is <span class="math display">\[\prod_{j=1}^{t} \frac{j}{j+1} = \frac{1}{t+1}.\]</span> The probability of never returning to state 1 from state 1 is0, and state 1 is recurrent. Thus, the probability of the first timetransition to state <span class="math inline">\(1\)</span> occurs attime <span class="math inline">\(t\)</span> is <span class="math display">\[r_{1,1}^{t} = \frac{1}{t} \cdot \frac{1}{t+1} = \frac{1}{t(t+1)}.\]</span> The expected number of steps until the first return to state 1when starting at state 1 is <span class="math display">\[\mu_{1} = \sum_{t = 1}^{\infty} t \cdot r_{1,1}^{t} = \sum_{t =1}^{\infty} \frac{1}{t+1} = \infty.\]</span> State 1 is recurrent but null recurrent.</p><blockquote><p>Lemma 2. In a finite Markov chain: 1. at least one state isrecurrent; and 2. all recurrent states are positive recurrent.</p></blockquote><p>Thus, all states of a finite, irreducible Markov chain are positiverecurrent.</p><h3 id="periodic-vs-aperiodic-states">Periodic vs Aperiodic states</h3><blockquote><p>Definition 7. A state <span class="math inline">\(j\)</span> in adiscrete time Markov chain is <strong>periodic</strong> if there existsan integer <span class="math inline">\(k&gt;1\)</span> such that <span class="math inline">\(P(X_{t+s}= j | X_t = j) = 0\)</span> unless <span class="math inline">\(s\)</span> is divisible by <span class="math inline">\(k\)</span>. A discrete time Markov chain isperiodic if any state in the chain is periodic. A state or chain that isnot periodic is <strong>aperiodic</strong>.</p></blockquote><p>A state <span class="math inline">\(i\)</span> is periodic means thatfor <span class="math inline">\(s = k, 2k, 3k,...\)</span>, <span class="math inline">\(P(X_{t+s}= j | X_t = j) &gt; 0\)</span>.</p><p><strong>NB: k &gt; 1</strong></p><h3 id="ergodic">Ergodic</h3><blockquote><p>Definition 8. An <strong>aperiodic</strong>, <strong>positiverecurrent</strong> state is an <strong>ergodic</strong> state. A Markovchain is ergodic if all its states are ergodic.</p></blockquote><blockquote><p>Corollary 1. Any finite, irreducible, and aperiodic Markov chain isan ergodic chain.</p></blockquote><h3 id="stationary-distribution">Stationary distribution</h3><p>Consider the two-state “broken printer” Markov chain:</p><figure><img src="/2023/07/12/Markov-Chains/2023-07-22-11-00-52.png" alt="Transition diagram for the two-state broken printer chain"><figcaption aria-hidden="true">Transition diagram for the two-statebroken printer chain</figcaption></figure><p>There are two state (0 and 1) in this Markov chain, and assume thatthe initial distribution is <span class="math display">\[P(X_0 = 0) = \frac{\beta}{\alpha+\beta}, \qquad P(X_0 = 1) =\frac{\alpha}{\alpha+\beta}.\]</span> Then, according to the transition probability matrix <span class="math inline">\(P\)</span>, after one step, the distribution is<span class="math display">\[\begin{align*}P(X_1 = 0) &amp;= P(X_0 = 0)P(X_1 = 0 | X_0 = 0) + P(X_0 = 1)P(X_1 = 0 |X_0 = 1) \\&amp;= \frac{\beta}{\alpha+\beta} \cdot (1-\alpha) +\frac{\alpha}{\alpha+\beta} \cdot \beta = \frac{\beta}{\alpha+\beta}, \\P(X_1 = 1) &amp;= P(X_0 = 0)P(X_1 = 1 | X_0 = 0) + P(X_0 = 1)P(X_1 = 1 |X_0 = 1) \\&amp;= \frac{\beta}{\alpha+\beta} \cdot \alpha +\frac{\alpha}{\alpha+\beta} \cdot (1-\beta) =\frac{\alpha}{\alpha+\beta}.\end{align*}\]</span> Apparently, the distribution of <span class="math inline">\(X_1\)</span> is the same as the initialdistribution. Similarly, we can prove that the distribution of <span class="math inline">\(X_t\)</span> is the same as the initialdistribution for any <span class="math inline">\(t\)</span>. Here, <span class="math inline">\(\pi = (\frac{\beta}{\alpha+\beta},\frac{\alpha}{\alpha+\beta})\)</span> is called <strong>stationarydistribution</strong>.</p><blockquote><p>Definition 9. A probability distribution <span class="math inline">\(\pi = (\pi_i)\)</span>, <span class="math inline">\(\sum_{i \in \Omega} \pi_i = 1\)</span>(<strong>rowvector</strong>) on the state space <span class="math inline">\(\Omega\)</span> is called a <strong>stationarydistribution</strong> (or an equilibrium distribution) for the Markovchain with transition probability matrix <span class="math inline">\(P\)</span> if <span class="math inline">\(\pi =\pi P\)</span>, equivalently, <span class="math inline">\(\pi_j =\sum_{i \in \Omega}\pi_i P_{i,j}\)</span> for all <span class="math inline">\(j \in \Omega\)</span>.</p></blockquote><ul><li><p>One interpretation of the stationary distribution: if we startedoff a <strong>thousand</strong> Markov chains, choosing each startingposition to be state <span class="math inline">\(i\)</span> withprobability <span class="math inline">\(\pi_i\)</span>, then(roughly)<strong><span class="math inline">\(1000 \pi_j\)</span></strong> of themwould be in state <span class="math inline">\(j\)</span> at any time inthe future – but not necessarily the same ones each time.</p></li><li><p>If a chain ever reaches a stationary distribution then itmaintains that distribution for all future time, and thus a stationarydistribution represents a steady state or an equilibrium in the chain’sbehavior.</p></li></ul><h4 id="finding-a-stationary-distribution">Finding a stationarydistribution</h4><p>Consider the following no-claims discount Markov chain with statespace <span class="math inline">\(\Omega = \{1,2,3\}\)</span> andtransition matrix <span class="math display">\[P =\begin{pmatrix}\frac{1}{4} &amp; \frac{3}{4} &amp; 0\\\frac{1}{4} &amp; 0 &amp; \frac{3}{4}\\0 &amp; \frac{1}{4} &amp; \frac{3}{4}\end{pmatrix}\]</span></p><ul><li><p>Step 1: Assume $= {_1, _2, _3} $ is a stationary distribution.According to the definition 9 of stationary distribution, we need tosolve the following equations: <span class="math display">\[\begin{align*}\pi_1 &amp;= \frac{1}{4}\pi_1 + \frac{1}{4}\pi_2, \\\pi_2 &amp;= \frac{3}{4}\pi_1 + \frac{1}{4}\pi_3, \\\pi_3 &amp;= \frac{3}{4}\pi_2 + \frac{3}{4}\pi_3.\end{align*}\]</span> Adding the normalising condition <span class="math inline">\(\pi_1 + \pi_2 + \pi_3 = 1\)</span>, we get fourequations in three unknown parameters.</p></li><li><p>Step 2: Choose one of the parameters, say <span class="math inline">\(\pi_1\)</span>, and solve for the other twoparameters in terms of <span class="math inline">\(\pi_1\)</span>. Weget <span class="math display">\[\pi_1 = \frac{1}{4}\pi_1 + \frac{1}{4}\pi_2 \Rightarrow \pi_2 = 3\pi_1,\qquad \pi_3 = 3\pi_2 = 9\pi_1.\]</span></p></li><li><p>Step 3: Combining with the normalising condition, we get <span class="math display">\[\pi_1 + 3\pi_1 + 9\pi_1 = 1 \Rightarrow \pi_1 = \frac{1}{13}, \qquad\pi_2 = \frac{3}{13}, \qquad \pi_3 = \frac{9}{13}.\]</span> Finally, we get the stationary distribution <span class="math inline">\(\pi = (\frac{1}{13}, \frac{3}{13},\frac{9}{13})\)</span>.</p></li></ul><h4 id="existence-and-uniqueness">Existence and uniqueness</h4><p>Given a Markov chaine, how can we know whether it has a stationarydistribution? If it has, is it unique? At this part, we will answerthese questions.</p><p>Some notations: - Hitting time to hit the state <span class="math inline">\(j\)</span>: <span class="math inline">\(H_{j} =\min \{ t \in \{0, 1, 2,...\}: X_t = j\}\)</span>. Note that here weinclude time <span class="math inline">\(t = 0\)</span>.</p><ul><li>Hitting probability to hit the state <span class="math inline">\(j\)</span> staring from state <span class="math inline">\(i\)</span>: <span class="math inline">\(h_{i,j} =P(X_t = j, \text{for some} \ t \geq 0 | X_0 = i) = P(H_{j} &lt; \infty |X_0 = i) = \sum_{t \geq 0} r_{i,j}^{t}\)</span>.</li></ul><p>Note that this is different from <span class="math inline">\(r_{i,j}^{t}\)</span>, which denotes theprobability that the chain, starting at state <span class="math inline">\(i\)</span>, the <strong>first</strong> timetransition to state <span class="math inline">\(j\)</span><strong>occurs at time <span class="math inline">\(t\)</span></strong>.</p><p>We also have <span class="math display">\[h_{i,j} =\begin{cases}\sum_{k \in \Omega}P_{i,k}h_{k,j} &amp; , &amp; \text{if} \quad j \ne i,\\1 &amp; , &amp; \text{if} \quad  j = i.\end{cases}\]</span> - Expected hitting time: <span class="math inline">\(\eta_{i,j} = E(H_{j} | X_0 = i) = \sum_{t \geq 0}t \cdot r_{i,j}^{t}\)</span>. The expected time until we hit state <span class="math inline">\(j\)</span> starting from state <span class="math inline">\(i\)</span>. We also have <span class="math display">\[\eta_{i,j} =\begin{cases}1 + \sum_{k \in \Omega}P_{i,k}\eta_{k,j} &amp; , &amp; if j \ne i, \\0 &amp; , &amp; if j = i.\end{cases}\]</span> (For the first case, we add 1 because we need to consider thefirst step from state <span class="math inline">\(i\)</span> to state<span class="math inline">\(k\)</span>.)</p><ul><li>Return time: <span class="math inline">\(M_i = \min \{ t \in \{1,2,...\}: X_t = i\}\)</span>. It is different from <span class="math inline">\(H_{i}\)</span>, as we exclude time <span class="math inline">\(t = 0\)</span>. It is the first time that thechain returns to state <span class="math inline">\(i\)</span> after<span class="math inline">\(t = 0\)</span>.</li><li>Return probability: <span class="math inline">\(m_{i} = P(X_t = i  \\text{for some} \ n \geq 1 | X_0 = i) = P(M_i &lt; \infty | X_0 = i) =\sum_{t&gt;1}r_{i,i}^{t}.\)</span></li><li>Expected return time: <span class="math inline">\(\mu_{i} = E(M_i |X_0 = i) = \sum_{t \geq 1} t \cdot r_{i,i}^{t}\)</span>. The expectedtime until we return to state <span class="math inline">\(i\)</span>starting from state <span class="math inline">\(i\)</span>. <span class="math display">\[m_{i} = \sum_{j \in \Omega} P_{i,j}h_{j,i},  \qquad \mu_{i} = 1 +\sum_{j \in \Omega} P_{i,j}\eta_{j,i}.\]</span></li></ul><table style="width:24%;"><colgroup><col style="width: 23%"></colgroup><tbody><tr><td>&gt; Theorem 1. Consider an irreducible Markov chain (<strong>finiteor infinite</strong>), &gt; (1) if it is <strong>positiverecurrent</strong>, <span class="math inline">\(\exists\)</span> anunique stationary distribution <span class="math inline">\(\pi\)</span>,such that <span class="math inline">\(\pi_i =\frac{1}{\mu_{i}}\)</span>. &gt; (2) if it is <strong>nullrecurrent</strong> or <strong>transient</strong>, no stationarydistribution exists.</td></tr><tr><td>Remark: If the chain is <strong>finite</strong> irreducible, it mustbe positive recurrent, thus it has an unique stationarydistribution.</td></tr><tr><td>Remark: If the Markov chain is not irreducible, we can decompose thestate space into several communicating classes. Then, we can considereach communicating class separately. - If none of the classes arepositive recurrent, then no stationary distribution exists. - If exactlyone of the classes is positive recurrent (and therefore closed), thenthere exists a unique stationary distribution, supported only on thatclosed class. - If more the one of the classes are positive recurrent,then many stationary distributions will exist.</td></tr><tr><td>Now, we give the proof of Theorem 1. We first prove that if a Markovchain is irreducible and positive recurrent, then there<strong>exists</strong> a stationary distribution. Next, we will provethe stationary distribution is <strong>unique</strong>. Since the secondpart with the null recurrent or transitive Markov chains is lessimportant and more complicated, we will omit it. If you are interestedin it, you can refer to the book <a href="https://www.statslab.cam.ac.uk/~james/Markov/">Markov Chains</a>by James Norris.</td></tr><tr><td>Proof. (1) Suppose that <span class="math inline">\((X_0, X_1...)\)</span> a recurrent Markov chain, which can be positive recurrentor null recurrent. Then we can desigh a stationary distribution asfollows. (If we can desigh a stationary distribution, then it must beexisted.)</td></tr><tr><td>Let <span class="math inline">\(\nu_i\)</span> be the expectednumber of visits to <span class="math inline">\(i\)</span> before wereturn back to <span class="math inline">\(k\)</span>, <span class="math display">\[\begin{align*}\nu_i &amp;= \mathbb{E}(\# \text{visits to $i$ before returning to } k |X_0 = k) \\&amp;= \mathbb{E}\sum_{t=1}^{M_k} P(X_t = i | X_0 = k) \\&amp;= \mathbb{E}\sum_{t = 0}^{M_k - 1} P(X_t = i | X_0 = k)\end{align*}\]</span> The last equation holds because of $ P(X_0 = i | X_0 = k) = 0$and $ P(X_{M_k} = i | X_0 = k) = 0$.</td></tr><tr><td>If we want design a stationary distribution, it must statisfy <span class="math inline">\(\pi P = \pi\)</span> and <span class="math inline">\(\sum_{i \in \Omega}\pi_i = 1\)</span>.</td></tr><tr><td>(a) We first prove that <span class="math inline">\(\nu P =\nu\)</span>. <span class="math display">\[\begin{align*}\sum_{i \in \Omega} \nu_i P_{i,j} &amp;= \mathbb{E}\sum_{i \in \Omega}\sum_{t = 0}^{M_k - 1} P(X_t = i, X_{t+1} = j | X_0 = k) \\&amp;= \mathbb{E}\sum_{t = 0}^{M_k - 1}  \sum_{i \in \Omega}  P(X_t = i,X_{t+1} = j | X_0 = k) \\&amp;=  \mathbb{E} \sum_{t = 0}^{M_k - 1} P(X_{t+1} = j | X_0 = k) \\&amp;= \mathbb{E} \sum_{t = 1}^{M_k } P(X_{t} = j | X_0 = k) \\&amp;= \mathbb{E} \sum_{t = 0}^{M_k - 1} \nu_i \\&amp;= \nu_j.\end{align*}\]</span> (b) Next, what we need to do is to normalize <span class="math inline">\(\nu\)</span> to get a stationary distribution. Wehave <span class="math display">\[\sum_{i \in \Omega} \nu_i = \sum_{i \in \Omega} \mathbb{E} \sum_{t =0}^{M_k - 1} P(X_t = i | X_0 = k) =\mathbb{E} \sum_{t = 0}^{M_k -1}  \sum_{i \in \Omega}  P(X_t = i | X_0 = k) = E(M_k | X_0 = i) =\mu_k.\]</span> Thus, we can define <span class="math inline">\(\pi_i =\nu_i/\mu_k\)</span>, <span class="math inline">\(\pi = \{\pi_i, i \in\Omega\}\)</span> is one of the stationary distribution.</td></tr><tr><td>(2) Next, we prove that if a Markov chain is irreducible andpositive recurrent, then the stationary distribution is<strong>unique</strong> and is given by <span class="math inline">\(\pi_j = \frac{1}{\mu_j}\)</span>.</td></tr><tr><td>Given a stationary distribution <span class="math inline">\(\pi\)</span>, if we prove that for all <span class="math inline">\(i\)</span>, <span class="math inline">\(\pi_j ==\frac{1}{\mu_j}\)</span>, then we prove that the stationary distributionis unique.</td></tr><tr><td>Remember that the expected hitting time: <span class="math display">\[\eta_{i,j} = 1 + \sum_{k \in \Omega}P_{i,k}\eta_{k,j},  j \ne i  \qquad(eq:1)\]</span> We multiply both sides of (eq:1) by <span class="math inline">\(\pi_i\)</span> and sum over <span class="math inline">\(i (i \ne j)\)</span> to get <span class="math display">\[\sum_{i \ne j} \pi_i \eta_{i,j} = \sum_{i \ne j} \pi_i + \sum_{i \ne j}\sum_{k \in \Omega} \pi_i P_{i,k}\eta_{k,j}\]</span> Since <span class="math inline">\(\eta_{j,j} = 0\)</span>, wecan rewrite the above equation as <span class="math display">\[\sum_{i \in \Omega} \pi_i \eta_{i,j} = \sum_{i \ne j} \pi_i + \sum_{i\ne j} \sum_{k \in \Omega} \pi_i P_{i,k}\eta_{k,j}. \qquad (eq:2)\]</span></td></tr><tr><td>(The above equality lacks <span class="math inline">\(j\)</span>,and we also want to design <span class="math inline">\(\pi_j =1/\mu_j\)</span>.) Remember that the expected return time: <span class="math display">\[ \mu_{j} = 1 + \sum_{i \in \Omega}P_{j,i}\eta_{i,j}. \qquad (eq:3) \]</span> We multiply both sides of(eq:2) by <span class="math inline">\(\pi_j\)</span> to get <span class="math display">\[\pi_j \mu_{j} =\pi_j +  \sum_{k \in \Omega} \pi_j P_{j,k}\eta_{k,j}\qquad (eq:4)\]</span> Adding (eq:2) and (eq:4), we get <span class="math display">\[\begin{align*}\sum_{i \in \Omega} \pi_i \eta_{i,j} + \pi_j \mu_{j} &amp;= \sum_{i \in\Omega} \pi_i + \sum_{i \in \Omega} \sum_{k \in \Omega} \pi_iP_{i,k}\eta_{k,j} \\&amp;= 1 + \sum_{k \in \Omega} \sum_{i \in \Omega}  \pi_iP_{i,k}\eta_{k,j} \\&amp;= 1 +  \sum_{k \in \Omega} \pi_k \eta_{k,j}  \qquad (\text{since}\sum_{i \in \Omega} \pi_i P_{i,k} = \pi_k) \\\end{align*}\]</span> <strong>Since the Markov chain is irreducible and positiverecurrent, that means all states belong to a communication class and theexpected return time of each state is finite. Thus, the space <span class="math inline">\(\Omega\)</span> is a finite dimensionalspace.</strong> We can substract <span class="math inline">\(\sum_{k \in\Omega} \pi_k \eta_{k,j}\)</span> and $_{i } <em>i </em>{i,j} $ (equal)from both sides of the above equation to get <span class="math display">\[\pi_j \mu_{j}=1,\]</span> which means <span class="math inline">\(\pi_j =1/\mu_j\)</span>. Similarly, we can prove that <span class="math inline">\(\pi_i = 1/\mu_i\)</span> for all <span class="math inline">\(i \in \Omega\)</span>.</td></tr></tbody></table><blockquote><p>Theorem 2 (Limit theorem) Consider an irreducible, aperiodic Markovchain (maybe infinite), we have <span class="math inline">\(\lim\limits_{t \to \infty} P_{i,j}^{t} =\frac{1}{\mu_{j}}\)</span>. Spectially, (1) Suppose the Markov chain ispositive recurrent. Then <span class="math inline">\(\lim\limits_{t \to\infty} P_{i,j}^{t} = \pi_j = \frac{1}{\mu_{j}}\)</span>. (2) Supposethe Markov chain is null recurrent or transient. Then there is no limiteprobability.</p></blockquote><ul><li>Three conditions for convergence to an equilibrium probabilitydistribution: irreducibility, aperiodicity, and positive recurrence. Thelimit probability <span class="math display">\[P =\begin{pmatrix}\pi_1 &amp; \pi_2 &amp; \cdots &amp; \pi_N\\\pi_1 &amp; \pi_2 &amp; \cdots &amp; \pi_N\\\vdots &amp; \vdots &amp; \ddots &amp; \vdots\\\pi_1 &amp; \pi_2 &amp; \cdots &amp; \pi_N\\\end{pmatrix}\]</span> where each row is identical.</li></ul><table style="width:7%;"><colgroup><col style="width: 6%"></colgroup><tbody><tr><td>Define <span class="math inline">\(V_{i,j}^{t} = |\{ n &lt; t | X_n= j\}|\)</span>. <span class="math inline">\(V_{i,j}^{t}\)</span> is thenumber of visits to state <span class="math inline">\(j\)</span> beforetime <span class="math inline">\(t\)</span> starting from state <span class="math inline">\(i\)</span>. Then we can interpret <span class="math inline">\(V_{i,j}^{t}/t\)</span> as the proportion of timeup to time <span class="math inline">\(t\)</span> spent in state <span class="math inline">\(j\)</span>.</td></tr><tr><td>&gt; Theorem 3 [Ergodic theorem] Consider an irreducible Markovchain, we have <span class="math inline">\(\lim\limits_{t \to \infty}V_{i,j}^{t}/t = \frac{1}{\mu_{j}}\)</span> <strong>almostsurely</strong>. Spectially, &gt; (1) Suppose the Markov chain ispositive recurrent. Then <span class="math inline">\(\lim\limits_{t \to\infty}  V_{i,j}^{t}/t = \pi_j = \frac{1}{\mu_{j}}\)</span><strong>almost surely</strong>. &gt; (2) Suppose the Markov chain isnull recurrent or transient. Then $ V_{i,j}^{t}/t $ <strong>almostsurely</strong> for all <span class="math inline">\(j\)</span>.</td></tr><tr><td><strong>almost surely</strong> means that the convergenceprobability of the event is 1.</td></tr></tbody></table><blockquote><p>Theorem 4[Detailed balance condition]. Consider a finite,irreducible, and ergodic Markov chain with transition matrix <span class="math inline">\(P\)</span>. If there are nonnegative numbers <span class="math inline">\(\bar{\pi} = (\pi_0, \pi_1, ..., \pi_n)\)</span>such that <span class="math inline">\(\sum_{i=0}^{n} \pi_i = 1\)</span>and if, for any pair of states <span class="math inline">\(i,j\)</span>, <span class="math display">\[\pi_i P_{i,j} = \pi_{j} P_{j,i},\]</span> then <span class="math inline">\(\bar{\pi}\)</span> is thestationary distribution corresponding to <span class="math inline">\(P\)</span>.</p></blockquote><p>Proof. <span class="math display">\[\sum_{i} \pi_i P_{i,j} = \sum_{i}\pi_{j} P_{j,i} = \pi_{j}\]</span> Thus, <span class="math inline">\(\bar{\pi} =\bar{\pi}P\)</span>. Since this is a finite, irreducible, and ergodicMarkov chain, <span class="math inline">\(\bar{\pi}\)</span> must be theunique stationary distribution of the Markov chain.</p><p>Remark: Theorem 2 is a sufficient but not necessary condition.</p><h2 id="reference">Reference</h2><ul><li>Mitzenmacher, M., &amp; Upfal, E. (2005). Probability and Computing.Cambridge University Press.</li><li><a href="https://mpaldridge.github.io/math2750/S09-recurrence-transience.html">Recurrenceand transience</a></li><li><a href="https://mpaldridge.github.io/math2750/S07-classes.html">Classstructure</a></li><li><a href="https://mpaldridge.github.io/math2750/S10-stationary-distributions.html">Stationarydistributions</a></li><li>Stirzaker, David. <a href="https://www.ctanujit.org/uploads/2/5/3/9/25393293/_elementary_probability.pdf">ElementaryProbability</a></li></ul>]]></content>
      
      
      <categories>
          
          <category> Stochastic Process </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Markov Chains </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Gradient Descent, Stochastic Gradient Descent, Variance Reduction</title>
      <link href="/2021/02/27/GD/"/>
      <url>/2021/02/27/GD/</url>
      
        <content type="html"><![CDATA[<h1 id="svrg2013-accelerating-stochastic-gradient-descent-using-predictive-variance-reduction">[SVRG2013]Accelerating Stochastic Gradient Descent using Predictive VarianceReduction</h1><h2 id="introduction">Introduction</h2><p>考虑如下优化问题 <span class="math display">\[\min P(\omega) = \frac{1}{n}\psi_{i}(\omega)\]</span></p><ul><li>如果采用平方损失，最小二乘回归；</li><li>考虑正则项，令 <span class="math inline">\(\psi_{i}(\omega) = \ln(1+ \exp(-\omega^{T}x_{i}y_{i})) + 0.5\lambda\omega^{T}\omega, y_{i} \in\{-1,1\}\)</span>，regularized logistic regression。</li></ul><p>梯度下降算法更新过程： <span class="math display">\[\omega^{(t)} = \omega^{(t-1)} - \eta_{t}\nabla P(\omega^{(t-1)}) =\omega^{(t-1)} - \frac{\eta_{t}}{n}\sum_{i=1}^{n}\nabla\psi_{i}(\omega^{(t-1)})\]</span></p><p>但是，在每一步，GD都需要计算 <span class="math inline">\(n\)</span>个一阶偏导，计算量大。所以，一个改进就是随机梯度下降SGD：在每一步迭代时，随机从<span class="math inline">\(\{1,...,n\}\)</span> 中抽取 <span class="math inline">\(i_{t}\)</span>，然后 <span class="math display">\[\omega^{(t)} = \omega^{(t-1)} - \eta_{t}\nabla\psi_{i_{t}}(\omega^{(t-1)})\]</span></p><p>期望 <span class="math inline">\(E[\omega^{(t)}|\omega^{(t-1)}]\)</span>同梯度更新的结果一致。 SGD的更一般表达形式为 <span class="math display">\[\omega^{(t)} = \omega^{(t-1)} - \eta_{t}g_{t}(\omega^{(t-1)},  \xi_{t})\]</span></p><p><span class="math inline">\(\xi_{t}\)</span> 为一个依赖于 <span class="math inline">\(\omega^{(t-1)}\)</span> 的随机变量， 并且期望<span class="math inline">\(E[g_{t}(\omega^{(t-1)},  \xi_{t})|\omega^{(t-1)}]= \nabla P(\omega^{(t-1)})\)</span>。</p><p>SGD的优势就是每步迭代只需要计算一个梯度，因此计算成本是GD的 <span class="math inline">\(\frac{1}{n}\)</span>。但是，SGD的一个缺点就是随机性引入了方差：虽然<span class="math inline">\(g_{t}(\omega^{(t-1)},  \xi_{t})\)</span>的期望等于梯度 <span class="math inline">\(\nablaP(\omega^{(t-1)})\)</span>，但是每个<span class="math inline">\(g_{t}(\omega^{(t-1)},  \xi_{t})\)</span>是不同的。方差的出现导致收敛速度变慢。对于SGD，由于随机取样带来的方差，一般都会要求其步长 <span class="math inline">\(\eta_{t} = O(1/t)\)</span>，从而得到一个sub-linear 收敛率 <span class="math inline">\(O(1/t)\)</span>。</p><ul><li>GD: 每步迭代计算慢，收敛快。</li><li>SGD：每步迭代计算快，收敛慢。</li></ul><p>为了改进SGD，一些学者开始设计算法以减少方差，从而可以使用较大的步长<span class="math inline">\(\eta_{t}\)</span>。有一些算法被提出来，比如：SAG(stochasticaveragegradient)，SDCA。但是这两个算法需要存储所有的梯度，一些情况下不太实际。因此作者提出了一个新的算法，该算法不需要存储所有的梯度信息，并且有较快的收敛速度，可以应用于非凸优化问题。</p><h2 id="stochastic-variance-reduced-gradient-svrg">Stochastic VarianceReduced Gradient (SVRG)</h2><p>为了保证收敛，SGD的步长必须衰减到0，从而导致收敛率变慢。需要较小步长的原因就是SGD的方差。作者提出一个解决方案。每进行<span class="math inline">\(m\)</span> 次SGD迭代后，记录当前参数 <span class="math inline">\(\tilde{\omega}\)</span> 以及平均梯度： <span class="math display">\[\tilde{\mu} = \nabla P(\tilde{\omega}) = \frac{1}{n}\sum_{i=1}^{n}\nabla \psi_{i}(\tilde{\omega}).\]</span></p><p>然后接下来的更新为：</p><p><span class="math display">\[\omega^{(t)} = \omega^{(t-1)} - \eta_{t}(\nabla\psi_{i_{t}}(\omega^{(t-1)}) - \nabla \psi_{i_{t}}(\tilde{\omega}) +\tilde{\mu})\]</span></p><p>注意到： <span class="math display">\[E[\omega^{(t)}|\omega^{(t-1)}] = \omega^{(t-1)} - \eta_{t}\nablaP(\omega^{(t-1)})\]</span></p><p>算法如下：</p><p><img src="/2021/02/27/GD/SVRG.png"></p><p>更新步骤中梯度的方差是减小的。当 <span class="math inline">\(\tilde{\omega}\)</span> 和 <span class="math inline">\(\omega^{(t)}\)</span> 收敛到最优参数 <span class="math inline">\(\omega_{*}\)</span>，<span class="math inline">\(\tilde{\mu} \to 0\)</span>， <span class="math inline">\(\nabla\psi_{i}(\tilde{\omega}) \to \nabla\psi_{i}(\omega_{*})\)</span>，有 <span class="math display">\[\nabla\psi_{i}(\omega^{(t-1)}) - \nabla\psi_{i}(\tilde{\omega}) + \tilde{\mu} \to \nabla\psi_{i}(\omega^{(t-1)}) -\nabla\psi_{i}(\omega_{*}) \to 0\]</span></p><p>SVRG的学习率不需要衰减，因此能有较快的收敛速度。作者提到，参数 <span class="math inline">\(m\)</span> 应该 <span class="math inline">\(O(n)\)</span>， 比如对于凸问题：<span class="math inline">\(m = 2n\)</span>，非凸问题：<span class="math inline">\(m = 2n\)</span>。</p><h1 id="saga2015-saga-a-fast-incremental-gradient-method-with-support-for-non-strongly-convex-composite-objectives">[SAGA2015]SAGA: A Fast Incremental Gradient Method With Support for Non-StronglyConvex Composite Objectives</h1><p>考虑最小化函数： <span class="math display">\[f(x) = \frac{1}{n}\sum_{i=1}^{n}f_{i}(x)\]</span></p><p>作者提出了一个叫做SAGA的算法，在目标函数为强凸函数时，SAGA的收敛速度优于SAG和SVRG。算法如下：</p><p><img src="/2021/02/27/GD/SAGA.png"></p><p>本文给出了variance reduction算法的一个解释： <img src="/2021/02/27/GD/SAGA1.png"></p><p>几种算法比较： <img src="/2021/02/27/GD/SAGA2.png"></p><h1 id="variance-reduced-stochastic-gradient-descent-with-neighbors-2015">[VarianceReduced Stochastic Gradient Descent with Neighbors 2015]</h1><h1 id="katyusha-2017-katyusha-the-first-direct-acceleration-of-stochastic-gradient-methods">[Katyusha2017] Katyusha: The First Direct Acceleration of Stochastic GradientMethods</h1><p>Nesterov's momentum通常用于加速梯度下降算法，但是，对于随机梯度下降，Nesterov's momentum可能无法对算法进行加速，即使优化目标为凸函数。因此，针对SGD，作者提出Katyusha 算法，借助于动量Katyusha momentum实现加速SGD。</p><p>考虑如下优化问题：</p><p><span class="math display">\[\min_{x \in \mathbb{R}^{d}} \{F(x) = f(x) + \psi(x) =\frac{1}{n}\sum_{i=1}^{n} f_{i}(x) + \psi(x)\}\]</span> 其中 <span class="math inline">\(f(x) =\frac{1}{n}\sum_{i=1}^{n} f_{i}(x)\)</span> 为凸函数，并且是 <span class="math inline">\(n\)</span> 个凸函数的有限平均。<span class="math inline">\(\psi(x)\)</span>为凸函数，可为近端函数。大多数假设 <span class="math inline">\(\psi(x)\)</span> 为 <span class="math inline">\(\sigma\)</span>-strongly，并且 <span class="math inline">\(f_{i}(x)\)</span> L-smooth。</p><p>作者提出一个可以求解上述优化问题的加速随机梯度下降算法-Katyusha：</p><p><img src="/2021/02/27/GD/ka.png"></p><p>其中，<span class="math inline">\(\tilde{x}\)</span> 为snapshotpoint，每经过 <span class="math inline">\(n\)</span>次迭代更新一次。<span class="math inline">\(\tilde{\nabla}_{k+1}\)</span> 为variance reduction中的梯度形式。<span class="math inline">\(\tau_{1}\)</span>，<span class="math inline">\(\tau_{2} \in[0,1]\)</span> 为两个动量参数，<span class="math inline">\(\alpha = \frac{1}{3\tau_{1}L}\)</span>。</p><h2 id="our-new-technique-katyusha-momentum">Our New Technique –Katyusha Momentum</h2><p>最novel的部分是 <span class="math inline">\(x_{k+1}\)</span>的更新步骤，是 <span class="math inline">\(y_{k}\)</span>, <span class="math inline">\(z_{k}\)</span> 以及 <span class="math inline">\(\tilde{x}\)</span> 的凸组合。理论建议参数 <span class="math inline">\(\tau_{2} = 0.5\)</span>，<span class="math inline">\(\tau_{1} = \min\{\sqrt{n\sigma/L},0.5\}\)</span>。</p><p>对于传统的加速梯度下降算法，<span class="math inline">\(x_{k+1}\)</span> 仅仅是 <span class="math inline">\(y_{k}\)</span> 和 <span class="math inline">\(z_{k}\)</span> 的凸组合，<span class="math inline">\(z_{k}\)</span>起到了一个“动量”的作用，即将历史加权的梯度信息添加到 <span class="math inline">\(y_{k+1}\)</span> 上。比如假设 <span class="math inline">\(\tau_{2} = 0, \tau_{1} = \tau\)</span>, <span class="math inline">\(x_{0} = y_{0} = z_{0}\)</span>，我们可以得到：</p><p><img src="/2021/02/27/GD/ka1.png"></p><p>由于 <span class="math inline">\(\alpha\)</span> 通常大于 <span class="math inline">\(1/3L\)</span>，上述递推过程意味着随着迭代进行，梯度<span class="math inline">\(\tilde{\nabla}_{t}\)</span>的贡献越高。比如，<span class="math inline">\(\tilde{\nabla}_{1}\)</span> 的权重不断增大 (<span class="math inline">\(\frac{1}{3L} &lt; ((1-\tau)\frac{1}{3L} +\tau\alpha) &lt; ((1 - \tau)^{2}\frac{1}{3L} + (1 - (1 -\tau)^{2})\alpha)\)</span>)。这就是一阶加速方法的核心思想。</p><p>在Katyusha算法中，作者认为 <span class="math inline">\(\tilde{x}\)</span> 同等重要，它能保证 <span class="math inline">\(x_{k+1}\)</span> 不要太远离 <span class="math inline">\(\tilde{x}\)</span>。<span class="math inline">\(\tilde{x}\)</span> 的添加可以看作是一个 “negativemomentum”，使 <span class="math inline">\(x_{k+1}\)</span> back to <span class="math inline">\(\tilde{x}\)</span>，抵消一部分前面迭代时的positive momentum”。</p><p>当 <span class="math inline">\(\tau_{1} = \tau_{2} = 0.5\)</span>时，Katyusha同SVRG几乎一致。</p><h1 id="l-svrg-l-katyusha-2019-dont-jump-through-hoops-and-remove-those-loops-svrg-and-katyusha-are-betterwithout-the-outer-loop">[L-SVRG,L-Katyusha 2019] Don’t Jump Through Hoops and Remove Those Loops: SVRGand Katyusha are BetterWithout the Outer Loop</h1><p>SVRG和Katyusha算法的共同关键结构就是两者都包含一个外层循环 (outerloop)。最初先在outerloop上使用所有样本计算梯度，然后计算出来的结果再用于内层循环 (innerloop)，结合新的随机梯度信息，构造variance-reduced梯度估计量。作者指出，由于SVRG和Katyusha算法都包括一个outerloop，所以存在一些问题，比如：算法很难分析；人们需要决定内部循环的次数。对于SVRG，理论上内部循环的最优次数取决于<span class="math inline">\(L\)</span>和 <span class="math inline">\(\mu\)</span>，但是 <span class="math inline">\(\mu\)</span>通常未知。由于这些问题存在，人们只能选择次优的inner loopsize，通常设置内部循环次数为 <span class="math inline">\(O(n)\)</span>或者 <span class="math inline">\(n\)</span>。</p><p>在这篇论文中，作者将外层循环 (outer loop)丢弃，在每次迭代时采用掷硬币技巧决定是否计算梯度，从而解决了上述问题。作者证明，新提出的算法和原始两个算法具有同样的理论性质。</p><p><img src="/2021/02/27/GD/loopless1.jpg"> <img src="/2021/02/27/GD/loopless1.jpg"></p><h1 id="l-svrg-l-katyusha-2019-l-svrg-and-l-katyusha-with-arbitrary-sampling">[L-SVRG,L-Katyusha 2019] L-SVRG and L-Katyusha with Arbitrary Sampling</h1><h1 id="参考文献">参考文献</h1><ul><li>Johnson, R. and Zhang, T. Accelerating stochastic gradient descentusing predictive variance reduction. In Advances in Neural InformationProcessing Systems 26, pp. 315–323, 2013a.<br></li><li>Defazio, A., Bach, F., and Lacoste-Julien, S. SAGA: a fastincremental gradient method with support for non-strongly convexcomposite objectives. In Advances in Neural Information ProcessingSystems, pp. 1646–1654, 2014.<br></li><li>Hofmann, T., Lucchi, A., Lacoste-Julien, S., and McWilliams, B.Variance reduced stochastic gradient descent with neighbors. In Advancesin Neural Information Processing Systems, pp.2305–2313, 2015.<br></li><li>Allen-Zhu, Z. Katyusha: The first direct acceleration of stochasticgradient methods. In Proceedings of the 49th Annual ACM SIGACT Symposiumon Theory of Computing, pp.1200–1205. ACM,2017.</li><li>Kovalev, D., Horváth, S., and Richtárik, P. Don’t jump through hoopsand remove those loops: SVRG and Katyusha are better without the outerloop. In Proceedings of the 31st International Conference on AlgorithmicLearning Theory, 2020.</li><li>Qian, X., Qu, Z., and Richtárik, P. L-SVRG and L-Katyusha witharbitrary sampling. arXiv preprint arXiv:1906.01481, 2019a.</li></ul>]]></content>
      
      
      <categories>
          
          <category> Machine Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> GD </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>ADMMdecentralized</title>
      <link href="/2021/02/03/ADMMdecentralized/"/>
      <url>/2021/02/03/ADMMdecentralized/</url>
      
        <content type="html"><![CDATA[<ul><li><a href="#abstract">Abstract</a></li><li><a href="#introduction">Introduction</a></li><li><a href="#preliminaries">Preliminaries</a><ul><li><a href="#notations-and-problem-setting">Notations and ProblemSetting</a></li></ul></li><li><a href="#model-propagation">Model Propagation</a><ul><li><a href="#asynchronous-gossip-algorithm">Asynchronous GossipAlgorithm</a></li></ul></li><li><a href="#collaborative-learning">Collaborative Learning</a><ul><li><a href="#problem-formulation">Problem Formulation</a></li><li><a href="#asynchronous-gossip-algorithm-1">Asynchronous GossipAlgorithm</a></li></ul></li><li><a href="#experiments">Experiments</a><ul><li><a href="#collaborative-linear-classification">Collaborative LinearClassification</a></li></ul></li><li><a href="#参考文献">参考文献</a></li></ul><!-- /TOC --><h1 id="abstract">Abstract</h1><p>考虑点对点的协作网络。本文解决的问题是：每个节点如何与具有相似目标的其他节点进行通信来改善本地模型？作者介绍了两种完全去中心化的算法，一种是受标签传播的启发，旨在平滑预先训练好的局部模型；第二种方法，节点基于本地数据核相邻节点进行迭代更新来共同学习和传播。</p><h1 id="introduction">Introduction</h1><p>数据不断产生，当前从数据中提取信息的主要方式是收集所有用户的个人数据于一个服务器上，然后进行数据挖掘。但是，中心化的方式存在一些问题，比如说一些用户拒绝提供个人数据，带宽和设备花费问题。即使一些算法允许数据分布在用户设备上，通常需要中心端来进行聚合和协调。</p><p>在本文中，作者考虑完全去中心化的点对点网络。不同于那些求解全局模型的算法，本文关注于每个节点可以根据自身目标函数学习一个个性化模型。作者假设网络结构已知，该网络结构能够反映出不同节点的相似度（如果两个节点具有相似的目标函数，那么这两个节点在网络中是邻居），每个节点只知道与其直接相邻的节点。一个节点不仅可以根据自身数据学习模型，还可以结合它的邻居。假设每个节点只知道相邻节点的信息，不知道整个网络结构。</p><p>作者提出两个算法。第一个是 modelpropagation：首先，每个节点先基于自己的局部数据学习到模型参数，然后，结合整个网络结构，平滑这些参数。第二个是collaborativelearning，这个算法更加灵活，它通过优化一个模型参数正则化（平滑）和局部模型准确性上的折中问题。作者基于分布式的ADMM算法提出一个异步gossip算法。</p><h1 id="preliminaries">Preliminaries</h1><h2 id="notations-and-problem-setting">Notations and ProblemSetting</h2><p>考虑 <span class="math inline">\(n\)</span> 个节点 <span class="math inline">\(V = [n] = \{1,...,n\}\)</span>。凸的损失函数 <span class="math inline">\(l: \mathbb{R}^{p} \times \mathcal{X} \times\mathcal{Y}\)</span>，节点 <span class="math inline">\(i\)</span>的目标是学习模型参数 <span class="math inline">\(\theta_{i} \in\mathbb{R}^{p}\)</span>，使得关于未知分布 <span class="math inline">\(\mu_{i}\)</span> 的期望损失 <span class="math inline">\(E_{(x_{i}, y_{i})\sim \mu_{i}}l(\theta_{i}; x_{i},y_{i})\)</span> 很小。节点 <span class="math inline">\(i\)</span> 具有<span class="math inline">\(m_{i}\)</span> 个来自分布 <span class="math inline">\(\mu_{i}\)</span> 的 i.i.d 的训练样本 <span class="math inline">\(S_{i} = \{(x_{i}^{j},y_{i}^{j})\}_{j=1}^{m_{i}}\)</span>。允许不同节点的样本量相差很大。每个节点可以最小化局部损失函数得到<span class="math inline">\(\theta_{i}^{sol}\)</span>:</p><p><span class="math display">\[\theta_{i}^{sol} \in \argmin_{\theta \in \mathbb{R}^{p}} L_{i}(\theta) =\sum_{j=1}^{m_{i}} l(\theta;x_{i}^{j}, y_{i}^{j}).\]</span></p><p>我们目标是通过结合其他节点信息，进一步改善上述模型。考虑一个加权网络结构<span class="math inline">\(G = (V, E)\)</span>，具有 <span class="math inline">\(V\)</span> 个节点，<span class="math inline">\(E\subseteq V \times V\)</span> 为无向边。定义 <span class="math inline">\(W \in \mathbb{R}^{n \times n}\)</span> 为由 <span class="math inline">\(G\)</span> 得到的对称非负加权矩阵，如果 <span class="math inline">\((i,j) \ne E\)</span> or <span class="math inline">\(i = j\)</span>， <span class="math inline">\(W_{ij} =0\)</span>。本文假设权重矩阵已知。定义对角阵 <span class="math inline">\(D\in \mathbb{R}^{n \times n}\)</span>，<span class="math inline">\(D_{ii} = \sum_{j=1}^{n} W_{ij}\)</span>。节点<span class="math inline">\(i\)</span> 的邻域 ：<span class="math inline">\(\mathcal{N}_{i} = \{j \ne i: W_{ij} &gt;0\}\)</span>。</p><h1 id="model-propagation">Model Propagation</h1><p>假设每个节点通过最小化局部损失函数得到各自的模型 <span class="math inline">\(\theta_{i}^{sol}\)</span>。由于每个节点上的模型都是在不同大小数据集上考虑得到，作者使用<span class="math inline">\(c_{i} \in (0,1]\)</span>定义每个节点模型的可信度。 <span class="math inline">\(c_{i}\)</span>的值应该和节点 <span class="math inline">\(i\)</span>的样本量大小呈正相关，可以设置为 <span class="math inline">\(c_{i} =\frac{m_{i}}{\max_{j} m_{j}}\)</span>。如果 <span class="math inline">\(m_{i}=0\)</span>，可以设置为一个小量。</p><p>定义 <span class="math inline">\(\Theta = [\theta_{1};\theta_{2};...;\theta_{n}] \in \mathbb{R}^{n \timesp}\)</span>，我们要优化的目标函数为：</p><figure><img src="https://cdn.jsdelivr.net/gh/sugar-xue/BlogImages@main/Picbed1612018569.jpg" alt="1612018569"><figcaption aria-hidden="true">1612018569</figcaption></figure><p>第一项二次函数用来平滑相邻节点的参数，当两个节点间权重越大时，节点间参数越相近；第二项的目的是使具有较高置信度的模型的参数不要太远离各自模型上的参数。具有较低置信度的模型的参数被允许具有较大的偏差，容易被相邻节点影响。<span class="math inline">\(D_{ii}\)</span> 的目的是为了normalization。</p><p><img src="https://cdn.jsdelivr.net/gh/sugar-xue/BlogImages@main/Picbed1612018994(1).png" alt="1612018994(1)"><br><img src="https://cdn.jsdelivr.net/gh/sugar-xue/BlogImages@main/Picbed1612019022(1).png" alt="1612019022(1)"></p><p>计算 (4)需要知道整个网络的信息以及所有节点的独立模型信息，这对于节点而言是未知的，因为每个节点只知道相邻节点的信息。因此，作者提出下面的迭代形式：</p><figure><img src="https://cdn.jsdelivr.net/gh/sugar-xue/BlogImages@main/Picbed1612019342(1).png" alt="1612019342(1)"><figcaption aria-hidden="true">1612019342(1)</figcaption></figure><p>作者证明，无论初始值 <span class="math inline">\(\Theta(0)\)</span>取何值，上述迭代序列收敛到 (4)。(5) 式可以进一步分解为</p><figure><img src="https://cdn.jsdelivr.net/gh/sugar-xue/BlogImages@main/Picbed1612019480(1).png" alt="1612019480(1)"><figcaption aria-hidden="true">1612019480(1)</figcaption></figure><p>考虑一个同步计算：在每一步，每个节点都和其所有相邻节点进行通信，收集它们当前参数，然后使用它们的参数更新上式。同步更新会导致很大的延迟，因为任何节点都必须等剩余节点更新完后才能进行下一步更新。并且，每一步，所有节点都需要和其邻居节点进行通信，降低了算法的效率。所以作者提出一个异步算法。</p><h2 id="asynchronous-gossip-algorithm">Asynchronous GossipAlgorithm</h2><p>在异步设置中，每个节点都有一个局部clock ticking at times of rate 1Poisson process.由于节点都是独立同分布的，所以相当于在每一步时等概率激活每个节点。</p><p>在时间 <span class="math inline">\(t\)</span>时，每个节点都存有相邻节点的信息。以数学形式表示，考虑矩阵 <span class="math inline">\(\tilde{\Theta}_{i}(t) \in \mathbb{R}^{n \timesp}\)</span>，第 <span class="math inline">\(i\)</span> 行 <span class="math inline">\(\tilde{\Theta}_{i}^{i}(t) \in\mathbb{R}^{p}\)</span> 为节点 <span class="math inline">\(i\)</span>在时刻 <span class="math inline">\(t\)</span> 的模型参数，<span class="math inline">\(\tilde{\Theta}_{i}^{j}(t) \in \mathbb{R}^{p} (j\ne i)\)</span> 为节点 <span class="math inline">\(i\)</span>储存的关于邻居节点 <span class="math inline">\(j\)</span> 的lastknowledge. 对于 <span class="math inline">\(j \notin \mathcal{N}_{i}\bigcup \{i\}\)</span>，<span class="math inline">\(\forall t &gt;0\)</span>，<span class="math inline">\(\tilde{\Theta}_{i}^{j}(t) =0\)</span>。令<span class="math inline">\(\tilde{\Theta} =[\tilde{\Theta}_{1}^{T}, ...,\tilde{\Theta}_{n}^{T}] \in\mathbb{R}^{n^{2} \times p}\)</span>。</p><p>如果在时间 <span class="math inline">\(t\)</span> 时，节点 <span class="math inline">\(i\)</span> wakes up，执行如下步骤：</p><ul><li><p>communication: 节点 <span class="math inline">\(i\)</span>随机选择一个邻居节点 <span class="math inline">\(j \in\mathcal{N}_{i}\)</span>，(先验概率 <span class="math inline">\(\pi_{i}^{j}\)</span>)，节点 <span class="math inline">\(i\)</span> 和节点 <span class="math inline">\(j\)</span> 同时更新它们的参数： <span class="math display">\[\tilde{\Theta}_{i}^{j}(t+1) = \tilde{\Theta}_{j}^{j}(t) \qquad\tilde{\Theta}_{j}^{i}(t+1) = \tilde{\Theta}_{i}^{i}(t),\]</span></p></li><li><p>update: 基于当前信息，节点 <span class="math inline">\(i\)</span>和节点 <span class="math inline">\(j\)</span> 更新自己的模型参数： <span class="math display">\[\tilde{\Theta}_{l}^{l}(t+1) = (\alpha +\bar{\alpha}c_{l})^{-1}(\alpha\sum_{k \in \mathcal{N}_{l}}\frac{W_{lk}}{D_{ll}}\tilde{\Theta}_{l}^{k}(t+1) +\bar{\alpha}c_{l}\theta^{sol}_{l}) \quad(l \in \{i,j\}).\]</span></p><p>网络中的其他变量保持不变。作者提出的算法属于 gossipalgorithms，每个节点每次最多只和一个邻居节点通信。</p><p>作者证明，上述算法可以收敛到使每个节点具有最优参数。</p><figure><img src="https://cdn.jsdelivr.net/gh/sugar-xue/BlogImages@main/Picbed1612340701.jpg" alt="1612340701"><figcaption aria-hidden="true">1612340701</figcaption></figure></li></ul><h1 id="collaborative-learning">Collaborative Learning</h1><p>上述算法先在局部节点上进行学习，然后在进行网络通信。在这部分，作者提出了一个使节点可以同时进行基于局部数据和邻居节点信息更新模型参数的算法。相较于前面的算法，该算法通信成本较高，但是估计精度高于前者。</p><h2 id="problem-formulation">Problem Formulation</h2><p>优化目标：</p><figure><img src="https://cdn.jsdelivr.net/gh/sugar-xue/BlogImages@main/Picbed1612341031(1).png" alt="1612341031(1)"><figcaption aria-hidden="true">1612341031(1)</figcaption></figure><p>注意到，这里的置信度通过 <span class="math inline">\(\mathcal{L}_{i}\)</span> 体现，因为 <span class="math inline">\(\mathcal{L}_{i}\)</span> 为局部节点 <span class="math inline">\(i\)</span> 上所有观测的损失函数和。</p><p>一般情况下，上述问题没有解析解，作者提出一个分散式迭代算法进行求解。</p><h2 id="asynchronous-gossip-algorithm-1">Asynchronous GossipAlgorithm</h2><p>作者基于ADMM提出了一个异步分散式算法。本文的目的不是寻找一个consensus解，因为我们的目标是为了学习到每个节点的personalized model.作者通过将问题 (7)进行变换为一个部分consensus问题，使用ADMMD进行求解。</p><p>令 <span class="math inline">\(\Theta_{i} \in\mathbb{R}^{(|\mathcal{N_{i}}|+1)\times p}\)</span> 为变量 <span class="math inline">\(\theta_{j} \in \mathbb{R}^{p}(j \in\mathcal{N_{i}} \bigcup \{i\})\)</span> 的集合。定义 <span class="math inline">\(\theta_{j}\)</span> 为 <span class="math inline">\(\Theta_{i}^{j}\)</span>。优化问题(7)重新写为：</p><figure><img src="https://cdn.jsdelivr.net/gh/sugar-xue/BlogImages@main/Picbed1612341938(1).png" alt="1612341938(1)"><figcaption aria-hidden="true">1612341938(1)</figcaption></figure><p>在这个目标函数中，所有的节点相互依赖，因为它们共享一个优化变量 <span class="math inline">\(\in\Theta\)</span>。为了使用ADMM，需要将各个节点的优化变量独立，对于每个节点<span class="math inline">\(i\)</span>，定义一个local copy <span class="math inline">\(\tilde{\Theta}_{i} \in\mathbb{R}^{(|\mathcal{N_{i}}|+1)\times p}\)</span>，添加等式约束：<span class="math inline">\(\tilde{\Theta}_{i}^{i} =\tilde{\Theta}_{j}^{i}\)</span>，对于所有的 <span class="math inline">\(i \in [n], j \in \mathcal{N}_{i}\)</span>。</p><figure><img src="https://cdn.jsdelivr.net/gh/sugar-xue/BlogImages@main/Picbed1612342502(1).png" alt="1612342502(1)"><figcaption aria-hidden="true">1612342502(1)</figcaption></figure><p>增广拉格朗日乘子：</p><figure><img src="https://cdn.jsdelivr.net/gh/sugar-xue/BlogImages@main/Picbed1612342580(1).png" alt="1612342580(1)"><figcaption aria-hidden="true">1612342580(1)</figcaption></figure><p>算法如下，假设时刻 <span class="math inline">\(t\)</span> 时节点<span class="math inline">\(i\)</span> wakes up，选取邻居节点 <span class="math inline">\(j \in \mathcal{N}_{i}\)</span>，定义 <span class="math inline">\(e = (i,j)\)</span>，</p><figure><img src="https://cdn.jsdelivr.net/gh/sugar-xue/BlogImages@main/Picbed1612342728(1).png" alt="1612342728(1)"><figcaption aria-hidden="true">1612342728(1)</figcaption></figure><figure><img src="https://cdn.jsdelivr.net/gh/sugar-xue/BlogImages@main/Picbed1612342765(1).png" alt="1612342765(1)"><figcaption aria-hidden="true">1612342765(1)</figcaption></figure><h1 id="experiments">Experiments</h1><h2 id="collaborative-linear-classification">Collaborative LinearClassification</h2><p>考虑100个节点，每个节点的目标是建立一个线性分类模型 in <span class="math inline">\(\mathbb{R}^{p}\)</span>。为了方便可视化，每个节点的真实参数位于2维子空间：将其参数看作是<span class="math inline">\(\mathbb{R}^{p}\)</span>空间中的向量，前两项从正态分布中随机产生，剩余项为0。两个节点 <span class="math inline">\(i\)</span> 和节点 <span class="math inline">\(j\)</span> 的相似度通过参数距离的高斯核定义，定义<span class="math inline">\(\phi_{ij}\)</span>为两个真实参数在单位圆上投影的夹角，<span class="math inline">\(W_{ij} =\exp(\cos \phi_{ij} - 1)/\sigma\)</span>，<span class="math inline">\(\sigma =0.1\)</span>。权重为负值的将被忽略。每个节点具有随机的训练样本，样本的标签为二元标签，由线性分类模型产生。以概率0.05随机使标签反转，以产生噪音数据。每个节点的损失函数为hinge损失：<span class="math inline">\(l(\theta;(x_{i}, y_{i})) = \max(0,1-y_{i}\theta^{T}x_{i})\)</span>。作者评估了模型在100个测试样本上的预测精度。</p><figure><img src="https://cdn.jsdelivr.net/gh/sugar-xue/BlogImages@main/Picbed1612343843(1).png" alt="1612343843(1)"><figcaption aria-hidden="true">1612343843(1)</figcaption></figure><h1 id="参考文献">参考文献</h1><ul><li>Vanhaesebrouck, P., Bellet, A. &amp; Tommasi, M.. (2017).Decentralized Collaborative Learning of Personalized Models overNetworks. Proceedings of the 20th International Conference on ArtificialIntelligence and Statistics, in PMLR 54:509-517</li></ul>]]></content>
      
      
      <categories>
          
          <category> Distributed Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> FL </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Lower Bounds and Optimal Algorithms for Personalized Federated Learning</title>
      <link href="/2021/01/26/AL2SGD/"/>
      <url>/2021/01/26/AL2SGD/</url>
      
        <content type="html"><![CDATA[<ul><li><a href="#introduction">Introduction</a></li><li><a href="#contributions">Contributions</a></li><li><a href="#lower-complexity-bounds">Lower complexity bounds</a><ul><li><a href="#lower-complexity-bounds-on-the-communication">Lowercomplexity bounds on the communication</a></li><li><a href="#lower-complexity-bounds-on-the-local-computation">Lowercomplexity bounds on the local computation</a></li></ul></li><li><a href="#优化算法">优化算法</a><ul><li><a href="#accelerated-proximal-gradient-descent-apgd-for-federated-learning">AcceleratedProximal Gradient Descent (APGD) for Federated Learning</a></li><li><a href="#beyond-proximal-oracle-inexact-apgd-iapgd">Beyond proximaloracle: Inexact APGD (IAPGD)</a></li><li><a href="#accelerated-l2sgd">Accelerated L2SGD+</a></li></ul></li><li><a href="#experiments">Experiments</a></li><li><a href="#参考文献">参考文献</a></li></ul><!-- /TOC --><h1 id="introduction">Introduction</h1><p>作者在前文考虑了一个新的优化问题： <span class="math display">\[\min_{\mathbf{x_{1},...,x_{n}} \in \mathbb{R}^{d}} \{ F(\mathbf{x}) :=f(\mathbf{x}) + \lambda \psi(\mathbf{x})\}\]</span> <span class="math display">\[f(\mathbf{x}):= \frac{1}{n}\sum_{i=1}^{n}f_{i}(\mathbf{x_{i}}), \quad\psi(\mathbf{x}):= \frac{1}{2n}\sum_{i=1}^{n}\|\mathbf{x_{i}} -\mathbf{\bar{x}}\|^{2}\]</span></p><p>Remark：该问题的最优解 <span class="math inline">\(\mathbf{x}^{*} =[\mathbf{x}_{1}^{*},..., \mathbf{x}_{n}^{*}] \in\mathbb{R}^{nd}\)</span> 可以被表示为 <span class="math inline">\(\mathbf{x}^{*}_{i} = \mathbf{\bar{x}}^{*} -\frac{1}{\lambda}\nabla f_{i}(\mathbf{x}_{i}^{*})\)</span>，其中 <span class="math inline">\(\mathbf{\bar{x}}^{*} = \frac{1}{n}\sum_{i=1}^{n}\mathbf{x}_{i}^{*}\)</span>，该形式与MAML相似。</p><h1 id="contributions">Contributions</h1><p>在这篇论文中，作者给出了求解上述优化问题的通信和局部计算复杂度（迭代次数）的最低界限，并且给出了几种能够达到最低界限的算法。</p><ul><li><p>lower bound on the communication complexity.作者证明对于任意一个满足一定假设条件的算法，会有一个L-smooth, <span class="math inline">\(\mu\)</span>-strongly convex 局部目标函数 <span class="math inline">\(f_{i}\)</span> 至少需要通信 <span class="math inline">\(O(\sqrt{\frac{\min\{L,  \lambda\}}{\mu}}\log\frac{1}{\epsilon})\)</span>轮才能得到最优解 <span class="math inline">\(\epsilon\)</span>邻域内的解。</p></li><li><p>lower complexity bound on the number of local oracle calls.作者证明对于局部近端梯度下降，至少需要迭代<span class="math inline">\(O(\sqrt{\frac{\min\{L,  \lambda\}}{\mu}}\log\frac{1}{\epsilon})\)</span>次；对于局部梯度下降，至少需要进行 <span class="math inline">\(O(\sqrt{\frac{L}{\mu}}\log\frac{1}{\epsilon})\)</span>次迭代；若每个目标函数为 <span class="math inline">\(m\)</span>个有限和形式（<span class="math inline">\(\tilde{L}\)</span>-smooth)，至少需要 <span class="math inline">\(O((m +\sqrt{\frac{m\tilde{L}}{\mu}})\log\frac{1}{\epsilon})\)</span>次。</p></li><li><p>作者讨论了不同的用于求解上述优化问题的算法，这些算法在不同设定下可以达到最优通信复杂度和最优局部梯度复杂度。。首先是加速近端梯度下降算法(APGD)，作者考虑两种不同的应用方式，第一种是：函数<span class="math inline">\(f\)</span> 采用梯度下降，<span class="math inline">\(\lambda \psi\)</span>采用近端梯度下降，第二种是反过来。对于第一种情况，当 <span class="math inline">\(L \leq \lambda\)</span>时，我们可以实现最优通信复杂度和局部梯度复杂度 <span class="math inline">\(O(\sqrt{\frac{L}{\mu}}\log\frac{1}{\epsilon})\)</span>；对于第二种情况，当<span class="math inline">\(L \geq \lambda\)</span>时，我们可以得到最优通信复杂度和局部近端复杂度 <span class="math inline">\(O(\sqrt{\frac{\lambda}{\mu}}\log\frac{1}{\epsilon})\)</span>。受一篇论文启发，作者提到局部近端可以由局部加速梯度下降 (Local AGD) 近似(inexactly) 得到，当目标函数为有限和形式，还可以采用Katyusha算法近似得到。Local AGD 可以得到 <span class="math inline">\(O(\sqrt{\frac{\lambda}{\mu}}\log\frac{1}{\epsilon})\)</span>的通信复杂度，以及 <span class="math inline">\(\tilde{O}(\sqrt{\frac{L+\lambda}{\mu}})\)</span>的局部梯度复杂度，当 <span class="math inline">\(L \geq\lambda\)</span>（取决于对数因子）时，两者都能达到最优。同样，当局部采用 Katyusha时，我们可以得到通信复杂度 <span class="math inline">\(O(\sqrt{\frac{\lambda}{\mu}}\log\frac{1}{\epsilon})\)</span>和局部梯度复杂度 <span class="math inline">\(\tilde{O}(m\sqrt{\frac{\lambda}{\mu}} + \sqrt{m\frac{\tilde{L}}{\mu}})\)</span>，前者当 <span class="math inline">\(L\geq \Lambda\)</span> 时能达到最优，后者当 <span class="math inline">\(m\lambda \leq \tilde{L}\)</span>（取决于对数因子）时达到最优。</p></li><li><p>作者提出了加速的L2SGD+算法-AL2SGD+，该算法可以实现最优通信复杂 度<span class="math inline">\(O(\sqrt{\frac{\min\{\tilde{L},  \lambda\}}{\mu}}\log\frac{1}{\epsilon})\)</span>，以及局部梯度复杂度<span class="math inline">\(O((m + \sqrt{\frac{m(\tilde{L} +\lambda)}{\mu}})\log\frac{1}{\epsilon})\)</span>，当 <span class="math inline">\(\lambda \leq \tilde{L}\)</span>时最优。但是，两者无法同时实现最优。</p><p><img src="/2021/01/26/AL2SGD/t1.jpg"></p></li></ul><h1 id="lower-complexity-bounds">Lower complexity bounds</h1><h2 id="lower-complexity-bounds-on-the-communication">Lower complexitybounds on the communication</h2><p><img src="/2021/01/26/AL2SGD/3.1.png"> <img src="/2021/01/26/AL2SGD/3.11.png"></p><h2 id="lower-complexity-bounds-on-the-local-computation">Lowercomplexity bounds on the local computation</h2><p><img src="/2021/01/26/AL2SGD/3.2.png"></p><h1 id="优化算法">优化算法</h1><h2 id="accelerated-proximal-gradient-descent-apgd-for-federated-learning">AcceleratedProximal Gradient Descent (APGD) for Federated Learning</h2><p>首先介绍非加速版本的近端梯度下降算法(PGD): <img src="/2021/01/26/AL2SGD/1.png"></p><p>根据另一篇论文，有两种不同的方式可以将梯度下降算法应用到上述优化问题上。最直接的方式是令<span class="math inline">\(h = f\)</span>，<span class="math inline">\(\phi =\lambda\psi\)</span>，那么可以得到如下更新步骤： <img src="/2021/01/26/AL2SGD/2.png"></p><p>另一种方式是令 <span class="math inline">\(h(\mathbf{x}) = \lambda\phi(\mathbf{x}) + \frac{\mu}{2n}\|\mathbf{x}\|^{2}\)</span>， <span class="math inline">\(\phi(\mathbf{x}) = f(\mathbf{x}) -\frac{\mu}{2n}\|\mathbf{x}\|^{2}\)</span>。由此得到的更新过程如下： <img src="/2021/01/26/AL2SGD/3.png"></p><p>同FedProx算法一致。</p><p>由于上述两种情况下，每次迭代都需要进行一轮通信，相应的通信复杂度次优。但是可以结合动量算法，程序(6)可以结合Nesterov'smomentum，能够得到最优通信复杂度，以及最优局部近端复杂度（当 <span class="math inline">\(\lambda \leqL)\)</span>，该算法定义为APGD1，具体如下：</p><p><img src="/2021/01/26/AL2SGD/a2.png"></p><p>将更新过程(5)和动量结合，可以得到最优通信复杂度以及最优局部近端复杂度（当<span class="math inline">\(\lambda \geqL）\)</span>。将该算法定义为APGD2，具体如下：</p><p><img src="/2021/01/26/AL2SGD/a3.png"></p><h2 id="beyond-proximal-oracle-inexact-apgd-iapgd">Beyond proximaloracle: Inexact APGD (IAPGD)</h2><p>在多数情况下，如果采用局部近端操作，每一步迭代时都需要得到子问题的精确解，这是不实际的。因此，作者提出了一个针对(6)的加速非精确的算法，每个节点只需要进行局部梯度运算（AGD, Katyusha)：</p><p><img src="/2021/01/26/AL2SGD/a1.png"></p><h2 id="accelerated-l2sgd">Accelerated L2SGD+</h2><p>作者给出L2SGD+算法的一个加速版本-AL2SGD+。作者指出AL2SGD+算法不过是L-Katyusha算法与非均匀抽样的结合。</p><p><img src="/2021/01/26/AL2SGD/a4.png"></p><h1 id="experiments">Experiments</h1><p>在第一个实验中，作者比较了当局部损失为有限和形式时，算法IAPGD+Katyusha、AL2SGD+以及L2SGD+的收敛速度。结果如下图：</p><p><img src="/2021/01/26/AL2SGD/5.jpg"></p><p>对于通信轮数，IAPGD+Katyusha和AL2SGD+都显著优于L2SGD+；对于局部计算次数，AL2SGD+表现最优，IAPGD+Katyusha不如L2SGD+。</p><p>第二个实验中，作者研究了数据异质性对算法的影响，结果如下图所示。可以看出，数据异质性不影响算法的收敛速度，各个算法的表现同第一个实验相似。</p><p><img src="/2021/01/26/AL2SGD/6.png"></p><p>在第三个实验中，作者比较了APGD算法的两种变形：APGD1和APGD2。作者不断改变参数<span class="math inline">\(\lambda\)</span>的取值，其余参数保持不变。在理论上，APGD2算法应该不受参数 <span class="math inline">\(\lambda\)</span> 影响，而APGD1 算法的收敛率会随着<span class="math inline">\(\lambda\)</span> 而增加 (<span class="math inline">\(\sqrt{\lambda}\)</span>)。 当 <span class="math inline">\(\lambda \leq L = 1\)</span>时，APGD1是最优选择；当<span class="math inline">\(\lambda &gt; L = 1\)</span> 时，APGD2应该是最优选择。实验结果如下图所示，结果与理论一致。</p><p><img src="/2021/01/26/AL2SGD/7.png"></p><h1 id="参考文献">参考文献</h1><ul><li>Filip Hanzely (KAUST) · Slavomír Hanzely (KAUST) · Samuel Horváth(King Abdullah University of Science and Technology)· Peter Richtarik(KAUST). Lower Bounds and Optimal Algorithms for Personalized FederatedLearning.arXiv e-prints.https://ui.adsabs.harvard.edu/abs/2020arXiv201002372H</li></ul>]]></content>
      
      
      <categories>
          
          <category> Distributed Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> FL </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Federated Learning of a Mixture of Global and Local Models</title>
      <link href="/2021/01/26/L2SGD/"/>
      <url>/2021/01/26/L2SGD/</url>
      
        <content type="html"><![CDATA[<ul><li><a href="#introduction">Introduction</a><ul><li><a href="#11-federated-learning">1.1 Federated learning</a></li></ul></li><li><a href="#contributions">Contributions</a></li><li><a href="#新的优化问题">新的优化问题</a></li><li><a href="#l2gd-loopless-local-gd">L2GD: Loopless Local GD</a><ul><li><a href="#收敛理论">收敛理论</a></li><li><a href="#收敛率优化">收敛率优化</a></li></ul></li><li><a href="#loopless-local-sgd-with-variance-reduction">Loopless LocalSGD with Variance Reduction</a><ul><li><a href="#问题设置">问题设置</a></li><li><a href="#理论">理论</a></li></ul></li><li><a href="#experiments">Experiments</a></li><li><a href="#附录">附录</a><ul><li><a href="#experimental-setup-and-further-experiments">ExperimentalSetup and further experiments</a></li><li><a href="#其余算法">其余算法</a></li></ul></li><li><a href="#参考文献">参考文献</a></li></ul><!-- /TOC --><h1 id="introduction">Introduction</h1><h2 id="federated-learning">1.1 Federated learning</h2><p>联邦学习的目标函数： <span class="math display">\[  \min_{\mathbf{x} \in \mathbb{R}^{d}}\frac{1}{n}\sum_{i=1}^{n}f_{i}(\mathbf{x})  \]</span> 其中 <span class="math inline">\(n\)</span>表示参与训练的节点个数，<span class="math inline">\(\mathbf{x} \in\mathbb{R}^{d}\)</span>为全模型优化变量。 <span class="math inline">\(f_{i}(\mathbf{x})\)</span>为节点<span class="math inline">\(i\)</span>上的损失函数。</p><h1 id="contributions">Contributions</h1><ul><li>提出了新的FL优化形式，尝试学习全局模型和局部模型的混合。</li><li>给出了新的优化形式的理论性质。作者证明了最优局部模型以<span class="math inline">\(O(1/\lambda)\)</span>收敛到传统的全局模型；作者证明了在局部模型上得到的损失不高于全局模型上的损失(定理3.1)；作者指出局部模型的最优解等于所有局部模型最优解的平均值减去对应局部模型上损失函数的一阶梯度，这一点和MAML一致。</li><li>Loopless LGD：作者提出了一个随机梯度算法 — Loopless Local GradientDescent(L2GD)（算法1）来解决提出的优化问题。该算法不是一个标准的SGD，它可以看作是一个关于损失函数和惩罚项的不均匀抽样。当抽到损失函数部分时，每个节点执行一次随机梯度下降；当抽到惩罚项时，进行信息聚合。</li><li>收敛理论。假设函数 <span class="math inline">\(f_{i}\)</span> 为<span class="math inline">\(L-smooth\)</span>，并且为 <span class="math inline">\(\mu-strong \, convex\)</span>，可以得到抽样概率<span class="math inline">\(p^{*} = \frac{\lambda}{\lambda +L}\)</span>，固定期望局部更新次数为 <span class="math inline">\(1 +\frac{L}{\lambda}\)</span>，作者证明通信 (communication)复杂度为（通信次数上界）为 <span class="math inline">\(\frac{2\lambda}{\lambda +L}\frac{L}{\mu}\log\frac{1}{\epsilon}\)</span>。当 <span class="math inline">\(\lambda \to 0\)</span>时，通信次数非常小；当$$时，根据新优化问题得到的解收敛到全局模型最优解，并且L2GD算法的通信上界为 <span class="math inline">\(O(\frac{L}{\mu}\log\frac{1}{\epsilon})\)</span>。</li><li>推广。部分连接，局部SGD，variancereduction（variance来自三部分：非均匀抽样，部分连接，从节点样本随机抽样）。</li><li>可用于异质数据。</li><li>经验表现不错。</li></ul><h1 id="新的优化问题">新的优化问题</h1><p><span class="math display">\[\min_{\mathbf{x_{1},...,x_{n}} \in \mathbb{R}^{d}} \{ F(\mathbf{x}) :=f(\mathbf{x}) + \lambda \psi(\mathbf{x})\}\]</span> <span class="math display">\[f(\mathbf{x}):= \frac{1}{n}\sum_{i=1}^{n}f_{i}(\mathbf{x_{i}}), \quad\psi(\mathbf{x}):= \frac{1}{2n}\sum_{i=1}^{n}\|\mathbf{x_{i}} -\mathbf{\bar{x}}\|^{2}\]</span></p><ul><li>Local model (<span class="math inline">\(\lambda = 0\)</span>)</li><li>Mixed model (<span class="math inline">\(\lambda \in (0,\infty)\)</span>)</li><li>Global model (<span class="math inline">\(\lambda =\infty\)</span>)</li></ul><h1 id="l2gd-loopless-local-gd">L2GD: Loopless Local GD</h1><p>在这一部分中，作者给出一个算法求解上述优化问题，该算法可以看作是一个非均匀SGD，要么抽取<span class="math inline">\(\nabla f\)</span>，要么抽取 <span class="math inline">\(\nabla \psi\)</span> 估计 <span class="math inline">\(\nabla F\)</span>。令 <span class="math inline">\(0 &lt; p &lt; 1\)</span>，定义一个随机梯度如下：<span class="math display">\[G(\mathbf{x}):= \begin{cases} \frac{\nabla f(\mathbf{x})}{1-p}, &amp;\text {概率 $1-p$} \\ \frac{\lambda \nabla \psi(\mathbf{x})}{p}, &amp;\text{概率 $p$ } \end{cases}\]</span> 显然，<span class="math inline">\(G(\mathbf{x})\)</span>为<span class="math inline">\(\nabla F(\mathbf{x})\)</span>的无偏估计量。每步的更新为: <span class="math display">\[\mathbf{x}^{k+1} = \mathbf{x}^{k} - \alpha G(\mathbf{x}).\]</span></p><figure><img src="/2021/01/26/L2SGD/l2gd.png" alt="Algorithm 1"><figcaption aria-hidden="true">Algorithm 1</figcaption></figure><p><span class="math inline">\(\textbf{Lemma 4.2}\)</span> 经过 <span class="math inline">\(k\)</span> 步迭代后，期望的通信次数为 <span class="math inline">\(p(1-p)k\)</span>。</p><h2 id="收敛理论">收敛理论</h2><p>作者首先证明梯度估计量<span class="math inline">\(G(\mathbf{x})\)</span>的期望具有光滑性质，然后证明了算法L2GD的收敛性质。（<span class="math inline">\(\mathbf{x(\lambda)}\)</span>为最优解，定理4.4表明，L2GD算法只能收敛到最优解邻域。） <img src="/2021/01/26/L2SGD/4.3.png"></p><h2 id="收敛率优化">收敛率优化</h2><p>作者给出最优抽样概率 <span class="math inline">\(p^{*} =\frac{\lambda}{L + \lambda}\)</span>，步长 <span class="math inline">\(\alpha\)</span> 要满足 <span class="math inline">\(\frac{\alpha\lambda}{np} \leq\frac{1}{2}\)</span>. <img src="/2021/01/26/L2SGD/4.4.png"></p><h1 id="loopless-local-sgd-with-variance-reduction">Loopless Local SGDwith Variance Reduction</h1><p>L2GD算法仅线性收敛到最优解的邻域，无法收敛到最优解。假设每个子目标函数具有有限和形式，作者提出了一个算法L2SGD+，在每个节点上进行随机梯度下降，并且具有线性收敛速度。L2SGD是一个具有variancereduction 的局部SGD算法，关于SGD的variance reduction，见另一篇博客：SGDwith variance reduction.</p><h2 id="问题设置">问题设置</h2><p>假设 <span class="math inline">\(f_{i}\)</span> 具有有限和结构：<span class="math display">\[f_{i} = \frac{1}{m}\sum_{j=1}^{m}f_{i,j}(\mathbf{x}_{i})\]</span></p><p>那么目标函数变为： <span class="math display">\[F(\mathbf{x}) =\frac{1}{n}\sum_{i=1}^{n}(\frac{1}{m}\sum_{i=1}^{m}f_{i,j}(\mathbf{x}_{i}))+ \lambda\frac{1}{2n}\sum_{i=1}^{n}\|\mathbf{x}_{i} -\mathbf{\bar{x}}\|^{2}\]</span></p><p><img src="/2021/01/26/L2SGD/l2sgd+.png"></p><p>L2SGD算法仅在两次抽样不同时才会发生通信，经过 <span class="math inline">\(k\)</span> 次迭代后，需要进行 <span class="math inline">\(p(1-p)k\)</span>次聚合平均。但是，L2SGD算法还需要通信控制变量 <span class="math inline">\(\mathbf{J_{i}I,  \Psi_{i}}\)</span>，因此通信次数变为原来的3倍。在附录中，作者给出了一个高效的L2SGD+，不需要通信控制变量。</p><h2 id="理论">理论</h2><p>作者给出了L2SGD算法的理论性质，并且给出最优抽样概率 <span class="math inline">\(p^{*}\)</span>。</p><p><img src="/2021/01/26/L2SGD/5.1.jpg"> <img src="/2021/01/26/L2SGD/5.2.png"> <img src="/2021/01/26/L2SGD/5.3.png"></p><h1 id="experiments">Experiments</h1><p>作者考虑Logistic回归问题，数据为LibSVM data(Chank &amp; Lin,2011)。数据首先进行normalized，以使得 <span class="math inline">\(f_{ij}\)</span>为1-smooth。步长根据定理5.2确定。每个数据集被划分为不同个数的节点，具体参数设置如下：<img src="/2021/01/26/L2SGD/table1.png"></p><p>作者考虑三种算法：L2SGD+, L2SGD(L2GD with local SGD), L2SGD2(L2GDwith local subsampling and control variates constructed for <span class="math inline">\(\Psi\)</span>)。根据理论分析，L2SGD+线性收敛到最优解，而L2SGD和L2SGD2收敛到最优解邻域。</p><p>作者考虑了两种数据分割方式。对于homogeneous data,首先将观测样本随机打乱，然后按照打乱后的数据划分到不同节点上；对于heterogeneousdata,首先根据观测样本的标签将样本排序，然后将排序后的数据依次划分到不同节点上(the worst-case heterogeneity)。</p><p><img src="/2021/01/26/L2SGD/figure3.png"></p><p>结果表明 - L2SGD+ (Full variance reduction)可以收敛到最优解，而L2SGD(without variance reduction)和 L2SGD2(with partial variancereduction) 只收敛到最优解邻域。 - 进行variancereduction是非常有必要的。它可以保证较快的全局收敛。 -数据异质性对算法收敛性没有影响。</p><h1 id="附录">附录</h1><h2 id="experimental-setup-and-further-experiments">Experimental Setupand further experiments</h2><ul><li>参数 <span class="math inline">\(p\)</span>如何影响算法L2SGD+的收敛速度</li><li>参数 <span class="math inline">\(\lambda\)</span>如何影响算法L2SGD+的收敛速度</li></ul><h2 id="其余算法">其余算法</h2><ul><li><p>Local GD with variance reduction</p><p>当每个节点采用梯度下降算法，且考虑variance reduction时，</p><p><img src="/2021/01/26/L2SGD/b1.png"> <img src="/2021/01/26/L2SGD/a3.png"></p></li><li><p>Efficient implementation of L2SGD+考虑到L2SGD+需要通信控制变量，增加了通信次数。作者给出了一个高效的版本，不需要通信控制变量，<span class="math inline">\(k\)</span>次迭代只需要通信 <span class="math inline">\(p(1-p)k\)</span>次。</p><p><img src="/2021/01/26/L2SGD/a4.png"></p></li><li><p>Local SGD with variance reduction – general method在这部分中，作者给出了一个使用性更广的版本。每个节点上目标函数可以包含一个非光滑正则项：</p><p><img src="/2021/01/26/L2SGD/b3.png"></p><p>另外，该版本算法允许从所有节点中任意抽样，允许节点结构任意（比如节点数据集大小，目标函数光滑程度，每个节点抽样方式任意）。</p><p><img src="/2021/01/26/L2SGD/a5.png"></p></li><li><p>Local stochastic algorithms</p><p>在这部分中，作者给出两个简单算法，不考虑variance reduction的LocalSGD(算法6)以及只考虑部分variance reduction的Local SGD (算法7)。</p><p><img src="/2021/01/26/L2SGD/a6.png"></p><p><img src="/2021/01/26/L2SGD/a7.png"></p></li></ul><h1 id="参考文献">参考文献</h1><ul><li>Hanzely, F. , &amp; Richtárik, Peter. (2020). Federated learning ofa mixture of global and local models.</li></ul>]]></content>
      
      
      <categories>
          
          <category> Distributed Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> FL </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>FLreview</title>
      <link href="/2021/01/21/FLreview/"/>
      <url>/2021/01/21/FLreview/</url>
      
        <content type="html"><![CDATA[<h1 id="aaai21-personalized-cross-silo-federated-learning-on-non-iid-data">[AAAI21]Personalized Cross-Silo Federated Learning on Non-IID Data</h1><p>该算法的目标函数为： <img src="https://cdn.jsdelivr.net/gh/sugar-xue/BlogImages@main/Picbed1613893223.png" alt="1613893223"></p><p>第二项<span class="math inline">\(A(\|\omega_{i} -\omega_{j}\|^{2})\)</span>的作用是使不同节点进行信息交流。该函数的定义如下：</p><figure><img src="https://cdn.jsdelivr.net/gh/sugar-xue/BlogImages@main/Picbed1614603445(1).png" alt="1614603445(1)"><figcaption aria-hidden="true">1614603445(1)</figcaption></figure><p>作者提出了一个求解上述目标函数的算法-FedAMP，具体如下：</p><figure><img src="https://cdn.jsdelivr.net/gh/sugar-xue/BlogImages@main/Picbed1613893256(1).png" alt="1613893256(1)"><figcaption aria-hidden="true">1613893256(1)</figcaption></figure><p>注意到，函数<span class="math inline">\(A(\cdot)\)</span>中的变量为<span class="math inline">\(\|\omega_{i} -\omega_{j}\|^{2}\)</span>，由于是子模型参数距离二范数的平方，在式(3)进行求导时，会出现<span class="math inline">\((\omega_{i} -\omega_{j})\)</span>项，进而式(3)可以表示为模型参数 <span class="math inline">\(\omega_{1}^{k-1},...,\omega_{m}^{k-1}\)</span>的线性组合：<img src="https://cdn.jsdelivr.net/gh/sugar-xue/BlogImages@main/Picbed1613893464(1).png" alt="1613893464(1)"></p><p>我们可以将 <span class="math inline">\(u_{i}\)</span> 看作是节点<span class="math inline">\(i\)</span>在云端子模型的参数，可以聚合各个节点的参数<span class="math inline">\(\omega_{1}^{k-1},...,\omega_{m}^{k-1}\)</span>信息。计算得到 <span class="math inline">\(u_{i}^{k}\)</span> 后，我们可以根据公式（4）在节点<span class="math inline">\(i\)</span> 上更新 <span class="math inline">\(\omega_{i}^{k}\)</span>:</p><figure><img src="https://cdn.jsdelivr.net/gh/sugar-xue/BlogImages@main/Picbed1613893769(1).png" alt="1613893769(1)"><figcaption aria-hidden="true">1613893769(1)</figcaption></figure><p>借助于<span class="math inline">\(u_{i}\)</span>聚合其他节点的参数，节点<span class="math inline">\(i\)</span>可以获取其他节点的信息。在云端优化完<span class="math inline">\(A(W)\)</span>后，对于每个节点，再利用式(6)优化损失函数<span class="math inline">\(F_{i}(w)\)</span>。</p><figure><img src="https://cdn.jsdelivr.net/gh/sugar-xue/BlogImages@main/Picbed1613893796(1).png" alt="1613893796(1)"><figcaption aria-hidden="true">1613893796(1)</figcaption></figure><figure><img src="https://cdn.jsdelivr.net/gh/sugar-xue/BlogImages@main/Picbed1613893828(1).png" alt="1613893828(1)"><figcaption aria-hidden="true">1613893828(1)</figcaption></figure><p>在聚合其他节点参数时，式(5)中不同节点参数的权重为 <span class="math display">\[\xi_{i,j} = \alpha_{k}A'(\|\omega_{i}^{k-1} -\omega_{j}^{k-1}\|^{2}), \quad (i \ne j)\]</span> 根据定义1，在<span class="math inline">\([0,\infty)\)</span>上，<span class="math inline">\(A\)</span>是一个increasing and concave函数，函数A的导数<span class="math inline">\(A'\)</span>在<span class="math inline">\((0, \infty)\)</span>上为non-negative andnon-increasing 函数，所以<span class="math inline">\(A'(\|\omega_{i}^{k-1} -\omega_{j}^{k-1}\|^{2})\)</span>相当于一个相似度函数，如果两个节点的参数<span class="math inline">\(w_{i}^{k-1}\)</span>和<span class="math inline">\(w_{j}^{k-1}\)</span>的欧氏距离小，那么这两个节点的相似度要高，对应到<span class="math inline">\(u_{i}^{k}\)</span>和<span class="math inline">\(u_{j}^{k}\)</span>中，它们的权重更高，因而<span class="math inline">\(u_{i}^{k}\)</span>和<span class="math inline">\(u_{j}^{k}\)</span>更接近，进一步，<span class="math inline">\(w_{i}^{k}\)</span>和<span class="math inline">\(w_{j}^{k}\)</span>更接近。</p><h1 id="aaai21-tornadoaggregate-accurate-and-scalable-federated-learning-via-the-ring-based-architecture">[AAAI21]TornadoAggregate: Accurate and Scalable Federated Learning via theRing-Based Architecture</h1><p>在这篇文章中，作者提出一种可以提高精度和稳定性的聚合方式，并且讨论了当前已有的各种聚合方式。</p><figure><img src="https://cdn.jsdelivr.net/gh/sugar-xue/BlogImages@main/Picbed1613894143.png" alt="1613894143"><figcaption aria-hidden="true">1613894143</figcaption></figure><p>作者指出STAR 这种全局聚合结构的稳定性差，相较而言，RING结构通过移除全局聚合，解决了STAR稳定性差的问题。但是，RING结构在FL中不切实际，假设共 <span class="math inline">\(|N|\)</span>个节点，RING需要进行的通信轮数是 STAR结构的 <span class="math inline">\(|N|\)</span> 倍数。除此之外，作者也总结讨论了其他已有聚合结构：STAR-stars, STAR-rings,RING-stars, RING-rings。</p><p>作者基于RING结构提出了两种新的聚合结构，通过减少RING结构带来的方差，提高了稳定性和精度。</p><h1 id="icml20-fedboost-communication-efficient-algorithms-for-federated-learning">[ICML20]FedBoost: Communication-Efficient Algorithms for Federated Learning</h1><p>作者借助集成的思想以减少FL中的通信成本。一些预先训练好的弱模型可以通过可获得的公共数据集训练。假设我们有<span class="math inline">\(q\)</span> 个已经训练好的弱模型 <span class="math inline">\(H =(h_{1},...,h_{q})\)</span>，本文的目标是学习组合权重 <span class="math inline">\(\alpha = \{ \alpha_{1}, ...,\alpha_{q}\}\)</span>，从而得到 <span class="math inline">\(\sum_{k=1}^{q} \alpha_{k}h_{k}\)</span>使得损失最小化。</p><figure><img src="https://cdn.jsdelivr.net/gh/sugar-xue/BlogImages@main/Picbed1613895221(1).png" alt="1613895221(1)"><figcaption aria-hidden="true">1613895221(1)</figcaption></figure><h1 id="icml20-fetchsgd-communication-efficient-federated-learning-with-sketching">[ICML20]FetchSGD: Communication-Efficient Federated Learning with Sketching</h1><p>作者提出一个新的算法，算法思想为：在每一轮，每个节点基于自己的局部信息计算得到一个梯度，然后在进行聚合前，作者使用一种叫做CountSketch的数据结构对梯度进行压缩。中心端保留momentum和error accumulationCount Sketches，每轮更新的权重参数根据error accumulationsketch得到。</p><h1 id="icml20-federated-learning-with-only-positive-labels">[ICML20]Federated Learning with Only Positive Labels</h1><h1 id="icml20-from-local-sgd-to-local-fixed-point-methods-for-federated-learning">[ICML20]From Local SGD to Local Fixed-Point Methods for Federated Learning</h1><h1 id="nips20-lower-bounds-and-optimal-algorithms-for-personalized-federated-learning">[NIPS20]Lower Bounds and Optimal Algorithms for Personalized Federatedlearning</h1><p>L2SGD # [NIPS20] Federated Bayesian Optimization # [NIPS20] FederatedMulti-Task Learning MOCHA # [NIPS20] FedSplit: An algorithmic frameworkfor fast federated optimization作者首先讨论了两种已有算法FedSGD和FedProx算法，作者证明这两种算法都不具有可行的收敛理论保证，因为它们得到的稳定点都不是它们预先要求解的目标函数的解。因此，作者提出FedSplit算法，该算法得到的稳定点是优化问题的最优解。</p><figure><img src="https://cdn.jsdelivr.net/gh/sugar-xue/BlogImages@main/Picbed1613897029.png" alt="1613897029"><figcaption aria-hidden="true">1613897029</figcaption></figure><h1 id="nips20-an-efficient-framework-for-clustered-federated-learning">[NIPS20]An Efficient Framework for Clustered Federated Learning</h1><p>作者提出一个迭代的聚类算法，论文假设所有的节点都能够被划分为若干类。由于每个节点所属类别未知，该算法可以交替估计每个节点所属的类别，并且通过梯度下降优化模型参数。论文中的算法可以解决数据分布的异质性问题。但是需要预先给定聚类个数<span class="math inline">\(k\)</span>。 <img src="https://cdn.jsdelivr.net/gh/sugar-xue/BlogImages@main/Picbed1613897504(1).png" alt="1613897504(1)"></p><h1 id="nips20-group-knowledge-transfer-federated-learning-of-large-cnns-at-the-edge">[NIPS20]Group Knowledge Transfer: Federated Learning of Large CNNs at theEdge</h1><p>作者提出一种新的交替最小化算法，该算法在每个节点上先训练较小的CNN网络，然后通过信息迁移训练一个较大的中心端CNN网络。<img src="https://cdn.jsdelivr.net/gh/sugar-xue/BlogImages@main/Picbed1613898317(1).jpg" alt="1613898317(1)"></p><p>上图展示了每个节点有一个特征提取器和分类器，可以在单个节点上进行模型训练。进行局部训练后，每个节点生成同样的张量，将其特征输出到中心端进行训练，然后借助于最小化预测标签和真实标签的KD损失函数训练参数。为了提升节点模型的表现，中心端会将其预测的标签发送给每个节点，然后每个节点可以基于其预测标签和中心端预测结果的损失函数训练子模型。</p><p><img src="https://cdn.jsdelivr.net/gh/sugar-xue/BlogImages@main/Picbed1613898899(1).png" alt="1613898899(1)"> # [NIPS20] Personalized Federated Learning withMoreau Envelopes为了解决异质性问题，作者考虑给每个节点的损失函数添加正则项： <span class="math display">\[f_{i}(\theta_{i}) + \frac{\lambda}{2}\|\theta_{i} - w\|^{2}，\]</span></p><p>优化问题表示为： <img src="https://cdn.jsdelivr.net/gh/sugar-xue/BlogImages@main/Picbed1613899301(1).jpg" alt="1613899301(1)"></p><h1 id="nips20-tackling-the-objective-inconsistency-problem-in-heterogeneous-federated-optimization">[NIPS20]Tackling the Objective Inconsistency Problem in Heterogeneous FederatedOptimization</h1><p>大多数论文在分析算法的收敛性时，往往会假设每个节点上进行局部更新的次数相同，它们的工作表明算法能够达到全局目标函数的稳定点。事实上，论文指出当不同节点局部更新次数不一致时，算法收敛到的稳定点不是原始目标函数的最优解，而是另一个目标函数。</p><p>解决这个问题的最简单想法就是固定每个节点的局部更新次数，在进行新的一轮迭代前，要等所有节点进行迭代完才能开始。这种方法能够保证目标函数的一致性，但是会带来训练成本。一些算法比如FedProx,VRLSGD以及SCAFFOLD用于处理non-IID问题，可以减少目标函数的不一致问题，但是要么有较慢的收敛速度，要么需要额外的通信成本和内存。</p><p>本文作者提出FedNova算法，可以保证目标函数的一致性问题。</p><h1 id="nips20-throughput-optimal-topology-design-for-cross-silo-federated-learning">[NIPS20]Throughput-Optimal Topology Design for Cross-Silo FederatedLearning</h1><h1 id="nips20-federated-principal-component-analysis">[NIPS20]Federated Principal Component Analysis</h1><h1 id="nips20-ensemble-distillation-for-robust-model-fusion-in-federated-learning">[NIPS20]Ensemble Distillation for Robust Model Fusion in Federated Learning</h1><h1 id="nips20-differentially-private-federated-linear-bandits">[NIPS20]Differentially-Private Federated Linear Bandits</h1><h1 id="nips20-inverting-gradients---how-easy-is-it-to-break-privacy-in-federated-learning">[NIPS20]Inverting Gradients - How easy is it to break privacy in federatedlearning?</h1><h1 id="nips20-distributionally-robust-federated-averaging">[NIPS20]Distributionally Robust Federated Averaging</h1><h1 id="iclr20-fair-resource-allocation-in-federated-learning">[ICLR20]FAIR RESOURCE ALLOCATION IN FEDERATED LEARNING</h1><p>作者提出 q-FFL算法，目的是解决FL中的公平问题：不同节点上的精度均匀。通过最小化一个加权的损失函数，具有较高损失的节点具有较高的权重。</p><figure><img src="https://cdn.jsdelivr.net/gh/sugar-xue/BlogImages@main/Picbed1613901146(1).png" alt="1613901146(1)"><figcaption aria-hidden="true">1613901146(1)</figcaption></figure><p>目标函数：</p><figure><img src="https://cdn.jsdelivr.net/gh/sugar-xue/BlogImages@main/Picbed1613901254(1).png" alt="1613901254(1)"><figcaption aria-hidden="true">1613901254(1)</figcaption></figure><p>具体算法略。 # [ICLR20] DIFFERENTIALLY PRIVATE META-LEARNING</p><h1 id="iclr20-dba-distributed-backdoor-attacks-against-federated-learning">[ICLR20]DBA: DISTRIBUTED BACKDOOR ATTACKS AGAINST FEDERATED LEARNING</h1><h1 id="iclr20-generative-models-for-effective-ml-on-private-decentralized-datasets">[ICLR20]GENERATIVE MODELS FOR EFFECTIVE ML ON PRIVATE, DECENTRALIZEDDATASETS</h1><h1 id="iclr20-attack-resistant-federated-learning-with-residual-based-reweighting">[ICLR20]ATTACK-RESISTANT FEDERATED LEARNING WITH RESIDUAL-BASED REWEIGHTING</h1>]]></content>
      
      
      <categories>
          
          <category> Distributed Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> FL </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>

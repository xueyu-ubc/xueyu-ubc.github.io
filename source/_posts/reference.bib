@article{Hanzely2020,
abstract = {We propose a new optimization formulation for training federated learning models. The standard formulation has the form of an empirical risk minimization problem constructed to find a single global model trained from the private data stored across all participating devices. In contrast, our formulation seeks an explicit trade-off between this traditional global model and the local models, which can be learned by each device from its own private data without any communication. Further, we develop several efficient variants of SGD (with and without partial participation and with and without variance reduction) for solving the new formulation and prove communication complexity guarantees. Notably, our methods are similar but not identical to federated averaging / local SGD, thus shedding some light on the essence of the elusive method. In particular, our methods do not perform full averaging steps and instead merely take steps towards averaging. We argue for the benefits of this new paradigm for federated learning.},
archivePrefix = {arXiv},
arxivId = {2002.05516},
author = {Hanzely, Filip and Richt{\'{a}}rik, Peter},
eprint = {2002.05516},
file = {:C$\backslash$:/Users/sugar/Desktop/NEW/Federated learning of a mixture of global and local models.pdf:pdf},
issn = {23318422},
journal = {arXiv},
title = {{Federated learning of a mixture of global and local models}},
year = {2020}
}
@article{Chang,
author = {Chang, C.-C. and Lin, C.-J},
journal = {ACM Transactions on Intelligent Systems and Technology(TIST)},
title = {{LibSVM: A library for support vector machines}},
year = {2011}
}